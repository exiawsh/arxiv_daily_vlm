[
    {
        "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM",
        "summary": "Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.",
        "url": "http://arxiv.org/abs/2510.15870v1",
        "published_date": "2025-10-17T17:59:59+00:00",
        "updated_date": "2025-10-17T17:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Hanrong Ye",
            "Chao-Han Huck Yang",
            "Arushi Goel",
            "Wei Huang",
            "Ligeng Zhu",
            "Yuanhang Su",
            "Sean Lin",
            "An-Chieh Cheng",
            "Zhen Wan",
            "Jinchuan Tian",
            "Yuming Lou",
            "Dong Yang",
            "Zhijian Liu",
            "Yukang Chen",
            "Ambrish Dantrey",
            "Ehsan Jahangiri",
            "Sreyan Ghosh",
            "Daguang Xu",
            "Ehsan Hosseini-Asl",
            "Danial Mohseni Taheri",
            "Vidya Murali",
            "Sifei Liu",
            "Jason Lu",
            "Oluwatobi Olabiyi",
            "Frank Wang",
            "Rafael Valle",
            "Bryan Catanzaro",
            "Andrew Tao",
            "Song Han",
            "Jan Kautz",
            "Hongxu Yin",
            "Pavlo Molchanov"
        ],
        "tldr": "The paper introduces OmniVinci, an open-source omni-modal LLM with novel architectural components and a data curation pipeline, achieving strong performance with significantly fewer training tokens compared to Qwen2.5-Omni.",
        "tldr_zh": "该论文介绍了OmniVinci，一个开源的，具有新颖架构组件和数据整理流程的Omni-modal LLM，与Qwen2.5-Omni相比，以明显更少的训练token实现了强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models",
        "summary": "The clinical adoption of biomedical vision-language models is hindered by\nprompt optimization techniques that produce either uninterpretable latent\nvectors or single textual prompts. This lack of transparency and failure to\ncapture the multi-faceted nature of clinical diagnosis, which relies on\nintegrating diverse observations, limits their trustworthiness in high-stakes\nsettings. To address this, we introduce BiomedXPro, an evolutionary framework\nthat leverages a large language model as both a biomedical knowledge extractor\nand an adaptive optimizer to automatically generate a diverse ensemble of\ninterpretable, natural-language prompt pairs for disease diagnosis. Experiments\non multiple biomedical benchmarks show that BiomedXPro consistently outperforms\nstate-of-the-art prompt-tuning methods, particularly in data-scarce few-shot\nsettings. Furthermore, our analysis demonstrates a strong semantic alignment\nbetween the discovered prompts and statistically significant clinical features,\ngrounding the model's performance in verifiable concepts. By producing a\ndiverse ensemble of interpretable prompts, BiomedXPro provides a verifiable\nbasis for model predictions, representing a critical step toward the\ndevelopment of more trustworthy and clinically-aligned AI systems.",
        "url": "http://arxiv.org/abs/2510.15866v1",
        "published_date": "2025-10-17T17:58:31+00:00",
        "updated_date": "2025-10-17T17:58:31+00:00",
        "categories": [
            "cs.CV",
            "cs.NE"
        ],
        "authors": [
            "Kaushitha Silva",
            "Mansitha Eashwara",
            "Sanduni Ubayasiri",
            "Ruwan Tennakoon",
            "Damayanthi Herath"
        ],
        "tldr": "BiomedXPro is introduced to generate a diverse ensemble of interpretable, natural-language prompt pairs for disease diagnosis by using a large language model as a biomedical knowledge extractor and adaptive optimizer, achieving SOTA performance on biomedical benchmarks.",
        "tldr_zh": "BiomedXPro被提出，通过使用大型语言模型作为生物医学知识提取器和自适应优化器，生成用于疾病诊断的多种可解释的自然语言提示对，并在生物医学基准测试中实现了 SOTA 性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.",
        "url": "http://arxiv.org/abs/2510.15857v1",
        "published_date": "2025-10-17T17:50:58+00:00",
        "updated_date": "2025-10-17T17:50:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiuhai Chen",
            "Le Xue",
            "Zhiyang Xu",
            "Xichen Pan",
            "Shusheng Yang",
            "Can Qin",
            "An Yan",
            "Honglu Zhou",
            "Zeyuan Chen",
            "Lifu Huang",
            "Tianyi Zhou",
            "Junnan Li",
            "Silvio Savarese",
            "Caiming Xiong",
            "Ran Xu"
        ],
        "tldr": "BLIP3o-NEXT is a new open-source vision-language model that achieves state-of-the-art native image generation and editing through a novel autoregressive + diffusion architecture, leveraging key insights about scaling, reinforcement learning, data quality, and post-training.",
        "tldr_zh": "BLIP3o-NEXT是一个新的开源视觉语言模型，通过一种新颖的自回归+扩散架构，实现了最先进的本地图像生成和编辑，利用了关于缩放、强化学习、数据质量和后期训练的关键见解。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
        "summary": "Despite rapid advances in text-to-video synthesis, generated video quality\nremains critically dependent on precise user prompts. Existing test-time\noptimization methods, successful in other domains, struggle with the\nmulti-faceted nature of video. In this work, we introduce VISTA (Video\nIterative Self-improvemenT Agent), a novel multi-agent system that autonomously\nimproves video generation through refining prompts in an iterative loop. VISTA\nfirst decomposes a user idea into a structured temporal plan. After generation,\nthe best video is identified through a robust pairwise tournament. This winning\nvideo is then critiqued by a trio of specialized agents focusing on visual,\naudio, and contextual fidelity. Finally, a reasoning agent synthesizes this\nfeedback to introspectively rewrite and enhance the prompt for the next\ngeneration cycle. Experiments on single- and multi-scene video generation\nscenarios show that while prior methods yield inconsistent gains, VISTA\nconsistently improves video quality and alignment with user intent, achieving\nup to 60% pairwise win rate against state-of-the-art baselines. Human\nevaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
        "url": "http://arxiv.org/abs/2510.15831v1",
        "published_date": "2025-10-17T17:12:08+00:00",
        "updated_date": "2025-10-17T17:12:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Do Xuan Long",
            "Xingchen Wan",
            "Hootan Nakhost",
            "Chen-Yu Lee",
            "Tomas Pfister",
            "Sercan Ö. Arık"
        ],
        "tldr": "The paper introduces VISTA, a multi-agent system that iteratively improves text-to-video generation by refining prompts based on visual, audio, and contextual feedback, achieving superior video quality and alignment with user intent compared to existing methods.",
        "tldr_zh": "该论文介绍了 VISTA，一个多智能体系统，通过基于视觉、音频和上下文反馈迭代优化提示来改进文本到视频的生成，与现有方法相比，实现了更高的视频质量和与用户意图的一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis",
        "summary": "Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.",
        "url": "http://arxiv.org/abs/2510.15710v1",
        "published_date": "2025-10-17T14:54:58+00:00",
        "updated_date": "2025-10-17T14:54:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junzhi Ning",
            "Wei Li",
            "Cheng Tang",
            "Jiashi Lin",
            "Chenglong Ma",
            "Chaoyang Zhang",
            "Jiyao Liu",
            "Ying Chen",
            "Shujian Gao",
            "Lihao Liu",
            "Yuandong Pu",
            "Huihui Xu",
            "Chenhui Gou",
            "Ziyan Huang",
            "Yi Xin",
            "Qi Qin",
            "Zhongying Deng",
            "Diping Song",
            "Bin Fu",
            "Guang Yang",
            "Yuanfeng Ji",
            "Tianbin Li",
            "Yanzhou Su",
            "Jin Ye",
            "Shixiang Tang",
            "Ming Hu",
            "Junjun He"
        ],
        "tldr": "The paper introduces UniMedVL, a unified multimodal model for medical image understanding and generation, trained on a large dataset (UniMed-5M) using progressive curriculum learning, demonstrating superior performance across various medical imaging tasks.",
        "tldr_zh": "本文介绍了UniMedVL，一个用于医学图像理解和生成的统一多模态模型。该模型在大型数据集（UniMed-5M）上使用渐进式课程学习进行训练，并在各种医学成像任务中表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning",
        "summary": "Long-form video reasoning remains a major challenge for Video Large Language\nModels (Video LLMs), as static uniform frame sampling leads to information\ndilution and obscures critical evidence. Furthermore, existing pixel-space\nvideo reasoning agents, which are designed to actively interact with the video\nto acquire new visual information, remain suboptimal due to their lack of\nrigorous reward mechanisms to enforce evidence purity and their inability to\nperform temporal information supplementation beyond pre-sampled frames. To\naddress this critical gap, we propose a novel evidence-prioritized adaptive\nframework built upon our core philosophy: \"Select Less, Reason More.\" Our core\ncontribution is the evidence-aware reinforcement learning (EARL) framework,\nwhich transforms the model into an active interrogator of evidence. EARL is\nprecisely engineered to dynamically select the most relevant frames and,\ncrucially, to perform localized re-sampling around the selected key frames to\naccess fine-grained temporal detail. Extensive experiments on five demanding\nvideo reasoning benchmarks demonstrate that our EARL-trained model achieves new\nstate-of-the-art among open-source Video LLMs, simultaneously learning an\neffective and high-purity visual evidence selection policy. Impressively, our\n7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on\nVideoMME. These results highlight the importance of prioritizing evidence\npurity and the effectiveness of our framework.",
        "url": "http://arxiv.org/abs/2510.15440v1",
        "published_date": "2025-10-17T08:52:40+00:00",
        "updated_date": "2025-10-17T08:52:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xuchen Li",
            "Xuzhao Li",
            "Shiyu Hu",
            "Kaiqi Huang"
        ],
        "tldr": "The paper introduces EARL, an evidence-aware reinforcement learning framework for Video LLMs that dynamically selects relevant frames and performs localized re-sampling to improve long-form video reasoning, achieving state-of-the-art results on several benchmarks.",
        "tldr_zh": "该论文介绍了一个名为EARL的证据感知强化学习框架，用于视频大型语言模型，它通过动态选择相关帧并执行局部重采样来改进长视频推理，并在多个基准测试中实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Salient Concept-Aware Generative Data Augmentation",
        "summary": "Recent generative data augmentation methods conditioned on both image and\ntext prompts struggle to balance between fidelity and diversity, as it is\nchallenging to preserve essential image details while aligning with varied text\nprompts. This challenge arises because representations in the synthesis process\noften become entangled with non-essential input image attributes such as\nenvironmental contexts, creating conflicts with text prompts intended to modify\nthese elements. To address this, we propose a personalized image generation\nframework that uses a salient concept-aware image embedding model to reduce the\ninfluence of irrelevant visual details during the synthesis process, thereby\nmaintaining intuitive alignment between image and text inputs. By generating\nimages that better preserve class-discriminative features with additional\ncontrolled variations, our framework effectively enhances the diversity of\ntraining datasets and thereby improves the robustness of downstream models. Our\napproach demonstrates superior performance across eight fine-grained vision\ndatasets, outperforming state-of-the-art augmentation methods with averaged\nclassification accuracy improvements by 0.73% and 6.5% under conventional and\nlong-tail settings, respectively.",
        "url": "http://arxiv.org/abs/2510.15194v1",
        "published_date": "2025-10-16T23:31:55+00:00",
        "updated_date": "2025-10-16T23:31:55+00:00",
        "categories": [
            "cs.CV",
            "68T45 (Machine learning)",
            "I.2.10; I.2.6; I.4.8; I.5.1; I.5.4"
        ],
        "authors": [
            "Tianchen Zhao",
            "Xuanbai Chen",
            "Zhihua Li",
            "Jun Fang",
            "Dongsheng An",
            "Xiang Xu",
            "Zhuowen Tu",
            "Yifan Xing"
        ],
        "tldr": "The paper introduces a salient concept-aware generative data augmentation framework that improves the balance between fidelity and diversity in image synthesis by reducing the influence of irrelevant visual details, leading to better downstream model robustness, particularly in long-tail settings.",
        "tldr_zh": "该论文介绍了一种基于显著概念的生成数据增强框架，通过减少不相关视觉细节的影响，改善了图像合成中保真度和多样性之间的平衡，从而提高了下游模型的鲁棒性，尤其是在长尾环境中。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
        "summary": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a\nmixture of image-text caption data and interleaved document data, while the\nhigh-quality data filtering towards image-text interleaved document data is\nunder-explored. We propose to train an efficient MLLM as a Unified Mulitmodal\nData Quality Classifier to Filter both high-quality image-text caption and\ninterleaved data (UniFilter). To address the challenge of collecting diverse\nlabeled multimodal data, we introduce a semi-synthetic approach that leverages\nreadily available raw images and generates corresponding text across four\nquality levels. This method enables efficient creation of sample-score pairs\nfor both caption and interleaved document data to train UniFilter. We apply\nUniFilter to curate high-quality caption data from DataComp caption dataset and\ninterleaved data from the OBELICS image-text interleaved dataset. MLLMs\npre-trained on the filtered data demonstrate significantly enhanced\ncapabilities compared to those trained on baseline-filtered data, achieving\nstronger zero-shot reasoning and in-context learning capabilities. After visual\nsupervised fine-tuning, these UniFilter-induced MLLMs achieve stronger\nperformance on various benchmarks, highlighting the downstream benefits of\nhigh-quality multimodal pre-training. We release the synthetic training data\nused for training UniFilter, the UniFilter model checkpoints, and the\nhigh-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to\nthe community for reproduction and further development.",
        "url": "http://arxiv.org/abs/2510.15162v1",
        "published_date": "2025-10-16T21:53:28+00:00",
        "updated_date": "2025-10-16T21:53:28+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Weizhi Wang",
            "Rongmei Lin",
            "Shiyang Li",
            "Colin Lockard",
            "Ritesh Sarkhel",
            "Sanket Lokegaonkar",
            "Jingbo Shang",
            "Xifeng Yan",
            "Nasser Zalmout",
            "Xian Li"
        ],
        "tldr": "This paper introduces UniFilter, a unified multimodal data quality classifier trained on semi-synthetic data to filter high-quality image-text caption and interleaved data for pre-training MLLMs, resulting in improved zero-shot reasoning and in-context learning capabilities.",
        "tldr_zh": "本文介绍了一种名为UniFilter的统一多模态数据质量分类器，该分类器使用半合成数据进行训练，以过滤高质量的图像-文本标题和交错数据，用于预训练MLLM，从而提高零样本推理和上下文学习能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Directional Reasoning Injection for Fine-Tuning MLLMs",
        "summary": "Multimodal large language models (MLLMs) are rapidly advancing, yet their\nreasoning ability often lags behind that of strong text-only counterparts.\nExisting methods to bridge this gap rely on supervised fine-tuning over\nlarge-scale multimodal reasoning data or reinforcement learning, both of which\nare resource-intensive. A promising alternative is model merging, which\ninterpolates parameters between reasoning-enhanced LLMs and multimodal\nvariants. However, our analysis shows that naive merging is not always a \"free\nlunch\": its effectiveness varies drastically across model families, with some\n(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance\ndegradation. To address this, we propose Directional Reasoning Injection for\nFine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning\nknowledge in the gradient space, without destabilizing multimodal alignment.\nDRIFT precomputes a reasoning prior as the parameter-space difference between\nreasoning and multimodal variants, then uses it to bias gradients during\nmultimodal fine-tuning. This approach preserves the simplicity of standard\nsupervised fine-tuning pipelines while enabling efficient reasoning transfer.\nExtensive experiments on multimodal reasoning benchmarks, including MathVista\nand MathVerse, demonstrate that DRIFT consistently improves reasoning\nperformance over naive merging and supervised fine-tuning, while matching or\nsurpassing training-heavy methods at a fraction of the cost.",
        "url": "http://arxiv.org/abs/2510.15050v1",
        "published_date": "2025-10-16T18:06:46+00:00",
        "updated_date": "2025-10-16T18:06:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chao Huang",
            "Zeliang Zhang",
            "Jiang Liu",
            "Ximeng Sun",
            "Jialian Wu",
            "Xiaodong Yu",
            "Ze Wang",
            "Chenliang Xu",
            "Emad Barsoum",
            "Zicheng Liu"
        ],
        "tldr": "The paper introduces DRIFT, a lightweight fine-tuning method that injects reasoning knowledge into MLLMs by biasing gradients with a precomputed reasoning prior, demonstrating improved reasoning performance compared to naive merging and supervised fine-tuning.",
        "tldr_zh": "本文介绍了一种轻量级的微调方法DRIFT，通过使用预先计算的推理先验来偏置梯度，将推理知识注入到MLLM中，与朴素合并和监督微调相比，推理性能得到了提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Comprehensive language-image pre-training for 3D medical image understanding",
        "summary": "Vision-language pre-training, i.e., aligning images with paired text, is a\npowerful paradigm to create encoders that can be directly used for tasks such\nas classification and retrieval, and for downstream tasks such as segmentation\nand report generation. In the 3D medical image domain, these capabilities allow\nvision-language encoders (VLEs) to support radiologists by retrieving patients\nwith similar abnormalities or predicting likelihoods of abnormality. While the\nmethodology holds promise, data availability limits the capabilities of current\n3D VLEs.\n  In this paper, we alleviate the lack of data by injecting additional\ninductive biases: introducing a report generation objective and pairing\nvision-language pre-training with vision-only pre-training. This allows us to\nleverage both image-only and paired image-text 3D datasets, increasing the\ntotal amount of data to which our model is exposed. Through these additional\ninductive biases, paired with best practices of the 3D medical imaging domain,\nwe develop the Comprehensive Language-image Pre-training (COLIPRI) encoder\nfamily. Our COLIPRI encoders achieve state-of-the-art performance in report\ngeneration, classification probing, and zero-shot classification, and remain\ncompetitive for semantic segmentation.",
        "url": "http://arxiv.org/abs/2510.15042v1",
        "published_date": "2025-10-16T18:01:31+00:00",
        "updated_date": "2025-10-16T18:01:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tassilo Wald",
            "Ibrahim Ethem Hamamci",
            "Yuan Gao",
            "Sam Bond-Taylor",
            "Harshita Sharma",
            "Maximilian Ilse",
            "Cynthia Lo",
            "Olesya Melnichenko",
            "Noel C. F. Codella",
            "Maria Teodora Wetscherek",
            "Klaus H. Maier-Hein",
            "Panagiotis Korfiatis",
            "Valentina Salvatelli",
            "Javier Alvarez-Valle",
            "Fernando Pérez-García"
        ],
        "tldr": "This paper introduces COLIPRI, a comprehensive language-image pre-training approach for 3D medical images that leverages both paired and unpaired data via report generation and vision-only pre-training, achieving state-of-the-art results on several tasks.",
        "tldr_zh": "本文介绍了COLIPRI，一种用于3D医学图像的综合语言-图像预训练方法，它通过报告生成和纯视觉预训练，利用配对和非配对数据，在多个任务上实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Composition-Grounded Instruction Synthesis for Visual Reasoning",
        "summary": "Pretrained multi-modal large language models (MLLMs) demonstrate strong\nperformance on diverse multimodal tasks, but remain limited in reasoning\ncapabilities for domains where annotations are difficult to collect. In this\nwork, we focus on artificial image domains such as charts, rendered documents,\nand webpages, which are abundant in practice yet lack large-scale human\nannotated reasoning datasets. We introduce COGS (COmposition-Grounded\ninstruction Synthesis), a data-efficient framework for equipping MLLMs with\nadvanced reasoning abilities from a small set of seed questions. The key idea\nis to decompose each seed question into primitive perception and reasoning\nfactors, which can then be systematically recomposed with new images to\ngenerate large collections of synthetic question-answer pairs. Each generated\nquestion is paired with subquestions and intermediate answers, enabling\nreinforcement learning with factor-level process rewards. Experiments on chart\nreasoning show that COGS substantially improves performance on unseen\nquestions, with the largest gains on reasoning-heavy and compositional\nquestions. Moreover, training with a factor-level mixture of different seed\ndata yields better transfer across multiple datasets, suggesting that COGS\ninduces generalizable capabilities rather than dataset-specific overfitting. We\nfurther demonstrate that the framework extends beyond charts to other domains\nsuch as webpages.",
        "url": "http://arxiv.org/abs/2510.15040v1",
        "published_date": "2025-10-16T18:00:48+00:00",
        "updated_date": "2025-10-16T18:00:48+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xinyi Gu",
            "Jiayuan Mao",
            "Zhang-Wei Hong",
            "Zhuoran Yu",
            "Pengyuan Li",
            "Dhiraj Joshi",
            "Rogerio Feris",
            "Zexue He"
        ],
        "tldr": "The paper introduces COGS, a data-efficient framework that uses a small set of seed questions to generate synthetic question-answer pairs for training MLLMs, improving reasoning capabilities in visual domains like charts and webpages where annotations are scarce.",
        "tldr_zh": "该论文介绍了一种名为COGS的数据高效框架，它使用少量种子问题来生成合成的问答对，以训练MLLM，从而提高在图表和网页等视觉领域中的推理能力，这些领域的注释数据稀缺。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Neuro-Symbolic Spatial Reasoning in Segmentation",
        "summary": "Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from\nan open set of categories, requiring generalization to unseen and unlabelled\nobjects. Using vision-language models (VLMs) to correlate local image patches\nwith potential unseen object categories suffers from a lack of understanding of\nspatial relations of objects in a scene. To solve this problem, we introduce\nneuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary\nVLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)\nto impose explicit spatial relational constraints by first order logic (FOL)\nformulated in a neural network architecture. This is the first attempt to\nexplore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically\nextracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them\nas first-order logic formulas using our proposed pseudo categories. Each pixel\nlearns to predict both a semantic category (e.g., \"cat\") and a spatial pseudo\ncategory (e.g., \"right of person\") simultaneously, enforcing relational\nconstraints (e.g., a \"cat\" pixel must lie to the right of a \"person\"). Finally,\nthese logic constraints are formulated in a deep network architecture by fuzzy\nlogic relaxation, enabling end-to-end learning of spatial-relationally\nconsistent segmentation. RelateSeg achieves state-of-the-art performance in\nterms of average mIoU across four benchmark datasets and particularly shows\nclear advantages on images containing multiple categories, with the cost of\nonly introducing a single auxiliary loss function and no additional parameters,\nvalidating the effectiveness of NeSy spatial reasoning in OVSS.",
        "url": "http://arxiv.org/abs/2510.15841v1",
        "published_date": "2025-10-17T17:35:34+00:00",
        "updated_date": "2025-10-17T17:35:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiayi Lin",
            "Jiabo Huang",
            "Shaogang Gong"
        ],
        "tldr": "This paper introduces RelateSeg, a neuro-symbolic approach to open-vocabulary semantic segmentation (OVSS) that incorporates spatial relational constraints using first-order logic and fuzzy logic relaxation within a neural network, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了一种神经符号方法 RelateSeg，用于开放词汇语义分割 (OVSS)，它使用一阶逻辑和模糊逻辑松弛在神经网络中结合了空间关系约束，从而实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning",
        "summary": "Scaling up model size and training data has advanced foundation models for\ninstance-level perception, achieving state-of-the-art in-domain and zero-shot\nperformance across object detection and segmentation. However, their high\ncomputational cost limits adoption on resource-constrained platforms. We first\nexamine the limitations of existing architectures in enabling efficient edge\ndeployment without compromising performance. We then introduce MOBIUS, a family\nof foundation models for universal instance segmentation, designed for\nPareto-optimal downscaling to support deployment across devices ranging from\nhigh-end accelerators to mobile hardware. To reduce training and inference\ndemands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale\nand multi-modal fusion, (ii) a language-guided uncertainty calibration loss for\nadaptive decoder pruning, and (iii) a streamlined, unified training strategy.\nUnlike efficient baselines that trade accuracy for reduced complexity, MOBIUS\nreduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,\nwhile maintaining state-of-the-art performance in just a third of the training\niterations. MOBIUS establishes a new benchmark for efficient segmentation on\nboth high-performance computing platforms and mobile devices.",
        "url": "http://arxiv.org/abs/2510.15026v1",
        "published_date": "2025-10-16T18:00:00+00:00",
        "updated_date": "2025-10-16T18:00:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mattia Segu",
            "Marta Tintore Gazulla",
            "Yongqin Xian",
            "Luc Van Gool",
            "Federico Tombari"
        ],
        "tldr": "MOBIUS introduces a family of efficient foundation models for universal instance segmentation, designed for Pareto-optimal downscaling to support deployment across devices ranging from high-end accelerators to mobile hardware while maintaining state-of-the-art performance.",
        "tldr_zh": "MOBIUS 提出了一系列高效的通用实例分割基础模型，旨在实现帕累托最优的降级，以支持从高端加速器到移动硬件等各种设备上的部署，同时保持最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation",
        "summary": "In the context of imitation learning, visuomotor-based diffusion policy\nlearning is one of the main directions in robotic manipulation. Most of these\napproaches rely on point clouds as observation inputs and construct scene\nrepresentations through point clouds feature learning, which enables them to\nachieve remarkable accuracy. However, the existing literature lacks an in-depth\nexploration of vision-only solutions that have significant potential. In this\npaper, we propose a Vision-Only and single-view Diffusion Policy learning\nmethod (VO-DP) that leverages pretrained visual foundation models to achieve\neffective fusion of semantic and geometric features. We utilize intermediate\nfeatures from VGGT incorporating semantic features from DINOv2 and geometric\nfeatures from Alternating Attention blocks. Features are fused via\ncross-attention and spatially compressed with a CNN to form the input to the\npolicy head. Extensive experiments demonstrate that VO-DP not only outperforms\nthe vision-only baseline DP significantly but also exhibits distinct\nperformance trends against the point cloud-based method DP3: in simulation\ntasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%\nand far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,\noutperforming both DP3 67.5% and DP 11.2% by a notable margin. Further\nrobustness evaluations confirm that VO-DP remains highly stable under varying\nconditions including color, size, background, and lighting. Lastly, we\nopen-source a training library for robotic manipulation. Built on Accelerate,\nthis library supports multi-machine and multi-GPU parallel training, as well as\nmixed precision training. It is compatible with visuomotor policies such as DP,\nDP3 and VO-DP, and also supports the RoboTwin simulator.",
        "url": "http://arxiv.org/abs/2510.15530v1",
        "published_date": "2025-10-17T11:01:33+00:00",
        "updated_date": "2025-10-17T11:01:33+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zehao Ni",
            "Yonghao He",
            "Lingfeng Qian",
            "Jilei Mao",
            "Fa Fu",
            "Wei Sui",
            "Hu Su",
            "Junran Peng",
            "Zhipeng Wang",
            "Bin He"
        ],
        "tldr": "The paper introduces VO-DP, a vision-only diffusion policy for robotic manipulation that fuses semantic and geometric features using a pretrained visual foundation model, achieving performance comparable to point cloud-based methods in simulation and superior performance in real-world tasks.",
        "tldr_zh": "该论文介绍了一种名为VO-DP的纯视觉扩散策略，用于机器人操作。该方法利用预训练的视觉基础模型融合语义和几何特征，在仿真环境中实现了与基于点云的方法相当的性能，并在真实世界任务中表现更优。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]