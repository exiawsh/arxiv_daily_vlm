[
    {
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
        "url": "http://arxiv.org/abs/2508.18265v1",
        "published_date": "2025-08-25T17:58:17+00:00",
        "updated_date": "2025-08-25T17:58:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiyun Wang",
            "Zhangwei Gao",
            "Lixin Gu",
            "Hengjun Pu",
            "Long Cui",
            "Xingguang Wei",
            "Zhaoyang Liu",
            "Linglin Jing",
            "Shenglong Ye",
            "Jie Shao",
            "Zhaokai Wang",
            "Zhe Chen",
            "Hongjie Zhang",
            "Ganlin Yang",
            "Haomin Wang",
            "Qi Wei",
            "Jinhui Yin",
            "Wenhao Li",
            "Erfei Cui",
            "Guanzhou Chen",
            "Zichen Ding",
            "Changyao Tian",
            "Zhenyu Wu",
            "Jingjing Xie",
            "Zehao Li",
            "Bowen Yang",
            "Yuchen Duan",
            "Xuehui Wang",
            "Songze Li",
            "Xiangyu Zhao",
            "Haodong Duan",
            "Nianchen Deng",
            "Bin Fu",
            "Yinan He",
            "Yi Wang",
            "Conghui He",
            "Botian Shi",
            "Junjun He",
            "Yingtong Xiong",
            "Han Lv",
            "Lijun Wu",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Huipeng Deng",
            "Biqing Qi",
            "Jiaye Ge",
            "Qipeng Guo",
            "Wenwei Zhang",
            "Wanli Ouyang",
            "Limin Wang",
            "Min Dou",
            "Xizhou Zhu",
            "Tong Lu",
            "Dahua Lin",
            "Jifeng Dai",
            "Bowen Zhou",
            "Weijie Su",
            "Kai Chen",
            "Yu Qiao",
            "Wenhai Wang",
            "Gen Luo"
        ],
        "tldr": "InternVL 3.5 introduces a new open-source multimodal model family featuring Cascade Reinforcement Learning for improved reasoning, Visual Resolution Router for efficiency, and Decoupled Vision-Language Deployment for balanced computational load, achieving state-of-the-art results and released publicly.",
        "tldr_zh": "InternVL 3.5 引入了一个新的开源多模态模型系列，采用级联强化学习以提高推理能力，视觉分辨率路由器以提高效率，以及解耦的视觉-语言部署以平衡计算负载，实现了最先进的结果并公开发布。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation",
        "summary": "Magnetic Resonance Imaging (MRI) is indispensable in clinical practice but\nremains constrained by fragmented, multi-stage workflows encompassing\nacquisition, reconstruction, segmentation, detection, diagnosis, and reporting.\nWhile deep learning has achieved progress in individual tasks, existing\napproaches are often anatomy- or application-specific and lack generalizability\nacross diverse clinical settings. Moreover, current pipelines rarely integrate\nimaging data with complementary language information that radiologists rely on\nin routine practice. Here, we introduce OmniMRI, a unified vision-language\nfoundation model designed to generalize across the entire MRI workflow. OmniMRI\nis trained on a large-scale, heterogeneous corpus curated from 60 public\ndatasets, over 220,000 MRI volumes and 19 million MRI slices, incorporating\nimage-only data, paired vision-text data, and instruction-response data. Its\nmulti-stage training paradigm, comprising self-supervised vision pretraining,\nvision-language alignment, multimodal pretraining, and multi-task instruction\ntuning, progressively equips the model with transferable visual\nrepresentations, cross-modal reasoning, and robust instruction-following\ncapabilities. Qualitative results demonstrate OmniMRI's ability to perform\ndiverse tasks within a single architecture, including MRI reconstruction,\nanatomical and pathological segmentation, abnormality detection, diagnostic\nsuggestion, and radiology report generation. These findings highlight OmniMRI's\npotential to consolidate fragmented pipelines into a scalable, generalist\nframework, paving the way toward foundation models that unify imaging and\nclinical language for comprehensive, end-to-end MRI interpretation.",
        "url": "http://arxiv.org/abs/2508.17524v1",
        "published_date": "2025-08-24T21:11:28+00:00",
        "updated_date": "2025-08-24T21:11:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xingxin He",
            "Aurora Rofena",
            "Ruimin Feng",
            "Haozhe Liao",
            "Zhaoye Zhou",
            "Albert Jang",
            "Fang Liu"
        ],
        "tldr": "OmniMRI is a unified vision-language foundation model for MRI interpretation, trained on a large dataset to perform various tasks like reconstruction, segmentation, detection, diagnosis, and report generation within a single architecture.",
        "tldr_zh": "OmniMRI是一个用于MRI解释的统一视觉语言基础模型，它在大型数据集上训练，可以在单个架构中执行重建、分割、检测、诊断和报告生成等各种任务。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs",
        "summary": "Vision-Language Models (VLMs) demonstrate impressive performance in\nunderstanding visual content with language instruction by converting visual\ninput to vision tokens. However, redundancy in vision tokens results in the\ndegenerated inference efficiency of VLMs. While many algorithms have been\nproposed to reduce the number of vision tokens, most of them apply only\nunimodal information (i.e., vision/text) for pruning and ignore the inherent\nmultimodal property of vision-language tasks. Moreover, it lacks a generic\ncriterion that can be applied to different modalities. To mitigate this\nlimitation, in this work, we propose to leverage both vision and text tokens to\nselect informative vision tokens by the criterion of coverage. We first\nformulate the subset selection problem as a maximum coverage problem.\nAfterward, a subset of vision tokens is optimized to cover the text tokens and\nthe original set of vision tokens, simultaneously. Finally, a VLM agent can be\nadopted to further improve the quality of text tokens for guiding vision\npruning. The proposed method MMTok is extensively evaluated on benchmark\ndatasets with different VLMs. The comparison illustrates that vision and text\ninformation are complementary, and combining multimodal information can surpass\nthe unimodal baseline with a clear margin. Moreover, under the maximum coverage\ncriterion on the POPE dataset, our method achieves a 1.87x speedup while\nmaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,\nwith only four vision tokens, it still preserves 87.7% of the original\nperformance on LLaVA-1.5-7B. These results highlight the effectiveness of\ncoverage in token selection.",
        "url": "http://arxiv.org/abs/2508.18264v1",
        "published_date": "2025-08-25T17:57:49+00:00",
        "updated_date": "2025-08-25T17:57:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sixun Dong",
            "Juhua Hu",
            "Mian Zhang",
            "Ming Yin",
            "Yanjie Fu",
            "Qi Qian"
        ],
        "tldr": "The paper introduces MMTok, a method for efficient VLM inference by pruning redundant vision tokens using a multimodal coverage maximization approach, leveraging both vision and text tokens. It demonstrates significant speedups with minimal performance loss on benchmark datasets.",
        "tldr_zh": "本文介绍了MMTok，一种通过多模态覆盖最大化方法修剪冗余视觉token以实现高效VLM推理的方法，该方法同时利用视觉和文本token。在基准数据集上，该方法在性能损失最小的情况下实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models",
        "summary": "Evaluating whether vision-language models (VLMs) reason consistently across\nrepresentations is challenging because modality comparisons are typically\nconfounded by task differences and asymmetric information. We introduce SEAM, a\nbenchmark that pairs semantically equivalent inputs across four domains that\nhave existing standardized textual and visual notations. By employing distinct\nnotation systems across modalities, in contrast to OCR-based image-text\npairing, SEAM provides a rigorous comparative assessment of the\ntextual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21\ncontemporary models, we observe systematic modality imbalance: vision\nfrequently lags language in overall performance, despite the problems\ncontaining semantically equivalent information, and cross-modal agreement is\nrelatively low. Our error analysis reveals two main drivers: textual perception\nfailures from tokenization in domain notation and visual perception failures\nthat induce hallucinations. We also show that our results are largely robust to\nvisual transformations. SEAM establishes a controlled, semantically equivalent\nsetting for measuring and improving modality-agnostic reasoning.",
        "url": "http://arxiv.org/abs/2508.18179v1",
        "published_date": "2025-08-25T16:33:07+00:00",
        "updated_date": "2025-08-25T16:33:07+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zhenwei Tang",
            "Difan Jiao",
            "Blair Yang",
            "Ashton Anderson"
        ],
        "tldr": "The paper introduces SEAM, a benchmark for evaluating the consistency of VLMs across modalities using semantically equivalent inputs and finds systematic modality imbalances where vision underperforms language.",
        "tldr_zh": "该论文介绍了SEAM，一个用于评估视觉语言模型在不同模态下一致性的基准，该基准使用语义等价的输入，并发现视觉通常比语言表现更差的系统性模态不平衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance",
        "summary": "This study proposes the dual technological innovation framework, including a\ncross-modal differ entiated quantization framework for vision-language models\n(VLMs) and a scene-aware vectorized\n  memory multi-agent system for visually impaired assistance. The modular\nframework was developed\n  implementing differentiated processing strategies, effectively reducing\nmemory requirements from\n  38GB to 16GB while maintaining model performance. The multi-agent\narchitecture combines\n  scene classification, vectorized memory, and multimodal interaction, enabling\npersistent storage\n  and efficient retrieval of scene memories. Through\nperception-memory-reasoning workflows, the\n  system provides environmental information beyond the current view using\nhistorical memories.\n  Experiments show the quantized 19B-parameter model only experiences a 2.05%\nperformance drop\n  on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9),\noutperforming smaller\n  models with equivalent memory requirements like the Molmo-7B series. The\nsystem maintains\n  response latency between 2.83-3.52 seconds from scene analysis to initial\nspeech output, substantially\n  faster than non-streaming methods. This research advances computational\nefficiency and assistive\n  technology, offering visually impaired users comprehensive real-time\nassistance in scene perception,\n  text recognition, and navigation.",
        "url": "http://arxiv.org/abs/2508.18177v1",
        "published_date": "2025-08-25T16:32:32+00:00",
        "updated_date": "2025-08-25T16:32:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Xiangxiang Wang",
            "Xuanyu Wang",
            "YiJia Luo",
            "Yongbin Yu",
            "Manping Fan",
            "Jingtao Zhang",
            "Liyong Ren"
        ],
        "tldr": "The paper introduces a scene-aware multi-agent system using quantized vision-language models for visually impaired assistance, achieving significant memory reduction and real-time performance while maintaining accuracy on relevant benchmarks.",
        "tldr_zh": "本文提出了一种基于量化视觉语言模型的场景感知多智能体系统，用于帮助视障人士，在保持相关基准测试准确性的同时，实现了显著的内存减少和实时性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation",
        "summary": "Camouflaged Object Segmentation (COS) poses a significant challenge due to\nthe intrinsic high similarity between targets and backgrounds, demanding models\ncapable of profound holistic understanding beyond superficial cues. Prevailing\nmethods, often limited by shallow feature representation, inadequate reasoning\nmechanisms, and weak cross-modal integration, struggle to achieve this depth of\ncognition, resulting in prevalent issues like incomplete target separation and\nimprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyed\nGiant-emphasizing holistic observation, omnidirectional focus, and intensive\nscrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thought\nframework underpinned by cross-modal synergy and omnidirectional reasoning\nwithin Vision-Language Models (VLMs). ArgusCogito orchestrates three\ncognitively-inspired stages: (1) Conjecture: Constructs a strong cognitive\nprior through global reasoning with cross-modal fusion (RGB, depth, semantic\nmaps), enabling holistic scene understanding and enhanced target-background\ndisambiguation. (2) Focus: Performs omnidirectional, attention-driven scanning\nand focused reasoning, guided by semantic priors from Conjecture, enabling\nprecise target localization and region-of-interest refinement. (3) Sculpting:\nProgressively sculpts high-fidelity segmentation masks by integrating\ncross-modal information and iteratively generating dense positive/negative\npoint prompts within focused regions, emulating Argus' intensive scrutiny.\nExtensive evaluations on four challenging COS benchmarks and three Medical\nImage Segmentation (MIS) benchmarks demonstrate that ArgusCogito achieves\nstate-of-the-art (SOTA) performance, validating the framework's exceptional\nefficacy, superior generalization capability, and robustness.",
        "url": "http://arxiv.org/abs/2508.18050v1",
        "published_date": "2025-08-25T14:08:17+00:00",
        "updated_date": "2025-08-25T14:08:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianwen Tan",
            "Huiyao Zhang",
            "Rui Xiong",
            "Han Zhou",
            "Hongfei Wang",
            "Ye Li"
        ],
        "tldr": "The paper introduces ArgusCogito, a novel zero-shot chain-of-thought framework for camouflaged object segmentation, leveraging cross-modal synergy and omnidirectional reasoning within VLMs to achieve SOTA results on COS and MIS benchmarks.",
        "tldr_zh": "该论文介绍了一种名为ArgusCogito 的新型零样本链式思考框架，用于伪装对象分割。它利用视觉语言模型中的跨模态协同和全方位推理，在伪装对象分割和医学图像分割基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization",
        "summary": "The increasing sophistication of image manipulation techniques demands robust\nforensic solutions that can both reliably detect alterations and precisely\nlocalize tampered regions. Recent Multimodal Large Language Models (MLLMs) show\npromise by leveraging world knowledge and semantic understanding for\ncontext-aware detection, yet they struggle with perceiving subtle, low-level\nforensic artifacts crucial for accurate manipulation localization. This paper\npresents a novel Propose-Rectify framework that effectively bridges semantic\nreasoning with forensic-specific analysis. In the proposal stage, our approach\nutilizes a forensic-adapted LLaVA model to generate initial manipulation\nanalysis and preliminary localization of suspicious regions based on semantic\nunderstanding and contextual reasoning. In the rectification stage, we\nintroduce a Forensics Rectification Module that systematically validates and\nrefines these initial proposals through multi-scale forensic feature analysis,\nintegrating technical evidence from several specialized filters. Additionally,\nwe present an Enhanced Segmentation Module that incorporates critical forensic\ncues into SAM's encoded image embeddings, thereby overcoming inherent semantic\nbiases to achieve precise delineation of manipulated regions. By\nsynergistically combining advanced multimodal reasoning with established\nforensic methodologies, our framework ensures that initial semantic proposals\nare systematically validated and enhanced through concrete technical evidence,\nresulting in comprehensive detection accuracy and localization precision.\nExtensive experimental validation demonstrates state-of-the-art performance\nacross diverse datasets with exceptional robustness and generalization\ncapabilities.",
        "url": "http://arxiv.org/abs/2508.17976v1",
        "published_date": "2025-08-25T12:43:53+00:00",
        "updated_date": "2025-08-25T12:43:53+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Keyang Zhang",
            "Chenqi Kong",
            "Hui Liu",
            "Bo Ding",
            "Xinghao Jiang",
            "Haoliang Li"
        ],
        "tldr": "The paper introduces a 'Propose-Rectify' framework that combines MLLMs with forensic analysis for improved image manipulation localization by integrating semantic understanding with forensic feature analysis, achieving SOTA performance.",
        "tldr_zh": "该论文介绍了一个“提议-纠正”框架，该框架结合了多模态大型语言模型和取证分析，通过将语义理解与取证特征分析相结合，从而改进了图像篡改定位，并实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops",
        "summary": "Human video comprehension demonstrates dynamic coordination between reasoning\nand visual attention, adaptively focusing on query-relevant details. However,\ncurrent long-form video question answering systems employ rigid pipelines that\ndecouple reasoning from perception, leading to either information loss through\npremature visual abstraction or computational inefficiency through exhaustive\nprocessing. The core limitation lies in the inability to adapt visual\nextraction to specific reasoning requirements, different queries demand\nfundamentally different visual evidence from the same video content. In this\nwork, we present CAVIA, a training-free framework that revolutionizes video\nunderstanding through reasoning, perception coordination. Unlike conventional\napproaches where visual processing operates independently of reasoning, CAVIA\ncreates a closed-loop system where reasoning continuously guides visual\nextraction based on identified information gaps. CAVIA introduces three\ninnovations: (1) hierarchical reasoning, guided localization to precise frames;\n(2) cross-modal semantic bridging for targeted extraction; (3)\nconfidence-driven iterative synthesis. CAVIA achieves state-of-the-art\nperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA\n(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic\nreasoning-perception coordination provides a scalable paradigm for video\nunderstanding.",
        "url": "http://arxiv.org/abs/2508.17932v1",
        "published_date": "2025-08-25T12:00:12+00:00",
        "updated_date": "2025-08-25T12:00:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T45, 68T05",
            "H.5.1; I.2.10; I.4.8; I.5.4"
        ],
        "authors": [
            "Zixuan Dong",
            "Baoyun Peng",
            "Yufei Wang",
            "Lin Liu",
            "Xinxin Dong",
            "Yunlong Cao",
            "Xiaodong Wang"
        ],
        "tldr": "The paper introduces CAVIA, a training-free framework for video question answering that uses a reasoning-perception loop to dynamically guide visual extraction, achieving state-of-the-art performance on several benchmarks.",
        "tldr_zh": "该论文介绍了一种名为 CAVIA 的免训练视频问答框架，它使用推理-感知循环来动态引导视觉提取，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model",
        "summary": "Affordance is crucial for intelligent robots in the context of object\nmanipulation. In this paper, we argue that affordance should be\ntask-/instruction-dependent, which is overlooked by many previous works. That\nis, different instructions can lead to different manipulation regions and\ndirections even for the same object. According to this observation, we present\na new dataset comprising fifteen thousand object-instruction-affordance\ntriplets. All scenes in the dataset are from an egocentric viewpoint, designed\nto approximate the perspective of a human-like robot. Furthermore, we\ninvestigate how to enable large multimodal models (LMMs) to serve as affordance\npredictors by implementing a ``search against verifiers'' pipeline. An LMM is\nasked to progressively predict affordances, with the output at each step being\nverified by itself during the iterative process, imitating a reasoning process.\nExperiments show that our method not only unlocks new instruction-oriented\naffordance prediction capabilities, but also achieves outstanding performance\nbroadly.",
        "url": "http://arxiv.org/abs/2508.17922v1",
        "published_date": "2025-08-25T11:40:31+00:00",
        "updated_date": "2025-08-25T11:40:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bokai Ji",
            "Jie Gu",
            "Xiaokang Ma",
            "Chu Tang",
            "Jingmin Chen",
            "Guangxia Li"
        ],
        "tldr": "This paper introduces a new egocentric dataset for instruction-oriented affordance prediction and proposes a method using Large Multimodal Models (LMMs) with a self-verification pipeline to predict affordances based on instructions.",
        "tldr_zh": "本文介绍了一个新的以自我为中心的指令导向可供性预测数据集，并提出了一种使用大型多模态模型 (LMM) 和自我验证管道的方法，以根据指令预测可供性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniAPO: Unified Multimodal Automated Prompt Optimization",
        "summary": "Prompting is fundamental to unlocking the full potential of large language\nmodels. To automate and enhance this process, automatic prompt optimization\n(APO) has been developed, demonstrating effectiveness primarily in text-only\ninput scenarios. However, extending existing APO methods to multimodal tasks,\nsuch as video-language generation introduces two core challenges: (i) visual\ntoken inflation, where long visual token sequences restrict context capacity\nand result in insufficient feedback signals; (ii) a lack of process-level\nsupervision, as existing methods focus on outcome-level supervision and\noverlook intermediate supervision, limiting prompt optimization. We present\nUniAPO: Unified Multimodal Automated Prompt Optimization, the first framework\ntailored for multimodal APO. UniAPO adopts an EM-inspired optimization process\nthat decouples feedback modeling and prompt refinement, making the optimization\nmore stable and goal-driven. To further address the aforementioned challenges,\nwe introduce a short-long term memory mechanism: historical feedback mitigates\ncontext limitations, while historical prompts provide directional guidance for\neffective prompt optimization. UniAPO achieves consistent gains across text,\nimage, and video benchmarks, establishing a unified framework for efficient and\ntransferable prompt optimization.",
        "url": "http://arxiv.org/abs/2508.17890v1",
        "published_date": "2025-08-25T10:56:39+00:00",
        "updated_date": "2025-08-25T10:56:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qipeng Zhu",
            "Yanzhe Chen",
            "Huasong Zhong",
            "Yan Li",
            "Jie Chen",
            "Zhixin Zhang",
            "Junping Zhang",
            "Zhenheng Yang"
        ],
        "tldr": "UniAPO introduces a unified framework for automated prompt optimization (APO) across text, image, and video modalities, addressing challenges like visual token inflation and lack of process-level supervision using an EM-inspired optimization and short-long term memory mechanisms.",
        "tldr_zh": "UniAPO 提出了一个统一的自动提示优化（APO）框架，适用于文本、图像和视频模态。该框架通过 EM 启发式优化和短长期记忆机制，解决了视觉 token 膨胀和缺乏过程级监督等挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering",
        "summary": "The advancement of Multimodal Large Language Models (MLLMs) has driven\nsignificant progress in Visual Question Answering (VQA), evolving from Single\nto Multi Image VQA (MVQA). However, the increased number of images in MVQA\ninevitably introduces substantial visual redundancy that is irrelevant to\nquestion answering, negatively impacting both accuracy and efficiency. To\naddress this issue, existing methods lack flexibility in controlling the number\nof compressed visual tokens and tend to produce discrete visual fragments,\nwhich hinder MLLMs' ability to comprehend images holistically. In this paper,\nwe propose a straightforward yet universal Adaptive Visual Anchoring strategy,\nwhich can be seamlessly integrated into existing MLLMs, offering significant\naccuracy improvements through adaptive compression. Meanwhile, to balance the\nresults derived from both global and compressed visual input, we further\nintroduce a novel collaborative decoding mechanism, enabling optimal\nperformance. Extensive experiments validate the effectiveness of our method,\ndemonstrating consistent performance improvements across various MLLMs. The\ncode will be publicly available.",
        "url": "http://arxiv.org/abs/2508.17860v1",
        "published_date": "2025-08-25T10:10:46+00:00",
        "updated_date": "2025-08-25T10:10:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kang Zeng",
            "Guojin Zhong",
            "Jintao Cheng",
            "Jin Yuan",
            "Zhiyong Li"
        ],
        "tldr": "The paper introduces AVAM, a training-free adaptive visual anchoring strategy for Multi-image VQA that compresses visual tokens to improve accuracy and efficiency in MLLMs, along with a collaborative decoding mechanism.",
        "tldr_zh": "该论文提出了AVAM，一种免训练的自适应视觉锚定策略，用于多图像VQA，通过压缩视觉令牌来提高MLLM的准确性和效率，并结合了一种协同解码机制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference",
        "summary": "In this study, we introduce a novel method called group-wise \\textbf{VI}sual\ntoken \\textbf{S}election and \\textbf{A}ggregation (VISA) to address the issue\nof inefficient inference stemming from excessive visual tokens in multimoal\nlarge language models (MLLMs). Compared with previous token pruning approaches,\nour method can preserve more visual information while compressing visual\ntokens. We first propose a graph-based visual token aggregation (VTA) module.\nVTA treats each visual token as a node, forming a graph based on semantic\nsimilarity among visual tokens. It then aggregates information from removed\ntokens into kept tokens based on this graph, producing a more compact visual\ntoken representation. Additionally, we introduce a group-wise token selection\nstrategy (GTS) to divide visual tokens into kept and removed ones, guided by\ntext tokens from the final layers of each group. This strategy progressively\naggregates visual information, enhancing the stability of the visual\ninformation extraction process. We conduct comprehensive experiments on\nLLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate\nthe efficacy of VISA. Our method consistently outperforms previous methods,\nachieving a superior trade-off between model performance and inference speed.\nThe code is available at https://github.com/mobiushy/VISA.",
        "url": "http://arxiv.org/abs/2508.17857v1",
        "published_date": "2025-08-25T10:07:07+00:00",
        "updated_date": "2025-08-25T10:07:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pengfei Jiang",
            "Hanjun Li",
            "Linglan Zhao",
            "Fei Chao",
            "Ke Yan",
            "Shouhong Ding",
            "Rongrong Ji"
        ],
        "tldr": "The paper introduces VISA, a method for efficient MLLM inference via group-wise visual token selection and aggregation using graph summarization, demonstrating improved performance and speed over existing token pruning methods on LLaVA models.",
        "tldr_zh": "该论文介绍了VISA，一种通过图摘要进行分组视觉token选择和聚合的有效MLLM推理方法，在LLaVA模型上展示了比现有token剪枝方法更好的性能和速度。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Alternating Training-based Label Smoothing Enhances Prompt Generalization",
        "summary": "Recent advances in pre-trained vision-language models have demonstrated\nremarkable zero-shot generalization capabilities. To further enhance these\nmodels' adaptability to various downstream tasks, prompt tuning has emerged as\na parameter-efficient fine-tuning method. However, despite its efficiency, the\ngeneralization ability of prompt remains limited. In contrast, label smoothing\n(LS) has been widely recognized as an effective regularization technique that\nprevents models from becoming over-confident and improves their generalization.\nThis inspires us to explore the integration of LS with prompt tuning. However,\nwe have observed that the vanilla LS even weakens the generalization ability of\nprompt tuning. To address this issue, we propose the Alternating Training-based\nLabel Smoothing (ATLaS) method, which alternately trains with standard one-hot\nlabels and soft labels generated by LS to supervise the prompt tuning.\nMoreover, we introduce two types of efficient offline soft labels, including\nClass-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide\ninter-class or instance-class relationships for prompt tuning. The theoretical\nproperties of the proposed ATLaS method are analyzed. Extensive experiments\ndemonstrate that the proposed ATLaS method, combined with CSL and ISL,\nconsistently enhances the generalization performance of prompt tuning.\nMoreover, the proposed ATLaS method exhibits high compatibility with prevalent\nprompt tuning methods, enabling seamless integration into existing methods.",
        "url": "http://arxiv.org/abs/2508.17846v1",
        "published_date": "2025-08-25T09:54:37+00:00",
        "updated_date": "2025-08-25T09:54:37+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yang Chen",
            "Yanbin Wei",
            "Ke Jin",
            "Yi Kong",
            "James Kwok",
            "Yu Zhang"
        ],
        "tldr": "This paper introduces Alternating Training-based Label Smoothing (ATLaS) with class-wise and instance-wise soft labels to improve the generalization ability of prompt tuning in vision-language models.",
        "tldr_zh": "本文介绍了一种基于交替训练的标签平滑（ATLaS）方法，结合类级别和实例级别的软标签，以提高视觉-语言模型中提示调优的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models",
        "summary": "Vision-Language Models (VLMs) typically process a significantly larger number\nof visual tokens compared to text tokens due to the inherent redundancy in\nvisual signals. Visual token pruning is a promising direction to reduce the\ncomputational cost of VLMs by eliminating redundant visual tokens. The\ntext-visual attention score is a widely adopted criterion for visual token\npruning as it reflects the relevance of visual tokens to the text input.\nHowever, many sequence models exhibit a recency bias, where tokens appearing\nlater in the sequence exert a disproportionately large influence on the model's\noutput. In VLMs, this bias manifests as inflated attention scores for tokens\ncorresponding to the lower regions of the image, leading to suboptimal pruning\nthat disproportionately retains tokens from the image bottom. In this paper, we\npresent an extremely simple yet effective approach to alleviate the recency\nbias in visual token pruning. We propose a straightforward reweighting\nmechanism that adjusts the attention scores of visual tokens according to their\nspatial positions in the image. Our method, termed Position-reweighted Visual\nToken Pruning, is a plug-and-play solution that can be seamlessly incorporated\ninto existing visual token pruning frameworks without any changes to the model\narchitecture or extra training. Extensive experiments on LVLMs demonstrate that\nour method improves the performance of visual token pruning with minimal\ncomputational overhead.",
        "url": "http://arxiv.org/abs/2508.17807v1",
        "published_date": "2025-08-25T08:56:32+00:00",
        "updated_date": "2025-08-25T08:56:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Zhao",
            "Wubang Yuan",
            "Alex Lingyu Hung",
            "Dan Zeng"
        ],
        "tldr": "This paper introduces a position-reweighted attention mechanism for visual token pruning in VLMs to address recency bias, improving performance without architectural changes or retraining.",
        "tldr_zh": "本文介绍了一种位置重加权的视觉token剪枝方法，用于解决VLM中的近因偏差，无需修改架构或重新训练即可提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Instant Preference Alignment for Text-to-Image Diffusion Models",
        "summary": "Text-to-image (T2I) generation has greatly enhanced creative expression, yet\nachieving preference-aligned generation in a real-time and training-free manner\nremains challenging. Previous methods often rely on static, pre-collected\npreferences or fine-tuning, limiting adaptability to evolving and nuanced user\nintents. In this paper, we highlight the need for instant preference-aligned\nT2I generation and propose a training-free framework grounded in multimodal\nlarge language model (MLLM) priors. Our framework decouples the task into two\ncomponents: preference understanding and preference-guided generation. For\npreference understanding, we leverage MLLMs to automatically extract global\npreference signals from a reference image and enrich a given prompt using\nstructured instruction design. Our approach supports broader and more\nfine-grained coverage of user preferences than existing methods. For\npreference-guided generation, we integrate global keyword-based control and\nlocal region-aware cross-attention modulation to steer the diffusion model\nwithout additional training, enabling precise alignment across both global\nattributes and local elements. The entire framework supports multi-round\ninteractive refinement, facilitating real-time and context-aware image\ngeneration. Extensive experiments on the Viper dataset and our collected\nbenchmark demonstrate that our method outperforms prior approaches in both\nquantitative metrics and human evaluations, and opens up new possibilities for\ndialog-based generation and MLLM-diffusion integration.",
        "url": "http://arxiv.org/abs/2508.17718v1",
        "published_date": "2025-08-25T06:51:15+00:00",
        "updated_date": "2025-08-25T06:51:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yang Li",
            "Songlin Yang",
            "Xiaoxuan Han",
            "Wei Wang",
            "Jing Dong",
            "Yueming Lyu",
            "Ziyu Xue"
        ],
        "tldr": "This paper introduces a training-free framework for instant preference alignment in text-to-image diffusion models, using multimodal large language models for preference understanding and guided generation, achieving real-time and interactive refinement.",
        "tldr_zh": "本文提出了一种无需训练的文本到图像扩散模型即时偏好对齐框架，利用多模态大型语言模型进行偏好理解和引导生成，实现实时和交互式优化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model",
        "summary": "Traditional dialogue retrieval aims to select the most appropriate utterance\nor image from recent dialogue history. However, they often fail to meet users'\nactual needs for revisiting semantically coherent content scattered across\nlong-form conversations. To fill this gap, we define the Fine-grained Fragment\nRetrieval (FFR) task, requiring models to locate query-relevant fragments,\ncomprising both utterances and images, from multimodal long-form dialogues. As\na foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue\nretrieval dataset to date, averaging 25.45 turns per dialogue, with each\nnaturally spanning three distinct topics. To evaluate generalization in\nreal-world scenarios, we curate and annotate a WeChat-based test set comprising\nreal-world multimodal dialogues with an average of 75.38 turns. Building on\nthese resources, we explore existing generation-based Vision-Language Models\n(VLMs) on FFR and observe that they often retrieve incoherent utterance-image\nfragments. While optimized for generating responses from visual-textual inputs,\nthese models lack explicit supervision to ensure semantic coherence within\nretrieved fragments. To this end, we propose F2RVLM, a generative retrieval\nmodel trained in a two-stage paradigm: (1) supervised fine-tuning to inject\nfragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning\nwith multi-objective rewards promoting semantic precision, relevance, and\ncontextual coherence. To handle varying intra-fragment complexity, from locally\ndense to sparsely distributed, we introduce difficulty-aware curriculum\nsampling that ranks training instances by model-predicted difficulty and\ngradually exposes the model to harder samples. This boosts reasoning ability in\nlong, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain\nand real-domain settings, demonstrating superior retrieval performance.",
        "url": "http://arxiv.org/abs/2508.17714v1",
        "published_date": "2025-08-25T06:42:47+00:00",
        "updated_date": "2025-08-25T06:42:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanbo Bi",
            "Zhiqiang Yuan",
            "Zexi Jia",
            "Jiapei Zhang",
            "Chongyang Li",
            "Peixiang Luo",
            "Ying Deng",
            "Xiaoyue Duan",
            "Jinchao Zhang"
        ],
        "tldr": "The paper introduces a new Fine-grained Fragment Retrieval (FFR) task for multimodal long-form dialogues and proposes F2RVLM, a generative retrieval model trained with a two-stage approach and difficulty-aware curriculum sampling to improve semantic coherence and retrieval performance.",
        "tldr_zh": "该论文提出了一个新的多模态长对话的细粒度片段检索（FFR）任务，并提出了一种生成式检索模型F2RVLM，该模型采用两阶段训练方法和难度感知课程采样，以提高语义连贯性和检索性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing",
        "summary": "Vision Language Models (VLMs) struggle with long-form videos due to the\nquadratic complexity of attention mechanisms. We propose Language-Guided\nTemporal Token Pruning (LGTTP), which leverages temporal cues from queries to\nadaptively prune video tokens, preserving contextual continuity while reducing\ncomputational overhead. Unlike uniform pruning or keyframe selection, LGTTP\nretains higher token density in temporally relevant segments. Our\nmodel-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a\n65% reduction in computation while preserving 97-99% of the original\nperformance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on\nCharades-STA, it retains 99.6% of R@1. It excels on queries with explicit\ntemporal markers and remains effective across general video understanding\ntasks.",
        "url": "http://arxiv.org/abs/2508.17686v1",
        "published_date": "2025-08-25T05:51:21+00:00",
        "updated_date": "2025-08-25T05:51:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yogesh Kumar"
        ],
        "tldr": "The paper introduces Language-Guided Temporal Token Pruning (LGTTP) to reduce the computational cost of VideoLLMs by adaptively pruning video tokens based on language queries, achieving significant computational savings with minimal performance loss.",
        "tldr_zh": "该论文提出了语言引导的时间令牌修剪（LGTTP），通过基于语言查询自适应地修剪视频令牌，来降低视频语言模型（VideoLLM）的计算成本，并在性能损失最小的情况下实现显著的计算节省。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection",
        "summary": "In trustworthy medical diagnosis systems, integrating out-of-distribution\n(OOD) detection aims to identify unknown diseases in samples, thereby\nmitigating the risk of misdiagnosis. In this study, we propose a novel OOD\ndetection framework based on vision-language models (VLMs), which integrates\nhierarchical visual information to cope with challenging unknown diseases that\nresemble known diseases. Specifically, a cross-scale visual fusion strategy is\nproposed to couple visual embeddings from multiple scales. This enriches the\ndetailed representation of medical images and thus improves the discrimination\nof unknown diseases. Moreover, a cross-scale hard pseudo-OOD sample generation\nstrategy is proposed to benefit OOD detection maximally. Experimental\nevaluations on three public medical datasets support that the proposed\nframework achieves superior OOD detection performance compared to existing\nmethods. The source code is available at https://openi.pcl.ac.cn/OpenMedIA/HVL.",
        "url": "http://arxiv.org/abs/2508.17667v1",
        "published_date": "2025-08-25T04:55:27+00:00",
        "updated_date": "2025-08-25T04:55:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Runhe Lai",
            "Xinhua Lu",
            "Kanghao Chen",
            "Qichao Chen",
            "Wei-Shi Zheng",
            "Ruixuan Wang"
        ],
        "tldr": "This paper introduces a vision-language model-based framework for medical out-of-distribution (OOD) detection, using cross-scale visual fusion and hard pseudo-OOD sample generation to improve the identification of unknown diseases.",
        "tldr_zh": "本文提出了一种基于视觉语言模型的医疗领域异常检测框架，通过跨尺度视觉融合和困难伪异常样本生成，以提高未知疾病的识别能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning",
        "summary": "Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects\nvisual features and then concatenates them with text tokens to form a unified\nsequence input for Large Language Models (LLMs). However, this paradigm leads\nto a significant increase in the length of the input sequence, resulting in\nsubstantial computational overhead. Existing methods attempt to fuse visual\ninformation into the intermediate layers of LLMs, which alleviate the sequence\nlength issue but often neglect the hierarchical semantic representations within\nthe model and the fine-grained visual information available in the shallower\nvisual encoding layers. To address this limitation, we propose DEHVF, an\nefficient vision-language fine-tuning method based on dynamic embedding and\nfusion of hierarchical visual features. Its core lies in leveraging the\ninherent hierarchical representation characteristics of visual encoders and\nlanguage models. Through a lightweight hierarchical visual fuser, it\ndynamically selects and fuses hierarchical features corresponding to semantic\ngranularity based on the internal representations of each layer in LLMs. The\nfused layer-related visual features are then projected and aligned before being\ndirectly embedded into the Feed-Forward Network (FFN) of the corresponding\nlayer in LLMs. This approach not only avoids sequence expansion but also\ndynamically fuses multi-layer visual information. By fine-tuning only a small\nnumber of parameters, DEHVF achieves precise alignment and complementarity of\ncross-modal information at the same semantic granularity. We conducted\nexperiments across various VL benchmarks, including visual question answering\non ScienceQA and image captioning on COCO Captions. The results demonstrate\nthat DEHVF achieves higher accuracy than existing parameter-efficient\nfine-tuning (PEFT) baselines while maintaining efficient training and\ninference.",
        "url": "http://arxiv.org/abs/2508.17638v1",
        "published_date": "2025-08-25T03:57:46+00:00",
        "updated_date": "2025-08-25T03:57:46+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Xinyu Wei",
            "Guoli Yang",
            "Jialu Zhou",
            "Mingyue Yang",
            "Leqian Li",
            "Kedi Zhang",
            "Chunping Qiu"
        ],
        "tldr": "The paper introduces DEHVF, a method for efficient vision-language fine-tuning that dynamically embeds hierarchical visual features into LLMs' FFN layers, avoiding sequence expansion and improving accuracy compared to existing PEFT methods.",
        "tldr_zh": "该论文介绍了DEHVF，一种高效的视觉语言微调方法，它将分层视觉特征动态嵌入到LLM的FFN层中，避免了序列扩展，并相比现有PEFT方法提高了准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
        "summary": "Visually-guided image editing, where edits are conditioned on both visual\ncues and textual prompts, has emerged as a powerful paradigm for fine-grained,\ncontrollable content generation. Although recent generative models have shown\nremarkable capabilities, existing evaluations remain simple and insufficiently\nrepresentative of real-world editing challenges. We present SpotEdit, a\ncomprehensive benchmark designed to systematically assess visually-guided image\nediting methods across diverse diffusion, autoregressive, and hybrid generative\nmodels, uncovering substantial performance disparities. To address a critical\nyet underexplored challenge, our benchmark includes a dedicated component on\nhallucination, highlighting how leading models, such as GPT-4o, often\nhallucinate the existence of a visual cue and erroneously perform the editing\ntask. Our code and benchmark are publicly released at\nhttps://github.com/SaraGhazanfari/SpotEdit.",
        "url": "http://arxiv.org/abs/2508.18159v1",
        "published_date": "2025-08-25T16:08:57+00:00",
        "updated_date": "2025-08-25T16:08:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sara Ghazanfari",
            "Wei-An Lin",
            "Haitong Tian",
            "Ersin Yumer"
        ],
        "tldr": "The paper introduces SpotEdit, a new benchmark for visually-guided image editing that reveals performance disparities among different generative models and specifically addresses the issue of hallucination in these models, including GPT-4o.",
        "tldr_zh": "该论文介绍了SpotEdit，这是一个用于视觉引导图像编辑的新基准，揭示了不同生成模型之间的性能差异，并专门解决了这些模型（包括GPT-4o）中存在的幻觉问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images",
        "summary": "Semantic segmentation of remote sensing (RS) images is pivotal for\ncomprehensive Earth observation, but the demand for interpreting new object\ncategories, coupled with the high expense of manual annotation, poses\nsignificant challenges. Although open-vocabulary semantic segmentation (OVSS)\noffers a promising solution, existing frameworks designed for natural images\nare insufficient for the unique complexities of RS data. They struggle with\nvast scale variations and fine-grained details, and their adaptation often\nrelies on extensive, costly annotations. To address this critical gap, this\npaper introduces SegEarth-OV, the first framework for annotation-free\nopen-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,\na universal upsampler that robustly restores high-resolution spatial details\nfrom coarse features, correcting distorted target shapes without any\ntask-specific post-training. We also present a simple yet effective Global Bias\nAlleviation operation to subtract the inherent global context from patch\nfeatures, significantly enhancing local semantic fidelity. These components\nempower SegEarth-OV to effectively harness the rich semantics of pre-trained\nVLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the\nframework's universality to other challenging RS modalities like SAR images,\nwhere large-scale VLMs are unavailable and expensive to create, we introduce\nAlignEarth, which is a distillation-based strategy and can efficiently transfer\nsemantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the\nneed to build SAR foundation models from scratch and enabling universal OVSS\nacross diverse sensor types. Extensive experiments on both optical and SAR\ndatasets validate that SegEarth-OV can achieve dramatic improvements over the\nSOTA methods, establishing a robust foundation for annotation-free and\nopen-world Earth observation.",
        "url": "http://arxiv.org/abs/2508.18067v1",
        "published_date": "2025-08-25T14:22:57+00:00",
        "updated_date": "2025-08-25T14:22:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyu Li",
            "Xiangyong Cao",
            "Ruixun Liu",
            "Shihong Wang",
            "Zixuan Jiang",
            "Zhi Wang",
            "Deyu Meng"
        ],
        "tldr": "The paper introduces SegEarth-OV, a novel annotation-free open-vocabulary segmentation framework for remote sensing images, addressing challenges in scale variation, fine-grained details, and modality transfer (optical to SAR). It achieves state-of-the-art results on both optical and SAR datasets.",
        "tldr_zh": "该论文介绍了 SegEarth-OV，一种用于遥感图像的新型无标注开放词汇分割框架，解决了尺度变化、精细细节和模态转移（光学到 SAR）方面的挑战。它在光学和 SAR 数据集上都取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation",
        "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
        "url": "http://arxiv.org/abs/2508.18032v1",
        "published_date": "2025-08-25T13:53:02+00:00",
        "updated_date": "2025-08-25T13:53:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaqi Li",
            "Peng Chen",
            "Mingyang Han",
            "Bu Pi",
            "Haoxiang Shi",
            "Runzhou Zhao",
            "Yang Yao",
            "Xuan Zhang",
            "Jun Song"
        ],
        "tldr": "This paper introduces Visual-CoG, a stage-aware reinforcement learning approach for text-to-image generation that provides immediate guidance through semantic reasoning, process refining, and outcome evaluation stages, resulting in improved performance on multiple benchmarks. They also introduce a new benchmark called VisCog-Bench.",
        "tldr_zh": "该论文介绍了一种用于文本到图像生成的阶段感知强化学习方法 Visual-CoG，它通过语义推理、过程优化和结果评估阶段提供即时指导，从而提高了在多个基准测试中的性能。 他们还介绍了一个名为 VisCog-Bench 的新基准测试。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Global to Local: Social Bias Transfer in CLIP",
        "summary": "The recycling of contrastive language-image pre-trained (CLIP) models as\nbackbones for a large number of downstream tasks calls for a thorough analysis\nof their transferability implications, especially their well-documented\nreproduction of social biases and human stereotypes. How do such biases,\nlearned during pre-training, propagate to downstream applications like visual\nquestion answering or image captioning? Do they transfer at all?\n  We investigate this phenomenon, referred to as bias transfer in prior\nliterature, through a comprehensive empirical analysis. Firstly, we examine how\npre-training bias varies between global and local views of data, finding that\nbias measurement is highly dependent on the subset of data on which it is\ncomputed. Secondly, we analyze correlations between biases in the pre-trained\nmodels and the downstream tasks across varying levels of pre-training bias,\nfinding difficulty in discovering consistent trends in bias transfer. Finally,\nwe explore why this inconsistency occurs, showing that under the current\nparadigm, representation spaces of different pre-trained CLIPs tend to converge\nwhen adapted for downstream tasks. We hope this work offers valuable insights\ninto bias behavior and informs future research to promote better bias\nmitigation practices.",
        "url": "http://arxiv.org/abs/2508.17750v1",
        "published_date": "2025-08-25T07:44:03+00:00",
        "updated_date": "2025-08-25T07:44:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ryan Ramos",
            "Yusuke Hirota",
            "Yuta Nakashima",
            "Noa Garcia"
        ],
        "tldr": "This paper investigates the transfer of social biases from pre-trained CLIP models to downstream tasks, finding inconsistencies and representation convergence that complicate bias mitigation.",
        "tldr_zh": "本文研究了预训练CLIP模型中社会偏见向下游任务的转移，发现不一致性和表征收敛使偏见缓解变得复杂。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints",
        "summary": "Reasoning about fine-grained spatial relationships in warehouse-scale\nenvironments poses a significant challenge for existing vision-language models\n(VLMs), which often struggle to comprehend 3D layouts, object arrangements, and\nmultimodal cues in real-world industrial settings. In this paper, we present\nTinyGiantVLM, a lightweight and modular two-stage framework designed for\nphysical spatial reasoning, distinguishing itself from traditional geographic\nreasoning in complex logistics scenes. Our approach encodes both global and\nregion-level features from RGB and depth modalities using pretrained visual\nbackbones. To effectively handle the complexity of high-modality inputs and\ndiverse question types, we incorporate a Mixture-of-Experts (MoE) fusion\nmodule, which dynamically combines spatial representations to support\ndownstream reasoning tasks and improve convergence. Training is conducted in a\ntwo-phase strategy: the first phase focuses on generating free-form answers to\nenhance spatial reasoning ability, while the second phase uses normalized\nanswers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our\n64M-parameter base model achieved 5th place on the leaderboard with a score of\n66.8861, demonstrating strong performance in bridging visual perception and\nspatial understanding in industrial environments. We further present an\n80M-parameter variant with expanded MoE capacity, which demonstrates improved\nperformance on spatial reasoning tasks.",
        "url": "http://arxiv.org/abs/2508.17595v1",
        "published_date": "2025-08-25T01:36:22+00:00",
        "updated_date": "2025-08-25T01:36:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vinh-Thuan Ly",
            "Hoang M. Truong",
            "Xuan-Huong Nguyen"
        ],
        "tldr": "TinyGiantVLM is a lightweight vision-language model designed for spatial reasoning in industrial environments, using a Mixture-of-Experts fusion module and a two-phase training strategy to achieve competitive performance on the AI City Challenge 2025.",
        "tldr_zh": "TinyGiantVLM 是一种轻量级的视觉语言模型，专为工业环境中的空间推理而设计，它采用混合专家融合模块和两阶段训练策略，在 AI City Challenge 2025 上取得了有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation",
        "summary": "Metamaterials are micro-architected structures whose geometry imparts highly\ntunable-often counter-intuitive-bulk properties. Yet their design is difficult\nbecause of geometric complexity and a non-trivial mapping from architecture to\nbehaviour. We address these challenges with three complementary contributions.\n(i) MetaDSL: a compact, semantically rich domain-specific language that\ncaptures diverse metamaterial designs in a form that is both human-readable and\nmachine-parsable. (ii) MetaDB: a curated repository of more than 150,000\nparameterized MetaDSL programs together with their\nderivatives-three-dimensional geometry, multi-view renderings, and simulated\nelastic properties. (iii) MetaBench: benchmark suites that test three core\ncapabilities of vision-language metamaterial assistants-structure\nreconstruction, property-driven inverse design, and performance prediction. We\nestablish baselines by fine-tuning state-of-the-art vision-language models and\ndeploy an omni-model within an interactive, CAD-like interface. Case studies\nshow that our framework provides a strong first step toward integrated design\nand understanding of structure-representation-property relationships.",
        "url": "http://arxiv.org/abs/2508.17568v1",
        "published_date": "2025-08-25T00:36:07+00:00",
        "updated_date": "2025-08-25T00:36:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CE",
            "cs.LG",
            "cs.PL"
        ],
        "authors": [
            "Liane Makatura",
            "Benjamin Jones",
            "Siyuan Bian",
            "Wojciech Matusik"
        ],
        "tldr": "The paper introduces MetaGen, a framework with a DSL, database, and benchmark for vision-language model assisted metamaterial generation, addressing the challenges of complex metamaterial design through structure reconstruction, inverse design, and performance prediction.",
        "tldr_zh": "该论文介绍了 MetaGen，一个包含 DSL、数据库和基准的框架，用于视觉语言模型辅助的超材料生成，通过结构重建、逆向设计和性能预测来解决复杂超材料设计的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images",
        "summary": "Depth estimation is a foundational component for 3D reconstruction in\nminimally invasive endoscopic surgeries. However, existing monocular depth\nestimation techniques often exhibit limited performance to the varying\nillumination and complex textures of the surgical environment. While powerful\nvisual foundation models offer a promising solution, their training on natural\nimages leads to significant domain adaptability limitations and semantic\nperception deficiencies when applied to endoscopy. In this study, we introduce\nEndoUFM, an unsupervised monocular depth estimation framework that innovatively\nintegrating dual foundation models for surgical scenes, which enhance the depth\nestimation performance by leveraging the powerful pre-learned priors. The\nframework features a novel adaptive fine-tuning strategy that incorporates\nRandom Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a\nResidual block based on Depthwise Separable Convolution (Res-DSC) to improve\nthe capture of fine-grained local features. Furthermore, we design a\nmask-guided smoothness loss to enforce depth consistency within anatomical\ntissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and\nEndoNeRF datasets confirm that our method achieves state-of-the-art performance\nwhile maintaining an efficient model size. This work contributes to augmenting\nsurgeons' spatial perception during minimally invasive procedures, thereby\nenhancing surgical precision and safety, with crucial implications for\naugmented reality and navigation systems.",
        "url": "http://arxiv.org/abs/2508.17916v1",
        "published_date": "2025-08-25T11:33:05+00:00",
        "updated_date": "2025-08-25T11:33:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinning Yao",
            "Bo Liu",
            "Bojian Li",
            "Jingjing Wang",
            "Jinghua Yue",
            "Fugen Zhou"
        ],
        "tldr": "The paper introduces EndoUFM, an unsupervised monocular depth estimation framework for endoscopic images that leverages foundation models and adaptive fine-tuning strategies to improve depth estimation performance in surgical scenes.",
        "tldr_zh": "该论文介绍了EndoUFM，一种用于内窥镜图像的无监督单目深度估计框架，它利用基础模型和自适应微调策略来提高手术场景中的深度估计性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]