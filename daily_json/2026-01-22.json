[
    {
        "title": "Unified Multi-Dataset Training for TBPS",
        "summary": "Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.",
        "url": "http://arxiv.org/abs/2601.14978v1",
        "published_date": "2026-01-21T13:26:28+00:00",
        "updated_date": "2026-01-21T13:26:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nilanjana Chatterjee",
            "Sidharatha Garg",
            "A V Subramanyam",
            "Brejesh Lall"
        ],
        "tldr": "The paper introduces Scale-TBPS, a unified Text-Based Person Search model trained across multiple datasets, addressing limitations of dataset-specific fine-tuning by using noise-aware curation and scalable identity learning.",
        "tldr_zh": "该论文介绍了Scale-TBPS，一个统一的基于文本的行人搜索模型，通过使用噪声感知的数据集管理和可扩展的身份学习，解决了数据集特定微调的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ReinPath: A Multimodal Reinforcement Learning Approach for Pathology",
        "summary": "Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.",
        "url": "http://arxiv.org/abs/2601.14757v1",
        "published_date": "2026-01-21T08:21:35+00:00",
        "updated_date": "2026-01-21T08:21:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kangcheng Zhou",
            "Jun Jiang",
            "Qing Zhang",
            "Shuang Zheng",
            "Qingli Li",
            "Shugong Xu"
        ],
        "tldr": "The paper introduces ReinPath, a multimodal pathology large language model with a semantic reward strategy and a new VQA dataset for improved reasoning and interpretability in computational pathology, demonstrating strong performance even with limited data.",
        "tldr_zh": "该论文介绍了ReinPath，一个多模态病理学大型语言模型，它具有语义奖励策略和一个新的VQA数据集，旨在提高计算病理学中的推理能力和可解释性。实验表明，即使在有限的数据下，该模型也表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
        "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
        "url": "http://arxiv.org/abs/2601.14750v1",
        "published_date": "2026-01-21T08:09:25+00:00",
        "updated_date": "2026-01-21T08:09:25+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yifan Wang",
            "Shiyu Li",
            "Peiming Li",
            "Xiaochen Yang",
            "Yang Tang",
            "Zheng Wei"
        ],
        "tldr": "The paper introduces Render-of-Thought (RoT), a framework that converts textual Chain-of-Thought steps into images to improve the efficiency and analyzability of VLMs in reasoning tasks, achieving token compression and inference acceleration.",
        "tldr_zh": "该论文介绍了Render-of-Thought (RoT)，它将文本的思维链步骤转换为图像，以提高视觉语言模型在推理任务中的效率和可分析性，实现了token压缩和推理加速。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
        "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
        "url": "http://arxiv.org/abs/2601.14724v1",
        "published_date": "2026-01-21T07:26:15+00:00",
        "updated_date": "2026-01-21T07:26:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haowei Zhang",
            "Shudong Yang",
            "Jinlan Fu",
            "See-Kiong Ng",
            "Xipeng Qiu"
        ],
        "tldr": "HERMES is a training-free architecture for real-time streaming video understanding that uses a hierarchical KV cache to improve speed and reduce memory overhead, achieving significant speedups and comparable accuracy compared to prior methods.",
        "tldr_zh": "HERMES是一种无需训练的架构，用于实时流视频理解，它使用分层KV缓存来提高速度并减少内存开销，与先前的方法相比，实现了显著的加速和相当的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving",
        "summary": "Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.",
        "url": "http://arxiv.org/abs/2601.14702v1",
        "published_date": "2026-01-21T06:29:09+00:00",
        "updated_date": "2026-01-21T06:29:09+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zecong Tang",
            "Zixu Wang",
            "Yifei Wang",
            "Weitong Lian",
            "Tianjian Gao",
            "Haoran Li",
            "Tengju Ru",
            "Lingyi Meng",
            "Zhejun Cui",
            "Yichen Zhu",
            "Qi Kang",
            "Kaixuan Wang",
            "Yu Zhang"
        ],
        "tldr": "The paper introduces AutoDriDM, a new benchmark specifically designed to evaluate the decision-making capabilities of Vision-Language Models (VLMs) in autonomous driving, addressing a gap in existing benchmarks that overemphasize perception.",
        "tldr_zh": "该论文介绍了AutoDriDM，这是一个新的基准测试，专门用于评估视觉语言模型(VLM)在自动驾驶中的决策能力，解决了现有基准测试过度强调感知能力的缺陷。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Consistent Taxonomic Classification through Hierarchical Reasoning",
        "summary": "While Vision-Language Models (VLMs) excel at visual understanding, they often fail to grasp hierarchical knowledge. This leads to common errors where VLMs misclassify coarser taxonomic levels even when correctly identifying the most specific level (leaf level). Existing approaches largely overlook this issue by failing to model hierarchical reasoning. To address this gap, we propose VL-Taxon, a two-stage, hierarchy-based reasoning framework designed to improve both leaf-level accuracy and hierarchical consistency in taxonomic classification. The first stage employs a top-down process to enhance leaf-level classification accuracy. The second stage then leverages this accurate leaf-level output to ensure consistency throughout the entire taxonomic hierarchy. Each stage is initially trained with supervised fine-tuning to instill taxonomy knowledge, followed by reinforcement learning to refine the model's reasoning and generalization capabilities. Extensive experiments reveal a remarkable result: our VL-Taxon framework, implemented on the Qwen2.5-VL-7B model, outperforms its original 72B counterpart by over 10% in both leaf-level and hierarchical consistency accuracy on average on the iNaturalist-2021 dataset. Notably, this significant gain was achieved by fine-tuning on just a small subset of data, without relying on any examples generated by other VLMs.",
        "url": "http://arxiv.org/abs/2601.14610v1",
        "published_date": "2026-01-21T03:00:00+00:00",
        "updated_date": "2026-01-21T03:00:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenghong Li",
            "Kecheng Zheng",
            "Haibin Ling"
        ],
        "tldr": "The paper introduces VL-Taxon, a two-stage, hierarchy-based reasoning framework for VLMs that significantly improves both leaf-level and hierarchical consistency in taxonomic classification, outperforming a larger model with a smaller, fine-tuned model.",
        "tldr_zh": "该论文介绍了VL-Taxon，一种针对视觉语言模型的两阶段、基于层次推理的框架，该框架显著提高了分类任务中叶级和层次一致性，并且在更小的微调模型上优于更大的模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "3D Space as a Scratchpad for Editable Text-to-Image Generation",
        "summary": "Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/",
        "url": "http://arxiv.org/abs/2601.14602v1",
        "published_date": "2026-01-21T02:40:19+00:00",
        "updated_date": "2026-01-21T02:40:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Oindrila Saha",
            "Vojtech Krs",
            "Radomir Mech",
            "Subhransu Maji",
            "Matheus Gadelha",
            "Kevin Blackburn-Matzen"
        ],
        "tldr": "This paper introduces a 3D spatial scratchpad for text-to-image generation, enabling better spatial reasoning and editing capabilities compared to existing 2D layout methods, leading to improved text alignment.",
        "tldr_zh": "本文介绍了一种用于文本到图像生成的3D空间草稿板，与现有的2D布局方法相比，它能够实现更好的空间推理和编辑功能，从而提高文本对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning",
        "summary": "Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.",
        "url": "http://arxiv.org/abs/2601.14594v1",
        "published_date": "2026-01-21T02:26:48+00:00",
        "updated_date": "2026-01-21T02:26:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lianying Chao",
            "Linfeng Yin",
            "Peiyu Ren",
            "Yifan Jiang",
            "Qiaoyu Ren",
            "Dingcheng Shan",
            "Jing-cheng Pang",
            "Sijie Wu",
            "Xubin Li",
            "Kai Zhang"
        ],
        "tldr": "The paper introduces a Learnable Frame Selector (LFS) for video captioning that selects temporally diverse and event-relevant frames, leveraging caption feedback from video-LLMs and achieving improved performance on multiple benchmarks, including a new human-consistent benchmark.",
        "tldr_zh": "该论文介绍了一种用于视频字幕的可学习帧选择器（LFS），该选择器选择时间上多样且与事件相关的帧，利用来自视频-LLM的字幕反馈，并在多个基准测试中取得了改进的性能，包括一个新的与人类认知一致的基准测试。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
        "summary": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.",
        "url": "http://arxiv.org/abs/2601.14490v1",
        "published_date": "2026-01-20T21:26:15+00:00",
        "updated_date": "2026-01-20T21:26:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Hunter Heidenreich",
            "Ben Elliott",
            "Olivia Dinica",
            "Yosheb Getachew"
        ],
        "tldr": "GutenOCR is a family of fine-tuned Qwen2.5-VL models providing a unified, prompt-based interface for grounded OCR tasks, achieving significant improvements over the base models on business and scientific documents.",
        "tldr_zh": "GutenOCR是一系列微调后的Qwen2.5-VL模型，为基于统一的提示界面上的有依据的OCR任务提供支持，并在商业和科学文档上实现了比基础模型显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data",
        "summary": "Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.",
        "url": "http://arxiv.org/abs/2601.14406v1",
        "published_date": "2026-01-20T19:09:12+00:00",
        "updated_date": "2026-01-20T19:09:12+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yixiong Chen",
            "Zongwei Zhou",
            "Wenxuan Li",
            "Alan Yuille"
        ],
        "tldr": "The paper introduces SegAE, a vision-language model for automated assessment of medical segmentation label quality, demonstrating significant improvements in data efficiency and training performance.",
        "tldr_zh": "该论文介绍了一个名为 SegAE 的视觉-语言模型，用于自动评估医学分割标签的质量，并在数据效率和训练性能方面表现出显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
        "summary": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.",
        "url": "http://arxiv.org/abs/2601.14251v1",
        "published_date": "2026-01-20T18:58:32+00:00",
        "updated_date": "2026-01-20T18:58:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Said Taghadouini",
            "Adrien Cavaillès",
            "Baptiste Aubertin"
        ],
        "tldr": "LightOnOCR-2-1B, a 1B-parameter multilingual vision-language model, achieves state-of-the-art OCR performance with improved speed and reduced size, also predicting bounding boxes for embedded images.",
        "tldr_zh": "LightOnOCR-2-1B是一个拥有10亿参数的多语种视觉语言模型，实现了最先进的OCR性能，同时提高了速度并减小了模型尺寸。此外，该模型还可以预测嵌入图像的边界框。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning",
        "summary": "Hateful videos pose serious risks by amplifying discrimination, inciting violence, and undermining online safety. Existing training-based hateful video detection methods are constrained by limited training data and lack of interpretability, while directly prompting large vision-language models often struggle to deliver reliable hate detection. To address these challenges, this paper introduces MARS, a training-free Multi-stage Adversarial ReaSoning framework that enables reliable and interpretable hateful content detection. MARS begins with the objective description of video content, establishing a neutral foundation for subsequent analysis. Building on this, it develops evidence-based reasoning that supports potential hateful interpretations, while in parallel incorporating counter-evidence reasoning to capture plausible non-hateful perspectives. Finally, these perspectives are synthesized into a conclusive and explainable decision. Extensive evaluation on two real-world datasets shows that MARS achieves up to 10% improvement under certain backbones and settings compared to other training-free approaches and outperforms state-of-the-art training-based methods on one dataset. In addition, MARS produces human-understandable justifications, thereby supporting compliance oversight and enhancing the transparency of content moderation workflows. The code is available at https://github.com/Multimodal-Intelligence-Lab-MIL/MARS.",
        "url": "http://arxiv.org/abs/2601.15115v1",
        "published_date": "2026-01-21T15:52:26+00:00",
        "updated_date": "2026-01-21T15:52:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuonan Yang",
            "Yuchen Zhang",
            "Zeyu Fu"
        ],
        "tldr": "The paper introduces MARS, a training-free, interpretable framework for hateful video detection using multi-stage adversarial reasoning, outperforming existing training-free methods and achieving competitive results against training-based methods while providing human-understandable justifications.",
        "tldr_zh": "该论文介绍了一个名为MARS的无需训练、可解释的框架，用于检测恶意视频，它采用多阶段对抗推理，性能优于现有的无训练方法，并在提供人类可理解的理由的同时，与基于训练的方法相比也取得了有竞争力的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis",
        "summary": "This study investigates the feature representations produced by publicly available open source medical vision-language models (VLMs). While medical VLMs are expected to capture diagnostically relevant features, their learned representations remain underexplored, and standard evaluations like classification accuracy do not fully reveal if they acquire truly discriminative, lesion-specific features. Understanding these representations is crucial for revealing medical image structures and improving downstream tasks in medical image analysis. This study aims to investigate the feature distributions learned by medical VLMs and evaluate the impact of medical specialization. We analyze the feature distribution of multiple image modalities extracted by some representative medical VLMs across lesion classification datasets on multiple modalities. These distributions were compared them with non-medical VLMs to assess the domain-specific medical training. Our experiments showed that medical VLMs can extract discriminative features that are effective for medical classification tasks. Moreover, it was found that non-medical VLMs with recent improvement with contextual enrichment such as LLM2CLIP produce more refined feature representations. Our results imply that enhancing text encoder is more crucial than training intensively on medical images when developing medical VLMs. Notably, non-medical models are particularly vulnerable to biases introduced by overlaied text strings on images. These findings underscore the need for careful consideration on model selection according to downstream tasks besides potential risks in inference due to background biases such as textual information in images.",
        "url": "http://arxiv.org/abs/2601.14774v1",
        "published_date": "2026-01-21T08:53:40+00:00",
        "updated_date": "2026-01-21T08:53:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keita Takeda",
            "Tomoya Sakai"
        ],
        "tldr": "The paper analyzes feature representations of medical VLMs, comparing them to non-medical VLMs, and surprisingly finds that enhancing the text encoder might be more crucial than extensive medical image training for medical VLM development, while also highlighting biases in non-medical models due to text overlays.",
        "tldr_zh": "本文分析了医学视觉语言模型（VLM）的特征表示，并将其与非医学VLM进行了比较。研究发现，增强文本编码器可能比对医学图像进行大量训练更关键，同时强调了非医学模型由于文本覆盖而产生的偏差。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis",
        "summary": "The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.",
        "url": "http://arxiv.org/abs/2601.14637v1",
        "published_date": "2026-01-21T04:23:33+00:00",
        "updated_date": "2026-01-21T04:23:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.HC"
        ],
        "authors": [
            "James Brock",
            "Ce Zhang",
            "Nantheera Anantrasirichai"
        ],
        "tldr": "The paper introduces Forest-Chat, an LLM-driven agent for interactive forest change analysis using satellite imagery, along with a new dataset, Forest-Change, demonstrating its effectiveness in change detection and captioning tasks.",
        "tldr_zh": "该论文介绍了Forest-Chat，一个利用卫星图像进行交互式森林变化分析的LLM驱动代理，以及一个新的数据集Forest-Change，展示了其在变化检测和描述任务中的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]