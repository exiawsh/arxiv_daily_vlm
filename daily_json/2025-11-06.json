[
    {
        "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought",
        "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generating intermediate visual images is essential for successful\nreasoning. Unlike traditional CoT methods that rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such as\nsketches, structural diagrams, or path drawings - to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546 multimodal problems,\nannotated with intermediate visual images and final answers. We also propose a\nunified evaluation protocol for MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, and Visual-CoT input with both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also report pass@k and majority voting accuracies under\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA.",
        "url": "http://arxiv.org/abs/2511.02779v1",
        "published_date": "2025-11-04T18:00:51+00:00",
        "updated_date": "2025-11-04T18:00:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyang Zhou",
            "Haoqin Tu",
            "Zijun Wang",
            "Zeyu Wang",
            "Niklas Muennighoff",
            "Fan Nie",
            "Yejin Choi",
            "James Zou",
            "Chaorui Deng",
            "Shen Yan",
            "Haoqi Fan",
            "Cihang Xie",
            "Huaxiu Yao",
            "Qinghao Ye"
        ],
        "tldr": "The paper introduces MIRA, a new multimodal benchmark for evaluating models' reasoning capabilities by requiring the generation and utilization of intermediate visual images, demonstrating significant performance gains when visual cues are provided.",
        "tldr_zh": "该论文介绍了MIRA，一个新的多模态基准，通过要求模型生成和利用中间视觉图像来评估模型的推理能力，表明在提供视觉线索时性能显著提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation",
        "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
        "url": "http://arxiv.org/abs/2511.02778v1",
        "published_date": "2025-11-04T18:00:18+00:00",
        "updated_date": "2025-11-04T18:00:18+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Kevin Qinghong Lin",
            "Yuhao Zheng",
            "Hangyu Ran",
            "Dantong Zhu",
            "Dongxing Mao",
            "Linjie Li",
            "Philip Torr",
            "Alex Jinpeng Wang"
        ],
        "tldr": "The paper introduces VCode, a multimodal coding benchmark using SVG as a symbolic visual representation for VLMs, revealing limitations in visual-centric coding and proposing an agentic framework, VCoder, to improve performance.",
        "tldr_zh": "该论文介绍了 VCode，一个使用 SVG 作为符号视觉表示的多模态编码基准，用于评估视觉语言模型（VLMs）。该基准揭示了 VLM 在视觉中心编码方面的局限性，并提出了一个名为 VCoder 的代理框架来提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]