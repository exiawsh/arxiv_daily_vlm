[
    {
        "title": "Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models",
        "summary": "Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.",
        "url": "http://arxiv.org/abs/2601.23253v1",
        "published_date": "2026-01-30T18:21:45+00:00",
        "updated_date": "2026-01-30T18:21:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yi Zhang",
            "Chun-Wun Cheng",
            "Angelica I. Aviles-Rivero",
            "Zhihai He",
            "Liang-Jie Zhang"
        ],
        "tldr": "The paper introduces TaTa, a training-free test-time adaptation method for vision-language models that uses Brownian Distance Covariance to dynamically adapt to new domains, improving efficiency and stability while achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了一种名为TaTa的免训练测试时自适应方法，用于视觉语言模型。该方法利用布朗距离协方差动态适应新领域，提高效率和稳定性，并达到最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structured Over Scale: Learning Spatial Reasoning from Educational Video",
        "summary": "Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \\textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.",
        "url": "http://arxiv.org/abs/2601.23251v1",
        "published_date": "2026-01-30T18:20:23+00:00",
        "updated_date": "2026-01-30T18:20:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bishoy Galoaa",
            "Xiangyu Bai",
            "Sarah Ostadabbas"
        ],
        "tldr": "This paper introduces DoraVQA, a dataset derived from Dora the Explorer, and demonstrates that fine-tuning VLMs on structured educational video content significantly improves spatial reasoning and generalization capabilities, achieving state-of-the-art performance on CVBench.",
        "tldr_zh": "该论文介绍了DoraVQA数据集，该数据集来源于《爱探险的朵拉》，并证明了在结构化教育视频内容上微调VLM可以显著提高空间推理和泛化能力，并在CVBench上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments",
        "summary": "Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.",
        "url": "http://arxiv.org/abs/2601.23281v1",
        "published_date": "2026-01-30T18:55:38+00:00",
        "updated_date": "2026-01-30T18:55:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junfeng Lin",
            "Yanming Xiu",
            "Maria Gorlatova"
        ],
        "tldr": "This paper investigates the performance of open-set object detection models under different user prompting strategies in XR environments, finding that ambiguity in prompts degrades performance and that prompt enhancement can improve robustness.",
        "tldr_zh": "本文研究了开放集对象检测模型在XR环境中不同用户提示策略下的性能，发现提示中的歧义会降低性能，而提示增强可以提高鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
        "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.",
        "url": "http://arxiv.org/abs/2601.23265v1",
        "published_date": "2026-01-30T18:33:37+00:00",
        "updated_date": "2026-01-30T18:33:37+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Dawei Zhu",
            "Rui Meng",
            "Yale Song",
            "Xiyu Wei",
            "Sujian Li",
            "Tomas Pfister",
            "Jinsung Yoon"
        ],
        "tldr": "PaperBanana is presented as an agentic framework automating the generation of publication-ready academic illustrations, outperforming baselines in metrics like faithfulness and aesthetics, using a curated benchmark.",
        "tldr_zh": "PaperBanana是一个自动生成可用于出版的学术插图的agent框架。它在诸如保真度和美学等指标上优于基线，并使用了一个精心策划的基准。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search",
        "summary": "In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.",
        "url": "http://arxiv.org/abs/2601.23232v1",
        "published_date": "2026-01-30T18:01:17+00:00",
        "updated_date": "2026-01-30T18:01:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tao Yu",
            "Haopeng Jin",
            "Hao Wang",
            "Shenghua Chai",
            "Yujia Yang",
            "Junhao Gong",
            "Jiaming Guo",
            "Minghui Zhang",
            "Xinlong Chen",
            "Zhenghao Zhang",
            "Yuxuan Zhou",
            "Yanpei Gong",
            "YuanCheng Liu",
            "Yiming Ding",
            "Kangwei Zeng",
            "Pengfei Yang",
            "Zhongtian Luo",
            "Yufei Xiong",
            "Shanbin Zhang",
            "Shaoxiong Cheng",
            "Huang Ruilin",
            "Li Shuo",
            "Yuxi Niu",
            "Xinyuan Zhang",
            "Yueya Xu",
            "Jie Mao",
            "Ruixuan Ji",
            "Yaru Zhao",
            "Mingchen Zhang",
            "Jiabing Yang",
            "Jiaqi Liu",
            "YiFan Zhang",
            "Hongzhu Yi",
            "Xinming Wang",
            "Cheng Zhong",
            "Xiao Ma",
            "Zhang Zhang",
            "Yan Huang",
            "Liang Wang"
        ],
        "tldr": "The paper introduces ShotFinder, a new benchmark for open-domain video shot retrieval, and proposes a three-stage retrieval pipeline, revealing challenges for multimodal LLMs in this area, particularly with color and visual style constraints.",
        "tldr_zh": "该论文介绍了ShotFinder，这是一个用于开放域视频片段检索的新基准，并提出了一个三阶段的检索流程。研究结果表明，多模态LLM在该领域面临挑战，尤其是在颜色和视觉风格约束方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]