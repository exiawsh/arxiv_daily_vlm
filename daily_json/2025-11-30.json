[
    {
        "title": "Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models",
        "summary": "Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.",
        "url": "http://arxiv.org/abs/2511.23478v1",
        "published_date": "2025-11-28T18:59:58+00:00",
        "updated_date": "2025-11-28T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Maaz",
            "Hanoona Rasheed",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "tldr": "This paper introduces Video-R2, a reinforcement learning approach to improve the temporal precision and reasoning consistency of multimodal language models for video understanding, addressing issues of logical inconsistency and weak grounding in visual evidence.",
        "tldr_zh": "本文介绍了一种名为Video-R2的强化学习方法，旨在提高多模态语言模型在视频理解方面的时序精确性和推理一致性，解决了逻辑不一致和视觉证据弱 grounding 的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video-CoM: Interactive Video Reasoning via Chain of Manipulations",
        "summary": "Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still \"think about videos\" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to \"think with videos\". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM",
        "url": "http://arxiv.org/abs/2511.23477v1",
        "published_date": "2025-11-28T18:59:57+00:00",
        "updated_date": "2025-11-28T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanoona Rasheed",
            "Mohammed Zumri",
            "Muhammad Maaz",
            "Ming-Hsuan Yang",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "tldr": "The paper introduces Video-CoM, a framework that allows MLLMs to interactively reason about videos through iterative visual manipulations, guided by reasoning-aware reinforcement learning, achieving state-of-the-art results on multiple video reasoning benchmarks with relatively small training data.",
        "tldr_zh": "该论文介绍了Video-CoM，一个允许MLLM通过迭代视觉操作交互式地推理视频的框架，该框架由具有推理意识的强化学习引导，并在多个视频推理基准测试中取得了最先进的结果，且训练数据相对较小。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Generation Tuning",
        "summary": "Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
        "url": "http://arxiv.org/abs/2511.23469v1",
        "published_date": "2025-11-28T18:57:13+00:00",
        "updated_date": "2025-11-28T18:57:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Guo",
            "Sinan Du",
            "Jingfeng Yao",
            "Wenyu Liu",
            "Bo Li",
            "Haoxiang Cao",
            "Kun Gai",
            "Chun Yuan",
            "Kai Wu",
            "Xinggang Wang"
        ],
        "tldr": "The paper introduces Visual Generation Tuning (VGT), a method to enable visual generation capabilities in pre-trained Vision Language Models (VLMs) by efficiently aligning semantic encoders with pixel decoders, achieving state-of-the-art results in image reconstruction and generation.",
        "tldr_zh": "本文介绍了视觉生成调优（VGT），一种通过有效地将语义编码器与像素解码器对齐，使预训练的视觉语言模型（VLM）具备视觉生成能力的方法，并在图像重建和生成方面取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model",
        "summary": "Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as \"open the door\", \"draw a torch\", or \"trigger an explosion\".",
        "url": "http://arxiv.org/abs/2511.23429v1",
        "published_date": "2025-11-28T18:26:39+00:00",
        "updated_date": "2025-11-28T18:26:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junshu Tang",
            "Jiacheng Liu",
            "Jiaqi Li",
            "Longhuang Wu",
            "Haoyu Yang",
            "Penghao Zhao",
            "Siruis Gong",
            "Xiang Yuan",
            "Shuai Shao",
            "Qinglin Lu"
        ],
        "tldr": "Hunyuan-GameCraft-2 introduces a new instruction-driven paradigm for generative game world modeling, enabling flexible user interaction through natural language and demonstrating temporally coherent and causally grounded responses to diverse instructions.",
        "tldr_zh": "Hunyuan-GameCraft-2 引入了一种新的指令驱动的生成游戏世界建模范式，通过自然语言实现灵活的用户交互，并展示了对各种指令在时间上连贯和因果关系上合理的响应。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]