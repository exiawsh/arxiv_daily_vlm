[
    {
        "title": "Visual Representation Alignment for Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) trained with visual instruction\ntuning have achieved strong performance across diverse tasks, yet they remain\nlimited in vision-centric tasks such as object counting or spatial reasoning.\nWe attribute this gap to the prevailing text-only supervision paradigm, which\nprovides only indirect guidance for the visual pathway and often leads MLLMs to\ndiscard fine-grained visual details during training. In this paper, we present\nVIsual Representation ALignment (VIRAL), a simple yet effective regularization\nstrategy that aligns the internal visual representations of MLLMs with those of\npre-trained vision foundation models (VFMs). By explicitly enforcing this\nalignment, VIRAL enables the model not only to retain critical visual details\nfrom the input vision encoder but also to complement additional visual\nknowledge from VFMs, thereby enhancing its ability to reason over complex\nvisual inputs. Our experiments demonstrate consistent improvements across all\ntasks on widely adopted multimodal benchmarks. Furthermore, we conduct\ncomprehensive ablation studies to validate the key design choices underlying\nour framework. We believe this simple finding opens up an important direction\nfor the effective integration of visual information in training MLLMs.",
        "url": "http://arxiv.org/abs/2509.07979v1",
        "published_date": "2025-09-09T17:59:14+00:00",
        "updated_date": "2025-09-09T17:59:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Heeji Yoon",
            "Jaewoo Jung",
            "Junwan Kim",
            "Hyungyu Choi",
            "Heeseong Shin",
            "Sangbeom Lim",
            "Honggyu An",
            "Chaehyun Kim",
            "Jisang Han",
            "Donghyun Kim",
            "Chanho Eom",
            "Sunghwan Hong",
            "Seungryong Kim"
        ],
        "tldr": "The paper introduces VIRAL, a regularization strategy to align visual representations in MLLMs with pre-trained vision foundation models (VFMs), improving performance on vision-centric tasks by retaining and complementing visual details.",
        "tldr_zh": "该论文介绍了VIRAL，一种将多模态大型语言模型（MLLM）中的视觉表征与预训练的视觉基础模型（VFM）对齐的正则化策略，通过保留和补充视觉细节，从而提高模型在以视觉为中心任务上的表现。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search",
        "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.",
        "url": "http://arxiv.org/abs/2509.07969v1",
        "published_date": "2025-09-09T17:54:21+00:00",
        "updated_date": "2025-09-09T17:54:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Xin Lai",
            "Junyi Li",
            "Wei Li",
            "Tao Liu",
            "Tianjian Li",
            "Hengshuang Zhao"
        ],
        "tldr": "The paper introduces Mini-o3, a system for visual search that uses a novel dataset, data collection pipeline, and training strategy to enable deep, multi-turn reasoning, achieving state-of-the-art performance. It demonstrates improved reasoning patterns and scalability compared to existing open-source approaches.",
        "tldr_zh": "该论文介绍了Mini-o3，一个用于视觉搜索的系统，它使用新的数据集、数据收集流程和训练策略来实现深度、多轮推理，并取得最先进的性能。它展示了相比现有开源方法更好的推理模式和可扩展性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
        "summary": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA.",
        "url": "http://arxiv.org/abs/2509.07966v1",
        "published_date": "2025-09-09T17:52:26+00:00",
        "updated_date": "2025-09-09T17:52:26+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Boammani Aser Lompo",
            "Marc Haraoui"
        ],
        "tldr": "The paper introduces Visual-TableQA, a large-scale, open-domain dataset for visual reasoning over table images, generated via collaborative LLMs, and demonstrates its effectiveness in improving model generalization.",
        "tldr_zh": "本文介绍了一个名为Visual-TableQA的大规模开放领域数据集，用于表格图像的视觉推理。该数据集通过协作式LLM生成，并证明其能有效提高模型的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics",
        "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks\nlike image captioning and visual question answering, but remain prone to\nhallucinations, where generated text conflicts with the visual input. Prior\nwork links this partly to insufficient visual attention, but existing\nattention-based detectors and mitigation typically apply uniform adjustments\nacross layers and heads, obscuring where errors originate. In this paper, we\nfirst show these methods fail to accurately localize problematic layers. Then,\nwe introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags\nanomalous layers, and Image Attention Focus (IAF) which scores attention heads\nwithin those layers. Analysis shows that LIAE pinpoints faulty layers and IAF\nreliably ranks heads that warrant correction. Guided by these signals, we\npropose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a\ntask-agnostic, attention-guided method that dynamically localizes and corrects\nerrors during inference with negligible overhead. Results show our D-LEAF\ndelivers a 53% relative improvement on standard captioning benchmarks, and on\nVQA both accuracy and F1-score improve by approximately 4%, substantially\nsuppressing hallucinations while preserving efficiency.",
        "url": "http://arxiv.org/abs/2509.07864v1",
        "published_date": "2025-09-09T15:51:15+00:00",
        "updated_date": "2025-09-09T15:51:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tiancheng Yang",
            "Lin Zhang",
            "Jiaye Lin",
            "Guimin Hu",
            "Di Wang",
            "Lijie Hu"
        ],
        "tldr": "The paper introduces D-LEAF, a method to localize and correct hallucinations in Multimodal LLMs by identifying problematic layers and heads using Layer Image Attention Entropy (LIAE) and Image Attention Focus (IAF). It demonstrates significant improvements in captioning and VQA tasks.",
        "tldr_zh": "该论文介绍了D-LEAF，一种通过使用层图像注意力熵（LIAE）和图像注意力焦点（IAF）来识别有问题层和头，从而定位和纠正多模态LLM中幻觉的方法。它在图像描述和VQA任务中展示了显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model",
        "summary": "3D object segmentation with Large Language Models (LLMs) has become a\nprevailing paradigm due to its broad semantics, task flexibility, and strong\ngeneralization. However, this paradigm is hindered by representation\nmisalignment: LLMs process high-level semantic tokens, whereas 3D point clouds\nconvey only dense geometric structures. In prior methods, misalignment limits\nboth input and output. At the input stage, dense point patches require heavy\npre-alignment, weakening object-level semantics and confusing similar\ndistractors. At the output stage, predictions depend only on dense features\nwithout explicit geometric cues, leading to a loss of fine-grained accuracy. To\naddress these limitations, we present the Point Linguist Model (PLM), a general\nframework that bridges the representation gap between LLMs and dense 3D point\nclouds without requiring large-scale pre-alignment between 3D-text or\n3D-images. Specifically, we introduce Object-centric Discriminative\nRepresentation (OcDR), which learns object-centric tokens that capture target\nsemantics and scene relations under a hard negative-aware training objective.\nThis mitigates the misalignment between LLM tokens and 3D points, enhances\nresilience to distractors, and facilitates semantic-level reasoning within\nLLMs. For accurate segmentation, we introduce the Geometric Reactivation\nDecoder (GRD), which predicts masks by combining OcDR tokens carrying\nLLM-inferred geometry with corresponding dense features, preserving\ncomprehensive dense features throughout the pipeline. Extensive experiments\nshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and\n+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains\nacross 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness\nof comprehensive object-centric reasoning for robust 3D understanding.",
        "url": "http://arxiv.org/abs/2509.07825v1",
        "published_date": "2025-09-09T15:01:28+00:00",
        "updated_date": "2025-09-09T15:01:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuoxu Huang",
            "Mingqi Gao",
            "Jungong Han"
        ],
        "tldr": "The paper introduces the Point Linguist Model (PLM) to bridge the gap between LLMs and 3D point clouds for object segmentation, using Object-centric Discriminative Representation (OcDR) and Geometric Reactivation Decoder (GRD) to achieve significant improvements in 3D referring segmentation.",
        "tldr_zh": "该论文介绍了点语言学家模型（PLM），旨在弥合LLM和3D点云之间的差距，用于对象分割。它使用对象中心区分表示（OcDR）和几何重激活解码器（GRD），在3D指代分割方面取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TextlessRAG: End-to-End Visual Document RAG by Speech Without Text",
        "summary": "Document images encapsulate a wealth of knowledge, while the portability of\nspoken queries enables broader and flexible application scenarios. Yet, no\nprior work has explored knowledge base question answering over visual document\nimages with queries provided directly in speech. We propose TextlessRAG, the\nfirst end-to-end framework for speech-based question answering over large-scale\ndocument images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR,\ndirectly interpreting speech, retrieving relevant visual knowledge, and\ngenerating answers in a fully textless pipeline. To further boost performance,\nwe integrate a layout-aware reranking mechanism to refine retrieval.\nExperiments demonstrate substantial improvements in both efficiency and\naccuracy. To advance research in this direction, we also release the first\nbilingual speech--document RAG dataset, featuring Chinese and English voice\nqueries paired with multimodal document content. Both the dataset and our\npipeline will be made available at\nrepository:https://github.com/xiepeijinhit-hue/textlessrag",
        "url": "http://arxiv.org/abs/2509.07538v1",
        "published_date": "2025-09-09T09:16:25+00:00",
        "updated_date": "2025-09-09T09:16:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peijin Xie",
            "Shun Qian",
            "Bingquan Liu",
            "Dexin Wang",
            "Lin Sun",
            "Xiangzheng Zhang"
        ],
        "tldr": "TextlessRAG is a novel end-to-end framework for speech-based question answering over document images, eliminating the need for ASR, TTS, and OCR by directly processing speech and retrieving visual knowledge. The authors also contribute a bilingual speech-document RAG dataset.",
        "tldr_zh": "TextlessRAG是一个新颖的端到端框架，用于基于语音的文档图像问答，无需ASR、TTS和OCR，而是直接处理语音并检索视觉知识。作者还贡献了一个双语语音-文档RAG数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting",
        "summary": "The emergence of advanced multimodal large language models (MLLMs) has\nsignificantly enhanced AI assistants' ability to process complex information\nacross modalities. Recently, egocentric videos, by directly capturing user\nfocus, actions, and context in an unified coordinate, offer an exciting\nopportunity to enable proactive and personalized AI user experiences with\nMLLMs. However, existing benchmarks overlook the crucial role of gaze as an\nindicator of user intent. To address this gap, we introduce EgoGazeVQA, an\negocentric gaze-guided video question answering benchmark that leverages gaze\ninformation to improve the understanding of longer daily-life videos.\nEgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by\nhuman annotators. Our experiments reveal that existing MLLMs struggle to\naccurately interpret user intentions. In contrast, our gaze-guided intent\nprompting methods significantly enhance performance by integrating spatial,\ntemporal, and intent-related cues. We further conduct experiments on\ngaze-related fine-tuning and analyze how gaze estimation accuracy impacts\nprompting effectiveness. These results underscore the value of gaze for more\npersonalized and effective AI assistants in egocentric settings.",
        "url": "http://arxiv.org/abs/2509.07447v1",
        "published_date": "2025-09-09T07:11:56+00:00",
        "updated_date": "2025-09-09T07:11:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taiying Peng",
            "Jiacheng Hua",
            "Miao Liu",
            "Feng Lu"
        ],
        "tldr": "This paper introduces EgoGazeVQA, a new benchmark for egocentric video question answering that uses gaze information to improve intent understanding in MLLMs, showing that existing MLLMs struggle with this task but can be significantly improved with gaze-guided prompting.",
        "tldr_zh": "本文介绍EgoGazeVQA，一个新的以自我为中心的视频问答基准，它使用注视信息来提高MLLM中对意图的理解。研究表明，现有的MLLM难以胜任此任务，但可以通过注视引导的提示得到显著改善。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reconstruction Alignment Improves Unified Multimodal Models",
        "summary": "Unified multimodal models (UMMs) unify visual understanding and generation\nwithin a single architecture. However, conventional training relies on\nimage-text pairs (or sequences) whose captions are typically sparse and miss\nfine-grained visual details--even when they use hundreds of words to describe a\nsimple image. We introduce Reconstruction Alignment (RecA), a\nresource-efficient post-training method that leverages visual understanding\nencoder embeddings as dense \"text prompts,\" providing rich supervision without\ncaptions. Concretely, RecA conditions a UMM on its own visual understanding\nembeddings and optimizes it to reconstruct the input image with a\nself-supervised reconstruction loss, thereby realigning understanding and\ngeneration. Despite its simplicity, RecA is broadly applicable: across\nautoregressive, masked-autoregressive, and diffusion-based UMMs, it\nconsistently improves generation and editing fidelity. With only 27 GPU-hours,\npost-training with RecA substantially improves image generation performance on\nGenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while\nalso boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit\n6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models\nand applies broadly across diverse UMM architectures, establishing it as an\nefficient and general post-training alignment strategy for UMMs",
        "url": "http://arxiv.org/abs/2509.07295v1",
        "published_date": "2025-09-08T23:59:32+00:00",
        "updated_date": "2025-09-08T23:59:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ji Xie",
            "Trevor Darrell",
            "Luke Zettlemoyer",
            "XuDong Wang"
        ],
        "tldr": "The paper introduces Reconstruction Alignment (RecA), a resource-efficient post-training method that uses visual embeddings as dense text prompts to improve the alignment between visual understanding and generation in unified multimodal models (UMMs). RecA shows significant improvements in image generation and editing across different UMM architectures with minimal computational cost.",
        "tldr_zh": "该论文介绍了一种名为重建对齐 (RecA) 的资源高效的后训练方法，该方法使用视觉嵌入作为密集文本提示，以提高统一多模态模型 (UMM) 中视觉理解和生成之间的对齐。RecA 在不同 UMM 架构的图像生成和编辑方面表现出显著改进，且计算成本极低。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis",
        "summary": "Ensuring reliable robot operation when visual input is degraded or\ninsufficient remains a central challenge in robotics. This letter introduces\nDepthVision, a framework for multimodal scene understanding designed to address\nthis problem. Unlike existing Vision-Language Models (VLMs), which use only\ncamera-based visual input alongside language, DepthVision synthesizes RGB\nimages from sparse LiDAR point clouds using a conditional generative\nadversarial network (GAN) with an integrated refiner network. These synthetic\nviews are then combined with real RGB data using a Luminance-Aware Modality\nAdaptation (LAMA), which blends the two types of data dynamically based on\nambient lighting conditions. This approach compensates for sensor degradation,\nsuch as darkness or motion blur, without requiring any fine-tuning of\ndownstream vision-language models. We evaluate DepthVision on real and\nsimulated datasets across various models and tasks, with particular attention\nto safety-critical tasks. The results demonstrate that our approach improves\nperformance in low-light conditions, achieving substantial gains over RGB-only\nbaselines while preserving compatibility with frozen VLMs. This work highlights\nthe potential of LiDAR-guided RGB synthesis for achieving robust robot\noperation in real-world environments.",
        "url": "http://arxiv.org/abs/2509.07463v1",
        "published_date": "2025-09-09T07:42:07+00:00",
        "updated_date": "2025-09-09T07:42:07+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Sven Kirchner",
            "Nils Purschke",
            "Ross Greer",
            "Alois C. Knoll"
        ],
        "tldr": "The paper introduces DepthVision, a framework that synthesizes RGB images from LiDAR data using a GAN to enhance VLM performance in degraded visual conditions without fine-tuning downstream models.",
        "tldr_zh": "该论文介绍了一种名为DepthVision的框架，它使用GAN从LiDAR数据合成RGB图像，以增强在视觉条件退化的情况下VLM的性能，而无需对下游模型进行微调。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning",
        "summary": "Background: Precise breast ultrasound (BUS) segmentation supports reliable\nmeasurement, quantitative analysis, and downstream classification, yet remains\ndifficult for small or low-contrast lesions with fuzzy margins and speckle\nnoise. Text prompts can add clinical context, but directly applying weakly\nlocalized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce\ncoarse, blob-like responses that smear boundaries unless additional mechanisms\nrecover fine edges. Methods: We propose XBusNet, a novel dual-prompt,\ndual-branch multimodal model that combines image features with clinically\ngrounded text. A global pathway based on a CLIP Vision Transformer encodes\nwhole-image semantics conditioned on lesion size and location, while a local\nU-Net pathway emphasizes precise boundaries and is modulated by prompts that\ndescribe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS)\nterms. Prompts are assembled automatically from structured metadata, requiring\nno manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using\nfive-fold cross-validation. Primary metrics are Dice and Intersection over\nUnion (IoU); we also conduct size-stratified analyses and ablations to assess\nthe roles of the global and local paths and the text-driven modulation.\nResults: XBusNet achieves state-of-the-art performance on BLU, with mean Dice\nof 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions\nshow the largest gains, with fewer missed regions and fewer spurious\nactivations. Ablation studies show complementary contributions of global\ncontext, local boundary modeling, and prompt-based modulation. Conclusions: A\ndual-prompt, dual-branch multimodal design that merges global semantics with\nlocal precision yields accurate BUS segmentation masks and improves robustness\nfor small, low-contrast lesions.",
        "url": "http://arxiv.org/abs/2509.07213v1",
        "published_date": "2025-09-08T20:45:55+00:00",
        "updated_date": "2025-09-08T20:45:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Raja Mallina",
            "Bryar Shareef"
        ],
        "tldr": "This paper introduces XBusNet, a dual-prompt, dual-branch multimodal model for breast ultrasound segmentation that leverages both global image semantics and local boundary information guided by clinically grounded text prompts. It achieves state-of-the-art performance on the BLU dataset, especially for small lesions.",
        "tldr_zh": "该论文介绍了XBusNet，一种双提示、双分支多模态模型，用于乳腺超声分割，利用全局图像语义和局部边界信息，并以临床文本提示为指导。它在BLU数据集上实现了最先进的性能，尤其是在小病灶方面。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]