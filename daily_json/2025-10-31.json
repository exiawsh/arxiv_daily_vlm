[
    {
        "title": "GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?",
        "summary": "Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.",
        "url": "http://arxiv.org/abs/2510.26339v1",
        "published_date": "2025-10-30T10:46:28+00:00",
        "updated_date": "2025-10-30T10:46:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingyu Sung",
            "Seungjae Ham",
            "Kangwoo Kim",
            "Yeokyoung Yoon",
            "Sangseok Yun",
            "Il-Min Kim",
            "Jae-Mo Kang"
        ],
        "tldr": "GLYPH-SR, a vision-language-guided diffusion framework, is presented to improve both text legibility and perceptual quality in image super-resolution, particularly for scene text, by using a Text-SR Fusion ControlNet and a ping-pong scheduler.",
        "tldr_zh": "GLYPH-SR是一个视觉语言引导的扩散框架，通过使用文本超分辨率融合控制网络和乒乓调度器，旨在提高图像超分辨率中，尤其是场景文本的文本可读性和感知质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Masked Diffusion Captioning for Visual Feature Learning",
        "summary": "We learn visual features by captioning images with an image-conditioned\nmasked diffusion language model, a formulation we call masked diffusion\ncaptioning (MDC). During training, text tokens in each image-caption pair are\nmasked at a randomly chosen ratio, and a decoder conditioned on visual features\nis trained to reconstruct the original text. After training, the learned visual\nfeatures can be applied to downstream vision tasks. Unlike autoregressive\ncaptioning, the strength of the visual learning signal in MDC does not depend\non each token's position in the sequence, reducing the need for auxiliary\nobjectives. Linear probing experiments across a variety of academic-scale\nmodels and datasets show that the learned visual features are competitive with\nthose produced by autoregressive and contrastive approaches.",
        "url": "http://arxiv.org/abs/2510.26799v1",
        "published_date": "2025-10-30T17:59:46+00:00",
        "updated_date": "2025-10-30T17:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chao Feng",
            "Zihao Wei",
            "Andrew Owens"
        ],
        "tldr": "The paper introduces Masked Diffusion Captioning (MDC), a method to learn visual features by training a masked diffusion language model to reconstruct masked captions conditioned on images, achieving competitive performance on downstream vision tasks compared to autoregressive and contrastive methods.",
        "tldr_zh": "该论文介绍了一种名为Masked Diffusion Captioning (MDC)的方法，通过训练一个图像条件下的掩码扩散语言模型来重建被掩盖的图像描述，从而学习视觉特征。实验表明，该方法在下游视觉任务上与自回归和对比学习方法相比具有竞争力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment",
        "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.",
        "url": "http://arxiv.org/abs/2510.26781v1",
        "published_date": "2025-10-30T17:56:31+00:00",
        "updated_date": "2025-10-30T17:56:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aniruddh Bansal",
            "Davit Soselia",
            "Dang Nguyen",
            "Tianyi Zhou"
        ],
        "tldr": "The paper introduces ChartAB, a new benchmark for evaluating vision-language models (VLMs) on chart grounding tasks, highlighting their limitations in fine-grained detail extraction and cross-chart reasoning.",
        "tldr_zh": "该论文介绍了一个新的基准测试 ChartAB，用于评估视觉语言模型 (VLM) 在图表基础任务上的表现，突出了它们在细粒度细节提取和跨图表推理方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models",
        "summary": "This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.",
        "url": "http://arxiv.org/abs/2510.26769v1",
        "published_date": "2025-10-30T17:52:39+00:00",
        "updated_date": "2025-10-30T17:52:39+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Anushka Sivakumar",
            "Andrew Zhang",
            "Zaber Hakim",
            "Chris Thomas"
        ],
        "tldr": "SteerVLM introduces a lightweight steering module to control VLMs at inference time by dynamically adjusting activations based on learned latent embeddings, improving instruction following and hallucination mitigation without modifying model weights.",
        "tldr_zh": "SteerVLM 引入了一个轻量级的转向模块，通过基于学习的潜在嵌入动态调整激活，在推理时控制视觉语言模型，从而改进指令遵循和减少幻觉，而无需修改模型权重。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
        "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.",
        "url": "http://arxiv.org/abs/2510.26641v1",
        "published_date": "2025-10-30T16:08:25+00:00",
        "updated_date": "2025-10-30T16:08:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sayed Pedram Haeri Boroujeni",
            "Niloufar Mehrabi",
            "Hazim Alzorgan",
            "Ahmad Sarlak",
            "Mahlagha Fazeli",
            "Abolfazl Razi"
        ],
        "tldr": "This survey paper reviews object detection in autonomous vehicles, focusing on the integration of VLMs, LLMs, and generative AI, and analyzes sensor fusion, datasets, and detection methodologies, providing a roadmap for future research.",
        "tldr_zh": "该综述论文回顾了自动驾驶车辆中的物体检测，重点关注VLM、LLM和生成式人工智能的集成，并分析了传感器融合、数据集和检测方法，为未来研究提供了路线图。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Emu3.5: Native Multimodal Models are World Learners",
        "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
        "url": "http://arxiv.org/abs/2510.26583v1",
        "published_date": "2025-10-30T15:11:16+00:00",
        "updated_date": "2025-10-30T15:11:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yufeng Cui",
            "Honghao Chen",
            "Haoge Deng",
            "Xu Huang",
            "Xinghang Li",
            "Jirong Liu",
            "Yang Liu",
            "Zhuoyan Luo",
            "Jinsheng Wang",
            "Wenxuan Wang",
            "Yueze Wang",
            "Chengyuan Wang",
            "Fan Zhang",
            "Yingli Zhao",
            "Ting Pan",
            "Xianduo Li",
            "Zecheng Hao",
            "Wenxuan Ma",
            "Zhuo Chen",
            "Yulong Ao",
            "Tiejun Huang",
            "Zhongyuan Wang",
            "Xinlong Wang"
        ],
        "tldr": "Emu3.5 is a large-scale multimodal world model pretrained on a massive vision-language dataset, demonstrating strong multimodal capabilities and world-modeling abilities with efficient inference via Discrete Diffusion Adaptation.",
        "tldr_zh": "Emu3.5 是一个大规模多模态世界模型，基于海量视觉-语言数据集进行预训练，展示了强大的多模态能力和世界建模能力，并通过离散扩散自适应实现高效推理。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "CATCH: A Modular Cross-domain Adaptive Template with Hook",
        "summary": "Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.",
        "url": "http://arxiv.org/abs/2510.26582v1",
        "published_date": "2025-10-30T15:10:02+00:00",
        "updated_date": "2025-10-30T15:10:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinjin Li",
            "Yulie Lu",
            "Jinghan Cao",
            "Yu Ma",
            "Zhenglin Li",
            "Yeyang Zhou"
        ],
        "tldr": "The paper introduces CATCH, a plug-and-play framework for cross-domain VQA that leverages domain classification and dual adapters (prompt and visual) injected via a unified hook interface to improve generalization without backbone retraining.",
        "tldr_zh": "该论文介绍了CATCH，一个即插即用的跨域VQA框架，它利用域分类和双适配器（提示和视觉），通过统一的钩子接口注入，以提高泛化能力，而无需重新训练骨干网络。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios",
        "summary": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.",
        "url": "http://arxiv.org/abs/2510.26580v1",
        "published_date": "2025-10-30T15:07:55+00:00",
        "updated_date": "2025-10-30T15:07:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manjunath Prasad Holenarasipura Rajiv",
            "B. M. Vidyavathi"
        ],
        "tldr": "The paper proposes a dynamic context-aware scene reasoning framework using vision-language alignment for zero-shot real-world scenarios, achieving up to 18% improvement in scene understanding accuracy on benchmark datasets.",
        "tldr_zh": "该论文提出了一种动态上下文感知场景推理框架，利用视觉-语言对齐来处理零样本真实世界场景，并在基准数据集上实现了高达18%的场景理解精度提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing",
        "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
        "url": "http://arxiv.org/abs/2510.26474v1",
        "published_date": "2025-10-30T13:26:58+00:00",
        "updated_date": "2025-10-30T13:26:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xin Guo",
            "Zhiheng Xi",
            "Yiwen Ding",
            "Yitao Zhai",
            "Xiaowei Shi",
            "Xunliang Cai",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "tldr": "The paper addresses the \"Matthew effect\" in self-improving LVLMs, where performance plateaus due to imbalance between learning from simple and complex data, and proposes re-balancing strategies to overcome this limitation, achieving performance gains.",
        "tldr_zh": "该论文解决了自提升LVLM中的“马太效应”问题，即由于从简单和复杂数据中学习的不平衡导致性能停滞，并提出了重新平衡策略来克服这一限制，从而实现性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition",
        "summary": "Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.",
        "url": "http://arxiv.org/abs/2510.26466v1",
        "published_date": "2025-10-30T13:11:23+00:00",
        "updated_date": "2025-10-30T13:11:23+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Pei Peng",
            "MingKun Xie",
            "Hang Hao",
            "Tong Jin",
            "ShengJun Huang"
        ],
        "tldr": "This paper introduces a representation-level counterfactual calibration method to debias zero-shot vision-language models by mitigating object-context shortcuts, achieving state-of-the-art performance on context-sensitive benchmarks without retraining or prompt engineering.",
        "tldr_zh": "本文提出了一种表示层面的反事实校准方法，通过减少对象-上下文的捷径来消除零样本视觉语言模型的偏差，在上下文敏感的基准测试中实现了最先进的性能，而无需重新训练或提示工程。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models",
        "summary": "Test-time prompt tuning (TPT) has emerged as a promising technique for\nadapting large vision-language models (VLMs) to unseen tasks without relying on\nlabeled data. However, the lack of dispersion between textual features can hurt\ncalibration performance, which raises concerns about VLMs' reliability,\ntrustworthiness, and safety. Current TPT approaches primarily focus on\nimproving prompt calibration by either maximizing average textual feature\ndispersion or enforcing orthogonality constraints to encourage angular\nseparation. However, these methods may not always have optimal angular\nseparation between class-wise textual features, which implies overlooking the\ncritical role of angular diversity. To address this, we propose A-TPT, a novel\nTPT framework that introduces angular diversity to encourage uniformity in the\ndistribution of normalized textual features induced by corresponding learnable\nprompts. This uniformity is achieved by maximizing the minimum pairwise angular\ndistance between features on the unit hypersphere. We show that our approach\nconsistently surpasses state-of-the-art TPT methods in reducing the aggregate\naverage calibration error while maintaining comparable accuracy through\nextensive experiments with various backbones on different datasets. Notably,\nour approach exhibits superior zero-shot calibration performance on natural\ndistribution shifts and generalizes well to medical datasets. We provide\nextensive analyses, including theoretical aspects, to establish the grounding\nof A-TPT. These results highlight the potency of promoting angular diversity to\nachieve well-dispersed textual features, significantly improving VLM\ncalibration during test-time adaptation. Our code will be made publicly\navailable.",
        "url": "http://arxiv.org/abs/2510.26441v1",
        "published_date": "2025-10-30T12:45:24+00:00",
        "updated_date": "2025-10-30T12:45:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shihab Aaqil Ahamed",
            "Udaya S. K. P. Miriya Thanthrige",
            "Ranga Rodrigo",
            "Muhammad Haris Khan"
        ],
        "tldr": "The paper introduces A-TPT, a novel test-time prompt tuning framework for VLMs that improves calibration by maximizing the minimum pairwise angular distance between textual features, leading to superior calibration error reduction and generalization.",
        "tldr_zh": "该论文介绍了A-TPT，一种用于VLM的新型测试时提示调优框架，通过最大化文本特征之间的最小成对角度距离来提高校准性能，从而显著降低校准误差并提高泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction",
        "summary": "Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.",
        "url": "http://arxiv.org/abs/2510.26151v1",
        "published_date": "2025-10-30T05:12:29+00:00",
        "updated_date": "2025-10-30T05:12:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shunjie-Fabian Zheng",
            "Hyeonjun Lee",
            "Thijs Kooi",
            "Ali Diba"
        ],
        "tldr": "The paper introduces MV-MLM, a novel multi-view mammography and language model trained on mammogram images and synthetic reports, achieving state-of-the-art performance in breast cancer classification and risk prediction while demonstrating strong data efficiency.",
        "tldr_zh": "该论文介绍了 MV-MLM，一种新颖的多视角乳腺钼靶和语言模型，该模型在乳房X光图像和合成报告上进行训练，在乳腺癌分类和风险预测方面取得了最先进的性能，同时展示了强大的数据效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoExo-Con: Exploring View-Invariant Video Temporal Understanding",
        "summary": "Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.26113v1",
        "published_date": "2025-10-30T03:53:22+00:00",
        "updated_date": "2025-10-30T03:53:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Minjoon Jung",
            "Junbin Xiao",
            "Junghyun Kim",
            "Byoung-Tak Zhang",
            "Angela Yao"
        ],
        "tldr": "This paper introduces EgoExo-Con, a benchmark for evaluating the view-consistency of Video-LLMs in temporal understanding, and proposes View-GRPO, a reinforcement learning framework to improve consistency.",
        "tldr_zh": "该论文介绍了EgoExo-Con，一个用于评估视频LLM在时间理解中视角一致性的基准，并提出了View-GRPO，一个用于提高一致性的强化学习框架。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic VLM-Guided Negative Prompting for Diffusion Models",
        "summary": "We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.",
        "url": "http://arxiv.org/abs/2510.26052v1",
        "published_date": "2025-10-30T01:10:25+00:00",
        "updated_date": "2025-10-30T01:10:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hoyeon Chang",
            "Seungjin Kim",
            "Yoonseok Choi"
        ],
        "tldr": "This paper introduces a dynamic negative prompting method for diffusion models, using VLMs to generate contextually relevant negative prompts during the denoising process, offering improvements over static negative prompting.",
        "tldr_zh": "该论文提出了一种用于扩散模型的动态负面提示方法，利用视觉语言模型在去噪过程中生成与上下文相关的负面提示，从而改进了静态负面提示。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders",
        "summary": "Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.",
        "url": "http://arxiv.org/abs/2510.26027v1",
        "published_date": "2025-10-29T23:50:57+00:00",
        "updated_date": "2025-10-29T23:50:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ali Rasekh",
            "Erfan Bagheri Soula",
            "Omid Daliran",
            "Simon Gottschalk",
            "Mohsen Fayyaz"
        ],
        "tldr": "The paper introduces a Video-LLM architecture with stacked temporal attention modules in the vision encoder to improve temporal understanding in videos, demonstrating significant performance gains on video question answering tasks.",
        "tldr_zh": "该论文提出了一种Video-LLM架构，在视觉编码器中引入堆叠的时间注意力模块，以提高视频中的时间理解能力，并在视频问答任务中展示了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments",
        "summary": "Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.",
        "url": "http://arxiv.org/abs/2510.26006v1",
        "published_date": "2025-10-29T22:34:26+00:00",
        "updated_date": "2025-10-29T22:34:26+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Rishika Bhagwatkar",
            "Syrielle Montariol",
            "Angelika Romanou",
            "Beatriz Borges",
            "Irina Rish",
            "Antoine Bosselut"
        ],
        "tldr": "The paper introduces CAVE, a novel benchmark for real-world visual anomaly detection, description, explanation, and justification, and demonstrates the limitations of current VLMs in this domain.",
        "tldr_zh": "该论文介绍了CAVE，这是一个用于真实世界视觉异常检测、描述、解释和辩护的新基准，并证明了当前VLM在该领域的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency",
        "summary": "Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).",
        "url": "http://arxiv.org/abs/2510.25897v1",
        "published_date": "2025-10-29T18:59:17+00:00",
        "updated_date": "2025-10-29T18:59:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nicolas Dufour",
            "Lucas Degeorge",
            "Arijit Ghosh",
            "Vicky Kalogeiton",
            "David Picard"
        ],
        "tldr": "The paper introduces MIRO, a method that conditions text-to-image models on multiple reward models during training, improving image quality, training efficiency, and achieving state-of-the-art performance on compositional benchmarks and user preference scores.",
        "tldr_zh": "该论文介绍了MIRO，一种在训练过程中将文本到图像模型与多个奖励模型结合的方法，从而提高图像质量、训练效率，并在组合基准测试和用户偏好得分上达到最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection",
        "summary": "Few-shot anomaly detection (FSAD) methods identify anomalous regions with few\nknown normal samples. Most existing methods rely on the generalization ability\nof pre-trained vision-language models (VLMs) to recognize potentially anomalous\nregions through feature similarity between text descriptions and images.\nHowever, due to the lack of detailed textual descriptions, these methods can\nonly pre-define image-level descriptions to match each visual patch token to\nidentify potential anomalous regions, which leads to the semantic misalignment\nbetween image descriptions and patch-level visual anomalies, achieving\nsub-optimal localization performance. To address the above issues, we propose\nthe Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and\nfine-grained textual descriptions for existing anomaly detection datasets with\nautomatic construction pipeline. Based on the MFSC, we propose a novel\nframework named FineGrainedAD to improve anomaly localization performance,\nwhich consists of two components: Multi-Level Learnable Prompt (MLLP) and\nMulti-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics\ninto multi-level learnable prompts through automatic replacement and\nconcatenation mechanism, while MLSA designs region aggregation strategy and\nmulti-level alignment training to facilitate learnable prompts better align\nwith corresponding visual regions. Experiments demonstrate that the proposed\nFineGrainedAD achieves superior overall performance in few-shot settings on\nMVTec-AD and VisA datasets.",
        "url": "http://arxiv.org/abs/2510.26464v1",
        "published_date": "2025-10-30T13:09:00+00:00",
        "updated_date": "2025-10-30T13:09:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanting Fan",
            "Jun Liu",
            "Xiaochen Chen",
            "Bin-Bin Gao",
            "Jian Li",
            "Yong Liu",
            "Jinlong Peng",
            "Chengjie Wang"
        ],
        "tldr": "The paper introduces a novel framework, FineGrainedAD, for few-shot anomaly detection using multi-level fine-grained semantic captions to address semantic misalignment issues in existing vision-language models, showing superior performance on anomaly detection datasets.",
        "tldr_zh": "该论文介绍了一种名为FineGrainedAD的新框架，用于少样本异常检测，该框架使用多层次的细粒度语义标题来解决现有视觉-语言模型中的语义错位问题，并在异常检测数据集上表现出卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models",
        "summary": "Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.",
        "url": "http://arxiv.org/abs/2510.26241v1",
        "published_date": "2025-10-30T08:21:50+00:00",
        "updated_date": "2025-10-30T08:21:50+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Shiho Matta",
            "Lis Kanashiro Pereira",
            "Peitao Han",
            "Fei Cheng",
            "Shigeru Kitazawa"
        ],
        "tldr": "This paper introduces AoT-PsyPhyBENCH, a psychophysically validated benchmark, to evaluate VLMs' ability to understand temporal direction in videos, finding that current models perform poorly compared to humans.",
        "tldr_zh": "本文介绍了 AoT-PsyPhyBENCH，一个经过心理物理学验证的基准，用于评估 VLM 理解视频中时间方向的能力，发现当前模型与人类相比表现不佳。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research",
        "summary": "As one of the earliest writing systems, Oracle Bone Script (OBS) preserves\nthe cultural and intellectual heritage of ancient civilizations. However,\ncurrent OBS research faces two major challenges: (1) the interpretation of OBS\ninvolves a complex workflow comprising multiple serial and parallel sub-tasks,\nand (2) the efficiency of OBS information organization and retrieval remains a\ncritical bottleneck, as scholars often spend substantial effort searching for,\ncompiling, and managing relevant resources. To address these challenges, we\npresent OracleAgent, the first agent system designed for the structured\nmanagement and retrieval of OBS-related information. OracleAgent seamlessly\nintegrates multiple OBS analysis tools, empowered by large language models\n(LLMs), and can flexibly orchestrate these components. Additionally, we\nconstruct a comprehensive domain-specific multimodal knowledge base for OBS,\nwhich is built through a rigorous multi-year process of data collection,\ncleaning, and expert annotation. The knowledge base comprises over 1.4M\nsingle-character rubbing images and 80K interpretation texts. OracleAgent\nleverages this resource through its multimodal tools to assist experts in\nretrieval tasks of character, document, interpretation text, and rubbing image.\nExtensive experiments demonstrate that OracleAgent achieves superior\nperformance across a range of multimodal reasoning and generation tasks,\nsurpassing leading mainstream multimodal large language models (MLLMs) (e.g.,\nGPT-4o). Furthermore, our case study illustrates that OracleAgent can\neffectively assist domain experts, significantly reducing the time cost of OBS\nresearch. These results highlight OracleAgent as a significant step toward the\npractical deployment of OBS-assisted research and automated interpretation\nsystems.",
        "url": "http://arxiv.org/abs/2510.26114v1",
        "published_date": "2025-10-30T03:54:53+00:00",
        "updated_date": "2025-10-30T03:54:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Caoshuo Li",
            "Zengmao Ding",
            "Xiaobin Hu",
            "Bang Li",
            "Donghao Luo",
            "Xu Peng",
            "Taisong Jin",
            "Yongge Liu",
            "Shengwei Han",
            "Jing Yang",
            "Xiaoping He",
            "Feng Gao",
            "AndyPian Wu",
            "SevenShu",
            "Chaoyang Wang",
            "Chengjie Wang"
        ],
        "tldr": "The paper introduces OracleAgent, an LLM-powered agent system with a comprehensive multimodal knowledge base for Oracle Bone Script research, demonstrating superior performance in multimodal reasoning and assistance to domain experts.",
        "tldr_zh": "该论文介绍了OracleAgent，一个由大型语言模型驱动的智能体系统，拥有全面的甲骨文知识库，在多模态推理和辅助领域专家方面表现出卓越的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI",
        "summary": "In this work, we address the TrackRAD2025 challenge of real-time tumor\ntracking in cine-MRI sequences of the thoracic and abdominal regions under\nstrong data scarcity constraints. Two complementary strategies were explored:\n(i) unsupervised registration with the IMPACT similarity metric and (ii)\nfoundation model-based segmentation leveraging SAM 2.1 and its recent variants\nthrough prompt-based interaction. Due to the one-second runtime constraint, the\nSAM-based method was ultimately selected. The final configuration used SAM2.1\nb+ with mask-based prompts from the first annotated slice, fine-tuned solely on\nthe small labeled subset from TrackRAD2025. Training was configured to minimize\noverfitting, using 1024x1024 patches (batch size 1), standard augmentations,\nand a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was\napplied to all modules (prompt encoder, decoder, Hiera backbone) to preserve\ngeneralization while adapting to annotator-specific styles. Training lasted 300\nepochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently\napplied across all anatomical sites and MRI field strengths. Test-time\naugmentation was considered but ultimately discarded due to negligible\nperformance gains. The final model was selected based on the highest Dice\nSimilarity Coefficient achieved on the validation set after fine-tuning. On the\nhidden test set, the model reached a Dice score of 0.8794, ranking 6th overall\nin the TrackRAD2025 challenge. These results highlight the strong potential of\nfoundation models for accurate and real-time tumor tracking in MRI-guided\nradiotherapy.",
        "url": "http://arxiv.org/abs/2510.25990v1",
        "published_date": "2025-10-29T21:57:12+00:00",
        "updated_date": "2025-10-29T21:57:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Valentin Boussot",
            "Cédric Hémon",
            "Jean-Claude Nunes",
            "Jean-Louis Dillenseger"
        ],
        "tldr": "This paper explores fine-tuning the Segment Anything Model (SAM) for real-time tumor tracking in cine-MRI under data scarcity, achieving a Dice score of 0.8794 on a hidden test set and ranking 6th in the TrackRAD2025 challenge.",
        "tldr_zh": "该论文探讨了微调 Segment Anything Model (SAM) 以在数据稀缺的情况下对电影 MRI 中的实时肿瘤进行跟踪，在隐藏的测试集上实现了 0.8794 的 Dice 分数，并在 TrackRAD2025 挑战赛中排名第 6。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]