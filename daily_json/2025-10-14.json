[
    {
        "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
        "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
        "url": "http://arxiv.org/abs/2510.11606v1",
        "published_date": "2025-10-13T16:45:28+00:00",
        "updated_date": "2025-10-13T16:45:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yicheng Xu",
            "Yue Wu",
            "Jiashuo Yu",
            "Ziang Yan",
            "Tianxiang Jiang",
            "Yinan He",
            "Qingsong Zhao",
            "Kai Chen",
            "Yu Qiao",
            "Limin Wang",
            "Manabu Okumura",
            "Yi Wang"
        ],
        "tldr": "The paper introduces ExpVid, a new benchmark for evaluating Multimodal Large Language Models (MLLMs) on scientific experiment videos, highlighting their limitations in fine-grained perception, procedural understanding, and scientific reasoning, especially compared to proprietary models.",
        "tldr_zh": "该论文介绍了ExpVid，一个新的用于评估多模态大型语言模型（MLLMs）在科学实验视频上的表现的基准，强调了它们在细粒度感知、程序理解和科学推理方面的局限性，特别是与专有模型相比。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
        "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
        "url": "http://arxiv.org/abs/2510.11718v1",
        "published_date": "2025-10-13T17:59:55+00:00",
        "updated_date": "2025-10-13T17:59:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chengqi Duan",
            "Kaiyue Sun",
            "Rongyao Fang",
            "Manyuan Zhang",
            "Yan Feng",
            "Ying Luo",
            "Yufang Liu",
            "Ke Wang",
            "Peng Pei",
            "Xunliang Cai",
            "Hongsheng Li",
            "Yi Ma",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces CodePlot-CoT, a code-driven Chain-of-Thought approach for mathematical visual reasoning, along with a large-scale bilingual dataset and benchmark (Math-VR) and a state-of-the-art image-to-code converter. The approach shows significant performance gains compared to base models.",
        "tldr_zh": "该论文介绍了CodePlot-CoT，一种用于数学视觉推理的代码驱动的思维链方法，以及一个大规模双语数据集和基准测试 (Math-VR) 和一个最先进的图像到代码转换器。 该方法相比于基线模型表现出显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Language-Centric Omnimodal Representation Learning",
        "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
        "url": "http://arxiv.org/abs/2510.11693v1",
        "published_date": "2025-10-13T17:53:52+00:00",
        "updated_date": "2025-10-13T17:53:52+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenghao Xiao",
            "Hou Pong Chan",
            "Hao Zhang",
            "Weiwen Xu",
            "Mahani Aljunied",
            "Yu Rong"
        ],
        "tldr": "This paper introduces LCO-Emb, a Language-Centric Omnimodal Embedding framework, demonstrating state-of-the-art performance by leveraging implicit cross-modal alignment in MLLMs and establishing a Generation-Representation Scaling Law.",
        "tldr_zh": "本文介绍了LCO-Emb，一种以语言为中心的通用模态嵌入框架，通过利用MLLM中隐含的跨模态对齐并建立生成-表示比例定律，展示了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?",
        "summary": "Omnidirectional images (ODIs) provide full 360x180 view which are widely\nadopted in VR, AR and embodied intelligence applications. While multi-modal\nlarge language models (MLLMs) have demonstrated remarkable performance on\nconventional 2D image and video understanding benchmarks, their ability to\ncomprehend the immersive environments captured by ODIs remains largely\nunexplored. To address this gap, we first present ODI-Bench, a novel\ncomprehensive benchmark specifically designed for omnidirectional image\nunderstanding. ODI-Bench contains 2,000 high-quality omnidirectional images and\nover 4,000 manually annotated question-answering (QA) pairs across 10\nfine-grained tasks, covering both general-level and spatial-level ODI\nunderstanding. Extensive experiments are conducted to benchmark 20\nrepresentative MLLMs, including proprietary and open-source models, under both\nclose-ended and open-ended settings. Experimental results reveal that current\nMLLMs still struggle to capture the immersive context provided by ODIs. To this\nend, we further introduce Omni-CoT, a training-free method which significantly\nenhances MLLMs' comprehension ability in the omnidirectional environment\nthrough chain-of-thought reasoning across both textual information and visual\ncues. Both the benchmark and the code will be released upon the publication.",
        "url": "http://arxiv.org/abs/2510.11549v1",
        "published_date": "2025-10-13T15:51:47+00:00",
        "updated_date": "2025-10-13T15:51:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liu Yang",
            "Huiyu Duan",
            "Ran Tao",
            "Juntao Cheng",
            "Sijing Wu",
            "Yunhao Li",
            "Jing Liu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "tldr": "The paper introduces ODI-Bench, a new benchmark for evaluating MLLMs on omnidirectional image understanding, and reveals that current MLLMs struggle with this task. They also propose Omni-CoT, a training-free method to improve MLLM performance in this domain.",
        "tldr_zh": "该论文介绍了 ODI-Bench，一个用于评估 MLLM 在全景图像理解方面的新基准，并揭示了当前 MLLM 在此任务上存在困难。他们还提出了 Omni-CoT，一种无需训练的方法来提高 MLLM 在该领域的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance",
        "summary": "Walking assistance in extreme or complex environments remains a significant\nchallenge for people with blindness or low vision (BLV), largely due to the\nlack of a holistic scene understanding. Motivated by the real-world needs of\nthe BLV community, we build mmWalk, a simulated multi-modal dataset that\nintegrates multi-view sensor and accessibility-oriented features for outdoor\nsafe navigation. Our dataset comprises 120 manually controlled,\nscenario-categorized walking trajectories with 62k synchronized frames. It\ncontains over 559k panoramic images across RGB, depth, and semantic modalities.\nFurthermore, to emphasize real-world relevance, each trajectory involves\noutdoor corner cases and accessibility-specific landmarks for BLV users.\nAdditionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual\nquestion-answer triplets across 9 categories tailored for safe and informed\nwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)\nusing zero- and few-shot settings and found they struggle with our risk\nassessment and navigational tasks. We validate our mmWalk-finetuned model on\nreal-world datasets and show the effectiveness of our dataset for advancing\nmulti-modal walking assistance.",
        "url": "http://arxiv.org/abs/2510.11520v1",
        "published_date": "2025-10-13T15:25:52+00:00",
        "updated_date": "2025-10-13T15:25:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kedi Ying",
            "Ruiping Liu",
            "Chongyan Chen",
            "Mingzhe Tao",
            "Hao Shi",
            "Kailun Yang",
            "Jiaming Zhang",
            "Rainer Stiefelhagen"
        ],
        "tldr": "The paper introduces mmWalk, a new multi-modal, multi-view dataset and VQA benchmark designed to advance walking assistance for people with blindness or low vision, and demonstrates its value by fine-tuning and testing VLMs.",
        "tldr_zh": "该论文介绍了mmWalk，一个新的多模态、多视点数据集和VQA基准，旨在推进针对盲人或低视力人群的步行辅助技术，并通过微调和测试VLM来证明其价值。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model",
        "summary": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs.",
        "url": "http://arxiv.org/abs/2510.11509v1",
        "published_date": "2025-10-13T15:17:18+00:00",
        "updated_date": "2025-10-13T15:17:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruiping Liu",
            "Junwei Zheng",
            "Yufan Chen",
            "Zirui Wang",
            "Kunyu Peng",
            "Kailun Yang",
            "Jiaming Zhang",
            "Marc Pollefeys",
            "Rainer Stiefelhagen"
        ],
        "tldr": "The paper introduces Situat3DChange, a large-scale dataset for situated 3D change understanding in dynamic environments, and an efficient 3D MLLM approach called SCReasoner for point cloud comparison.",
        "tldr_zh": "该论文介绍了一个名为Situat3DChange的大规模数据集，用于理解动态环境中基于情境的3D变化，以及一种名为SCReasoner的高效3D MLLM方法，用于点云比较。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
        "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoR",
        "url": "http://arxiv.org/abs/2510.11496v1",
        "published_date": "2025-10-13T15:04:38+00:00",
        "updated_date": "2025-10-13T15:04:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhiwei Jin",
            "Xiaohui Song",
            "Nan Wang",
            "Yafei Liu",
            "Chao Li",
            "Xin Li",
            "Ruichen Wang",
            "Zhihao Li",
            "Qi Qi",
            "Long Cheng",
            "Dongze Hao",
            "Quanlong Zheng",
            "Yanhao Zhang",
            "Haobo Ji",
            "Jian Ma",
            "Zhitong Zheng",
            "Zhenyi Lin",
            "Haolin Deng",
            "Xin Zou",
            "Xiaojie Yin",
            "Ruilin Wang",
            "Liankai Cai",
            "Haijing Liu",
            "Yuqing Qiu",
            "Ke Chen",
            "Zixian Li",
            "Chi Xie",
            "Huafei Li",
            "Chenxing Li",
            "Chuangchuang Wang",
            "Kai Tang",
            "Zhiguang Zhu",
            "Kai Tang",
            "Wenmei Gao",
            "Rui Wang",
            "Jun Wu",
            "Chao Liu",
            "Qin Xie",
            "Chen Chen",
            "Haonan Lu"
        ],
        "tldr": "AndesVL introduces a suite of mobile-side MLLMs (0.6B-4B parameters) based on Qwen3, achieving competitive performance on various VQA benchmarks, addressing the limitations of large cloud-based models on edge devices.",
        "tldr_zh": "AndesVL介绍了一系列基于Qwen3的移动端多模态大语言模型（参数量0.6B-4B），在各种VQA基准测试中取得了具有竞争力的性能，解决了大型云端模型在边缘设备上的限制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
        "summary": "Recent approaches for vision-language models (VLMs) have shown remarkable\nsuccess in achieving fast downstream adaptation. When applied to real-world\ndownstream tasks, VLMs inevitably encounter both the in-distribution (ID) data\nand out-of-distribution (OOD) data. The OOD datasets often include both\ncovariate shifts (e.g., known classes with changes in image styles) and\nsemantic shifts (e.g., test-time unseen classes). This highlights the\nimportance of improving VLMs' generalization ability to covariate-shifted OOD\ndata, while effectively detecting open-set semantic-shifted OOD classes. In\nthis paper, inspired by the substantial energy change observed in closed-set\ndata when re-aligning vision-language modalities (specifically by directly\nreducing the maximum cosine similarity to a low value), we introduce a novel\nOOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the\nvanilla energy-based OOD score and provides a more reliable approach for OOD\ndetection. Furthermore, {\\Delta}Energy can simultaneously improve OOD\ngeneralization under covariate shifts, which is achieved by lower-bound\nmaximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to\nnot only enhance OOD detection but also yields a domain-consistent Hessian,\nwhich serves as a strong indicator for OOD generalization. Based on this\nfinding, we developed a unified fine-tuning framework that allows for improving\nVLMs' robustness in both OOD generalization and OOD detection. Extensive\nexperiments on challenging OOD detection and generalization benchmarks\ndemonstrate the superiority of our method, outperforming recent approaches by\n10% to 25% in AUROC.",
        "url": "http://arxiv.org/abs/2510.11296v1",
        "published_date": "2025-10-13T11:36:58+00:00",
        "updated_date": "2025-10-13T11:36:58+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Lin Zhu",
            "Yifeng Yang",
            "Xinbing Wang",
            "Qinying Gu",
            "Nanyang Ye"
        ],
        "tldr": "This paper introduces a novel OOD score, ΔEnergy, for Vision-Language Models, which improves both OOD detection and generalization by optimizing energy change during vision-language alignment and using energy based model training.",
        "tldr_zh": "本文介绍了一种新的视觉语言模型（VLM）的OOD分数 ΔEnergy，通过优化视觉-语言对齐过程中的能量变化并使用基于能量模型的训练，从而提高了OOD检测和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering",
        "summary": "Large vision-language models (VLMs) achieve strong performance in Visual\nQuestion Answering but still rely heavily on supervised fine-tuning (SFT) with\nmassive labeled datasets, which is costly due to human annotations. Crucially,\nreal-world datasets often exhibit human uncertainty (HU) -- variation in human\nconfidence across annotations -- but standard SFT simply optimizes toward the\nmost frequent label, disregarding HU distributions. This leaves two open\nquestions: How does HU affect SFT, and how can HU be effectively leveraged in\ntraining? In this work, we first conduct a systematic evaluation of VLMs across\nvarying HU levels. We have two key findings: (i) surprisingly, high-HU samples\ncontribute little or even degrade model performance, and (ii) naively training\non the full dataset yields under-calibrated models that fail to capture HU\ndistributions. Motivated by these findings, we introduce HaDola, a human\nuncertainty-aware data selection and automatic labeling framework. HaDola\noperates in four stages -- discriminate, self-annotate, error trigger, and\ntraining -- to iteratively identify harmful samples, prioritize informative\nones, and bootstrap from a small seed set (5\\% of data). Our approach\nsubstantially reduces reliance on costly HU annotations and makes VLMs more\naccurate and better calibrated. Extensive experiments on VQAv2 and VizWiz\ndatasets demonstrate that HaDola consistently matches or outperforms\nstate-of-the-art baselines with less training data. Our work highlights the\nimportance of explicitly modeling HU in SFT, suggesting that better utilization\nof HU is more effective than merely scaling up dataset size.",
        "url": "http://arxiv.org/abs/2510.11295v1",
        "published_date": "2025-10-13T11:35:30+00:00",
        "updated_date": "2025-10-13T11:35:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Lan",
            "Zhicheng Liu",
            "Udo Schlegel",
            "Raoyuan Zhao",
            "Yihong Liu",
            "Hinrich Schütze",
            "Michael A. Hedderich",
            "Thomas Seidl"
        ],
        "tldr": "This paper introduces HaDola, a framework for Visual Question Answering that leverages human uncertainty to improve data selection and automatic labeling, reducing reliance on costly human annotations and enhancing model accuracy and calibration.",
        "tldr_zh": "本文介绍了一种名为HaDola的视觉问答框架，该框架利用人类不确定性来改进数据选择和自动标注，从而减少对昂贵的人工标注的依赖，并提高模型的准确性和校准。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations",
        "summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT)\nexplanations that sound plausible yet fail to reflect the underlying decision\nprocess, undermining trust in high-stakes clinical use. Existing evaluations\nrarely catch this misalignment, prioritizing answer accuracy or adherence to\nformats. We present a clinically grounded framework for chest X-ray visual\nquestion answering (VQA) that probes CoT faithfulness via controlled text and\nimage modifications across three axes: clinical fidelity, causal attribution,\nand confidence calibration. In a reader study (n=4), evaluator-radiologist\ncorrelations fall within the observed inter-radiologist range for all axes,\nwith strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate\nalignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone\n($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows\nthat answer accuracy and explanation quality are decoupled, acknowledging\ninjected cues does not ensure grounding, and text cues shift explanations more\nthan visual cues. While some open-source models match final answer accuracy,\nproprietary models score higher on attribution (25.0% vs. 1.4%) and often on\nfidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to\nevaluate beyond final answer accuracy.",
        "url": "http://arxiv.org/abs/2510.11196v1",
        "published_date": "2025-10-13T09:28:22+00:00",
        "updated_date": "2025-10-13T09:28:22+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Johannes Moll",
            "Markus Graf",
            "Tristan Lemke",
            "Nicolas Lenhart",
            "Daniel Truhn",
            "Jean-Benoit Delbrouck",
            "Jiazhen Pan",
            "Daniel Rueckert",
            "Lisa C. Adams",
            "Keno K. Bressem"
        ],
        "tldr": "This paper introduces a framework for evaluating the faithfulness of chain-of-thought explanations in medical VLMs, revealing that answer accuracy and explanation quality are often decoupled and highlighting the importance of evaluating beyond final answer accuracy. They use controlled text and image perturbations to probe faithfulness.",
        "tldr_zh": "本文提出了一个评估医疗VLM中思维链解释忠实度的框架，揭示了答案准确性和解释质量通常是脱钩的，并强调了评估超越最终答案准确性的重要性。他们使用受控的文本和图像扰动来探测忠实度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) face an inherent trade-off between\nfaithfulness and creativity, as different tasks require varying degrees of\nassociative reasoning. However, existing methods lack the flexibility to\nmodulate this reasoning strength, limiting MLLMs' adaptability across factual\nand creative scenarios. To bridge this gap, we propose equipping MLLMs with\nmechanisms that enable flexible control over associative reasoning. We begin by\ninvestigating the internal mechanisms underlying associative behavior in MLLMs\nand find that: (1) middle layers play a pivotal role in shaping model's\nassociative tendencies, (2) modifying representations in these layers\neffectively regulates associative reasoning strength, and (3) hallucinations\ncan be exploited to derive steering vectors that guide this modulation.\nBuilding on these findings, we introduce Flexible Association Control (FlexAC),\na lightweight and training-free framework for modulating associative behavior\nin MLLMs. FlexAC first induces hallucination-guided intermediate\nrepresentations to encode associative directions. Then, it selects\nhigh-association instances to construct effective associative steering vectors,\nwhose strengths are adaptively calibrated to balance creative guidance with\noutput stability. Finally, recognizing the multi-dimensional nature of\nassociative reasoning, FlexAC incorporates task-specific associative vectors\nderived from a forward pass on a few target-domain samples, enabling models to\nfollow diverse associative directions and better adapt to creative tasks.\nNotably, our method achieves up to a 5.8x improvement in creativity on\nCreation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing\nexisting baselines and demonstrating its effectiveness in enabling flexible\ncontrol over associative reasoning in MLLMs. Our code is available at\nhttps://github.com/ylhz/FlexAC.",
        "url": "http://arxiv.org/abs/2510.11190v1",
        "published_date": "2025-10-13T09:22:12+00:00",
        "updated_date": "2025-10-13T09:22:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengming Yuan",
            "Xinyu Lyu",
            "Shuailong Wang",
            "Beitao Chen",
            "Jingkuan Song",
            "Lianli Gao"
        ],
        "tldr": "The paper introduces FlexAC, a training-free framework that enables flexible control over associative reasoning in MLLMs by modulating intermediate representations using hallucination-guided steering vectors, demonstrating improved creativity and reduced hallucination rates.",
        "tldr_zh": "该论文介绍了 FlexAC，一个无需训练的框架，通过使用幻觉引导的控制向量来调节中间表示，从而实现对 MLLM 中联想推理的灵活控制，展示了创造力的提高和幻觉率的降低。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models",
        "summary": "As vision-language models (VLMs) are deployed globally, their ability to\nunderstand culturally situated knowledge becomes essential. Yet, existing\nevaluations largely assess static recall or isolated visual grounding, leaving\nunanswered whether VLMs possess robust and transferable cultural understanding.\nWe introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to\nevaluate the robustness of everyday cultural knowledge in VLMs across\nlinguistic rephrasings and visual modalities. Building on the BLEnD dataset,\nBLEnD-Vis constructs 313 culturally grounded question templates spanning 16\nregions and generates three aligned multiple-choice formats: (i) a text-only\nbaseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant\n(Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated\nimages. The resulting benchmark comprises 4,916 images and over 21,000\nmultiple-choice question (MCQ) instances, validated through human annotation.\nBLEnD-Vis reveals significant fragility in current VLM cultural knowledge;\nmodels exhibit performance drops under linguistic rephrasing and, whilst visual\ncues often aid performance, low cross-modal consistency highlights challenges\nin robustly integrating textual and visual understanding, particularly for\nlower-resource regions. BLEnD-Vis thus provides a crucial testbed for\nsystematically analysing cultural robustness and multimodal grounding, exposing\nlimitations and guiding the development of more culturally competent VLMs.",
        "url": "http://arxiv.org/abs/2510.11178v1",
        "published_date": "2025-10-13T09:10:05+00:00",
        "updated_date": "2025-10-13T09:10:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Bryan Chen Zhengyu Tan",
            "Zheng Weihua",
            "Zhengyuan Liu",
            "Nancy F. Chen",
            "Hwaran Lee",
            "Kenny Tsu Wei Choo",
            "Roy Ka-Wei Lee"
        ],
        "tldr": "The paper introduces BLEnD-Vis, a new multimodal benchmark for evaluating the cultural understanding and robustness of Vision-Language Models (VLMs) across different linguistic rephrasings and visual modalities, revealing significant limitations in current VLMs.",
        "tldr_zh": "该论文介绍了BLEnD-Vis，一个新的多模态基准，用于评估视觉语言模型（VLM）在不同语言释义和视觉模态下的文化理解和鲁棒性，揭示了当前VLM的显著局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory",
        "summary": "Continuous, high-frame-rate, high-resolution processing of long video streams\nis critical for future AI agents, yet current video-understanding LLMs struggle\nto scale. Offline, fixed-frame-number methods require the stream length to\nadapt frame rates; streaming methods constrain memory by merging or discarding\ntokens, losing information. We propose video-SALMONN S, a streaming\naudio-visual LLM that, to our knowledge, is the first to process 3-hour videos\nat 1 FPS and 360p resolution under a fixed memory budget. Our model introduces\n(i) a test-time-training (TTT) memory module that continually updates token\nrepresentations to capture long-range dependencies by replacing token merging,\nand (ii) a prompt-dependent memory reader that selectively retrieves\ncontext-relevant content from fixed-size memory. The TTT module is optimised\nwith a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient\nadaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),\nvideo-SALMONN S sustains high-quality understanding on multi-hour videos with\n10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and\n67.8% on the Video-MME long split, outperforming both offline and streaming\nbaselines.",
        "url": "http://arxiv.org/abs/2510.11129v1",
        "published_date": "2025-10-13T08:20:15+00:00",
        "updated_date": "2025-10-13T08:20:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guangzhi Sun",
            "Yixuan Li",
            "Xiaodong Wu",
            "Yudong Yang",
            "Wei Li",
            "Zejun Ma",
            "Chao Zhang"
        ],
        "tldr": "The paper introduces video-SALMONN S, a streaming audio-visual LLM capable of processing 3-hour videos with fixed memory using a test-time-training memory module and a prompt-dependent memory reader, achieving state-of-the-art performance on long-video benchmarks.",
        "tldr_zh": "该论文介绍了 video-SALMONN S，一种流式音视频 LLM，它能够使用测试时训练的记忆模块和提示依赖的记忆读取器，以固定内存处理 3 小时的视频，并在长视频基准测试中实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning",
        "summary": "Few-shot learning (FSL) addresses the challenge of classifying novel classes\nwith limited training samples. While some methods leverage semantic knowledge\nfrom smaller-scale models to mitigate data scarcity, these approaches often\nintroduce noise and bias due to the data's inherent simplicity. In this paper,\nwe propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which\neffectively transfers diverse and complementary knowledge from large multimodal\nmodels to empower the off-the-shelf few-shot learner. Specifically, SynTrans\nemploys CLIP as a robust teacher and uses a few-shot vision encoder as a weak\nstudent, distilling semantic-aligned visual knowledge via an unsupervised proxy\ntask. Subsequently, a training-free synergistic knowledge mining module\nfacilitates collaboration among large multimodal models to extract high-quality\nsemantic knowledge. Building upon this, a visual-semantic bridging module\nenables bi-directional knowledge transfer between visual and semantic spaces,\ntransforming explicit visual and implicit semantic knowledge into\ncategory-specific classifier weights. Finally, SynTrans introduces a visual\nweight generator and a semantic weight reconstructor to adaptively construct\noptimal multimodal FSL classifiers. Experimental results on four FSL datasets\ndemonstrate that SynTrans, even when paired with a simple few-shot vision\nencoder, significantly outperforms current state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2510.11115v1",
        "published_date": "2025-10-13T08:06:23+00:00",
        "updated_date": "2025-10-13T08:06:23+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Hao Tang",
            "Shengfeng He",
            "Jing Qin"
        ],
        "tldr": "The paper introduces Synergistic Knowledge Transfer (SynTrans), a framework that leverages large multimodal models like CLIP to improve few-shot learning by transferring diverse knowledge to a few-shot learner through distillation and knowledge mining.",
        "tldr_zh": "该论文介绍了协同知识迁移（SynTrans）框架，该框架利用CLIP等大型多模态模型，通过蒸馏和知识挖掘将多样化的知识迁移到少样本学习器，从而改善少样本学习效果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\ntasks such as image captioning, visual question answering, and cross-modal\nreasoning by integrating visual and textual modalities. However, their\nmultimodal nature also exposes them to adversarial threats, where attackers can\nperturb either modality or both jointly to induce harmful, misleading, or\npolicy violating outputs. Existing defense strategies, such as adversarial\ntraining and input purification, face notable limitations: adversarial training\ntypically improves robustness only against known attacks while incurring high\ncomputational costs, whereas conventional purification approaches often suffer\nfrom degraded image quality and insufficient generalization to complex\nmultimodal tasks.\n  In this work, we focus on defending the visual modality, which frequently\nserves as the primary entry point for adversarial manipulation. We propose a\nsupervised diffusion based denoising framework that leverages paired\nadversarial clean image datasets to fine-tune diffusion models with\ndirectional, task specific guidance. Unlike prior unsupervised purification\nmethods such as DiffPure, our approach achieves higher quality reconstructions\nwhile significantly improving defense robustness in multimodal tasks.\nFurthermore, we incorporate prompt optimization as a complementary defense\nmechanism, enhancing resistance against diverse and unseen attack strategies.\n  Extensive experiments on image captioning and visual question answering\ndemonstrate that our method not only substantially improves robustness but also\nexhibits strong transferability to unknown adversarial attacks. These results\nhighlight the effectiveness of supervised diffusion based denoising for\nmultimodal defense, paving the way for more reliable and secure deployment of\nMLLMs in real world applications.",
        "url": "http://arxiv.org/abs/2510.11096v1",
        "published_date": "2025-10-13T07:44:54+00:00",
        "updated_date": "2025-10-13T07:44:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fengling Zhu",
            "Boshi Liu",
            "Jingyu Hua",
            "Sheng Zhong"
        ],
        "tldr": "The paper introduces CoDefend, a novel defense mechanism against adversarial attacks on Multimodal Large Language Models (MLLMs). It uses supervised diffusion-based denoising and prompt optimization to enhance robustness and transferability, particularly focusing on defending the visual modality.",
        "tldr_zh": "该论文介绍了一种名为CoDefend的新型防御机制，用于抵抗多模态大型语言模型（MLLMs）上的对抗攻击。它采用监督扩散去噪和提示优化来增强鲁棒性和可迁移性，特别关注于防御视觉模态。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
        "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
        "url": "http://arxiv.org/abs/2510.11027v1",
        "published_date": "2025-10-13T05:51:22+00:00",
        "updated_date": "2025-10-13T05:51:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ganlin Yang",
            "Tianyi Zhang",
            "Haoran Hao",
            "Weiyun Wang",
            "Yibin Liu",
            "Dehui Wang",
            "Guanzhou Chen",
            "Zijian Cai",
            "Junting Chen",
            "Weijie Su",
            "Wengang Zhou",
            "Yu Qiao",
            "Jifeng Dai",
            "Jiangmiao Pang",
            "Gen Luo",
            "Wenhai Wang",
            "Yao Mu",
            "Zhi Hou"
        ],
        "tldr": "The paper introduces Vlaser, a VLA model designed to bridge the gap between high-level VLM reasoning and low-level VLA control, achieving state-of-the-art results on several embodied reasoning benchmarks and demonstrating improved VLA fine-tuning.",
        "tldr_zh": "本文介绍了Vlaser，一个旨在弥合高层VLM推理和底层VLA控制之间差距的VLA模型。该模型在多个具身推理基准测试中取得了最先进的结果，并展示了改进的VLA微调。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
        "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\n\\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
        "url": "http://arxiv.org/abs/2510.11026v1",
        "published_date": "2025-10-13T05:50:44+00:00",
        "updated_date": "2025-10-13T05:50:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongxiang Li",
            "Yaowei Li",
            "Bin Lin",
            "Yuwei Niu",
            "Yuhang Yang",
            "Xiaoshuang Huang",
            "Jiayin Cai",
            "Xiaolong Jiang",
            "Yao Hu",
            "Long Chen"
        ],
        "tldr": "The paper introduces GIR-Bench, a new benchmark for evaluating the reasoning capabilities of unified multimodal models in image generation, focusing on consistency, text-to-image generation with constraints, and multi-step editing.",
        "tldr_zh": "该论文介绍了GIR-Bench，一个新的基准，用于评估统一多模态模型在图像生成中的推理能力，重点关注一致性、带约束的文本到图像生成和多步骤编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation",
        "summary": "Auxiliary lines are essential for solving complex geometric problems but\nremain challenging for large vision-language models (LVLMs). Rather than\nediting diagrams to draw auxiliary lines, which current image editing models\nstruggle to render with geometric precision, we generate textual descriptions\nof auxiliary-line constructions to better align with the representational\nstrengths of LVLMs. To bridge the gap between textual descriptions and spatial\nstructure, we propose a reinforcement learning framework that enhances\ndiagram-text alignment. At the core of our approach is a cross-modal reward\nthat evaluates how well the generated auxiliary-line description for an\noriginal diagram matches a ground-truth auxiliary-line diagram. Built on this\nreward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line\nreasoning in solid geometry. This fine-grained signal drives a GRPO-based RL\nstage, yielding precise diagram-text alignment. To support training, we develop\na scalable data creation pipeline and construct AuxSolidMath, a dataset of\n3,018 real-exam geometry problems with paired diagrams and aligned textual\nfields. At the 3B and 7B scales, GeoVLMath achieves competitive and often\nsuperior performance compared with strong open-source and proprietary LVLMs on\nauxiliary-line reasoning benchmarks.",
        "url": "http://arxiv.org/abs/2510.11020v1",
        "published_date": "2025-10-13T05:33:51+00:00",
        "updated_date": "2025-10-13T05:33:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shasha Guo",
            "Liang Pang",
            "Xi Wang",
            "Yanling Wang",
            "Huawei Shen",
            "Jing Zhang"
        ],
        "tldr": "The paper introduces GeoVLMath, an LVLM that enhances geometry reasoning by generating textual descriptions of auxiliary line constructions and uses a cross-modal reward-based reinforcement learning framework to improve diagram-text alignment. They also introduce a new dataset, AuxSolidMath, to support training.",
        "tldr_zh": "该论文介绍了 GeoVLMath，一个通过生成辅助线构造的文本描述来增强几何推理的 LVLM，并使用基于跨模态奖励的强化学习框架来改进图文对齐。他们还引入了一个新的数据集 AuxSolidMath 来支持训练。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models",
        "summary": "Compositional reasoning remains a persistent weakness of modern vision\nlanguage models (VLMs): they often falter when a task hinges on understanding\nhow multiple objects, attributes, and relations interact within an image.\nMultiple research works have attempted to improve compositionality performance\nby creative tricks such as improving prompt structure, chain of thought\nreasoning, etc. A more recent line of work attempts to impart additional\nreasoning in VLMs using well-trained Large Language Models (LLMs), which are\nfar superior in linguistic understanding than VLMs to compensate for the\nlimited linguistic prowess of VLMs. However, these approaches are either\nresource-intensive or do not provide an interpretable reasoning process. In\nthis paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs\nwith carefully designed neurosymbolic concept trees learned from LLMs to\nimprove VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning\nprocess boosts compositionality performance and provides a rationale behind VLM\npredictions. Empirical results on four compositionality benchmarks, Winoground,\nEqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with\nvarying sizes, demonstrate that COCO-Tree significantly improves compositional\ngeneralization by 5-10% over baselines.",
        "url": "http://arxiv.org/abs/2510.11012v1",
        "published_date": "2025-10-13T05:07:13+00:00",
        "updated_date": "2025-10-13T05:07:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sanchit Sinha",
            "Guangzhi Xiong",
            "Aidong Zhang"
        ],
        "tldr": "The paper introduces COCO-Tree, a neurosymbolic approach using LLM-derived concept trees to enhance compositional reasoning in VLMs, demonstrating 5-10% improvement on benchmarks.",
        "tldr_zh": "该论文介绍了COCO-Tree，一种利用LLM生成的神经符号概念树来增强VLM中组合推理能力的方法，并在基准测试中表现出5-10%的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning",
        "summary": "The capabilities of Large Vision-Language Models (LVLMs) have reached\nstate-of-the-art on many visual reasoning tasks, including chart reasoning, yet\nthey still falter on out-of-distribution (OOD) data, and degrade further when\nasked to produce their chain-of-thought (CoT) rationales, limiting\nexplainability. We present Chart-RVR, a general framework that fine-tunes LVLMs\nto be more robust and explainable for chart reasoning by coupling Group\nRelative Policy Optimization (GRPO) with automatically verifiable rewards. Our\nframework comprises of three rewards that maximize: (i) correct chart-type\nclassification, (ii) faithful chart table reconstruction, and (iii) process\nconformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently\noutperforms standard supervised fine-tuning (SFT) on both in-distribution and\nout-of-distribution datasets, closing the OOD performance gap while improving\nrationale fidelity. The resulting models, the Chart-RVR-3B series, achieve\nstate-of-the-art results on six chart-reasoning benchmarks spanning in-domain\nand OOD settings, surpassing all existing models of comparable size. Beyond\naccuracy, Chart-RVR yields more interpretable CoT rationales, strengthening\ntrust and reliability - showcasing the power of verifiable rewards with GRPO\nfor training reliable, interpretable chart-reasoning models.",
        "url": "http://arxiv.org/abs/2510.10973v1",
        "published_date": "2025-10-13T03:25:35+00:00",
        "updated_date": "2025-10-13T03:25:35+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sanchit Sinha",
            "Oana Frunza",
            "Kashif Rasul",
            "Yuriy Nevmyvaka",
            "Aidong Zhang"
        ],
        "tldr": "Chart-RVR is a framework that fine-tunes LVLMs using reinforcement learning with verifiable rewards for more robust and explainable chart reasoning, achieving state-of-the-art results on multiple benchmarks.",
        "tldr_zh": "Chart-RVR 是一个框架，它使用强化学习和可验证的奖励来微调 LVLM，以实现更强大和可解释的图表推理，并在多个基准测试中实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation",
        "summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often\nstruggle to preserve logic, object identity, and style in multimodal image-text\ngeneration. This limitation significantly hinders the generalization capability\nof VLMs in complex image-text input-output scenarios. To address this issue, we\npropose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which\nenhances existing interleaved VLMs through explicit structured reasoning,\nthereby mitigating context drift in logic, entity identity, and style. The\nproposed framework operates in two stages. (1) A dynamic IUT-Plug extraction\nmodule parses visual scenes into hierarchical symbolic structures. (2) A\ncoordinated narrative-flow and image synthesis mechanism ensures cross-modal\nconsistency. To evaluate our approach, we construct a novel benchmark based on\n3,000 real human-generated question-answer pairs over fine-tuned large models,\nintroducing a dynamic evaluation protocol for quantifying context drift in\ninterleaved VLMs. Experimental results demonstrate that IUT-Plug not only\nimproves accuracy on established benchmarks but also effectively alleviates the\nthree critical forms of context drift across diverse multimodal question\nanswering (QA) scenarios.",
        "url": "http://arxiv.org/abs/2510.10969v1",
        "published_date": "2025-10-13T03:19:45+00:00",
        "updated_date": "2025-10-13T03:19:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeteng Lin",
            "Xingxing Li",
            "Wen You",
            "Xiaoyang Li",
            "Zehan Lu",
            "Yujun Cai",
            "Jing Tang"
        ],
        "tldr": "The paper introduces IUT-Plug, a module grounded in an Image Understanding Tree, designed to enhance VLMs by improving logical consistency, object identity preservation, and style adherence in interleaved image-text generation, which it demonstrates using a new benchmark.",
        "tldr_zh": "该论文介绍了IUT-Plug，一个基于图像理解树的模块，旨在通过改善逻辑一致性、对象身份保持和风格一致性来增强视觉语言模型在交错图像-文本生成中的表现，并通过一个新的基准进行了演示。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
        "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
        "url": "http://arxiv.org/abs/2510.10921v1",
        "published_date": "2025-10-13T02:32:07+00:00",
        "updated_date": "2025-10-13T02:32:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Chunyu Xie",
            "Bin Wang",
            "Fanjing Kong",
            "Jincheng Li",
            "Dawei Liang",
            "Ji Ao",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "tldr": "FG-CLIP 2 is a bilingual (English and Chinese) vision-language model that enhances fine-grained alignment using techniques like region-text matching and a new loss function, achieving state-of-the-art results and a new benchmark for Chinese multimodal understanding.",
        "tldr_zh": "FG-CLIP 2是一个双语（英语和中文）视觉-语言模型，它通过区域-文本匹配和新的损失函数等技术来增强细粒度对齐，从而在中文多模态理解方面取得了最先进的成果，并建立了一个新的基准。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
        "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
        "url": "http://arxiv.org/abs/2510.11650v1",
        "published_date": "2025-10-13T17:29:55+00:00",
        "updated_date": "2025-10-13T17:29:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxuan Xue",
            "Xianghui Xie",
            "Margaret Kostyrko",
            "Gerard Pons-Moll"
        ],
        "tldr": "The paper introduces InfiniHuman, a framework that distills foundation models to generate a large-scale, richly annotated 3D human dataset (InfiniHumanData) and a diffusion-based generative pipeline (InfiniHumanGen) for controllable avatar generation, demonstrating improvements in quality, speed, and control.",
        "tldr_zh": "该论文介绍了 InfiniHuman，一个利用基础模型生成大规模、带有丰富注释的 3D 人类数据集 (InfiniHumanData) 和基于扩散的生成管道 (InfiniHumanGen) 的框架，用于可控的头像生成，并在质量、速度和控制方面展示了改进。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
        "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
        "url": "http://arxiv.org/abs/2510.11631v1",
        "published_date": "2025-10-13T17:12:02+00:00",
        "updated_date": "2025-10-13T17:12:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.NE"
        ],
        "authors": [
            "Tobias Preintner",
            "Weixuan Yuan",
            "Adrian König",
            "Thomas Bäck",
            "Elena Raponi",
            "Niki van Stein"
        ],
        "tldr": "EvoCAD uses evolutionary optimization with vision language models to generate CAD objects from symbolic representations, outperforming prior methods, especially in topological correctness, evaluated with novel Euler characteristic-based metrics.",
        "tldr_zh": "EvoCAD 使用视觉语言模型和进化优化生成 CAD 对象的符号表示，优于现有方法，尤其是在拓扑正确性方面，通过基于欧拉特征的新型指标进行评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SNAP: Towards Segmenting Anything in Any Point Cloud",
        "summary": "Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/",
        "url": "http://arxiv.org/abs/2510.11565v1",
        "published_date": "2025-10-13T16:07:00+00:00",
        "updated_date": "2025-10-13T16:07:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aniket Gupta",
            "Hanhui Wang",
            "Charles Saunders",
            "Aruni RoyChowdhury",
            "Hanumant Singh",
            "Huaizu Jiang"
        ],
        "tldr": "SNAP is a unified model for interactive 3D point cloud segmentation that supports both point-based and text-based prompts across diverse domains, achieving state-of-the-art performance on several zero-shot benchmarks.",
        "tldr_zh": "SNAP 是一个统一的交互式 3D 点云分割模型，支持跨不同领域的基于点和基于文本的提示，并在多个零样本基准测试中实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion",
        "summary": "Existing Infrared and Visible Image Fusion (IVIF) methods typically assume\nhigh-quality inputs. However, when handing degraded images, these methods\nheavily rely on manually switching between different pre-processing techniques.\nThis decoupling of degradation handling and image fusion leads to significant\nperformance degradation. In this paper, we propose a novel VLM-Guided\nDegradation-Coupled Fusion network (VGDCFusion), which tightly couples\ndegradation modeling with the fusion process and leverages vision-language\nmodels (VLMs) for degradation-aware perception and guided suppression.\nSpecifically, the proposed Specific-Prompt Degradation-Coupled Extractor\n(SPDCE) enables modality-specific degradation awareness and establishes a joint\nmodeling of degradation suppression and intra-modal feature extraction. In\nparallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates\ncross-modal degradation perception and couples residual degradation filtering\nwith complementary cross-modal feature fusion. Extensive experiments\ndemonstrate that our VGDCFusion significantly outperforms existing\nstate-of-the-art fusion approaches under various degraded image scenarios. Our\ncode is available at https://github.com/Lmmh058/VGDCFusion.",
        "url": "http://arxiv.org/abs/2510.11456v1",
        "published_date": "2025-10-13T14:26:33+00:00",
        "updated_date": "2025-10-13T14:26:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianpei Zhang",
            "Jufeng Zhao",
            "Yiming Zhu",
            "Guangmang Cui"
        ],
        "tldr": "The paper introduces VGDCFusion, a novel Vision-Language Model (VLM)-guided network for infrared and visible image fusion that explicitly addresses degradation in input images by tightly coupling degradation modeling with the fusion process.",
        "tldr_zh": "该论文介绍了一种新颖的视觉语言模型（VLM）引导网络VGDCFusion，用于红外和可见图像融合，通过将退化建模与融合过程紧密结合，明确解决了输入图像中的退化问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
        "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.",
        "url": "http://arxiv.org/abs/2510.11341v1",
        "published_date": "2025-10-13T12:38:04+00:00",
        "updated_date": "2025-10-13T12:38:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haomin Wang",
            "Jinhui Yin",
            "Qi Wei",
            "Wenguang Zeng",
            "Lixin Gu",
            "Shenglong Ye",
            "Zhangwei Gao",
            "Yaohui Wang",
            "Yanting Zhang",
            "Yuanqi Li",
            "Yanwen Guo",
            "Wenhai Wang",
            "Kai Chen",
            "Yu Qiao",
            "Hongjie Zhang"
        ],
        "tldr": "The paper introduces InternSVG, a multimodal large language model for unified SVG understanding, editing, and generation, along with a new dataset and benchmark. It claims substantial performance gains over existing methods.",
        "tldr_zh": "该论文介绍了InternSVG，一个用于统一SVG理解、编辑和生成的多模态大型语言模型，以及一个新的数据集和基准测试。 它声称比现有方法有了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models",
        "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.",
        "url": "http://arxiv.org/abs/2510.11302v1",
        "published_date": "2025-10-13T11:48:48+00:00",
        "updated_date": "2025-10-13T11:48:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Samer Al-Hamadani"
        ],
        "tldr": "This paper analyzes the cost-effectiveness of supervised object detection (YOLO) versus zero-shot VLM detection (Gemini), establishing break-even thresholds based on deployment volume, category stability, budget, and accuracy requirements. It reveals scenarios where VLMs offer lower cost per detection despite lower accuracy.",
        "tldr_zh": "本文分析了有监督目标检测 (YOLO) 与零样本 VLM 检测 (Gemini) 的性价比，并根据部署量、类别稳定性、预算和准确性要求，建立了盈亏平衡阈值。 研究表明，在某些情况下，尽管准确率较低，但 VLM 提供的每次检测成本更低。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation",
        "summary": "Existing works on reasoning segmentation either connect hidden features from\na language model directly to a mask decoder or represent positions in text,\nwhich limits interpretability and semantic detail. To solve this, we present\nCoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model\nthat bridges language reasoning to segmentation through a differentiable and\ninterpretable positional prior instantiated as a heatmap. By making the\nreasoning process clear via MCoT and expressing it as a dense, differentiable\nheatmap, this interface enhances interpretability and diagnostic analysis and\nyields more concentrated evidence on the target. A learnable concentration\ntoken aggregates features of the image and reasoning text to generate this\npositional prior, which is decoded to precise masks through a lightweight\ndecoder, providing a direct connection between reasoning and segmentation.\nAcross the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best\nreported metrics on each standard split under comparable protocols, with\nperformance at or above prior state of the art across both validation and test\npartitions. Extensive experiments reveal that the quality of the heatmap\nstrongly influences the resulting mask quality, supporting a consistent\nassociation between the reasoning output and downstream mask generation.\nCollectively, these findings support the utility of this paradigm in bridging\nreasoning and segmentation and show advantages in concentration driven by\nreasoning and predicting masks more precisely. Code, checkpoints and logs are\nreleased at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.",
        "url": "http://arxiv.org/abs/2510.11173v1",
        "published_date": "2025-10-13T09:07:54+00:00",
        "updated_date": "2025-10-13T09:07:54+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Zhenyu Lu",
            "Liupeng Li",
            "Jinpeng Wang",
            "Yan Feng",
            "Bin Chen",
            "Ke Chen",
            "Yaowei Wang"
        ],
        "tldr": "The paper introduces CoPRS, a Multi-modal Chain-of-Thought based model for reasoning segmentation, using a learnable positional prior heatmap to connect language reasoning to segmentation, achieving state-of-the-art results on RefCOCO and ReasonSeg datasets.",
        "tldr_zh": "该论文介绍了CoPRS，一种基于多模态思维链的推理分割模型，它使用可学习的位置先验热图将语言推理连接到分割，在RefCOCO和ReasonSeg数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts",
        "summary": "Recently, the powerful generalization ability exhibited by foundation models\nhas brought forth new solutions for zero-shot anomaly segmentation tasks.\nHowever, guiding these foundation models correctly to address downstream tasks\nremains a challenge. This paper proposes a novel two-stage framework, for\nzero-shot anomaly segmentation tasks in industrial anomaly detection. This\nframework excellently leverages the powerful anomaly localization capability of\nCLIP and the boundary perception ability of SAM.(1) To mitigate SAM's\ninclination towards object segmentation, we propose the Co-Feature Point Prompt\nGeneration (PPG) module. This module collaboratively utilizes CLIP and SAM to\ngenerate positive and negative point prompts, guiding SAM to focus on\nsegmenting anomalous regions rather than the entire object. (2) To further\noptimize SAM's segmentation results and mitigate rough boundaries and isolated\nnoise, we introduce the Cascaded Prompts for SAM (CPS) module. This module\nemploys hybrid prompts cascaded with a lightweight decoder of SAM, achieving\nprecise segmentation of anomalous regions. Across multiple datasets, consistent\nexperimental validation demonstrates that our approach achieves\nstate-of-the-art zero-shot anomaly segmentation results. Particularly\nnoteworthy is our performance on the Visa dataset, where we outperform the\nstate-of-the-art methods by 10.3\\% and 7.7\\% in terms of {$F_1$-max} and AP\nmetrics, respectively.",
        "url": "http://arxiv.org/abs/2510.11028v1",
        "published_date": "2025-10-13T05:53:49+00:00",
        "updated_date": "2025-10-13T05:53:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanning Hou",
            "Ke Xu",
            "Junfa Li",
            "Yanran Ruan",
            "Jianfeng Qiu"
        ],
        "tldr": "This paper introduces a two-stage framework using CLIP and SAM with novel prompt generation and cascaded prompting techniques to achieve state-of-the-art zero-shot anomaly segmentation, demonstrating significant performance gains on industrial anomaly detection datasets.",
        "tldr_zh": "本文提出了一种两阶段框架，利用CLIP和SAM，结合新颖的提示生成和级联提示技术，实现了最先进的零样本异常分割，并在工业异常检测数据集上展示了显著的性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Topological Alignment of Shared Vision-Language Embedding Space",
        "summary": "Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot\ncapabilities. However, their cross-modal alignment remains biased toward\nEnglish due to limited multilingual multimodal data. Recent multilingual\nextensions have alleviated this gap but enforce instance-level alignment while\nneglecting the global geometry of the shared embedding space. We address this\nproblem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a\ntopology-aware framework aligning embedding spaces with topology-preserving\nconstraints. The proposed method applies persistent homology to define a\ntopological alignment loss and approximates persistence diagram with\ntheoretical error bounds using graph sparsification strategy. This work\nvalidates the proposed approach, showing enhanced structural coherence of\nmultilingual representations, higher zero-shot accuracy on the CIFAR-100, and\nstronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the\nproposed approach provides a general method for incorporating topological\nalignment into representation learning.",
        "url": "http://arxiv.org/abs/2510.10889v1",
        "published_date": "2025-10-13T01:36:38+00:00",
        "updated_date": "2025-10-13T01:36:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Junwon You",
            "Dasol Kang",
            "Jae-Hun Jung"
        ],
        "tldr": "The paper introduces ToMCLIP, a topology-aware framework for aligning multilingual Vision-Language Models (VLMs) by incorporating topology-preserving constraints using persistent homology, enhancing multilingual representation coherence and zero-shot accuracy.",
        "tldr_zh": "该论文介绍了 ToMCLIP，一个拓扑感知的框架，通过使用持久同调并结合保持拓扑结构的约束来对齐多语言视觉-语言模型 (VLM)，从而增强多语言表示的连贯性和零样本准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales",
        "summary": "Vision-language models (VLMs) have advanced rapidly, yet their capacity for\nimage-grounded geolocation in open-world conditions, a task that is challenging\nand of demand in real life, has not been comprehensively evaluated. We present\nEarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates\nvisual recognition, step-by-step reasoning, and evidence use. EarthWhere\ncomprises 810 globally distributed images across two complementary geolocation\nscales: WhereCountry (i.e., 500 multiple-choice question-answering, with\ncountry-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained\nstreet-level identification tasks requiring multi-step reasoning with optional\nweb search). For evaluation, we adopt the final-prediction metrics: location\naccuracies within k km (Acc@k) for coordinates and hierarchical path scores for\ntextual localization. Beyond this, we propose to explicitly score intermediate\nreasoning chains using human-verified key visual clues and a Shapley-reweighted\nthinking score that attributes credit to each clue's marginal contribution. We\nbenchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere\nand report different types of final answer accuracies as well as the calibrated\nmodel thinking scores. Overall, Gemini-2.5-Pro achieves the best average\naccuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches\n34.71%. We reveal that web search and reasoning do not guarantee improved\nperformance when visual clues are limited, and models exhibit regional biases,\nachieving up to 42.7% higher scores in certain areas than others. These\nfindings highlight not only the promise but also the persistent challenges of\nmodels to mitigate bias and achieve robust, fine-grained localization. We\nopen-source our benchmark at https://github.com/UCSC-VLAA/EarthWhere.",
        "url": "http://arxiv.org/abs/2510.10880v1",
        "published_date": "2025-10-13T01:12:21+00:00",
        "updated_date": "2025-10-13T01:12:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaofang Qian",
            "Hardy Chen",
            "Zeyu Wang",
            "Li Zhang",
            "Zijun Wang",
            "Xiaoke Huang",
            "Hui Liu",
            "Xianfeng Tang",
            "Zeyu Zheng",
            "Haoqin Tu",
            "Cihang Xie",
            "Yuyin Zhou"
        ],
        "tldr": "The paper introduces EarthWhere, a new benchmark for evaluating vision-language models' geolocation capabilities across different scales, revealing limitations in web search reliance and regional biases in current models.",
        "tldr_zh": "该论文介绍了EarthWhere，一个新的基准，用于评估视觉语言模型在不同尺度上的地理定位能力，揭示了当前模型在网络搜索依赖性和区域偏差方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]