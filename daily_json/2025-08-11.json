[
    {
        "title": "Freeze and Reveal: Exposing Modality Bias in Vision-Language Models",
        "summary": "Vision Language Models achieve impressive multi-modal performance but often\ninherit gender biases from their training data. This bias might be coming from\nboth the vision and text modalities. In this work, we dissect the contributions\nof vision and text backbones to these biases by applying targeted debiasing\nusing Counterfactual Data Augmentation and Task Vector methods. Inspired by\ndata-efficient approaches in hate-speech classification, we introduce a novel\nmetric, Degree of Stereotypicality and a corresponding debiasing method, Data\nAugmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with\nminimal computational cost. We curate a gender annotated dataset and evaluate\nall methods on VisoGender benchmark to quantify improvements and identify\ndominant source of bias. Our results show that CDA reduces the gender gap by 6%\nand DAUDoS by 3% but using only one-third of the data. Both methods also\nimprove the model's ability to correctly identify gender in images by 3%, with\nDAUDoS achieving this improvement using only almost one-third of training data.\nFrom our experiment's, we observed that CLIP's vision encoder is more biased\nwhereas PaliGemma2's text encoder is more biased. By identifying whether bias\nstems more from vision or text encoders, our work enables more targeted and\neffective bias mitigation strategies in future multi-modal systems.",
        "url": "http://arxiv.org/abs/2508.07432v1",
        "published_date": "2025-08-10T17:08:10+00:00",
        "updated_date": "2025-08-10T17:08:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Vivek Hruday Kavuri",
            "Vysishtya Karanam",
            "Venkata Jahnavi Venkamsetty",
            "Kriti Madumadukala",
            "Lakshmipathi Balaji Darur",
            "Ponnurangam Kumaraguru"
        ],
        "tldr": "This paper investigates and mitigates gender bias in Vision-Language Models by dissecting the contributions of vision and text encoders, finding that CLIP's vision encoder and PaliGemma2's text encoder are more biased, and introducing a new debiasing method called DAUDoS.",
        "tldr_zh": "本文研究并缓解了视觉语言模型中的性别偏见，通过剖析视觉和文本编码器的贡献，发现CLIP的视觉编码器和PaliGemma2的文本编码器偏见较大，并引入了一种新的去偏见方法DAUDoS。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots",
        "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain.",
        "url": "http://arxiv.org/abs/2508.07406v1",
        "published_date": "2025-08-10T16:07:23+00:00",
        "updated_date": "2025-08-10T16:07:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xiaobei Zhao",
            "Xingqi Lyu",
            "Xiang Li"
        ],
        "tldr": "This paper introduces a new Vision-and-Language Navigation (VLN) benchmark (A2A) and baseline (AgriVLN) specifically designed for agricultural robots, addressing the limitations of existing VLN methods in agricultural settings. They also propose a Subtask List (STL) instruction decomposition module to improve performance.",
        "tldr_zh": "该论文介绍了一个新的视觉语言导航（VLN）基准（A2A）和基线（AgriVLN），专门为农业机器人设计，解决了现有VLN方法在农业环境中的局限性。他们还提出了一个子任务列表（STL）指令分解模块来提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation",
        "summary": "The unified autoregressive (AR) model excels at multimodal understanding and\ngeneration, but its potential for customized image generation remains\nunderexplored. Existing customized generation methods rely on full fine-tuning\nor adapters, making them costly and prone to overfitting or catastrophic\nforgetting. In this paper, we propose \\textbf{CoAR}, a novel framework for\ninjecting subject concepts into the unified AR models while keeping all\npre-trained parameters completely frozen. CoAR learns effective, specific\nsubject representations with only a minimal number of parameters using a\nLayerwise Multimodal Context Learning strategy. To address overfitting and\nlanguage drift, we further introduce regularization that preserves the\npre-trained distribution and anchors context tokens to improve subject fidelity\nand re-contextualization. Additionally, CoAR supports training-free subject\ncustomization in a user-provided style. Experiments demonstrate that CoAR\nachieves superior performance on both subject-driven personalization and style\npersonalization, while delivering significant gains in computational and memory\nefficiency. Notably, CoAR tunes less than \\textbf{0.05\\%} of the parameters\nwhile achieving competitive performance compared to recent Proxy-Tuning. Code:\nhttps://github.com/KZF-kzf/CoAR",
        "url": "http://arxiv.org/abs/2508.07341v1",
        "published_date": "2025-08-10T13:36:39+00:00",
        "updated_date": "2025-08-10T13:36:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fangtai Wu",
            "Mushui Liu",
            "Weijie He",
            "Wanggui He",
            "Hao Jiang",
            "Zhao Wang",
            "Yunlong Yu"
        ],
        "tldr": "The paper introduces CoAR, a parameter-efficient method for personalized text-to-image generation using autoregressive models, achieving competitive performance with minimal tuning and support for style customization.",
        "tldr_zh": "该论文介绍了CoAR，一种参数高效的个性化文本到图像生成方法，它使用自回归模型，以最少的微调实现了具有竞争力的性能，并支持风格定制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos",
        "summary": "Vision-language alignment in video must address the complexity of language,\nevolving interacting entities, their action chains, and semantic gaps between\nlanguage and vision. This work introduces Planner-Refiner, a framework to\novercome these challenges. Planner-Refiner bridges the semantic gap by\niteratively refining visual elements' space-time representation, guided by\nlanguage until semantic gaps are minimal. A Planner module schedules language\nguidance by decomposing complex linguistic prompts into short sentence chains.\nThe Refiner processes each short sentence, a noun-phrase and verb-phrase pair,\nto direct visual tokens' self-attention across space then time, achieving\nefficient single-step refinement. A recurrent system chains these steps,\nmaintaining refined visual token representations. The final representation\nfeeds into task-specific heads for alignment generation. We demonstrate\nPlanner-Refiner's effectiveness on two video-language alignment tasks:\nReferring Video Object Segmentation and Temporal Grounding with varying\nlanguage complexity. We further introduce a new MeViS-X benchmark to assess\nmodels' capability with long queries. Superior performance versus\nstate-of-the-art methods on these benchmarks shows the approach's potential,\nespecially for complex prompts.",
        "url": "http://arxiv.org/abs/2508.07330v1",
        "published_date": "2025-08-10T13:03:40+00:00",
        "updated_date": "2025-08-10T13:03:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tuyen Tran",
            "Thao Minh Le",
            "Quang-Hung Le",
            "Truyen Tran"
        ],
        "tldr": "The paper introduces Planner-Refiner, a framework for improved video-language alignment that refines visual elements' space-time representation based on decomposed language prompts, demonstrating superior performance on video-language tasks and a new benchmark for long queries.",
        "tldr_zh": "该论文介绍了 Planner-Refiner，一种用于改进视频-语言对齐的框架，它基于分解的语言提示细化视觉元素的时空表示，并在视频-语言任务和用于长查询的新基准测试中表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM",
        "summary": "Personalizing Vision-Language Models (VLMs) to transform them into daily\nassistants has emerged as a trending research direction. However, leading\ncompanies like OpenAI continue to increase model size and develop complex\ndesigns such as the chain of thought (CoT). While large VLMs are proficient in\ncomplex multi-modal understanding, their high training costs and limited access\nvia paid APIs restrict direct personalization. Conversely, small VLMs are\neasily personalized and freely available, but they lack sufficient reasoning\ncapabilities. Inspired by this, we propose a novel collaborative framework\nnamed Small-Large Collaboration (SLC) for large VLM personalization, where the\nsmall VLM is responsible for generating personalized information, while the\nlarge model integrates this personalized information to deliver accurate\nresponses. To effectively incorporate personalized information, we develop a\ntest-time reflection strategy, preventing the potential hallucination of the\nsmall VLM. Since SLC only needs to train a meta personalized small VLM for the\nlarge VLMs, the overall process is training-efficient. To the best of our\nknowledge, this is the first training-efficient framework that supports both\nopen-source and closed-source large VLMs, enabling broader real-world\npersonalized applications. We conduct thorough experiments across various\nbenchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC\nframework. The code will be released at https://github.com/Hhankyangg/SLC.",
        "url": "http://arxiv.org/abs/2508.07260v1",
        "published_date": "2025-08-10T09:24:31+00:00",
        "updated_date": "2025-08-10T09:24:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sihan Yang",
            "Huitong Ji",
            "Shaolin Lu",
            "Jiayi Chen",
            "Binxiao Xu",
            "Ming Lu",
            "Yuanxing Zhang",
            "Wenhui Dong",
            "Wentao Zhang"
        ],
        "tldr": "The paper proposes a Small-Large Collaboration (SLC) framework for personalizing large VLMs by leveraging a meta-personalized small VLM to provide personalized information, which is then integrated by the large VLM, achieving training efficiency and broader real-world applications.",
        "tldr_zh": "该论文提出了一种小型-大型协作（SLC）框架，通过利用元个性化的小型VLM来提供个性化信息，从而实现大型VLM的个性化。大型VLM整合这些信息，实现了训练效率并扩展了实际应用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LET-US: Long Event-Text Understanding of Scenes",
        "summary": "Event cameras output event streams as sparse, asynchronous data with\nmicrosecond-level temporal resolution, enabling visual perception with low\nlatency and a high dynamic range. While existing Multimodal Large Language\nModels (MLLMs) have achieved significant success in understanding and analyzing\nRGB video content, they either fail to interpret event streams effectively or\nremain constrained to very short sequences. In this paper, we introduce LET-US,\na framework for long event-stream--text comprehension that employs an adaptive\ncompression mechanism to reduce the volume of input events while preserving\ncritical visual details. LET-US thus establishes a new frontier in cross-modal\ninferential understanding over extended event sequences. To bridge the\nsubstantial modality gap between event streams and textual representations, we\nadopt a two-stage optimization paradigm that progressively equips our model\nwith the capacity to interpret event-based scenes. To handle the voluminous\ntemporal information inherent in long event streams, we leverage text-guided\ncross-modal queries for feature reduction, augmented by hierarchical clustering\nand similarity computation to distill the most representative event features.\nMoreover, we curate and construct a large-scale event-text aligned dataset to\ntrain our model, achieving tighter alignment of event features within the LLM\nembedding space. We also develop a comprehensive benchmark covering a diverse\nset of tasks -- reasoning, captioning, classification, temporal localization\nand moment retrieval. Experimental results demonstrate that LET-US outperforms\nprior state-of-the-art MLLMs in both descriptive accuracy and semantic\ncomprehension on long-duration event streams. All datasets, codes, and models\nwill be publicly available.",
        "url": "http://arxiv.org/abs/2508.07401v1",
        "published_date": "2025-08-10T16:02:41+00:00",
        "updated_date": "2025-08-10T16:02:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Chen",
            "Xingyu Chen",
            "Shaoan Wang",
            "Shihan Kong",
            "Junzhi Yu"
        ],
        "tldr": "The paper introduces LET-US, a framework for long event-stream--text comprehension using an adaptive compression mechanism and a large-scale event-text aligned dataset, outperforming existing MLLMs on various tasks related to long-duration event streams.",
        "tldr_zh": "该论文介绍了LET-US，一个用于长事件流-文本理解的框架，它使用自适应压缩机制和一个大规模的事件-文本对齐数据集，并在与长时事件流相关的各种任务上优于现有的MLLM。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning",
        "summary": "Image captioning aims to generate natural language descriptions for input\nimages in an open-form manner. To accurately generate descriptions related to\nthe image, a critical step in image captioning is to identify objects and\nunderstand their relations within the image. Modern approaches typically\ncapitalize on object detectors or combine detectors with Graph Convolutional\nNetwork (GCN). However, these models suffer from redundant detection\ninformation, difficulty in GCN construction, and high training costs. To\naddress these issues, a Retrieval-based Objects and Relations Prompt for Image\nCaptioning (RORPCap) is proposed, inspired by the fact that image-text\nretrieval can provide rich semantic information for input images. RORPCap\nemploys an Objects and relations Extraction Model to extract object and\nrelation words from the image. These words are then incorporate into predefined\nprompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping\nnetwork is designed to quickly map image embeddings extracted by CLIP to\nvisual-text embeddings. Finally, the resulting prompt embeddings and\nvisual-text embeddings are concatenated to form textual-enriched feature\nembeddings, which are fed into a GPT-2 model for caption generation. Extensive\nexperiments conducted on the widely used MS-COCO dataset show that the RORPCap\nrequires only 2.6 hours under cross-entropy loss training, achieving 120.5%\nCIDEr score and 22.0% SPICE score on the \"Karpathy\" test split. RORPCap\nachieves comparable performance metrics to detector-based and GCN-based models\nwith the shortest training time and demonstrates its potential as an\nalternative for image captioning.",
        "url": "http://arxiv.org/abs/2508.07318v1",
        "published_date": "2025-08-10T12:27:27+00:00",
        "updated_date": "2025-08-10T12:27:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinjing Gu",
            "Tianbao Qin",
            "Yuanyuan Pu",
            "Zhengpeng Zhao"
        ],
        "tldr": "The paper proposes RORPCap, a retrieval-based approach for image captioning that uses objects and relations extracted from images as prompts, achieving comparable performance to detector-based and GCN-based models with significantly reduced training time.",
        "tldr_zh": "该论文提出了RORPCap，一种基于检索的图像字幕方法，它使用从图像中提取的对象和关系作为提示，在显著减少训练时间的情况下，实现了与基于检测器和基于GCN的模型相当的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding",
        "summary": "Understanding multi-page documents poses a significant challenge for\nmultimodal large language models (MLLMs), as it requires fine-grained visual\ncomprehension and multi-hop reasoning across pages. While prior work has\nexplored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,\nits application to multi-page document understanding remains underexplored. In\nthis paper, we introduce DocR1, an MLLM trained with a novel RL framework,\nEvidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware\nreward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the\nmodel to first retrieve relevant pages before generating answers. This training\nparadigm enables us to build high-quality models with limited supervision. To\nsupport this, we design a two-stage annotation pipeline and a curriculum\nlearning strategy, based on which we construct two datasets: EviBench, a\nhigh-quality training set with 4.8k examples, and ArxivFullQA, an evaluation\nbenchmark with 8.6k QA pairs based on scientific papers. Extensive experiments\nacross a wide range of benchmarks demonstrate that DocR1 achieves\nstate-of-the-art performance on multi-page tasks, while consistently\nmaintaining strong results on single-page benchmarks.",
        "url": "http://arxiv.org/abs/2508.07313v1",
        "published_date": "2025-08-10T12:03:45+00:00",
        "updated_date": "2025-08-10T12:03:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyu Xiong",
            "Yonghui Wang",
            "Weichao Zhao",
            "Chenyu Liu",
            "Bing Yin",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "tldr": "The paper introduces DocR1, an MLLM trained with a novel Reinforcement Learning framework (EviGRPO) for multi-page document understanding, achieving state-of-the-art results by incorporating an evidence-aware reward mechanism for coarse-to-fine reasoning.",
        "tldr_zh": "该论文介绍了DocR1，一种通过新型强化学习框架（EviGRPO）训练的多模态大型语言模型，用于理解多页文档。它通过结合证据感知奖励机制进行粗到精的推理，实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MobileViCLIP: An Efficient Video-Text Model for Mobile Devices",
        "summary": "Efficient lightweight neural networks are with increasing attention due to\ntheir faster reasoning speed and easier deployment on mobile devices. However,\nexisting video pre-trained models still focus on the common ViT architecture\nwith high latency, and few works attempt to build efficient architecture on\nmobile devices. This paper bridges this gap by introducing temporal structural\nreparameterization into an efficient image-text model and training it on a\nlarge-scale high-quality video-text dataset, resulting in an efficient\nvideo-text model that can run on mobile devices with strong zero-shot\nclassification and retrieval capabilities, termed as MobileViCLIP. In\nparticular, in terms of inference speed on mobile devices, our\nMobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster\nthan InternVideo2-S14. In terms of zero-shot retrieval performance, our\nMobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains\n6.9\\% better than InternVideo2-S14 on MSR-VTT. The code is available at\nhttps://github.com/MCG-NJU/MobileViCLIP.",
        "url": "http://arxiv.org/abs/2508.07312v1",
        "published_date": "2025-08-10T12:01:58+00:00",
        "updated_date": "2025-08-10T12:01:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Min Yang",
            "Zihan Jia",
            "Zhilin Dai",
            "Sheng Guo",
            "Limin Wang"
        ],
        "tldr": "The paper introduces MobileViCLIP, an efficient video-text model designed for mobile devices, achieving significant speed improvements and competitive zero-shot performance compared to larger models. It uses temporal structural reparameterization and is trained on a large-scale video-text dataset.",
        "tldr_zh": "该论文介绍了 MobileViCLIP，一种专为移动设备设计的高效视频文本模型，与大型模型相比，在速度上实现了显着提升，并具有竞争力的零样本性能。它使用时间结构重参数化，并在大型视频文本数据集上进行训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark",
        "summary": "Continual learning aims to equip AI systems with the ability to continuously\nacquire and adapt to new knowledge without forgetting previously learned\ninformation, similar to human learning. While traditional continual learning\nmethods focusing on unimodal tasks have achieved notable success, the emergence\nof Multimodal Large Language Models has brought increasing attention to\nMultimodal Continual Learning tasks involving multiple modalities, such as\nvision and language. In this setting, models are expected to not only mitigate\ncatastrophic forgetting but also handle the challenges posed by cross-modal\ninteractions and coordination. To facilitate research in this direction, we\nintroduce MCITlib, a comprehensive and constantly evolving code library for\ncontinual instruction tuning of Multimodal Large Language Models. In MCITlib,\nwe have currently implemented 8 representative algorithms for Multimodal\nContinual Instruction Tuning and systematically evaluated them on 2 carefully\nselected benchmarks. MCITlib will be continuously updated to reflect advances\nin the Multimodal Continual Learning field. The codebase is released at\nhttps://github.com/Ghy0501/MCITlib.",
        "url": "http://arxiv.org/abs/2508.07307v1",
        "published_date": "2025-08-10T11:42:36+00:00",
        "updated_date": "2025-08-10T11:42:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haiyang Guo",
            "Fei Zhu",
            "Hongbo Zhao",
            "Fanhu Zeng",
            "Wenzhuo Liu",
            "Shijie Ma",
            "Da-Han Wang",
            "Xu-Yao Zhang"
        ],
        "tldr": "The paper introduces MCITlib, a code library and benchmark for multimodal continual instruction tuning of VLMs, aiming to facilitate research in this area.",
        "tldr_zh": "该论文介绍了MCITlib，一个用于VLM多模态持续指令调整的代码库和基准，旨在促进该领域的研究。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]