[
    {
        "title": "F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search",
        "summary": "The proliferation of digital food content has intensified the need for robust\nand accurate systems capable of fine-grained visual understanding and\nretrieval. In this work, we address the challenging task of food image-to-text\nmatching, a critical component in applications such as dietary monitoring,\nsmart kitchens, and restaurant automation. We propose F4-ITS: Fine-grained\nFeature Fusion for Food Image-Text Search, a training-free, vision-language\nmodel (VLM)-guided framework that significantly improves retrieval performance\nthrough enhanced multi-modal feature representations. Our approach introduces\ntwo key contributions: (1) a uni-directional(and bi-directional) multi-modal\nfusion strategy that combines image embeddings with VLM-generated textual\ndescriptions to improve query expressiveness, and (2) a novel feature-based\nre-ranking mechanism for top-k retrieval, leveraging predicted food ingredients\nto refine results and boost precision. Leveraging open-source image-text\nencoders, we demonstrate substantial gains over standard baselines - achieving\n~10% and ~7.7% improvements in top-1 retrieval under dense and sparse caption\nscenarios, and a ~28.6% gain in top-k ingredient-level retrieval. Additionally,\nwe show that smaller models (e.g., ViT-B/32) can match or outperform larger\ncounterparts (e.g., ViT-H, ViT-G, ViT-bigG) when augmented with textual fusion,\nhighlighting the effectiveness of our method in resource-constrained settings.\nCode and test datasets will be made publicly available at:\nhttps://github.com/mailcorahul/f4-its",
        "url": "http://arxiv.org/abs/2508.17037v1",
        "published_date": "2025-08-23T14:36:31+00:00",
        "updated_date": "2025-08-23T14:36:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Raghul Asokan"
        ],
        "tldr": "The paper introduces F4-ITS, a training-free, VLM-guided framework for food image-text matching that uses fine-grained feature fusion and re-ranking based on predicted ingredients to improve retrieval performance, especially in resource-constrained settings.",
        "tldr_zh": "该论文介绍了一种名为F4-ITS的无需训练的VLM引导框架，用于食物图像-文本匹配。该框架通过细粒度的特征融合和基于预测成分的重排序来提升检索性能，尤其是在资源受限的环境中。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WebSight: A Vision-First Architecture for Robust Web Agents",
        "summary": "We introduce WebSight, a vision-based autonomous web agent, designed to\ninteract with web environments purely through visual perception, eliminating\ndependence on HTML or DOM-based inputs. Central to our approach we introduce\nour new model, WebSight-7B, a fine-tuned vision-language model optimized for UI\nelement interaction, trained using LoRA on a web-focused subset of the\nWave-UI-25K dataset. WebSight integrates this model into a modular multi-agent\narchitecture, comprising planning, reasoning, vision-action, and verification\nagents, coordinated through an episodic memory mechanism.\n  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks\nbenchmark, outperforming several larger generalist models while maintaining\nlower latency. The full WebSight agent achieves a 68.0% success rate on the\nWebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and\nHCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly\n97.14% of the time, indicating high precision. Together, WebSight and\nWebSight-7B establish a new standard for interpretable, robust, and efficient\nvisual web navigation.",
        "url": "http://arxiv.org/abs/2508.16987v1",
        "published_date": "2025-08-23T11:02:59+00:00",
        "updated_date": "2025-08-23T11:02:59+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Tanvir Bhathal",
            "Asanshay Gupta"
        ],
        "tldr": "The paper introduces WebSight, a vision-based web agent architecture with a fine-tuned VLM (WebSight-7B) that outperforms existing systems on web navigation benchmarks, demonstrating robust and efficient visual web interaction.",
        "tldr_zh": "该论文介绍了WebSight，一个基于视觉的Web代理架构，带有一个微调的VLM (WebSight-7B)， 在Web导航基准测试中优于现有系统，展示了强大而高效的可视化Web交互。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis",
        "summary": "Evaluating human actions with clear and detailed feedback is important in\nareas such as sports, healthcare, and robotics, where decisions rely not only\non final outcomes but also on interpretable reasoning. However, most existing\nmethods provide only a final score without explanation or detailed analysis,\nlimiting their practical applicability. To address this, we introduce\nHieroAction, a vision-language model that delivers accurate and structured\nassessments of human actions. HieroAction builds on two key ideas: (1) Stepwise\nAction Reasoning, a tailored chain of thought process designed specifically for\naction assessment, which guides the model to evaluate actions step by step,\nfrom overall recognition through sub action analysis to final scoring, thus\nenhancing interpretability and structured understanding; and (2) Hierarchical\nPolicy Learning, a reinforcement learning strategy that enables the model to\nlearn fine grained sub action dynamics and align them with high level action\nquality, thereby improving scoring precision. The reasoning pathway structures\nthe evaluation process, while policy learning refines each stage through reward\nbased optimization. Their integration ensures accurate and interpretable\nassessments, as demonstrated by superior performance across multiple benchmark\ndatasets. Code will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2508.16942v1",
        "published_date": "2025-08-23T08:19:27+00:00",
        "updated_date": "2025-08-23T08:19:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junhao Wu",
            "Xiuer Gu",
            "Zhiying Li",
            "Yeying Jin",
            "Yunfeng Diao",
            "Zhiyu Li",
            "Zhenbo Song",
            "Xiaomei Zhang",
            "Zhaoxin Fan"
        ],
        "tldr": "HieroAction is a vision-language model that provides accurate and interpretable assessments of human actions by using stepwise action reasoning and hierarchical policy learning.",
        "tldr_zh": "HieroAction是一种视觉语言模型，通过逐步动作推理和分层策略学习，提供对人类动作的准确且可解释的评估。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Align 3D Representation and Text Embedding for 3D Content Personalization",
        "summary": "Recent advances in NeRF and 3DGS have significantly enhanced the efficiency\nand quality of 3D content synthesis. However, efficient personalization of\ngenerated 3D content remains a critical challenge. Current 3D personalization\napproaches predominantly rely on knowledge distillation-based methods, which\nrequire computationally expensive retraining procedures. To address this\nchallenge, we propose \\textbf{Invert3D}, a novel framework for convenient 3D\ncontent personalization. Nowadays, vision-language models such as CLIP enable\ndirect image personalization through aligned vision-text embedding spaces.\nHowever, the inherent structural differences between 3D content and 2D images\npreclude direct application of these techniques to 3D personalization. Our\napproach bridges this gap by establishing alignment between 3D representations\nand text embedding spaces. Specifically, we develop a camera-conditioned\n3D-to-text inverse mechanism that projects 3D contents into a 3D embedding\naligned with text embeddings. This alignment enables efficient manipulation and\npersonalization of 3D content through natural language prompts, eliminating the\nneed for computationally retraining procedures. Extensive experiments\ndemonstrate that Invert3D achieves effective personalization of 3D content. Our\nwork is available at: https://github.com/qsong2001/Invert3D.",
        "url": "http://arxiv.org/abs/2508.16932v1",
        "published_date": "2025-08-23T07:43:26+00:00",
        "updated_date": "2025-08-23T07:43:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qi Song",
            "Ziyuan Luo",
            "Ka Chun Cheung",
            "Simon See",
            "Renjie Wan"
        ],
        "tldr": "The paper introduces Invert3D, a novel framework that aligns 3D representations with text embeddings, enabling efficient 3D content personalization via natural language prompts without retraining.",
        "tldr_zh": "该论文介绍了 Invert3D，一种新的框架，通过将 3D 表示与文本嵌入对齐，实现通过自然语言提示高效地进行 3D 内容个性化，而无需重新训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Delta-SVD: Efficient Compression for Personalized Text-to-Image Models",
        "summary": "Personalized text-to-image models such as DreamBooth require fine-tuning\nlarge-scale diffusion backbones, resulting in significant storage overhead when\nmaintaining many subject-specific models. We present Delta-SVD, a post-hoc,\ntraining-free compression method that targets the parameter weights update\ninduced by DreamBooth fine-tuning. Our key observation is that these delta\nweights exhibit strong low-rank structure due to the sparse and localized\nnature of personalization. Delta-SVD first applies Singular Value Decomposition\n(SVD) to factorize the weight deltas, followed by an energy-based rank\ntruncation strategy to balance compression efficiency and reconstruction\nfidelity. The resulting compressed models are fully plug-and-play and can be\nre-constructed on-the-fly during inference. Notably, the proposed approach is\nsimple, efficient, and preserves the original model architecture. Experiments\non a multiple subject dataset demonstrate that Delta-SVD achieves substantial\ncompression with negligible loss in generation quality measured by CLIP score,\nSSIM and FID. Our method enables scalable and efficient deployment of\npersonalized diffusion models, making it a practical solution for real-world\napplications that require storing and deploying large-scale subject\ncustomizations.",
        "url": "http://arxiv.org/abs/2508.16863v1",
        "published_date": "2025-08-23T01:21:46+00:00",
        "updated_date": "2025-08-23T01:21:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tangyuan Zhang",
            "Shangyu Chen",
            "Qixiang Chen",
            "Jianfei Cai"
        ],
        "tldr": "Delta-SVD is a post-hoc compression method for personalized text-to-image models that leverages SVD on delta weights to achieve significant compression with minimal quality loss, facilitating efficient deployment of personalized diffusion models.",
        "tldr_zh": "Delta-SVD是一种用于个性化文本到图像模型的后处理压缩方法，它利用SVD对增量权重进行处理，以实现显著的压缩效果，同时保持最小的质量损失，从而方便个性化扩散模型的高效部署。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes",
        "summary": "3D object detection plays a crucial role in autonomous systems, yet existing\nmethods are limited by closed-set assumptions and struggle to recognize novel\nobjects and their attributes in real-world scenarios. We propose OVODA, a novel\nframework enabling both open-vocabulary 3D object and attribute detection with\nno need to know the novel class anchor size. OVODA uses foundation models to\nbridge the semantic gap between 3D features and texts while jointly detecting\nattributes, e.g., spatial relationships, motion states, etc. To facilitate such\nresearch direction, we propose OVAD, a new dataset that supplements existing 3D\nobject detection benchmarks with comprehensive attribute annotations. OVODA\nincorporates several key innovations, including foundation model feature\nconcatenation, prompt tuning strategies, and specialized techniques for\nattribute detection, including perspective-specified prompts and horizontal\nflip augmentation. Our results on both the nuScenes and Argoverse 2 datasets\nshow that under the condition of no given anchor sizes of novel classes, OVODA\noutperforms the state-of-the-art methods in open-vocabulary 3D object detection\nwhile successfully recognizing object attributes. Our OVAD dataset is released\nhere: https://doi.org/10.5281/zenodo.16904069 .",
        "url": "http://arxiv.org/abs/2508.16812v1",
        "published_date": "2025-08-22T22:02:49+00:00",
        "updated_date": "2025-08-22T22:02:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinhao Xiang",
            "Kuan-Chuan Peng",
            "Suhas Lohit",
            "Michael J. Jones",
            "Jiawei Zhang"
        ],
        "tldr": "The paper introduces OVODA, a new framework and dataset (OVAD) for open-vocabulary 3D object detection with attributes, leveraging foundation models to overcome limitations in existing closed-set methods.",
        "tldr_zh": "该论文介绍了一个名为OVODA的新框架和一个名为OVAD的数据集，用于开放词汇的3D物体检测，并利用基础模型来克服现有封闭集方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation",
        "summary": "We present WebMMU, a multilingual benchmark that evaluates three core web\ntasks: (1) website visual question answering, (2) code editing involving\nHTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks\nthat treat these tasks separately, WebMMU unifies them using expert-annotated,\nreal-world web data to assess models' abilities in complex multi-step\nreasoning, precise element grounding, and functional UI comprehension and\ncoding. Our evaluation shows that while multimodal large language models\n(MLLMs) perform well on basic information extraction, they struggle with\nreasoning and grounding, editing code to preserve functionality, and generating\ndesign-to-code that maintains hierarchy and supports multilingual content.\nThese findings reveal key limitations in current MLLMs and underscore the need\nfor improved multimodal and cross-lingual reasoning to build future web agents\ncapable of automating diverse web development tasks.",
        "url": "http://arxiv.org/abs/2508.16763v1",
        "published_date": "2025-08-22T19:41:02+00:00",
        "updated_date": "2025-08-22T19:41:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rabiul Awal",
            "Mahsa Massoud",
            "Aarash Feizi",
            "Zichao Li",
            "Suyuchen Wang",
            "Christopher Pal",
            "Aishwarya Agrawal",
            "David Vazquez",
            "Siva Reddy",
            "Juan A. Rodriguez",
            "Perouz Taslakian",
            "Spandana Gella",
            "Sai Rajeswar"
        ],
        "tldr": "The paper introduces WebMMU, a new multilingual benchmark for evaluating multimodal models on web-related tasks like VQA, code editing, and mockup-to-code generation, highlighting current MLLMs' limitations in reasoning, grounding, and code generation, especially in multilingual contexts.",
        "tldr_zh": "本文介绍了WebMMU，一个新的多语言基准，用于评估多模态模型在网页相关任务上的性能，例如VQA、代码编辑和模型到代码生成。该基准突出了当前MLLM在推理、定位和代码生成方面的局限性，尤其是在多语言环境中。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding",
        "summary": "Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) have\nachieved remarkable progress in natural language processing and multimodal\nunderstanding. Despite their impressive generalization capabilities, current\nLVLMs often exhibit insufficient robustness, proneness to hallucination, and\nreasoning errors in complex real-world scenarios, particularly when precise\nimage region localization and fine-grained visual reasoning are required. To\naddress these limitations, we propose the Hierarchical Contextual Grounding\nLVLM (HCG-LVLM), a novel architecture that mimics human coarse-to-fine\ncognitive processing. HCG-LVLM employs a two-layered approach: a Global\nContextual Perception layer for initial broad understanding and a Fine-grained\nLocal Grounding layer. The latter incorporates a Local Detail Enhancement\nModule to extract high-resolution features and a Semantic Consistency Validator\nto ensure accurate, hallucination-free visual-language alignment. Through an\nadaptive fusion mechanism, information from both layers is integrated for\nrobust and precise outputs. Extensive experiments on challenging datasets,\nincluding GQA, A-OKVQA for fine-grained VQA, and RefCOCO/+/g for Referring\nExpression Comprehension, demonstrate that HCG-LVLM consistently outperforms\nstate-of-the-art models such as Flamingo, BLIP-2, and MiniGPT-4. Our model\nachieves superior accuracy and significantly reduces hallucination, validating\nthe effectiveness of its hierarchical design in enhancing fine-grained\nvisual-language understanding and precise grounding capabilities.",
        "url": "http://arxiv.org/abs/2508.16974v1",
        "published_date": "2025-08-23T09:57:52+00:00",
        "updated_date": "2025-08-23T09:57:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Leilei Guo",
            "Antonio Carlos Rivera",
            "Peiyu Tang",
            "Haoxuan Ren",
            "Zheyu Song"
        ],
        "tldr": "The paper introduces HCG-LVLM, a hierarchical LVLM architecture designed for robust and precise visual-language understanding, particularly in fine-grained scenarios, demonstrating superior performance on several challenging datasets.",
        "tldr_zh": "该论文介绍了HCG-LVLM，一种分层LVLM架构，旨在实现鲁棒和精确的视觉-语言理解，尤其是在细粒度场景中，并在几个具有挑战性的数据集上展示了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams",
        "summary": "Large Language Models (LLMs) and their multimodal variants (LVLMs) hold\nimmense promise for scientific and engineering applications, particularly in\nprocessing visual information like scientific diagrams. However, their\npractical deployment is hindered by a critical lack of robustness to common\nvisual perturbations such as noise, blur, and occlusions, which are prevalent\nin real-world scientific documents. Existing evaluation benchmarks largely\noverlook this challenge, leaving the robust reasoning capabilities of LVLMs on\nvisually degraded scientific diagrams underexplored. To address this, we\nintroduce the Robust Diagram Reasoning (RDR) framework, a novel approach\ndesigned to enhance and rigorously evaluate LVLMs' performance under such\nconditions. At its core, RDR employs an Adaptive Multi-View & Consistency\nVerification (AMCV) mechanism, which involves generating multiple perturbed\nversions of a diagram, performing parallel inference, and then applying a\nconsistency-based self-correction loop. We also propose two new metrics,\nPerturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC),\nto quantify robustness. Furthermore, we construct SciDiagram-Robust, the first\nlarge-scale scientific diagram question-answering dataset specifically\naugmented with diverse, programmatically generated visual perturbations. Our\nextensive experiments demonstrate that even state-of-the-art closed-source\nLVLMs like GPT-4V exhibit significant performance degradation when faced with\nperturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).",
        "url": "http://arxiv.org/abs/2508.16972v1",
        "published_date": "2025-08-23T09:50:58+00:00",
        "updated_date": "2025-08-23T09:50:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minghao Zhou",
            "Rafael Souza",
            "Yaqian Hu",
            "Luming Che"
        ],
        "tldr": "The paper introduces a framework (RDR) and dataset (SciDiagram-Robust) to improve and evaluate the robustness of LVLMs on visually perturbed scientific diagrams, revealing performance degradation in models like GPT-4V.",
        "tldr_zh": "该论文介绍了一个框架 (RDR) 和数据集 (SciDiagram-Robust)，用于提升和评估 LVLMs 在视觉扰动科学图上的鲁棒性，揭示了像 GPT-4V 这样的模型在视觉扰动下的性能下降。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Do Multimodal LLMs See Sentiment?",
        "summary": "Understanding how visual content communicates sentiment is critical in an era\nwhere online interaction is increasingly dominated by this kind of media on\nsocial platforms. However, this remains a challenging problem, as sentiment\nperception is closely tied to complex, scene-level semantics. In this paper, we\npropose an original framework, MLLMsent, to investigate the sentiment reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) through three\nperspectives: (1) using those MLLMs for direct sentiment classification from\nimages; (2) associating them with pre-trained LLMs for sentiment analysis on\nautomatically generated image descriptions; and (3) fine-tuning the LLMs on\nsentiment-labeled image descriptions. Experiments on a recent and established\nbenchmark demonstrate that our proposal, particularly the fine-tuned approach,\nachieves state-of-the-art results outperforming Lexicon-, CNN-, and\nTransformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively,\nacross different levels of evaluators' agreement and sentiment polarity\ncategories. Remarkably, in a cross-dataset test, without any training on these\nnew data, our model still outperforms, by up to 8.26%, the best runner-up,\nwhich has been trained directly on them. These results highlight the potential\nof the proposed visual reasoning scheme for advancing affective computing,\nwhile also establishing new benchmarks for future research.",
        "url": "http://arxiv.org/abs/2508.16873v1",
        "published_date": "2025-08-23T02:11:46+00:00",
        "updated_date": "2025-08-23T02:11:46+00:00",
        "categories": [
            "cs.CV",
            "cs.SI"
        ],
        "authors": [
            "Neemias B. da Silva",
            "John Harrison",
            "Rodrigo Minetto",
            "Myriam R. Delgado",
            "Bogdan T. Nassu",
            "Thiago H. Silva"
        ],
        "tldr": "This paper investigates the sentiment reasoning capabilities of Multimodal Large Language Models (MLLMs) through direct classification, image description analysis, and fine-tuning, achieving state-of-the-art results on sentiment-labeled image benchmarks.",
        "tldr_zh": "本文研究了多模态大型语言模型 (MLLM) 的情感推理能力，通过直接分类、图像描述分析和微调，在情感标记图像基准测试中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]