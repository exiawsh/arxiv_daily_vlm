[
    {
        "title": "CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions",
        "summary": "We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.",
        "url": "http://arxiv.org/abs/2512.01095v1",
        "published_date": "2025-11-30T21:28:41+00:00",
        "updated_date": "2025-11-30T21:28:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Simon Kohaut",
            "Daniel Ochs",
            "Shun Zhang",
            "Benedict Flade",
            "Julian Eggert",
            "Kristian Kersting",
            "Devendra Singh Dhami"
        ],
        "tldr": "The paper introduces CycliST, a new benchmark for evaluating VLMs on their ability to reason about cyclical state transitions in videos, revealing limitations in current models' temporal understanding and quantitative scene analysis.",
        "tldr_zh": "该论文介绍了CycliST，一个新的基准数据集，用于评估VLM在视频中推理循环状态转换的能力，揭示了当前模型在时间理解和定量场景分析方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models",
        "summary": "Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.",
        "url": "http://arxiv.org/abs/2512.01048v1",
        "published_date": "2025-11-30T19:36:46+00:00",
        "updated_date": "2025-11-30T19:36:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maya Varma",
            "Jean-Benoit Delbrouck",
            "Sophie Ostmeier",
            "Akshay Chaudhari",
            "Curtis Langlotz"
        ],
        "tldr": "The paper introduces TRoVe, an automated approach to discover error-inducing static feature biases in temporal vision-language models, demonstrating its effectiveness in identifying and mitigating these biases, leading to improved model performance.",
        "tldr_zh": "该论文介绍了一种名为 TRoVe 的自动化方法，用于发现时序视觉语言模型中导致错误的静态特征偏差。实验结果表明，TRoVe 可以有效识别和减轻这些偏差，从而提高模型性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency",
        "summary": "Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.",
        "url": "http://arxiv.org/abs/2512.01008v1",
        "published_date": "2025-11-30T18:02:14+00:00",
        "updated_date": "2025-11-30T18:02:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongbin Guo",
            "Jiahe Liu",
            "Wenyu Gao",
            "Yushan Li",
            "Chengzhi Li",
            "Ping Jian"
        ],
        "tldr": "LISA-3D is a two-stage framework that lifts language-image segmentation to 3D using multi-view consistency, improving language-to-3D accuracy with a geometry-aware LoRA approach and a frozen SAM-3D reconstructor.",
        "tldr_zh": "LISA-3D是一个两阶段框架，通过多视角一致性将语言-图像分割提升到3D，利用几何感知的LoRA方法和冻结的SAM-3D重建器提高了语言到3D的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]