[
    {
        "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
        "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.",
        "url": "http://arxiv.org/abs/2512.22047v1",
        "published_date": "2025-12-26T14:51:52+00:00",
        "updated_date": "2025-12-26T14:51:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanzhang Zhou",
            "Xu Zhang",
            "Panrong Tong",
            "Jianan Zhang",
            "Liangyu Chen",
            "Quyu Kong",
            "Chenglin Cai",
            "Chen Liu",
            "Yue Wang",
            "Jingren Zhou",
            "Steven Hoi"
        ],
        "tldr": "The paper introduces MAI-UI, a family of foundation GUI agents addressing key challenges in real-world deployment through a self-evolving data pipeline, device-cloud collaboration, and online RL. It achieves state-of-the-art performance on GUI grounding and mobile navigation benchmarks.",
        "tldr_zh": "该论文介绍了MAI-UI，一系列基础GUI代理，通过自进化数据管道、设备-云协作和在线RL，解决了现实部署中的关键挑战。 在GUI grounding和移动导航基准测试中，它实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception",
        "summary": "Multimodal Large Language Models (MLLMs) show strong potential for interpreting and interacting with complex, pixel-rich Graphical User Interface (GUI) environments. However, building agents that are both efficient for high-level tasks and precise for fine-grained interactions remains challenging. GUI agents must perform routine actions efficiently while also handling tasks that demand exact visual grounding, yet existing approaches struggle when accuracy depends on identifying specific interface elements. These MLLMs also remain large and cannot adapt their reasoning depth to the task at hand. In this work, we introduce iSHIFT: Implicit Slow-fast Hybrid Inference with Flexible Tokens, a lightweight agent that integrates latent thinking (implicit chain-of-thought) with a perception control module. iSHIFT enables an MLLM to switch between a slow mode, which leverages detailed visual grounding for high precision and a fast mode that uses global cues for efficiency. Special perception tokens guide attention to relevant screen regions, allowing the model to decide both how to reason and where to focus. Despite its compact 2.5B size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets.",
        "url": "http://arxiv.org/abs/2512.22009v1",
        "published_date": "2025-12-26T12:09:15+00:00",
        "updated_date": "2025-12-26T12:09:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sarthak Mehrotra",
            "Sairam V C Rebbapragada",
            "Mani Hemanth Reddy Bonthu",
            "Vineeth N Balasubramanian"
        ],
        "tldr": "The paper introduces iSHIFT, a lightweight (2.5B) GUI agent with a slow-fast inference mechanism and adaptive perception tokens, achieving SOTA performance on benchmark datasets while being efficient.",
        "tldr_zh": "该论文介绍了 iSHIFT，一个轻量级 (2.5B) 的 GUI 代理，具有慢速-快速推理机制和自适应感知令牌，在基准数据集上实现了 SOTA 性能，同时保持了效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
        "summary": "While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inherently limits their optimization potential. To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \\textbf{A}ctivate-\\textbf{L}ocate-\\textbf{E}dit \\textbf{A}dversarially paradigm. Specifically, we first construct an activation dataset that comprises grounded responses (positive samples attentively anchored in visual features) and hallucinatory responses (negative samples reflecting LLM prior bias and internal knowledge artifacts). Next, we identify critical hallucination-prone parameter clusters by analyzing differential hidden states of response pairs. Then, these clusters are fine-tuned using prompts injected with adversarial tuned prefixes that are optimized to maximize visual neglect, thereby forcing the model to prioritize visual evidence over inherent parametric biases. Evaluations on both generative and discriminative VLM tasks demonstrate the significant effectiveness of ALEAHallu in alleviating hallucinations. Our code is available at https://github.com/hujiayu1223/ALEAHallu.",
        "url": "http://arxiv.org/abs/2512.21999v1",
        "published_date": "2025-12-26T11:56:45+00:00",
        "updated_date": "2025-12-26T11:56:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jiayu Hu",
            "Beibei Li",
            "Jiangwei Xia",
            "Yanjun Qin",
            "Bing Ji",
            "Zhongshi He"
        ],
        "tldr": "This paper proposes an adversarial parametric editing framework, ALEAHallu, to mitigate hallucinations in VLMs by fine-tuning hallucination-prone parameters using adversarial prompts to force the model to prioritize visual evidence.",
        "tldr_zh": "该论文提出了一种对抗参数编辑框架ALEAHallu，通过使用对抗性提示微调容易产生幻觉的参数，从而缓解VLM中的幻觉问题，迫使模型优先考虑视觉证据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LVLM-Aided Alignment of Task-Specific Vision Models",
        "summary": "In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model's dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.",
        "url": "http://arxiv.org/abs/2512.21985v1",
        "published_date": "2025-12-26T11:11:25+00:00",
        "updated_date": "2025-12-26T11:11:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alexander Koebler",
            "Lukas Kuhn",
            "Ingo Thon",
            "Florian Buettner"
        ],
        "tldr": "This paper presents a method (LVLM-VA) to align task-specific vision models with human domain knowledge using a Large Vision Language Model, improving model reliability and reducing reliance on spurious correlations.",
        "tldr_zh": "本文提出了一种方法 (LVLM-VA)，利用大型视觉语言模型将特定任务的视觉模型与人类领域知识对齐，从而提高模型的可靠性并减少对虚假相关的依赖。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models",
        "summary": "Medical Multi-modal Large Language Models (MLLMs) have shown promising clinical performance. However, their sensitivity to real-world input perturbations, such as imaging artifacts and textual errors, critically undermines their clinical applicability. Systematic analysis of such noise impact on medical MLLMs remains largely unexplored. Furthermore, while several works have investigated the MLLMs' robustness in general domains, they primarily focus on text modality and rely on costly fine-tuning. They are inadequate to address the complex noise patterns and fulfill the strict safety standards in medicine. To bridge this gap, this work systematically analyzes the impact of various perturbations on medical MLLMs across both visual and textual modalities. Building on our findings, we introduce a training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' inherent denoising capabilities following the perceive-and-calibrate principle for cross-modal robustness enhancement. For the visual modality, we propose a Perturbation-aware Denoising Calibration (PDC) which leverages MLLMs' own vision encoder to identify noise patterns and perform prototype-guided feature calibration. For text denoising, we design a Self-instantiated Multi-agent System (SMS) that exploits the MLLMs' self-assessment capabilities to refine noisy text through a cooperative hierarchy of agents. We construct a benchmark containing 11 types of noise across both image and text modalities on 2 datasets. Experimental results demonstrate our method achieves the state-of-the-art performance across multiple modalities, showing potential to enhance MLLMs' robustness in real clinical scenarios.",
        "url": "http://arxiv.org/abs/2512.21964v1",
        "published_date": "2025-12-26T10:23:30+00:00",
        "updated_date": "2025-12-26T10:23:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dunyuan XU",
            "Xikai Yang",
            "Yaoqian Li",
            "Juzheng Miao",
            "Jinpeng Li",
            "Pheng-Ann Heng"
        ],
        "tldr": "This paper analyzes the robustness of medical MLLMs to real-world noise and proposes a training-free calibration framework (IMC) involving visual and textual denoising methods to enhance robustness without fine-tuning.",
        "tldr_zh": "该论文分析了医学多模态大型语言模型（MLLM）对真实世界噪声的鲁棒性，并提出了一种无需训练的校准框架（IMC），该框架包括视觉和文本去噪方法，旨在无需微调即可提高鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models",
        "summary": "Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM's last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.",
        "url": "http://arxiv.org/abs/2512.21860v1",
        "published_date": "2025-12-26T04:51:23+00:00",
        "updated_date": "2025-12-26T04:51:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masayuki Kawarada",
            "Kosuke Yamada",
            "Antonio Tejero-de-Pablos",
            "Naoto Inoue"
        ],
        "tldr": "The paper introduces DIOR, a training-free method leveraging LVLMs to generate conditional image embeddings by prompting the LVLM to describe an image with a single word related to a given condition. DIOR outperforms existing training-free and training-based baselines on conditional image similarity tasks.",
        "tldr_zh": "本文介绍了一种名为DIOR的免训练方法，该方法利用大型视觉语言模型(LVLM)，通过提示LVLM用一个与给定条件相关的词来描述图像，从而生成条件图像嵌入。DIOR在条件图像相似性任务上优于现有的免训练和基于训练的基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "url": "http://arxiv.org/abs/2512.21815v1",
        "published_date": "2025-12-26T01:01:25+00:00",
        "updated_date": "2025-12-26T01:01:25+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mengqi He",
            "Xinyu Tian",
            "Xin Shen",
            "Jinhong Ni",
            "Shu Zou",
            "Zhaoyuan Yang",
            "Jing Zhang"
        ],
        "tldr": "This paper introduces an entropy-guided adversarial attack (EGA) targeting vision-language models (VLMs), demonstrating that focusing perturbations on high-entropy tokens significantly degrades performance and exposes critical safety risks with improved transferability.",
        "tldr_zh": "本文介绍了一种针对视觉语言模型 (VLM) 的熵引导对抗攻击 (EGA)，表明将扰动集中在高熵 token 上可以显著降低性能，暴露关键安全风险，并提高可迁移性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models",
        "summary": "Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.",
        "url": "http://arxiv.org/abs/2512.21778v1",
        "published_date": "2025-12-25T20:31:36+00:00",
        "updated_date": "2025-12-25T20:31:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nimrod Berman",
            "Adam Botach",
            "Emanuel Ben-Baruch",
            "Shunit Haviv Hakimi",
            "Asaf Gendler",
            "Ilan Naiman",
            "Erez Yosef",
            "Igor Kviatkovsky"
        ],
        "tldr": "The paper introduces Scene-VLM, a fine-tuned vision-language model for video scene segmentation that leverages multimodal inputs and sequential reasoning to achieve state-of-the-art performance, also providing explainability through natural language rationales.",
        "tldr_zh": "该论文介绍了一种名为Scene-VLM的微调视觉语言模型，用于视频场景分割。该模型利用多模态输入和序列推理，实现了最先进的性能，并通过自然语言的理由提供了解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]