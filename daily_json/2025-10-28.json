[
    {
        "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
        "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.",
        "url": "http://arxiv.org/abs/2510.23538v1",
        "published_date": "2025-10-27T17:13:49+00:00",
        "updated_date": "2025-10-27T17:13:49+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.SE"
        ],
        "authors": [
            "Qiushi Sun",
            "Jingyang Gong",
            "Yang Liu",
            "Qiaosheng Chen",
            "Lei Li",
            "Kai Chen",
            "Qipeng Guo",
            "Ben Kao",
            "Fei Yuan"
        ],
        "tldr": "The paper introduces JanusCoder and JanusCoderV, VLM models trained on a newly created large-scale multimodal code corpus (JanusCode-800K) for generating code from text or visual inputs, demonstrating superior performance compared to existing and commercial models.",
        "tldr_zh": "该论文介绍了JanusCoder和JanusCoderV，这些VLM模型是在新建的大规模多模态代码语料库(JanusCode-800K)上训练的，用于从文本或视觉输入生成代码，并且表现出优于现有模型和商业模型的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
        "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
        "url": "http://arxiv.org/abs/2510.23095v1",
        "published_date": "2025-10-27T08:00:46+00:00",
        "updated_date": "2025-10-27T08:00:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jie Huang",
            "Xuejing Liu",
            "Sibo Song",
            "Ruibing Hou",
            "Hong Chang",
            "Junyang Lin",
            "Shuai Bai"
        ],
        "tldr": "This paper analyzes and improves multimodal Rotary Positional Embedding (RoPE) in vision-language models, proposing two simple and effective variants, MHRoPE and MRoPE-I, that outperform existing methods on diverse benchmarks.",
        "tldr_zh": "本文分析并改进了视觉语言模型中的多模态旋转位置嵌入（RoPE），提出了两种简单有效的变体MHRoPE和MRoPE-I，并在不同的基准测试中优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity",
        "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
        "url": "http://arxiv.org/abs/2510.23603v1",
        "published_date": "2025-10-27T17:59:32+00:00",
        "updated_date": "2025-10-27T17:59:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuqian Yuan",
            "Wenqiao Zhang",
            "Xin Li",
            "Shihao Wang",
            "Kehan Li",
            "Wentong Li",
            "Jun Xiao",
            "Lei Zhang",
            "Beng Chin Ooi"
        ],
        "tldr": "PixelRefer introduces a unified framework for fine-grained object referring in images and videos using a scale-adaptive object tokenizer and an efficient object-centric infusion module, achieving leading performance with fewer resources.",
        "tldr_zh": "PixelRefer 提出了一个统一的框架，用于图像和视频中细粒度的对象指代，它使用了一个尺度自适应对象标记器和一个高效的以对象为中心的融合模块，以更少的资源实现了领先的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
        "summary": "Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties.",
        "url": "http://arxiv.org/abs/2510.23576v1",
        "published_date": "2025-10-27T17:46:43+00:00",
        "updated_date": "2025-10-27T17:46:43+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Anqi Li",
            "Zhiyong Wang",
            "Jiazhao Zhang",
            "Minghan Li",
            "Yunpeng Qi",
            "Zhibo Chen",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "tldr": "The paper introduces UrbanVLA, a Vision-Language-Action framework for urban micromobility that aligns visual observations with noisy route waypoints, trained with a two-stage SFT and RFT pipeline to achieve scalable and robust navigation in real-world urban environments.",
        "tldr_zh": "本文介绍了一个名为UrbanVLA的视觉-语言-动作框架，用于城市微出行。该框架将视觉观察与嘈杂的路线航点对齐，并通过两阶段的SFT和RFT流程进行训练，以在真实的城市环境中实现可扩展且稳健的导航。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
        "summary": "Egocentric video reasoning centers on an unobservable agent behind the camera\nwho dynamically shapes the environment, requiring inference of hidden\nintentions and recognition of fine-grained interactions. This core challenge\nlimits current multimodal large language models MLLMs, which excel at visible\nevent reasoning but lack embodied, first-person understanding. To bridge this\ngap, we introduce EgoThinker, a novel framework that endows MLLMs with robust\negocentric reasoning capabilities through spatio-temporal chain-of-thought\nsupervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,\na large-scale egocentric QA dataset constructed from 13M diverse egocentric\nvideo clips. This dataset features multi-minute segments annotated with\ndetailed CoT rationales and dense hand-object grounding. Second, we employ SFT\non EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning\nRFT to further enhance spatio-temporal localization. Experimental results show\nthat EgoThinker outperforms existing methods across multiple egocentric\nbenchmarks, while achieving substantial improvements in fine-grained\nspatio-temporal localization tasks. Full code and data are released at\nhttps://github.com/InternRobotics/EgoThinker.",
        "url": "http://arxiv.org/abs/2510.23569v1",
        "published_date": "2025-10-27T17:38:17+00:00",
        "updated_date": "2025-10-27T17:38:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Baoqi Pei",
            "Yifei Huang",
            "Jilan Xu",
            "Yuping He",
            "Guo Chen",
            "Fei Wu",
            "Yu Qiao",
            "Jiangmiao Pang"
        ],
        "tldr": "The paper introduces EgoThinker, a framework for improving egocentric video reasoning in MLLMs through spatio-temporal chain-of-thought supervision and a large-scale dataset EgoRe-5M, demonstrating improved performance on egocentric benchmarks.",
        "tldr_zh": "该论文介绍了EgoThinker，一个通过时空链式思考监督和大规模数据集EgoRe-5M来提高MLLM中以自我为中心的视频推理能力的框架，并在以自我为中心的基准测试中展示了改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
        "summary": "This paper proposes FreeFuse, a novel training-free approach for\nmulti-subject text-to-image generation through automatic fusion of multiple\nsubject LoRAs. In contrast to existing methods that either focus on\npre-inference LoRA weight merging or rely on segmentation models and complex\ntechniques like noise blending to isolate LoRA outputs, our key insight is that\ncontext-aware dynamic subject masks can be automatically derived from\ncross-attention layer weights. Mathematical analysis shows that directly\napplying these masks to LoRA outputs during inference well approximates the\ncase where the subject LoRA is integrated into the diffusion model and used\nindividually for the masked region. FreeFuse demonstrates superior practicality\nand efficiency as it requires no additional training, no modification to LoRAs,\nno auxiliary models, and no user-defined prompt templates or region\nspecifications. Alternatively, it only requires users to provide the LoRA\nactivation words for seamless integration into standard workflows. Extensive\nexperiments validate that FreeFuse outperforms existing approaches in both\ngeneration quality and usability under the multi-subject generation tasks. The\nproject page is at https://future-item.github.io/FreeFuse/",
        "url": "http://arxiv.org/abs/2510.23515v1",
        "published_date": "2025-10-27T16:54:08+00:00",
        "updated_date": "2025-10-27T16:54:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaoli Liu",
            "Yao-Xiang Ding",
            "Kun Zhou"
        ],
        "tldr": "The paper introduces FreeFuse, a training-free method for multi-subject text-to-image generation by automatically fusing multiple LoRAs using context-aware masks derived from cross-attention, without requiring additional training or models.",
        "tldr_zh": "该论文介绍了FreeFuse，一种无需训练的多主体文本到图像生成方法，通过使用从交叉注意力导出的上下文感知掩码自动融合多个LoRA，而无需额外的训练或模型。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation",
        "summary": "Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher.",
        "url": "http://arxiv.org/abs/2510.23497v1",
        "published_date": "2025-10-27T16:32:12+00:00",
        "updated_date": "2025-10-27T16:32:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Walid Bousselham",
            "Hilde Kuehne",
            "Cordelia Schmid"
        ],
        "tldr": "The paper introduces VOLD, a framework that transfers reasoning capabilities from text-only LLMs to VLMs using on-policy distillation and reinforcement learning, achieving state-of-the-art performance on several VLM reasoning benchmarks. Cold-start alignment is crucial for effective transfer.",
        "tldr_zh": "该论文介绍了 VOLD，一个使用 on-policy distillation 和强化学习将纯文本 LLM 的推理能力转移到 VLM 的框架，在多个 VLM 推理基准测试中实现了最先进的性能。冷启动对齐对于有效的转移至关重要。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On the Faithfulness of Visual Thinking: Measurement and Enhancement",
        "summary": "Recent large vision-language models (LVLMs) can generate vision-text\nmultimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning\n(RFT). However, we observe that the visual information incorporated in MCoT is\noften inaccurate, though still yield correct answers, indicating a lack of\nfaithfulness in the MCoT reasoning process. We attribute this unfaithfulness to\nthe RL reward in RFT, which solely incentivizes the format of interleaved\nvision-text cues, ie, it encourages the model to incorporate visual information\ninto its text reasoning steps without considering the correctness of the visual\ninformation. In this paper, we first probe the faithfulness of MCoT by\nmeasuring how much the prediction changes when its visual and textual thoughts\nare intervened. Surprisingly, the model's predictions remain nearly unchanged\nunder visual intervention but change significantly under textual intervention,\nindicating that the visual evidence is largely ignored. To further analyze\nvisual information, we introduce an automated LVLM-based evaluation metric that\nquantifies the faithfulness of visual cues from two perspectives: reliability\nand sufficiency. Our evaluation reveals that the visual information in current\nMCoT traces is simultaneously unreliable and insufficient. To address this\nissue, we propose a novel MCoT learning strategy termed Sufficient-Component\nCause Model (SCCM) learning. This approach encourages the MCoT to generate\nsufficient yet minimal visual components that are independently capable of\nleading to correct answers. We note that the proposed SCCM is annotation-free\nand compatible with various RFT for MCoT in a plug-and-play manner. Empirical\nresults demonstrate that SCCM consistently improves the visual faithfulness\nacross a suite of fine-grained perception and reasoning benchmarks. Code is\navailable at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",
        "url": "http://arxiv.org/abs/2510.23482v1",
        "published_date": "2025-10-27T16:15:54+00:00",
        "updated_date": "2025-10-27T16:15:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zujing Liu",
            "Junwen Pan",
            "Qi She",
            "Yuan Gao",
            "Guisong Xia"
        ],
        "tldr": "This paper identifies and addresses the issue of unfaithful visual information usage in Vision-Language Models' chain-of-thought reasoning by introducing a novel learning strategy called Sufficient-Component Cause Model (SCCM) learning.",
        "tldr_zh": "本文指出并解决了视觉语言模型在链式思考推理中视觉信息使用不忠实的问题，并提出了一种新的学习策略，称为充分成分因果模型（SCCM）学习。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding",
        "summary": "Vision-language alignment in multi-modal large language models (MLLMs)\ntypically relies on supervised fine-tuning (SFT) or reinforcement learning\n(RL). SFT is stable and efficient but requires large-scale human annotations\nand cannot capture subtle preferences, while RL brings in a reward signal for\ntraining, but suffers from overhead and instability. These limitations\nhighlight a trade-off between scalability, robustness, and alignment quality.\nTo address this, we propose MergeMix, a training-time augmentation paradigm\nthat bridges SFT and RL. It first applies an attention-aware image mixing via\ntoken merge with more cluster representation and spatial context, and then\npresents a preference-driven training paradigm for MLLMs by building preference\npairs with mixed images and raw images, and optimizing via SimPO loss. As a\nmixup augmentation, MergeMix enhances attention consistency and efficiency,\nsurpassing other heuristic-based methods in classification. Extensive\nexperiments demonstrate that MergeMix achieves competitive accuracy with\nimproved efficiency, providing a scalable approach to preference alignment in\nclassification and MLLMs.",
        "url": "http://arxiv.org/abs/2510.23479v1",
        "published_date": "2025-10-27T16:12:40+00:00",
        "updated_date": "2025-10-27T16:12:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Jin",
            "Siyuan Li",
            "Siyong Jian",
            "Kai Yu",
            "Huan Wang"
        ],
        "tldr": "MergeMix is a novel augmentation paradigm that combines SFT and RL for vision-language alignment in MLLMs, using attention-aware image mixing and preference-driven training with SimPO loss to improve accuracy and efficiency.",
        "tldr_zh": "MergeMix 是一种新颖的增强范式，它结合了 SFT 和 RL，用于 MLLM 中的视觉语言对齐，通过使用注意力感知的图像混合和基于偏好的训练与 SimPO 损失来提高准确性和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
        "summary": "Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs.",
        "url": "http://arxiv.org/abs/2510.23473v1",
        "published_date": "2025-10-27T16:10:45+00:00",
        "updated_date": "2025-10-27T16:10:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shijian Wang",
            "Jiarui Jin",
            "Xingjian Wang",
            "Linxin Song",
            "Runhao Fu",
            "Hecheng Wang",
            "Zongyuan Ge",
            "Yuan Lu",
            "Xuelian Cheng"
        ],
        "tldr": "The paper introduces Video-Thinker, a reinforcement learning-based approach that empowers MLLMs to perform video reasoning by leveraging grounding and captioning capabilities, achieving state-of-the-art performance on various benchmarks.",
        "tldr_zh": "该论文介绍了Video-Thinker，一种基于强化学习的方法，它使MLLM能够通过利用定位和字幕功能来执行视频推理，并在各种基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
        "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.",
        "url": "http://arxiv.org/abs/2510.23451v1",
        "published_date": "2025-10-27T15:53:20+00:00",
        "updated_date": "2025-10-27T15:53:20+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zhuoran Jin",
            "Hongbang Yuan",
            "Kejian Zhu",
            "Jiachun Li",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "tldr": "The paper introduces Omni-Reward, a framework for omni-modal reward modeling that addresses modality imbalance and preference rigidity using a new benchmark, dataset, and reward model.",
        "tldr_zh": "该论文介绍了Omni-Reward，一个用于全模态奖励建模的框架，通过新的基准、数据集和奖励模型来解决模态不平衡和偏好刚性问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Video Is Not Worth a Thousand Words",
        "summary": "As we become increasingly dependent on vision language models (VLMs) to\nanswer questions about the world around us, there is a significant amount of\nresearch devoted to increasing both the difficulty of video question answering\n(VQA) datasets, and the context lengths of the models that they evaluate. The\nreliance on large language models as backbones has lead to concerns about\npotential text dominance, and the exploration of interactions between\nmodalities is underdeveloped. How do we measure whether we're heading in the\nright direction, with the complexity that multi-modal models introduce? We\npropose a joint method of computing both feature attributions and modality\nscores based on Shapley values, where both the features and modalities are\narbitrarily definable. Using these metrics, we compare $6$ VLM models of\nvarying context lengths on $4$ representative datasets, focusing on\nmultiple-choice VQA. In particular, we consider video frames and whole textual\nelements as equal features in the hierarchy, and the multiple-choice VQA task\nas an interaction between three modalities: video, question and answer. Our\nresults demonstrate a dependence on text and show that the multiple-choice VQA\ntask devolves into a model's ability to ignore distractors. Code available at\nhttps://github.com/sjpollard/a-video-is-not-worth-a-thousand-words.",
        "url": "http://arxiv.org/abs/2510.23253v1",
        "published_date": "2025-10-27T12:15:02+00:00",
        "updated_date": "2025-10-27T12:15:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sam Pollard",
            "Michael Wray"
        ],
        "tldr": "This paper proposes a Shapley value-based method for computing feature attributions and modality scores in VLMs, revealing a dependence on text and the importance of distractor rejection in multiple-choice VQA tasks.",
        "tldr_zh": "该论文提出了一种基于 Shapley 值的 VLM 特征归因和模态评分方法，揭示了对文本的依赖性以及多项选择 VQA 任务中排除干扰项的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment",
        "summary": "The rapid digitization of histopathology slides has opened up new\npossibilities for computational tools in clinical and research workflows. Among\nthese, content-based slide retrieval stands out, enabling pathologists to\nidentify morphologically and semantically similar cases, thereby supporting\nprecise diagnoses, enhancing consistency across observers, and assisting\nexample-based education. However, effective retrieval of whole slide images\n(WSIs) remains challenging due to their gigapixel scale and the difficulty of\ncapturing subtle semantic differences amid abundant irrelevant content. To\novercome these challenges, we present PathSearch, a retrieval framework that\nunifies fine-grained attentive mosaic representations with global-wise slide\nembeddings aligned through vision-language contrastive learning. Trained on a\ncorpus of 6,926 slide-report pairs, PathSearch captures both fine-grained\nmorphological cues and high-level semantic patterns to enable accurate and\nflexible retrieval. The framework supports two key functionalities: (1)\nmosaic-based image-to-image retrieval, ensuring accurate and efficient slide\nresearch; and (2) multi-modal retrieval, where text queries can directly\nretrieve relevant slides. PathSearch was rigorously evaluated on four public\npathology datasets and three in-house cohorts, covering tasks including\nanatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,\nand grading across diverse organs such as breast, lung, kidney, liver, and\nstomach. External results show that PathSearch outperforms traditional\nimage-to-image retrieval frameworks. A multi-center reader study further\ndemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,\nand enhances inter-observer agreement among pathologists in real clinical\nscenarios. These results establish PathSearch as a scalable and generalizable\nretrieval solution for digital pathology.",
        "url": "http://arxiv.org/abs/2510.23224v1",
        "published_date": "2025-10-27T11:22:28+00:00",
        "updated_date": "2025-10-27T11:22:28+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Hongyi Wang",
            "Zhengjie Zhu",
            "Jiabo Ma",
            "Fang Wang",
            "Yue Shi",
            "Bo Luo",
            "Jili Wang",
            "Qiuyu Cai",
            "Xiuming Zhang",
            "Yen-Wei Chen",
            "Lanfen Lin",
            "Hao Chen"
        ],
        "tldr": "The paper introduces PathSearch, a multimodal pathology retrieval framework using attentive vision-language alignment for accurate and scalable slide retrieval, demonstrating improved diagnostic accuracy and inter-observer agreement in clinical scenarios.",
        "tldr_zh": "该论文介绍了PathSearch，一个多模态病理检索框架，它使用注意力的视觉-语言对齐来实现准确和可扩展的幻灯片检索，并在临床场景中展示了诊断准确性和观察者间一致性的提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluation of Vision-LLMs in Surveillance Video",
        "summary": "The widespread use of cameras in our society has created an overwhelming\namount of video data, far exceeding the capacity for human monitoring. This\npresents a critical challenge for public safety and security, as the timely\ndetection of anomalous or criminal events is crucial for effective response and\nprevention. The ability for an embodied agent to recognize unexpected events is\nfundamentally tied to its capacity for spatial reasoning. This paper\ninvestigates the spatial reasoning of vision-language models (VLMs) by framing\nanomalous action recognition as a zero-shot, language-grounded task, addressing\nthe embodied perception challenge of interpreting dynamic 3D scenes from sparse\n2D video. Specifically, we investigate whether small, pre-trained vision--LLMs\ncan act as spatially-grounded, zero-shot anomaly detectors by converting video\ninto text descriptions and scoring labels via textual entailment. We evaluate\nfour open models on UCF-Crime and RWF-2000 under prompting and\nprivacy-preserving conditions. Few-shot exemplars can improve accuracy for some\nmodels, but may increase false positives, and privacy filters -- especially\nfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.\nThese results chart where current vision--LLMs succeed (simple, spatially\nsalient events) and where they falter (noisy spatial cues, identity\nobfuscation). Looking forward, we outline concrete paths to strengthen spatial\ngrounding without task-specific training: structure-aware prompts, lightweight\nspatial memory across clips, scene-graph or 3D-pose priors during description,\nand privacy methods that preserve action-relevant geometry. This positions\nzero-shot, language-grounded pipelines as adaptable building blocks for\nembodied, real-world video understanding. Our implementation for evaluating\nVLMs is publicly available at:\nhttps://github.com/pascalbenschopTU/VLLM_AnomalyRecognition",
        "url": "http://arxiv.org/abs/2510.23190v1",
        "published_date": "2025-10-27T10:27:02+00:00",
        "updated_date": "2025-10-27T10:27:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pascal Benschop",
            "Cristian Meo",
            "Justin Dauwels",
            "Jelte P. Mense"
        ],
        "tldr": "This paper evaluates the spatial reasoning capabilities of small, pre-trained Vision-Language Models (VLMs) for zero-shot anomaly detection in surveillance video, finding limitations in noisy environments and with privacy filters, and suggests improvements for spatial grounding.",
        "tldr_zh": "本文评估了小型预训练视觉语言模型 (VLM) 在监控视频中进行零样本异常检测的空间推理能力，发现其在嘈杂环境和使用隐私过滤器时存在局限性，并提出了改进空间基础的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Finding 3D Scene Analogies with Multimodal Foundation Models",
        "summary": "Connecting current observations with prior experiences helps robots adapt and\nplan in new, unseen 3D environments. Recently, 3D scene analogies have been\nproposed to connect two 3D scenes, which are smooth maps that align scene\nregions with common spatial relationships. These maps enable detailed transfer\nof trajectories or waypoints, potentially supporting demonstration transfer for\nimitation learning or task plan transfer across scenes. However, existing\nmethods for the task require additional training and fixed object vocabularies.\nIn this work, we propose to use multimodal foundation models for finding 3D\nscene analogies in a zero-shot, open-vocabulary setting. Central to our\napproach is a hybrid neural representation of scenes that consists of a sparse\ngraph based on vision-language model features and a feature field derived from\n3D shape foundation models. 3D scene analogies are then found in a\ncoarse-to-fine manner, by first aligning the graph and refining the\ncorrespondence with feature fields. Our method can establish accurate\ncorrespondences between complex scenes, and we showcase applications in\ntrajectory and waypoint transfer.",
        "url": "http://arxiv.org/abs/2510.23184v1",
        "published_date": "2025-10-27T10:23:31+00:00",
        "updated_date": "2025-10-27T10:23:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junho Kim",
            "Young Min Kim"
        ],
        "tldr": "This paper introduces a zero-shot, open-vocabulary method for finding 3D scene analogies using multimodal foundation models, enabling trajectory and waypoint transfer between scenes.",
        "tldr_zh": "本文提出了一种使用多模态基础模型寻找3D场景类比的零样本、开放词汇方法，从而能够在场景之间进行轨迹和航路点转移。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
        "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
        "url": "http://arxiv.org/abs/2510.22946v1",
        "published_date": "2025-10-27T02:59:57+00:00",
        "updated_date": "2025-10-27T02:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Wang",
            "Zilong Chen",
            "Chenhui Gou",
            "Feng Li",
            "Chaorui Deng",
            "Deyao Zhu",
            "Kunchang Li",
            "Weihao Yu",
            "Haoqin Tu",
            "Haoqi Fan",
            "Cihang Xie"
        ],
        "tldr": "The paper introduces LightBagel, a light-weighted double fusion framework that efficiently combines publicly available models for unified multimodal understanding and generation, achieving competitive performance with significantly fewer computational resources.",
        "tldr_zh": "该论文介绍了一种轻量级的双重融合框架LightBagel，它有效地结合了公开可用的模型，用于统一的多模态理解和生成，并以更少的计算资源实现了有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Positional Preservation Embedding for Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks, yet often suffer from inefficiencies due to redundant\nvisual tokens. Existing token merging methods reduce sequence length but\nfrequently disrupt spatial layouts and temporal continuity by disregarding\npositional relationships. In this work, we propose a novel encoding operator\ndubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding\n(\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal\nstructure during visual token compression. PPE explicitly introduces the\ndisentangled encoding of 3D positions in the token dimension, enabling each\ncompressed token to encapsulate different positions from multiple original\ntokens. Furthermore, we show that PPE can effectively support cascade\nclustering -- a progressive token compression strategy that leads to better\nperformance retention. PPE is a parameter-free and generic operator that can be\nseamlessly integrated into existing token merging methods without any\nadjustments. Applied to state-of-the-art token merging framework, PPE achieves\nconsistent improvements of $2\\%\\sim5\\%$ across multiple vision-language\nbenchmarks, including MMBench (general vision understanding), TextVQA (layout\nunderstanding) and VideoMME (temporal understanding). These results demonstrate\nthat preserving positional cues is critical for efficient and effective MLLM\nreasoning.",
        "url": "http://arxiv.org/abs/2510.22936v1",
        "published_date": "2025-10-27T02:40:02+00:00",
        "updated_date": "2025-10-27T02:40:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mouxiao Huang",
            "Borui Jiang",
            "Dehua Zheng",
            "Hailin Hu",
            "Kai Han",
            "Xinghao Chen"
        ],
        "tldr": "This paper introduces Positional Preservation Embedding (PPE), a parameter-free operator for visual token compression in MLLMs that preserves spatiotemporal structure, leading to performance improvements on various vision-language benchmarks.",
        "tldr_zh": "该论文介绍了位置保持嵌入（PPE），一种用于多模态大语言模型中视觉令牌压缩的无参数算子，能够保持时空结构，从而在各种视觉-语言基准测试中带来性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models",
        "summary": "Wind turbine blades operate in harsh environments, making timely damage\ndetection essential for preventing failures and optimizing maintenance.\nDrone-based inspection and deep learning are promising, but typically depend on\nlarge, labeled datasets, which limit their ability to detect rare or evolving\ndamage types. To address this, we propose a zero-shot-oriented inspection\nframework that integrates Retrieval-Augmented Generation (RAG) with\nVision-Language Models (VLM). A multimodal knowledge base is constructed,\ncomprising technical documentation, representative reference images, and\ndomain-specific guidelines. A hybrid text-image retriever with keyword-aware\nreranking assembles the most relevant context to condition the VLM at\ninference, injecting domain knowledge without task-specific training. We\nevaluate the framework on 30 labeled blade images covering diverse damage\ncategories. Although the dataset is small due to the difficulty of acquiring\nverified blade imagery, it covers multiple representative defect types. On this\ntest set, the RAG-grounded VLM correctly classified all samples, whereas the\nsame VLM without retrieval performed worse in both accuracy and precision. We\nfurther compare against open-vocabulary baselines and incorporate uncertainty\nClopper-Pearson confidence intervals to account for the small-sample setting.\nAblation studies indicate that the key advantage of the framework lies in\nexplainability and generalizability: retrieved references ground the reasoning\nprocess and enable the detection of previously unseen defects by leveraging\ndomain knowledge rather than relying solely on visual cues. This research\ncontributes a data-efficient solution for industrial inspection that reduces\ndependence on extensive labeled datasets.",
        "url": "http://arxiv.org/abs/2510.22868v1",
        "published_date": "2025-10-26T23:19:28+00:00",
        "updated_date": "2025-10-26T23:19:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Zhang",
            "Qianyu Zhou",
            "Farhad Imani",
            "Jiong Tang"
        ],
        "tldr": "This paper introduces a zero-shot wind turbine blade inspection framework using a Retrieval-Augmented Generation (RAG) grounded Vision-Language Model (VLM) to detect rare or evolving damages, achieving high classification accuracy on a small, labeled dataset.",
        "tldr_zh": "本文介绍了一种基于检索增强生成 (RAG) 的视觉语言模型 (VLM) 的零样本风力涡轮机叶片检测框架，用于检测罕见或演变的损伤，并在小型标记数据集上实现了高分类精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models",
        "summary": "The \"style trap\" poses a significant challenge for Large Vision-Language\nModels (LVLMs), hindering robust semantic understanding across diverse visual\nstyles, especially in in-context learning (ICL). Existing methods often fail to\neffectively decouple style from content, hindering generalization. To address\nthis, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR),\na novel framework for stable semantic understanding and adaptive cross-style\nvisual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) for\nstyle-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD)\nfor efficient few-shot style adaptation, and an Adaptive Semantic Consistency\nModule (ASCM) employing multi-task contrastive learning to enforce cross-style\nsemantic invariance. Extensive experiments on a challenging multi-style dataset\ndemonstrate SP-CSVR's state-of-the-art performance across visual captioning,\nvisual question answering, and in-context style adaptation. Comprehensive\nevaluations, including ablation studies and generalization analysis, confirm\nSP-CSVR's efficacy in enhancing robustness, generalization, and efficiency\nacross diverse visual styles.",
        "url": "http://arxiv.org/abs/2510.22838v1",
        "published_date": "2025-10-26T21:11:46+00:00",
        "updated_date": "2025-10-26T21:11:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aya Nakayama",
            "Brian Wong",
            "Yuji Nishimura",
            "Kaito Tanaka"
        ],
        "tldr": "The paper introduces Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR) to improve Large Vision-Language Models' robustness across diverse visual styles by decoupling style from content. It achieves state-of-the-art performance in visual captioning, visual question answering, and in-context style adaptation.",
        "tldr_zh": "该论文介绍了语义保持跨风格视觉推理器（SP-CSVR），通过将风格与内容解耦，提高大型视觉语言模型在不同视觉风格下的鲁棒性。在视觉描述生成、视觉问题解答和上下文风格适应方面取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment",
        "summary": "Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how\nwell images match prompts and how models treat social attributes. Common\nproxies -- face classifiers and contrastive similarity -- reward surface cues,\nlack calibrated abstention, and miss attributes only weakly visible (for\nexample, religion, culture, disability). We present FairJudge, a lightweight\nprotocol that treats instruction-following multimodal LLMs as fair judges. It\nscores alignment with an explanation-oriented rubric mapped to [-1, 1];\nconstrains judgments to a closed label set; requires evidence grounded in the\nvisible content; and mandates abstention when cues are insufficient. Unlike\nCLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions;\nunlike mitigation that alters generators, it targets evaluation fairness. We\nevaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to\nreligion, culture, and disability; and assess profession correctness and\nalignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions.\nWe also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes.\nAcross datasets, judge models outperform contrastive and face-centric baselines\non demographic prediction and improve mean alignment while maintaining high\nprofession accuracy, enabling more reliable, reproducible fairness audits.",
        "url": "http://arxiv.org/abs/2510.22827v1",
        "published_date": "2025-10-26T20:43:48+00:00",
        "updated_date": "2025-10-26T20:43:48+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zahraa Al Sahili",
            "Maryam Fetanat",
            "Maimuna Nowaz",
            "Ioannis Patras",
            "Matthew Purver"
        ],
        "tldr": "The paper introduces FairJudge, a lightweight protocol using instruction-following MLLMs to evaluate text-to-image systems for fairness across social attributes and prompt alignment, outperforming existing methods.",
        "tldr_zh": "该论文介绍了FairJudge，一种轻量级协议，使用指令遵循的多模态LLM来评估文本到图像系统的公平性，包括社会属性和提示对齐，性能优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedXplain-VQA: Multi-Component Explainable Medical Visual Question Answering",
        "summary": "Explainability is critical for the clinical adoption of medical visual\nquestion answering (VQA) systems, as physicians require transparent reasoning\nto trust AI-generated diagnoses. We present MedXplain-VQA, a comprehensive\nframework integrating five explainable AI components to deliver interpretable\nmedical image analysis. The framework leverages a fine-tuned BLIP-2 backbone,\nmedical query reformulation, enhanced Grad-CAM attention, precise region\nextraction, and structured chain-of-thought reasoning via multi-modal language\nmodels. To evaluate the system, we introduce a medical-domain-specific\nframework replacing traditional NLP metrics with clinically relevant\nassessments, including terminology coverage, clinical structure quality, and\nattention region relevance. Experiments on 500 PathVQA histopathology samples\ndemonstrate substantial improvements, with the enhanced system achieving a\ncomposite score of 0.683 compared to 0.378 for baseline methods, while\nmaintaining high reasoning confidence (0.890). Our system identifies 3-5\ndiagnostically relevant regions per sample and generates structured\nexplanations averaging 57 words with appropriate clinical terminology. Ablation\nstudies reveal that query reformulation provides the most significant initial\nimprovement, while chain-of-thought reasoning enables systematic diagnostic\nprocesses. These findings underscore the potential of MedXplain-VQA as a\nrobust, explainable medical VQA system. Future work will focus on validation\nwith medical experts and large-scale clinical datasets to ensure clinical\nreadiness.",
        "url": "http://arxiv.org/abs/2510.22803v1",
        "published_date": "2025-10-26T19:23:20+00:00",
        "updated_date": "2025-10-26T19:23:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hai-Dang Nguyen",
            "Minh-Anh Dang",
            "Minh-Tan Le",
            "Minh-Tuan Le"
        ],
        "tldr": "MedXplain-VQA introduces a novel explainable VQA framework for medical images, incorporating multiple AI components and clinically relevant evaluation metrics, showing significant performance gains on histopathology samples. It enhances trust in AI diagnoses through transparent reasoning.",
        "tldr_zh": "MedXplain-VQA 提出了一种新的可解释的 VQA 框架，用于医学图像，集成了多种 AI 组件和临床相关的评估指标，并在组织病理学样本上显示出显着的性能提升。它通过透明的推理增强了对 AI 诊断的信任。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Calibrated Consistency can Fight Back for Adversarial Robustness in Vision-Language Models",
        "summary": "Pre-trained vision-language models (VLMs) such as CLIP have demonstrated\nstrong zero-shot capabilities across diverse domains, yet remain highly\nvulnerable to adversarial perturbations that disrupt image-text alignment and\ncompromise reliability. Existing defenses typically rely on adversarial\nfine-tuning with labeled data, limiting their applicability in zero-shot\nsettings. In this work, we identify two key weaknesses of current CLIP\nadversarial attacks -- lack of semantic guidance and vulnerability to view\nvariations -- collectively termed semantic and viewpoint fragility. To address\nthese challenges, we propose Self-Calibrated Consistency (SCC), an effective\ntest-time defense. SCC consists of two complementary modules: Semantic\nconsistency, which leverages soft pseudo-labels from counterattack warm-up and\nmulti-view predictions to regularize cross-modal alignment and separate the\ntarget embedding from confusable negatives; and Spatial consistency, aligning\nperturbed visual predictions via augmented views to stabilize inference under\nadversarial perturbations. Together, these modules form a plug-and-play\ninference strategy. Extensive experiments on 22 benchmarks under diverse attack\nsettings show that SCC consistently improves the zero-shot robustness of CLIP\nwhile maintaining accuracy, and can be seamlessly integrated with other VLMs\nfor further gains. These findings highlight the great potential of establishing\nan adversarially robust paradigm from CLIP, with implications extending to\nbroader vision-language domains such as BioMedCLIP.",
        "url": "http://arxiv.org/abs/2510.22785v1",
        "published_date": "2025-10-26T18:37:12+00:00",
        "updated_date": "2025-10-26T18:37:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxiang Liu",
            "Jiawei Du",
            "Xiao Liu",
            "Prayag Tiwari",
            "Mingkun Xu"
        ],
        "tldr": "This paper introduces Self-Calibrated Consistency (SCC), a test-time defense strategy to improve the adversarial robustness of CLIP and other VLMs in zero-shot settings by addressing semantic and viewpoint fragility. It demonstrates improved robustness and maintained accuracy across various benchmarks.",
        "tldr_zh": "本文提出了自校准一致性（SCC），一种测试时防御策略，通过解决语义和视点脆弱性来提高CLIP和其他VLM在零样本设置中的对抗鲁棒性。结果表明，该方法在各种基准测试中提高了鲁棒性并保持了准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
        "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.",
        "url": "http://arxiv.org/abs/2510.23571v1",
        "published_date": "2025-10-27T17:41:38+00:00",
        "updated_date": "2025-10-27T17:41:38+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yash Jangir",
            "Yidi Zhang",
            "Kashu Yamazaki",
            "Chenyu Zhang",
            "Kuan-Hsun Tu",
            "Tsung-Wei Ke",
            "Lei Ke",
            "Yonatan Bisk",
            "Katerina Fragkiadaki"
        ],
        "tldr": "The paper introduces RobotArena ∞, a novel benchmarking framework that translates real-world robot demonstrations into simulated environments for scalable VLA policy evaluation using VLM-guided scoring and human feedback.",
        "tldr_zh": "该论文介绍了RobotArena ∞，这是一个新的基准测试框架，它将真实世界的机器人演示转换为模拟环境，用于使用VLM引导的评分和人工反馈进行可扩展的VLA策略评估。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations",
        "summary": "Video temporal grounding (VTG) aims to locate precise segments in videos\nbased on language queries, which is a fundamental challenge in video\nunderstanding. While recent Multimodal Large Language Models (MLLMs) have shown\npromise in tackling VTG through reinforcement learning (RL), they overlook the\nchallenges arising from both the quality and difficulty of training samples.\n(1) Partially annotated samples. Many samples contain relevant segments beyond\nthe annotated interval, introducing ambiguous supervision. (2) Hard-to-ground\nsamples. Samples with poor zero-shot performance produce consistently low and\nindistinguishable rewards during RL training, exhibiting no clear preference\namong multiple outputs and thus hindering learning efficiency. To address these\nchallenges, we propose VideoTG-R1, a novel curriculum RL framework with\nreflected boundary annotations, enabling data-efficient training. Specifically,\nwe propose a Boundary Reflection Agent that utilizes MLLMs to predict\nquery-relevant timestamps outside the annotated intervals, allowing us to\nidentify and filter out partially annotated samples, thereby reducing\nambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess\nthe training difficulty of each sample and design a curriculum RL strategy that\ndynamically masks the videos of hard-to-ground samples according to the\ntraining steps, easing the training difficulty and providing clearer\npreference. Experiments on the VTG and grounded VideoQA tasks demonstrate the\neffectiveness of our method. Remarkably, with only 10% of the training samples\nand 21% of the computational budget, VideoTG-R1 outperforms full-data\ncounterparts under both group relative policy optimization (GRPO) and\nsupervised fine-tuning (SFT). The code is available at\nhttps://github.com/ldong1111/VideoTG-R1.",
        "url": "http://arxiv.org/abs/2510.23397v1",
        "published_date": "2025-10-27T14:55:38+00:00",
        "updated_date": "2025-10-27T14:55:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lu Dong",
            "Haiyu Zhang",
            "Han Lin",
            "Ziang Yan",
            "Xiangyu Zeng",
            "Hongjie Zhang",
            "Yifei Huang",
            "Yi Wang",
            "Zhen-Hua Ling",
            "Limin Wang",
            "Yali Wang"
        ],
        "tldr": "The paper introduces VideoTG-R1, a curriculum reinforcement learning framework for video temporal grounding that addresses challenges related to partially annotated and hard-to-ground samples using a Boundary Reflection Agent and a Difficulty Estimation Agent, achieving state-of-the-art results with reduced data and computation.",
        "tldr_zh": "该论文介绍了VideoTG-R1，一个用于视频时间定位的课程强化学习框架，它使用边界反射代理和难度估计代理来解决与部分注释和难以定位的样本相关的挑战，并以减少的数据和计算量实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark",
        "summary": "Text-to-image models are known to struggle with generating images that\nperfectly align with textual prompts. Several previous studies have focused on\nevaluating image-text alignment in text-to-image generation. However, these\nevaluations either address overly simple scenarios, especially overlooking the\ndifficulty of prompts with multiple different instances belonging to the same\ncategory, or they introduce metrics that do not correlate well with human\nevaluation. In this study, we introduce M$^3$T2IBench, a large-scale,\nmulti-category, multi-instance, multi-relation along with an\nobject-detection-based evaluation metric, $AlignScore$, which aligns closely\nwith human evaluation. Our findings reveal that current open-source\ntext-to-image models perform poorly on this challenging benchmark.\nAdditionally, we propose the Revise-Then-Enforce approach to enhance image-text\nalignment. This training-free post-editing method demonstrates improvements in\nimage-text alignment across a broad range of diffusion models. \\footnote{Our\ncode and data has been released in supplementary material and will be made\npublicly available after the paper is accepted.}",
        "url": "http://arxiv.org/abs/2510.23020v1",
        "published_date": "2025-10-27T05:32:50+00:00",
        "updated_date": "2025-10-27T05:32:50+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Huixuan Zhang",
            "Xiaojun Wan"
        ],
        "tldr": "The paper introduces M$^3$T2IBench, a challenging text-to-image benchmark, along with an evaluation metric, and proposes a training-free post-editing method to improve image-text alignment.",
        "tldr_zh": "该论文介绍了一个具有挑战性的文本到图像基准测试 M$^3$T2IBench，以及一个评估指标，并提出了一种无需训练的后编辑方法来提高图像-文本对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression",
        "summary": "Modeling open-vocabulary language fields in 3D is essential for intuitive\nhuman-AI interaction and querying within physical environments.\nState-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting\nto efficiently construct these language fields, encoding features distilled\nfrom high-dimensional models like CLIP. However, this efficiency is currently\noffset by the requirement to train a scene-specific language autoencoder for\nfeature compression, introducing a costly, per-scene optimization bottleneck\nthat hinders deployment scalability. In this work, we introduce Gen-LangSplat,\nthat eliminates this requirement by replacing the scene-wise autoencoder with a\ngeneralized autoencoder, pre-trained extensively on the large-scale ScanNet\ndataset. This architectural shift enables the use of a fixed, compact latent\nspace for language features across any new scene without any scene-specific\ntraining. By removing this dependency, our entire language field construction\nprocess achieves a efficiency boost while delivering querying performance\ncomparable to, or exceeding, the original LangSplat method. To validate our\ndesign choice, we perform a thorough ablation study empirically determining the\noptimal latent embedding dimension and quantifying representational fidelity\nusing Mean Squared Error and cosine similarity between the original and\nreprojected 512-dimensional CLIP embeddings. Our results demonstrate that\ngeneralized embeddings can efficiently and accurately support open-vocabulary\nquerying in novel 3D scenes, paving the way for scalable, real-time interactive\n3D AI applications.",
        "url": "http://arxiv.org/abs/2510.22930v1",
        "published_date": "2025-10-27T02:13:38+00:00",
        "updated_date": "2025-10-27T02:13:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pranav Saxena"
        ],
        "tldr": "Gen-LangSplat replaces the scene-specific autoencoder in LangSplat with a pre-trained generalized autoencoder, improving efficiency and scalability for open-vocabulary 3D language field construction without sacrificing query performance.",
        "tldr_zh": "Gen-LangSplat 使用预训练的通用自编码器替换了 LangSplat 中特定于场景的自编码器，提高了开放词汇 3D 语言场构建的效率和可扩展性，且不牺牲查询性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LLM-based Fusion of Multi-modal Features for Commercial Memorability Prediction",
        "summary": "This paper addresses the prediction of commercial (brand) memorability as\npart of \"Subtask 2: Commercial/Ad Memorability\" within the \"Memorability:\nPredicting movie and commercial memorability\" task at the MediaEval 2025\nworkshop competition. We propose a multimodal fusion system with a Gemma-3 LLM\nbackbone that integrates pre-computed visual (ViT) and textual (E5) features by\nmulti-modal projections. The model is adapted using Low-Rank Adaptation (LoRA).\nA heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key\ncontribution is the use of LLM-generated rationale prompts, grounded in\nexpert-derived aspects of memorability, to guide the fusion model. The results\ndemonstrate that the LLM-based system exhibits greater robustness and\ngeneralization performance on the final test set, compared to the baseline.\n  The paper's codebase can be found at\nhttps://github.com/dsgt-arc/mediaeval-2025-memorability",
        "url": "http://arxiv.org/abs/2510.22829v1",
        "published_date": "2025-10-26T20:51:52+00:00",
        "updated_date": "2025-10-26T20:51:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Aleksandar Pramov"
        ],
        "tldr": "This paper explores a Gemma-3 LLM-based multimodal fusion system, enhanced with LLM-generated rationale prompts, for predicting commercial memorability, showing improved robustness and generalization compared to a baseline.",
        "tldr_zh": "本文提出了一种基于Gemma-3 LLM的多模态融合系统，利用LLM生成的理由提示来预测商业记忆性，与基线相比表现出更好的鲁棒性和泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification",
        "summary": "Accurate vertex-level contact prediction between humans and surrounding\nobjects is a prerequisite for high fidelity human object interaction models\nused in robotics, AR/VR, and behavioral simulation. DECO was the first in the\nwild estimator for this task but is limited to binary contact maps and\nstruggles with soft surfaces, occlusions, children, and false-positive foot\ncontacts. We address these issues and introduce DecoDINO, a three-branch\nnetwork based on DECO's framework. It uses two DINOv2 ViT-g/14 encoders,\nclass-balanced loss weighting to reduce bias, and patch-level cross-attention\nfor improved local reasoning. Vertex features are finally passed through a\nlightweight MLP with a softmax to assign semantic contact labels. We also\ntested a vision-language model (VLM) to integrate text features, but the\nsimpler architecture performed better and was used instead. On the DAMON\nbenchmark, DecoDINO (i) raises the binary-contact F1 score by 7$\\%$, (ii)\nhalves the geodesic error, and (iii) augments predictions with object-level\nsemantic labels. Ablation studies show that LoRA fine-tuning and the dual\nencoders are key to these improvements. DecoDINO outperformed the challenge\nbaseline in both tasks of the DAMON Challenge. Our code is available at\nhttps://github.com/DavidePasero/deco/tree/main.",
        "url": "http://arxiv.org/abs/2510.23203v1",
        "published_date": "2025-10-27T10:46:22+00:00",
        "updated_date": "2025-10-27T10:46:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lukas Bierling",
            "Davide Pasero",
            "Fleur Dolmans",
            "Helia Ghasemi",
            "Angelo Broere"
        ],
        "tldr": "DecoDINO improves 3D human-scene contact prediction with semantic classification using a dual-encoder network and class-balanced loss, achieving significant performance gains on the DAMON benchmark.",
        "tldr_zh": "DecoDINO通过使用双编码器网络和类别平衡损失，改进了具有语义分类的3D人体-场景接触预测，并在DAMON基准测试中取得了显著的性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]