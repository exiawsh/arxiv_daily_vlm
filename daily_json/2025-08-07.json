[
    {
        "title": "ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models",
        "summary": "Prompt tuning has emerged as an efficient and effective technique for\nadapting vision-language models (VLMs) with low computational overhead.\nHowever, existing methods often overlook the vulnerability of prompt-tuned VLMs\nto weak semantic perturbations-such as subtle image or text noise-that degrade\ntheir generalization to unseen classes. To address this limitation, we propose\nANPrompt, a novel prompt tuning framework designed to enhance robustness under\nsuch perturbations. ANPrompt first constructs weak noise text features by\nfusing original and noise-perturbed text embeddings, which are then clustered\nto form noise prompts. These noise prompts are integrated with learnable prompt\ntokens to generate anti-noise prompts, which are injected into the deeper\nlayers of both image and text encoders. To further capture the noise-aware\nvisual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype\n(NRVPP) by averaging the output prompt tokens from the vision encoder. Finally,\nANPrompt introduces alignment, robustness, and anti-noise objectives by\ncomputing a Weak semantic noise Alignment Loss (WALoss) alongside the standard\ncross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that\nANPrompt consistently outperforms existing prompt tuning approaches, achieving\nsuperior robustness to semantic noise and improved generalization to novel\ncategories.",
        "url": "http://arxiv.org/abs/2508.04677v1",
        "published_date": "2025-08-06T17:42:30+00:00",
        "updated_date": "2025-08-06T17:42:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yansheng Gao",
            "Yufei Zheng",
            "Jinghan Qu",
            "Zixi Zhu",
            "Yukuan Zhang",
            "Shengsheng Wang"
        ],
        "tldr": "The paper introduces ANPrompt, a novel prompt tuning framework for Vision-Language Models (VLMs) that enhances robustness against weak semantic perturbations by incorporating noise prompts and a noise-resistant visual prompt prototype, demonstrating improved generalization across multiple benchmarks.",
        "tldr_zh": "该论文介绍了一种名为ANPrompt的新型提示调优框架，用于视觉-语言模型（VLMs），通过结合噪声提示和抗噪声视觉提示原型，增强了对弱语义扰动的鲁棒性，并在多个基准测试中展示了改进的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "X-SAM: From Segment Anything to Any Segmentation",
        "summary": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.",
        "url": "http://arxiv.org/abs/2508.04655v1",
        "published_date": "2025-08-06T17:19:10+00:00",
        "updated_date": "2025-08-06T17:19:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hao Wang",
            "Limeng Qiao",
            "Zequn Jie",
            "Zhijian Huang",
            "Chengjian Feng",
            "Qingfang Zheng",
            "Lin Ma",
            "Xiangyuan Lan",
            "Xiaodan Liang"
        ],
        "tldr": "The paper introduces X-SAM, a Multimodal Large Language Model framework, which extends the Segment Anything Model to handle more diverse segmentation tasks and incorporates visual grounding for improved pixel-level understanding.",
        "tldr_zh": "该论文介绍了X-SAM，一种多模态大型语言模型框架，它扩展了Segment Anything Model以处理更多样化的分割任务，并结合了视觉基础以提高像素级的理解。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding",
        "summary": "In this work, we address the problem of grounding abnormalities in medical\nimages, where the goal is to localize clinical findings based on textual\ndescriptions. While generalist Vision-Language Models (VLMs) excel in natural\ngrounding tasks, they often struggle in the medical domain due to rare,\ncompositional, and domain-specific terms that are poorly aligned with visual\npatterns. Specialized medical VLMs address this challenge via large-scale\ndomain pretraining, but at the cost of substantial annotation and computational\nresources. To overcome these limitations, we propose \\textbf{Knowledge to Sight\n(K2Sight)}, a framework that introduces structured semantic supervision by\ndecomposing clinical concepts into interpretable visual attributes, such as\nshape, density, and anatomical location. These attributes are distilled from\ndomain ontologies and encoded into concise instruction-style prompts, which\nguide region-text alignment during training. Unlike conventional report-level\nsupervision, our approach explicitly bridges domain knowledge and spatial\nstructure, enabling data-efficient training of compact models. We train compact\nmodels with 0.23B and 2B parameters using only 1.5\\% of the data required by\nstate-of-the-art medical VLMs. Despite their small size and limited training\ndata, these models achieve performance on par with or better than 7B+ medical\nVLMs, with up to 9.82\\% improvement in $mAP_{50}$. Code and models:\n\\href{https://lijunrio.github.io/K2Sight/}{\\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.",
        "url": "http://arxiv.org/abs/2508.04572v1",
        "published_date": "2025-08-06T15:54:44+00:00",
        "updated_date": "2025-08-06T15:54:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jun Li",
            "Che Liu",
            "Wenjia Bai",
            "Mingxuan Liu",
            "Rossella Arcucci",
            "Cosmin I. Bercea",
            "Julia A. Schnabel"
        ],
        "tldr": "The paper introduces K2Sight, a framework that uses knowledge decomposition to improve abnormality grounding in medical images with data-efficient training of compact models, achieving comparable or better performance than larger models.",
        "tldr_zh": "该论文介绍了 K2Sight，一种利用知识分解来改进医学图像中异常定位的框架。它通过数据高效的小型模型训练，实现了与大型模型相当甚至更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Analyzing and Mitigating Object Hallucination: A Training Bias Perspective",
        "summary": "As scaling up training data has significantly improved the general multimodal\ncapabilities of Large Vision-Language Models (LVLMs), they still suffer from\nthe hallucination issue, generating text that is inconsistent with the visual\ninput. This phenomenon motivates us to systematically investigate the role of\ntraining data in hallucination. We introduce a new benchmark, POPEv2, which\nconsists of counterfactual images collected from the training data of LVLMs\nwith certain objects masked. Through comprehensive evaluation on POPEv2, we\nfind that current LVLMs suffer from training bias: they fail to fully leverage\ntheir training data and hallucinate more frequently on images seen during\ntraining. Specifically, they perform poorly on counterfactual images, often\nincorrectly answering ``Yes'' to questions about masked objects. To understand\nthis issue, we conduct probing experiments on the models' internal components,\nrevealing that this training bias is primarily located in the language modeling\n(LM) head. Based on these findings, we propose Obliviate, an efficient and\nlightweight unlearning method designed to mitigate object hallucination via\ntraining bias unlearning. Obliviate identifies the discrepancy between\nground-truth labels and model outputs on the training data as a proxy for bias\nand adopts a parameter- and data-efficient fine-tuning strategy that only\nupdates the LM head. Extensive experiments demonstrate the effectiveness of our\napproach. While only reusing the training data and updating approximately 2\\%\nof the parameters, Obliviate significantly reduces hallucination across both\ndiscriminative and generative tasks. Furthermore, it demonstrates strong\nscalability with respect to both model size (2B to 72B) and training data\nvolume, and exhibits promising generalization to hallucination types beyond\nobject-level hallucination. Our code and data will be publicly released.",
        "url": "http://arxiv.org/abs/2508.04567v1",
        "published_date": "2025-08-06T15:51:02+00:00",
        "updated_date": "2025-08-06T15:51:02+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yifan Li",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Lei Fang",
            "Ji-Rong Wen"
        ],
        "tldr": "This paper identifies and mitigates object hallucination in LVLMs by analyzing training data bias, proposing a new benchmark (POPEv2) and an efficient unlearning method (Obliviate) focused on the language modeling head.",
        "tldr_zh": "本文通过分析训练数据偏差，识别并缓解了大型视觉语言模型中的物体幻觉问题。论文提出了一个新的基准测试(POPEv2)和一个有效的非学习方法(Obliviate)，该方法专注于语言建模头。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding",
        "summary": "The deployment of vision-language models remains constrained by substantial\ncomputational requirements. We present \\textbf{FrEVL}, a framework exploring\nwhether frozen pretrained embeddings can support effective vision-language\nunderstanding. Our analysis reveals that frozen embeddings contain rich\ninformation for discriminative tasks, achieving 85\\% to 95\\% of\nstate-of-the-art performance on standard benchmarks with only 68.4M trainable\nparameters. This performance dichotomy reveals a critical insight: frozen\nembedding effectiveness depends on alignment between pretraining objectives and\ndownstream task requirements. When accounting for end-to-end computation\nincluding embedding extraction, FrEVL provides $2.3\\times$ speedup with 52\\%\nlower energy consumption, making it suitable for scenarios with pre-computable\ninputs or when deployment constraints outweigh marginal performance gains. Our\nevaluation provides practitioners with guidance on when frozen embedding\napproaches represent viable alternatives to full model deployment. We will\nrelease our complete implementation and evaluation framework to facilitate\nfurther research into efficient multi-modal understanding.",
        "url": "http://arxiv.org/abs/2508.04469v1",
        "published_date": "2025-08-06T14:12:05+00:00",
        "updated_date": "2025-08-06T14:12:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Emmanuelle Bourigault",
            "Pauline Bourigault"
        ],
        "tldr": "FrEVL explores using frozen pretrained embeddings for efficient vision-language understanding, achieving near state-of-the-art performance with significantly reduced computational cost and energy consumption. The paper provides guidance on when frozen embeddings are viable alternatives.",
        "tldr_zh": "FrEVL探索使用冻结的预训练嵌入来实现高效的视觉-语言理解，在显著降低计算成本和能耗的同时，实现了接近最先进的性能。该论文提供了关于何时冻结嵌入是可行替代方案的指导。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion",
        "summary": "Large Vision-Language Models (LVLMs) have experienced significant\nadvancements in recent years. However, their performance still falls short in\ntasks requiring deep visual perception, such as identifying subtle differences\nbetween images. A potential cause is the scarcity of visual knowledge in\npopular instruction-tuning corpora, resulting in inadequate visual perception\nand reasoning capabilities. To address this challenge, we introduce a\nself-improvement framework grounded in a novel visual knowledge-intensive task,\n\\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion\n(CVC). This task requires LVLMs to infer the masked object in an image based on\nits \\textit{causal} relationships with the other visible information. We first\nobtain rich examples cheaply through our automated instance construction\npipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or\nhuman assistance. Then, LVLMs effectively self-improve through trial and error\nlearning using these created instances. Our experiments demonstrate substantial\ngains across four challenging specialized tasks and four widely-used\ncomprehensive benchmarks. Especially on specialized tasks, our method achieves\nan average improvement of 5.4\\% and 4.0\\% compared to the corresponding\nbaselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code\nis available at https://github.com/XMUDeepLIT/CVC.",
        "url": "http://arxiv.org/abs/2508.04453v1",
        "published_date": "2025-08-06T13:54:49+00:00",
        "updated_date": "2025-08-06T13:54:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingguo Hu",
            "Ante Wang",
            "Jia Song",
            "Delai Qiu",
            "Qingsong Liu",
            "Jinsong Su"
        ],
        "tldr": "The paper introduces a self-improvement framework for LVLMs using a novel causality-driven visual object completion task (CVC) and demonstrates performance gains on several benchmarks.",
        "tldr_zh": "该论文介绍了一个LVLM的自改进框架，该框架使用一种新的因果驱动的视觉对象补全任务（CVC），并在多个基准测试中展示了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models",
        "summary": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.",
        "url": "http://arxiv.org/abs/2508.04350v1",
        "published_date": "2025-08-06T11:42:54+00:00",
        "updated_date": "2025-08-06T11:42:54+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Nima Iji",
            "Kia Dashtipour"
        ],
        "tldr": "The paper introduces Chain of Questions (CoQ), a curiosity-driven reasoning framework for multimodal language models that dynamically generates questions to guide the model to selectively activate relevant modalities for improved reasoning. Experiments on a novel multimodal benchmark demonstrate improved accuracy, interpretability, and alignment.",
        "tldr_zh": "该论文介绍了链式问题（CoQ）框架，这是一种好奇心驱动的推理方法，用于多模态语言模型，它动态生成问题来引导模型选择性地激活相关模态，从而改进推理。在新颖的多模态基准上的实验表明，准确性、可解释性和对齐性得到了提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting",
        "summary": "Vision-language models (VLMs) have achieved impressive performance across\ndiverse multimodal tasks by leveraging large-scale pre-training. However,\nenabling them to learn continually from non-stationary data remains a major\nchallenge, as their cross-modal alignment and generalization capabilities are\nparticularly vulnerable to catastrophic forgetting. Unlike traditional unimodal\ncontinual learning (CL), VLMs face unique challenges such as cross-modal\nfeature drift, parameter interference due to shared architectures, and\nzero-shot capability erosion. This survey offers the first focused and\nsystematic review of continual learning for VLMs (VLM-CL). We begin by\nidentifying the three core failure modes that degrade performance in VLM-CL.\nBased on these, we propose a challenge-driven taxonomy that maps solutions to\ntheir target problems: (1) \\textit{Multi-Modal Replay Strategies} address\ncross-modal drift through explicit or implicit memory mechanisms; (2)\n\\textit{Cross-Modal Regularization} preserves modality alignment during\nupdates; and (3) \\textit{Parameter-Efficient Adaptation} mitigates parameter\ninterference with modular or low-rank updates. We further analyze current\nevaluation protocols, datasets, and metrics, highlighting the need for better\nbenchmarks that capture VLM-specific forgetting and compositional\ngeneralization. Finally, we outline open problems and future directions,\nincluding continual pre-training and compositional zero-shot learning. This\nsurvey aims to serve as a comprehensive and diagnostic reference for\nresearchers developing lifelong vision-language systems. All resources are\navailable at:\nhttps://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.",
        "url": "http://arxiv.org/abs/2508.04227v1",
        "published_date": "2025-08-06T09:03:10+00:00",
        "updated_date": "2025-08-06T09:03:10+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yuyang Liu",
            "Qiuhe Hong",
            "Linlan Huang",
            "Alexandra Gomez-Villa",
            "Dipam Goswami",
            "Xialei Liu",
            "Joost van de Weijer",
            "Yonghong Tian"
        ],
        "tldr": "This survey paper identifies failure modes in continual learning for VLMs (VLM-CL) and proposes a taxonomy of solutions based on multi-modal replay, cross-modal regularization, and parameter-efficient adaptation. It also highlights the need for better benchmarks and outlines future research directions.",
        "tldr_zh": "这篇综述文章识别了视觉语言模型持续学习(VLM-CL)中的失败模式，并提出了基于多模态重放、跨模态正则化和参数高效自适应的解决方案分类。它还强调了对更好基准的需求，并概述了未来的研究方向。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What Holds Back Open-Vocabulary Segmentation?",
        "summary": "Standard segmentation setups are unable to deliver models that can recognize\nconcepts outside the training taxonomy. Open-vocabulary approaches promise to\nclose this gap through language-image pretraining on billions of image-caption\npairs. Unfortunately, we observe that the promise is not delivered due to\nseveral bottlenecks that have caused the performance to plateau for almost two\nyears. This paper proposes novel oracle components that identify and decouple\nthese bottlenecks by taking advantage of the groundtruth information. The\npresented validation experiments deliver important empirical findings that\nprovide a deeper insight into the failures of open-vocabulary models and\nsuggest prominent approaches to unlock the future research.",
        "url": "http://arxiv.org/abs/2508.04211v1",
        "published_date": "2025-08-06T08:46:47+00:00",
        "updated_date": "2025-08-06T08:46:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Josip Šarić",
            "Ivan Martinović",
            "Matej Kristan",
            "Siniša Šegvić"
        ],
        "tldr": "This paper analyzes the bottlenecks hindering the performance of open-vocabulary segmentation models, which have plateaued despite language-image pretraining, and proposes oracle components to identify and decouple these issues, suggesting future research directions.",
        "tldr_zh": "本文分析了阻碍开放词汇分割模型性能的瓶颈，尽管进行了语言-图像预训练，但其性能已停滞不前。文章提出了预言机组件来识别和分离这些问题，并提出了未来的研究方向。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs",
        "summary": "In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs\nwhen a model generates a correct answer but follows an incorrect reasoning\npath. Existing methods based on specific multi-step reasoning datasets and\nreinforcement learning strategies, leading to high training costs and limited\ngeneralization. In this work, we propose ViFP, a general framework for\nenhancing visual reasoning reliability. It improves both answer accuracy and\nreasoning soundness by detecting FPs. ViFP tackles the limitations of dataset\ndependency and poor generalization by constructing sub-question templates\ngrounded in the core dimensions of visual reasoning, such as object\nlocalization, characteristic description, and object discovery. ViFP then\nbuilds effective reasoning paths via multi-turn QA to improve reasoning\naccuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning\npath to identify potential FPs, and introduces a targeted chain-of-thought\n(CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby\nreducing logical errors in the reasoning path while preserving accuracy.\nFinally, we introduce a reliability evaluation metric-VoC, which integrates\nanswer accuracy and the FP rate, providing a quantitative tool to assess\nwhether a VLM not only answers correctly, but also reasons reliably. Our\nexperiments on closed-source VLMs show that ViFP consistently improves\nperformance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP\nimproves accuracy by up to 5.4%, surpassing the previous state-of-the-art by\n4.3%, and significantly reduces the number of FPs, validating its benefits in\nenhancing reasoning reliability.",
        "url": "http://arxiv.org/abs/2508.04201v1",
        "published_date": "2025-08-06T08:31:11+00:00",
        "updated_date": "2025-08-06T08:31:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ben Zhang",
            "LuLu Yu",
            "Lei Gao",
            "Jing Liu",
            "QuanJiang Guo",
            "Hui Gao"
        ],
        "tldr": "The paper introduces ViFP, a framework for detecting false positives in VLM reasoning to improve answer accuracy and reasoning soundness, using sub-question templates and dynamic consistency analysis. It achieves state-of-the-art performance on A-OKVQA.",
        "tldr_zh": "该论文提出了 ViFP 框架，用于检测视觉语言模型（VLM）推理中的假阳性，从而提高答案准确性和推理的可靠性。ViFP 使用子问题模板和动态一致性分析，在 A-OKVQA 数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization",
        "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities across diverse domains, their application to specialized anomaly\ndetection (AD) remains constrained by domain adaptation challenges. Existing\nGroup Relative Policy Optimization (GRPO) based approaches suffer from two\ncritical limitations: inadequate training data utilization when models produce\nuniform responses, and insufficient supervision over reasoning processes that\nencourage immediate binary decisions without deliberative analysis. We propose\na comprehensive framework addressing these limitations through two synergistic\ninnovations. First, we introduce a multi-stage deliberative reasoning process\nthat guides models from region identification to focused examination,\ngenerating diverse response patterns essential for GRPO optimization while\nenabling structured supervision over analytical workflows. Second, we develop a\nfine-grained reward mechanism incorporating classification accuracy and\nlocalization supervision, transforming binary feedback into continuous signals\nthat distinguish genuine analytical insight from spurious correctness.\nComprehensive evaluation across multiple industrial datasets demonstrates\nsubstantial performance improvements in adapting general vision-language models\nto specialized anomaly detection. Our method achieves superior accuracy with\nefficient adaptation of existing annotations, effectively bridging the gap\nbetween general-purpose MLLM capabilities and the fine-grained visual\ndiscrimination required for detecting subtle manufacturing defects and\nstructural irregularities.",
        "url": "http://arxiv.org/abs/2508.04175v1",
        "published_date": "2025-08-06T08:00:27+00:00",
        "updated_date": "2025-08-06T08:00:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyi Liao",
            "Yongyi Su",
            "Rong-Cheng Tu",
            "Zhao Jin",
            "Wenhao Sun",
            "Yiting Li",
            "Dacheng Tao",
            "Xun Xu",
            "Xulei Yang"
        ],
        "tldr": "The paper proposes a novel framework, AD-FM, to improve MLLMs for anomaly detection by using multi-stage reasoning and fine-grained reward optimization, addressing limitations of existing Group Relative Policy Optimization (GRPO) methods.",
        "tldr_zh": "该论文提出了一个名为AD-FM的新框架，通过多阶段推理和细粒度奖励优化来改进MLLM在异常检测方面的应用，解决了现有基于GRPO方法的一些局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval",
        "summary": "Few-shot fine-grained visual classification (FGVC) aims to leverage limited\ndata to enable models to discriminate subtly distinct categories. Recent works\nmostly finetuned the pre-trained visual language models to achieve performance\ngain, yet suffering from overfitting and weak generalization. To deal with\nthis, we introduce UniFGVC, a universal training-free framework that\nreformulates few-shot FGVC as multimodal retrieval. First, we propose the\nCategory-Discriminative Visual Captioner (CDV-Captioner) to exploit the\nopen-world knowledge of multimodal large language models (MLLMs) to generate a\nstructured text description that captures the fine-grained attribute features\ndistinguishing closely related classes. CDV-Captioner uses chain-of-thought\nprompting and visually similar reference images to reduce hallucination and\nenhance discrimination of generated captions. Using it we can convert each\nimage into an image-description pair, enabling more comprehensive feature\nrepresentation, and construct the multimodal category templates using few-shot\nsamples for the subsequent retrieval pipeline. Then, off-the-shelf vision and\ntext encoders embed query and template pairs, and FGVC is accomplished by\nretrieving the nearest template in the joint space. UniFGVC ensures broad\ncompatibility with diverse MLLMs and encoders, offering reliable generalization\nand adaptability across few-shot FGVC scenarios. Extensive experiments on 12\nFGVC benchmarks demonstrate its consistent superiority over prior few-shot\nCLIP-based methods and even several fully-supervised MLLMs-based approaches.",
        "url": "http://arxiv.org/abs/2508.04136v1",
        "published_date": "2025-08-06T07:02:39+00:00",
        "updated_date": "2025-08-06T07:02:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongyu Guo",
            "Kuan Zhu",
            "Xiangzhao Hao",
            "Haiyun Guo",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "tldr": "The paper introduces UniFGVC, a training-free few-shot fine-grained visual classification framework that leverages multimodal retrieval and a category-discriminative visual captioner to achieve state-of-the-art performance across multiple benchmarks.",
        "tldr_zh": "该论文介绍了UniFGVC，一个无需训练的少样本细粒度视觉分类框架，它利用多模态检索和一个类别区分视觉字幕器，在多个基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode",
        "summary": "Reference Expression Segmentation (RES) aims to segment image regions\nspecified by referring expressions and has become popular with the rise of\nmultimodal large models (MLLMs). While MLLMs excel in semantic understanding,\ntheir token-generation paradigm struggles with pixel-level dense prediction.\nExisting RES methods either couple MLLMs with the parameter-heavy Segment\nAnything Model (SAM) with 632M network parameters or adopt SAM-free lightweight\npipelines that sacrifice accuracy. To address the trade-off between performance\nand cost, we specifically propose MLLMSeg, a novel framework that fully\nexploits the inherent visual detail features encoded in the MLLM vision encoder\nwithout introducing an extra visual encoder. Besides, we propose a\ndetail-enhanced and semantic-consistent feature fusion module (DSFF) that fully\nintegrates the detail-related visual feature with the semantic-related feature\noutput by the large language model (LLM) of MLLM. Finally, we establish a\nlight-weight mask decoder with only 34M network parameters that optimally\nleverages detailed spatial features from the visual encoder and semantic\nfeatures from the LLM to achieve precise mask prediction. Extensive experiments\ndemonstrate that our method generally surpasses both SAM-based and SAM-free\ncompetitors, striking a better balance between performance and cost. Code is\navailable at https://github.com/jcwang0602/MLLMSeg.",
        "url": "http://arxiv.org/abs/2508.04107v1",
        "published_date": "2025-08-06T06:06:52+00:00",
        "updated_date": "2025-08-06T06:06:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingchao Wang",
            "Zhijian Wu",
            "Dingjiang Huang",
            "Yefeng Zheng",
            "Hong Wang"
        ],
        "tldr": "The paper introduces MLLMSeg, a novel and lightweight framework for Referring Expression Segmentation that leverages MLLMs and a detail-enhanced feature fusion module to achieve high accuracy with significantly fewer parameters than SAM-based approaches.",
        "tldr_zh": "该论文介绍了MLLMSeg，一个新颖且轻量级的指代表达式分割框架，它利用MLLM和一个细节增强的特征融合模块，以比基于SAM的方法少得多的参数实现了高精度。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding",
        "summary": "Computer-aided medical image analysis is crucial for disease diagnosis and\ntreatment planning, yet limited annotated datasets restrict medical-specific\nmodel development. While vision-language models (VLMs) like CLIP offer strong\ngeneralization capabilities, their direct application to medical imaging\nanalysis is impeded by a significant domain gap. Existing approaches to bridge\nthis gap, including prompt learning and one-way modality interaction\ntechniques, typically focus on introducing domain knowledge to a single\nmodality. Although this may offer performance gains, it often causes modality\nmisalignment, thereby failing to unlock the full potential of VLMs. In this\npaper, we propose \\textbf{NEARL-CLIP} (i\\underline{N}teracted qu\\underline{E}ry\n\\underline{A}daptation with o\\underline{R}thogona\\underline{L} Regularization),\na novel cross-modality interaction VLM-based framework that contains two\ncontributions: (1) Unified Synergy Embedding Transformer (USEformer), which\ndynamically generates cross-modality queries to promote interaction between\nmodalities, thus fostering the mutual enrichment and enhancement of multi-modal\nmedical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA\nintroduces an orthogonality technique to decouple the new knowledge from\nUSEformer into two distinct components: the truly novel information and the\nincremental knowledge. By isolating the learning process from the interference\nof incremental knowledge, OCA enables a more focused acquisition of new\ninformation, thereby further facilitating modality interaction and unleashing\nthe capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in\na parameter-efficient style, which only introduces \\textbf{1.46M} learnable\nparameters.",
        "url": "http://arxiv.org/abs/2508.04101v1",
        "published_date": "2025-08-06T05:44:01+00:00",
        "updated_date": "2025-08-06T05:44:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zelin Peng",
            "Yichen Zhao",
            "Yu Huang",
            "Piao Yang",
            "Feilong Tang",
            "Zhengqin Xu",
            "Xiaokang Yang",
            "Wei Shen"
        ],
        "tldr": "The paper introduces NEARL-CLIP, a parameter-efficient framework for medical vision-language understanding that uses a Unified Synergy Embedding Transformer and an Orthogonal Cross-Attention Adapter to improve cross-modality interaction and knowledge acquisition. It aims to address the domain gap between general VLMs and medical imaging.",
        "tldr_zh": "该论文介绍了NEARL-CLIP，一个用于医学视觉语言理解的参数高效框架，它使用统一协同嵌入变换器和正交交叉注意力适配器来改善跨模态交互和知识获取。旨在解决通用VLM和医学影像之间的领域差距。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography",
        "summary": "Positron emission tomography (PET) is a cornerstone of modern oncologic and\nneurologic imaging, distinguished by its unique ability to illuminate dynamic\nmetabolic processes that transcend the anatomical focus of traditional imaging\ntechnologies. Radiology reports are essential for clinical decision making, yet\ntheir manual creation is labor-intensive and time-consuming. Recent\nadvancements of vision-language models (VLMs) have shown strong potential in\nmedical applications, presenting a promising avenue for automating report\ngeneration. However, existing applications of VLMs in the medical domain have\npredominantly focused on structural imaging modalities, while the unique\ncharacteristics of molecular PET imaging have largely been overlooked. To\nbridge the gap, we introduce PET2Rep, a large-scale comprehensive benchmark for\nevaluation of general and medical VLMs for radiology report generation for PET\nimages. PET2Rep stands out as the first dedicated dataset for PET report\ngeneration with metabolic information, uniquely capturing whole-body\nimage-report pairs that cover dozens of organs to fill the critical gap in\nexisting benchmarks and mirror real-world clinical comprehensiveness. In\naddition to widely recognized natural language generation metrics, we introduce\na series of clinical efficiency metrics to evaluate the quality of radiotracer\nuptake pattern description in key organs in generated reports. We conduct a\nhead-to-head comparison of 30 cutting-edge general-purpose and\nmedical-specialized VLMs. The results show that the current state-of-the-art\nVLMs perform poorly on PET report generation task, falling considerably short\nof fulfilling practical needs. Moreover, we identify several key insufficiency\nthat need to be addressed to advance the development in medical applications.",
        "url": "http://arxiv.org/abs/2508.04062v1",
        "published_date": "2025-08-06T03:46:51+00:00",
        "updated_date": "2025-08-06T03:46:51+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yichi Zhang",
            "Wenbo Zhang",
            "Zehui Ling",
            "Gang Feng",
            "Sisi Peng",
            "Deshu Chen",
            "Yuchen Liu",
            "Hongwei Zhang",
            "Shuqi Wang",
            "Lanlan Li",
            "Limei Han",
            "Yuan Cheng",
            "Zixin Hu",
            "Yuan Qi",
            "Le Xue"
        ],
        "tldr": "The paper introduces PET2Rep, a new benchmark dataset for PET radiology report generation, and evaluates the performance of 30 VLMs, finding they fall short of practical needs, highlighting key areas for improvement in molecular imaging report generation.",
        "tldr_zh": "该论文介绍了PET2Rep，一个新的PET放射学报告生成基准数据集，并评估了30个视觉语言模型的性能。研究发现这些模型距离实际需求仍有差距，并指出了分子影像报告生成方面需要改进的关键领域。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval",
        "summary": "Recently, prompt learning has demonstrated remarkable success in adapting\npre-trained Vision-Language Models (VLMs) to various downstream tasks such as\nimage classification. However, its application to the downstream Image-Text\nRetrieval (ITR) task is more challenging. We find that the challenge lies in\ndiscriminating both fine-grained attributes and similar subcategories of the\ndownstream data. To address this challenge, we propose Dual prompt Learning\nwith Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning\nframework to achieve precise image-text matching. The framework dynamically\nadjusts prompt vectors from both semantic and visual dimensions to improve the\nperformance of CLIP on the downstream ITR task. Based on the prompt paradigm,\nDCAR jointly optimizes attribute and class features to enhance fine-grained\nrepresentation learning. Specifically, (1) at the attribute level, it\ndynamically updates the weights of attribute descriptions based on text-image\nmutual information correlation; (2) at the category level, it introduces\nnegative samples from multiple perspectives with category-matching weighting to\nlearn subcategory distinctions. To validate our method, we construct the\nFine-class Described Retrieval Dataset (FDRD), which serves as a challenging\nbenchmark for ITR in downstream data domains. It covers over 1,500 downstream\nfine categories and 230,000 image-caption pairs with detailed attribute\nannotations. Extensive experiments on FDRD demonstrate that DCAR achieves\nstate-of-the-art performance over existing baselines.",
        "url": "http://arxiv.org/abs/2508.04028v1",
        "published_date": "2025-08-06T02:44:08+00:00",
        "updated_date": "2025-08-06T02:44:08+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Yifan Wang",
            "Tao Wang",
            "Chenwei Tang",
            "Caiyang Yu",
            "Zhengqing Zang",
            "Mengmi Zhang",
            "Shudong Huang",
            "Jiancheng Lv"
        ],
        "tldr": "The paper introduces DCAR, a dual-prompt learning framework, for adapting Vision-Language Models to downstream Image-Text Retrieval tasks, achieving state-of-the-art performance on a newly constructed fine-grained dataset (FDRD). It focuses on fine-grained attribute and subcategory discrimination.",
        "tldr_zh": "该论文介绍了一种双提示学习框架DCAR，用于将视觉语言模型适应于下游图像-文本检索任务，并在新构建的细粒度数据集（FDRD）上实现了最先进的性能。它侧重于细粒度属性和子类别区分。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification",
        "summary": "In this paper, we introduce RAVID, the first framework for AI-generated image\ndetection that leverages visual retrieval-augmented generation (RAG). While RAG\nmethods have shown promise in mitigating factual inaccuracies in foundation\nmodels, they have primarily focused on text, leaving visual knowledge\nunderexplored. Meanwhile, existing detection methods, which struggle with\ngeneralization and robustness, often rely on low-level artifacts and\nmodel-specific features, limiting their adaptability. To address this, RAVID\ndynamically retrieves relevant images to enhance detection. Our approach\nutilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with\ncategory-related prompts to improve representation learning. We further\nintegrate a vision-language model (VLM) to fuse retrieved images with the\nquery, enriching the input and improving accuracy. Given a query image, RAVID\ngenerates an embedding using RAVID CLIP, retrieves the most relevant images\nfrom a database, and combines these with the query image to form an enriched\ninput for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the\nUniversalFakeDetect benchmark, which covers 19 generative models, show that\nRAVID achieves state-of-the-art performance with an average accuracy of 93.85%.\nRAVID also outperforms traditional methods in terms of robustness, maintaining\nhigh accuracy even under image degradations such as Gaussian blur and JPEG\ncompression. Specifically, RAVID achieves an average accuracy of 80.27% under\ndegradation conditions, compared to 63.44% for the state-of-the-art model\nC2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG\ncompression scenarios. The code will be publicly available upon acceptance.",
        "url": "http://arxiv.org/abs/2508.03967v1",
        "published_date": "2025-08-05T23:10:56+00:00",
        "updated_date": "2025-08-05T23:10:56+00:00",
        "categories": [
            "cs.CV",
            "cs.CR",
            "cs.IR"
        ],
        "authors": [
            "Mamadou Keita",
            "Wassim Hamidouche",
            "Hessen Bougueffa Eutamene",
            "Abdelmalik Taleb-Ahmed",
            "Abdenour Hadid"
        ],
        "tldr": "RAVID is a novel framework for AI-generated image detection using retrieval-augmented generation (RAG), achieving state-of-the-art performance and robustness against image degradations by leveraging a fine-tuned CLIP model and a VLM.",
        "tldr_zh": "RAVID是一个新颖的AI生成图像检测框架，它使用检索增强生成（RAG），通过利用微调的CLIP模型和VLM，实现了最先进的性能和对图像退化的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts",
        "summary": "Multimodal vision-language models (VLMs) continue to achieve ever-improving\nscores on chart understanding benchmarks. Yet, we find that this progress does\nnot fully capture the breadth of visual reasoning capabilities essential for\ninterpreting charts. We introduce EncQA, a novel benchmark informed by the\nvisualization literature, designed to provide systematic coverage of visual\nencodings and analytic tasks that are crucial for chart understanding. EncQA\nprovides 2,076 synthetic question-answer pairs, enabling balanced coverage of\nsix visual encoding channels (position, length, area, color quantitative, color\nnominal, and shape) and eight tasks (find extrema, retrieve value, find\nanomaly, filter values, compute derived value exact, compute derived value\nrelative, correlate values, and correlate values relative). Our evaluation of 9\nstate-of-the-art VLMs reveals that performance varies significantly across\nencodings within the same task, as well as across tasks. Contrary to\nexpectations, we observe that performance does not improve with model size for\nmany task-encoding pairs. Our results suggest that advancing chart\nunderstanding requires targeted strategies addressing specific visual reasoning\ngaps, rather than solely scaling up model or dataset size.",
        "url": "http://arxiv.org/abs/2508.04650v1",
        "published_date": "2025-08-06T17:17:46+00:00",
        "updated_date": "2025-08-06T17:17:46+00:00",
        "categories": [
            "cs.CV",
            "I.2.0"
        ],
        "authors": [
            "Kushin Mukherjee",
            "Donghao Ren",
            "Dominik Moritz",
            "Yannick Assogba"
        ],
        "tldr": "The paper introduces EncQA, a new benchmark for evaluating vision-language models on chart understanding, highlighting that current models struggle with specific visual encodings and analytic tasks despite overall progress.",
        "tldr_zh": "该论文介绍了EncQA，一个新的基准测试，用于评估视觉语言模型在图表理解方面的能力，强调尽管整体取得了进展，但当前的模型在处理特定的视觉编码和分析任务时仍然存在困难。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case",
        "summary": "Collecting real-world data for rare high-risk scenarios, long-tailed driving\nevents, and complex interactions remains challenging, leading to poor\nperformance of existing autonomous driving systems in these critical\nsituations. In this paper, we propose RoboTron-Sim that improves real-world\ndriving in critical situations by utilizing simulated hard cases. First, we\ndevelop a simulated dataset called Hard-case Augmented Synthetic Scenarios\n(HASS), which covers 13 high-risk edge-case categories, as well as balanced\nenvironmental conditions such as day/night and sunny/rainy. Second, we\nintroduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder\n(I2E Encoder) to enable multimodal large language models to effectively learn\nreal-world challenging driving skills from HASS, via adapting to environmental\ndeviations and hardware differences between real-world and simulated scenarios.\nExtensive experiments on nuScenes show that RoboTron-Sim improves driving\nperformance in challenging scenarios by around 50%, achieving state-of-the-art\nresults in real-world open-loop planning. Qualitative results further\ndemonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk\ndriving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/",
        "url": "http://arxiv.org/abs/2508.04642v1",
        "published_date": "2025-08-06T17:07:25+00:00",
        "updated_date": "2025-08-06T17:07:25+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Baihui Xiao",
            "Chengjian Feng",
            "Zhijian Huang",
            "Feng yan",
            "Yujie Zhong",
            "Lin Ma"
        ],
        "tldr": "RoboTron-Sim improves autonomous driving in challenging scenarios by using a simulated dataset of hard cases and a multimodal large language model trained with scenario-aware prompt engineering to bridge the sim-to-real gap.",
        "tldr_zh": "RoboTron-Sim 通过使用模拟的极端场景数据集和多模态大语言模型来提升自动驾驶在挑战性场景下的表现，该模型采用场景感知提示工程来弥合仿真到真实的差距。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging",
        "summary": "We present FinMMR, a novel bilingual multimodal benchmark tailored to\nevaluate the reasoning capabilities of multimodal large language models (MLLMs)\nin financial numerical reasoning tasks. Compared to existing benchmarks, our\nwork introduces three significant advancements. (1) Multimodality: We\nmeticulously transform existing financial reasoning benchmarks, and construct\nnovel questions from the latest Chinese financial research reports. FinMMR\ncomprises 4.3K questions and 8.7K images spanning 14 categories, including\ntables, bar charts, and ownership structure charts. (2) Comprehensiveness:\nFinMMR encompasses 14 financial subdomains, including corporate finance,\nbanking, and industry analysis, significantly exceeding existing benchmarks in\nfinancial domain knowledge breadth. (3) Challenge: Models are required to\nperform multi-step precise numerical reasoning by integrating financial\nknowledge with the understanding of complex financial images and text. The\nbest-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe\nthat FinMMR will drive advancements in enhancing the reasoning capabilities of\nMLLMs in real-world scenarios.",
        "url": "http://arxiv.org/abs/2508.04625v1",
        "published_date": "2025-08-06T16:51:09+00:00",
        "updated_date": "2025-08-06T16:51:09+00:00",
        "categories": [
            "cs.CV",
            "cs.CE"
        ],
        "authors": [
            "Zichen Tang",
            "Haihong E",
            "Jiacheng Liu",
            "Zhongjun Yang",
            "Rongjin Li",
            "Zihua Rong",
            "Haoyang He",
            "Zhuodi Hao",
            "Xinyang Hu",
            "Kun Ji",
            "Ziyan Ma",
            "Mengyuan Ji",
            "Jun Zhang",
            "Chenghao Ma",
            "Qianhe Zheng",
            "Yang Liu",
            "Yiling Huang",
            "Xinyi Hu",
            "Qing Huang",
            "Zijian Xie",
            "Shiyao Peng"
        ],
        "tldr": "FinMMR introduces a new bilingual multimodal benchmark for financial numerical reasoning, designed to challenge and evaluate MLLMs with complex financial data and multi-step reasoning tasks.",
        "tldr_zh": "FinMMR 提出了一个新的双语多模态金融数值推理基准，旨在利用复杂的金融数据和多步骤推理任务来挑战和评估 MLLM。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model",
        "summary": "Concept Erasure, which aims to prevent pretrained text-to-image models from\ngenerating content associated with semantic-harmful concepts (i.e., target\nconcepts), is getting increased attention. State-of-the-art methods formulate\nthis task as an optimization problem: they align all target concepts with\nsemantic-harmless anchor concepts, and apply closed-form solutions to update\nthe model accordingly. While these closed-form methods are efficient, we argue\nthat existing methods have two overlooked limitations: 1) They often result in\nincomplete erasure due to \"non-zero alignment residual\", especially when text\nprompts are relatively complex. 2) They may suffer from generation quality\ndegradation as they always concentrate parameter updates in a few deep layers.\nTo address these issues, we propose a novel closed-form method ErasePro: it is\ndesigned for more complete concept erasure and better preserving overall\ngenerative quality. Specifically, ErasePro first introduces a strict\nzero-residual constraint into the optimization objective, ensuring perfect\nalignment between target and anchor concept features and enabling more complete\nerasure. Secondly, it employs a progressive, layer-wise update strategy that\ngradually transfers target concept features to those of the anchor concept from\nshallow to deep layers. As the depth increases, the required parameter changes\ndiminish, thereby reducing deviations in sensitive deep layers and preserving\ngenerative quality. Empirical results across different concept erasure tasks\n(including instance, art style, and nudity erasure) have demonstrated the\neffectiveness of our ErasePro.",
        "url": "http://arxiv.org/abs/2508.04472v1",
        "published_date": "2025-08-06T14:19:32+00:00",
        "updated_date": "2025-08-06T14:19:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hongxu Chen",
            "Zhen Wang",
            "Taoran Mei",
            "Lin Li",
            "Bowei Zhu",
            "Runshi Li",
            "Long Chen"
        ],
        "tldr": "The paper introduces ErasePro, a novel closed-form method for concept erasure in text-to-image models that addresses limitations of existing methods by ensuring zero-residual alignment and employing a progressive, layer-wise update strategy.",
        "tldr_zh": "该论文介绍了一种名为ErasePro的新型封闭式概念擦除方法，用于文本到图像模型，通过确保零残差对齐和采用渐进式分层更新策略来解决现有方法的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in vision-language tasks, yet they still face challenges when\nprocessing long-duration video inputs. The limitation arises from MLLMs'\ncontext limit and training costs, necessitating sparse frame sampling before\nfeeding videos into MLLMs. Existing video MLLMs adopt training-free uniform\nsampling or keyframe search, which may miss critical events or be constrained\nby the pre-trained models' event understanding capabilities. Meanwhile,\nbuilding a training-based method remains challenging due to the unsupervised\nand non-differentiable nature of sparse frame sampling. To address these\nproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancing\nMLLMs' long-form video-language understanding via reinforcement learning.\nSpecifically, we first propose a trainable event-aware temporal agent, which\ncaptures event-query correlation for performing probabilistic keyframe\nselection. Then, we propose the TSPO reinforcement learning paradigm, which\nmodels keyframe selection and language generation as a joint decision-making\nprocess, enabling end-to-end group relative optimization with efficient\nrule-based rewards. Furthermore, for the TSPO's training, we propose a long\nvideo training data construction pipeline with comprehensive temporal data and\nvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answering\naccuracy and temporal locating reward mechanisms to optimize the temporal\nsampling policy. Comprehensive experiments show that our TSPO achieves\nstate-of-the-art performance across multiple long video understanding\nbenchmarks, and shows transferable ability across different cutting-edge\nVideo-MLLMs.",
        "url": "http://arxiv.org/abs/2508.04369v1",
        "published_date": "2025-08-06T12:03:36+00:00",
        "updated_date": "2025-08-06T12:03:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Canhui Tang",
            "Zifan Han",
            "Hongbo Sun",
            "Sanping Zhou",
            "Xuchong Zhang",
            "Xin Wei",
            "Ye Yuan",
            "Jinglin Xu",
            "Hao Sun"
        ],
        "tldr": "The paper proposes Temporal Sampling Policy Optimization (TSPO), a reinforcement learning based approach to improve long-form video understanding in MLLMs by learning an event-aware temporal sampling policy. It outperforms existing methods on long video understanding benchmarks.",
        "tldr_zh": "本文提出了时间采样策略优化(TSPO)，一种基于强化学习的方法，通过学习事件感知的时序采样策略，来提高MLLM对长视频的理解能力。 该方法在长视频理解基准测试中优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations",
        "summary": "The 2025 Global Risks Report identifies state-based armed conflict and\nsocietal polarisation among the most pressing global threats, with social media\nplaying a central role in amplifying toxic discourse. Memes, as a widely used\nmode of online communication, often serve as vehicles for spreading harmful\ncontent. However, limitations in data accessibility and the high cost of\ndataset curation hinder the development of robust meme moderation systems. To\naddress this challenge, in this work, we introduce a first-of-its-kind dataset\nof 6,300 real-world meme-based posts annotated in two stages: (i) binary\nclassification into toxic and normal, and (ii) fine-grained labelling of toxic\nmemes as hateful, dangerous, or offensive. A key feature of this dataset is\nthat it is enriched with auxiliary metadata of socially relevant tags,\nenhancing the context of each meme. In addition, we propose a tag generation\nmodule that produces socially grounded tags, because most in-the-wild memes\noften do not come with tags. Experimental results show that incorporating these\ntags substantially enhances the performance of state-of-the-art VLMs detection\ntasks. Our contributions offer a novel and scalable foundation for improved\ncontent moderation in multimodal online environments.",
        "url": "http://arxiv.org/abs/2508.04166v1",
        "published_date": "2025-08-06T07:46:14+00:00",
        "updated_date": "2025-08-06T07:46:14+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Subhankar Swain",
            "Naquee Rizwan",
            "Nayandeep Deb",
            "Vishwajeet Singh Solanki",
            "Vishwa Gangadhar S",
            "Animesh Mukherjee"
        ],
        "tldr": "This paper introduces ToxicTAGS, a novel dataset of 6,300 memes annotated for toxicity and enriched with socially relevant tags, and demonstrates that incorporating these tags improves the performance of VLMs in detecting toxic content.",
        "tldr_zh": "该论文介绍了ToxicTAGS，一个包含6300个模因的新数据集，这些模因被标注为具有毒性并富含社会相关标签，并证明了结合这些标签可以提高VLM在检测有害内容方面的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CLIPVehicle: A Unified Framework for Vision-based Vehicle Search",
        "summary": "Vehicles, as one of the most common and significant objects in the real\nworld, the researches on which using computer vision technologies have made\nremarkable progress, such as vehicle detection, vehicle re-identification, etc.\nTo search an interested vehicle from the surveillance videos, existing methods\nfirst pre-detect and store all vehicle patches, and then apply vehicle\nre-identification models, which is resource-intensive and not very practical.\nIn this work, we aim to achieve the joint detection and re-identification for\nvehicle search. However, the conflicting objectives between detection that\nfocuses on shared vehicle commonness and re-identification that focuses on\nindividual vehicle uniqueness make it challenging for a model to learn in an\nend-to-end system. For this problem, we propose a new unified framework, namely\nCLIPVehicle, which contains a dual-granularity semantic-region alignment module\nto leverage the VLMs (Vision-Language Models) for vehicle discrimination\nmodeling, and a multi-level vehicle identification learning strategy to learn\nthe identity representation from global, instance and feature levels. We also\nconstruct a new benchmark, including a real-world dataset CityFlowVS, and two\nsynthetic datasets SynVS-Day and SynVS-All, for vehicle search. Extensive\nexperimental results demonstrate that our method outperforms the\nstate-of-the-art methods of both vehicle Re-ID and person search tasks.",
        "url": "http://arxiv.org/abs/2508.04120v1",
        "published_date": "2025-08-06T06:36:44+00:00",
        "updated_date": "2025-08-06T06:36:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Likai Wang",
            "Ruize Han",
            "Xiangqun Zhang",
            "Wei Feng"
        ],
        "tldr": "The paper introduces CLIPVehicle, a unified framework leveraging Vision-Language Models (VLMs) for joint vehicle detection and re-identification, along with a new benchmark dataset for vehicle search.",
        "tldr_zh": "该论文介绍了CLIPVehicle，一个利用视觉语言模型（VLMs）进行车辆联合检测和重识别的统一框架，并提出了一个新的车辆搜索基准数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models",
        "summary": "Occlusion perception, a critical foundation for human-level spatial\nunderstanding, embodies the challenge of integrating visual recognition and\nreasoning. Though multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities, their performance on occlusion perception remains\nunder-explored. To address this gap, we introduce O-Bench, the first visual\nquestion answering (VQA) benchmark specifically designed for occlusion\nperception. Based on SA-1B, we construct 1,365 images featuring semantically\ncoherent occlusion scenarios through a novel layered synthesis approach. Upon\nthis foundation, we annotate 4,588 question-answer pairs in total across five\ntailored tasks, employing a reliable, semi-automatic workflow. Our extensive\nevaluation of 22 representative MLLMs against the human baseline reveals a\nsignificant performance gap between current MLLMs and humans, which, we find,\ncannot be sufficiently bridged by model scaling or thinking process. We further\nidentify three typical failure patterns, including an overly conservative bias,\na fragile gestalt prediction, and a struggle with quantitative tasks. We\nbelieve O-Bench can not only provide a vital evaluation tool for occlusion\nperception, but also inspire the development of MLLMs for better visual\nintelligence. Our benchmark will be made publicly available upon paper\npublication.",
        "url": "http://arxiv.org/abs/2508.04059v1",
        "published_date": "2025-08-06T03:39:21+00:00",
        "updated_date": "2025-08-06T03:39:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaochen Liu",
            "Kaiwen Gao",
            "Shuyi Liang",
            "Bin Xiao",
            "Limeng Qiao",
            "Lin Ma",
            "Tingting Jiang"
        ],
        "tldr": "The paper introduces O-Bench, a new VQA benchmark for evaluating occlusion perception in MLLMs, revealing a significant performance gap between current models and humans and identifying common failure patterns.",
        "tldr_zh": "该论文介绍了O-Bench，一个新的VQA基准，用于评估MLLM中的遮挡感知，揭示了当前模型与人类之间的显著性能差距，并识别了常见的失败模式。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience",
        "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
        "url": "http://arxiv.org/abs/2508.04700v1",
        "published_date": "2025-08-06T17:58:46+00:00",
        "updated_date": "2025-08-06T17:58:46+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.MA",
            "cs.MM"
        ],
        "authors": [
            "Zeyi Sun",
            "Ziyu Liu",
            "Yuhang Zang",
            "Yuhang Cao",
            "Xiaoyi Dong",
            "Tong Wu",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "tldr": "The paper introduces SEAgent, a self-evolving computer use agent that learns to use novel software autonomously through trial-and-error and a specialist-to-generalist training strategy, achieving improved performance over existing methods.",
        "tldr_zh": "该论文介绍了SEAgent，一种自进化的计算机使用代理，通过试错法和专家到通才的训练策略自主学习使用新的软件，与现有方法相比，性能有所提高。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]