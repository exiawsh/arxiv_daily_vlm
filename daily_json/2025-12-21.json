[
    {
        "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
        "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
        "url": "http://arxiv.org/abs/2512.17909v1",
        "published_date": "2025-12-19T18:59:57+00:00",
        "updated_date": "2025-12-19T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shilong Zhang",
            "He Zhang",
            "Zhifei Zhang",
            "Chongjian Ge",
            "Shuchen Xue",
            "Shaoteng Liu",
            "Mengwei Ren",
            "Soo Ye Kim",
            "Yuqian Zhou",
            "Qing Liu",
            "Daniil Pakhomov",
            "Kai Zhang",
            "Zhe Lin",
            "Ping Luo"
        ],
        "tldr": "This paper addresses the challenges of using representation encoders in latent diffusion models for text-to-image generation and editing by proposing a semantic-pixel reconstruction objective to regularize the latent space, achieving SOTA results.",
        "tldr_zh": "本文提出了一种语义像素重建目标来规范潜在空间，解决了在文本到图像生成和编辑中使用潜在扩散模型中的表示编码器时遇到的挑战，并取得了SOTA结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
        "summary": "A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.",
        "url": "http://arxiv.org/abs/2512.17875v1",
        "published_date": "2025-12-19T18:26:58+00:00",
        "updated_date": "2025-12-19T18:26:58+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Haiwen Feng",
            "Long Lian",
            "Lisa Dunlap",
            "Jiahao Shu",
            "XuDong Wang",
            "Renhao Wang",
            "Trevor Darrell",
            "Alane Suhr",
            "Angjoo Kanazawa"
        ],
        "tldr": "This paper demonstrates that visually prompted VLMs are surprisingly fragile to changes in visual marker design and other low-level details, leading to unstable benchmark rankings. They propose VPBench, a more robust benchmark with multiple marker variants, to address this issue.",
        "tldr_zh": "该论文表明，视觉提示的 VLM 对视觉标记设计和其他底层细节的变化非常敏感，导致基准排名不稳定。他们提出了 VPBench，一个具有多种标记变体的更强大的基准，以解决此问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adversarial Robustness of Vision in Open Foundation Models",
        "summary": "With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.",
        "url": "http://arxiv.org/abs/2512.17902v1",
        "published_date": "2025-12-19T18:59:16+00:00",
        "updated_date": "2025-12-19T18:59:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Jonathon Fox",
            "William J Buchanan",
            "Pavlos Papadopoulos"
        ],
        "tldr": "The paper investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B when attacked via the visual input modality, finding that Llama 3.2 Vision is more robust than LLaVA despite lower baseline accuracy.",
        "tldr_zh": "该论文研究了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B在视觉输入模态攻击下的对抗鲁棒性，发现尽管Llama 3.2 Vision的基线准确率较低，但其鲁棒性优于LLaVA。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]