[
    {
        "title": "ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving",
        "summary": "In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.",
        "url": "http://arxiv.org/abs/2601.19582v1",
        "published_date": "2026-01-27T13:17:50+00:00",
        "updated_date": "2026-01-27T13:17:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujin Wang",
            "Yutong Zheng",
            "Wenxian Fan",
            "Tianyi Wang",
            "Hongqing Chu",
            "Daxin Tian",
            "Bingzhao Gao",
            "Jianqiang Wang",
            "Hong Chen"
        ],
        "tldr": "The paper introduces ScenePilot-Bench, a large-scale dataset and benchmark for evaluating vision-language models in autonomous driving, featuring a four-axis evaluation suite and safety-aware metrics.",
        "tldr_zh": "该论文介绍了ScenePilot-Bench，一个大规模数据集和基准，用于评估自动驾驶中的视觉语言模型，具有四轴评估套件和安全感知指标。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
        "summary": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.",
        "url": "http://arxiv.org/abs/2601.19686v1",
        "published_date": "2026-01-27T15:02:23+00:00",
        "updated_date": "2026-01-27T15:02:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyue Wang",
            "Sheng Jin",
            "Zhongrong Zuo",
            "Jiawei Wu",
            "Han Qiu",
            "Qi She",
            "Hao Zhang",
            "Xudong Jiang"
        ],
        "tldr": "Video-KTR enhances video reasoning in VLMs by using reinforcement learning with a novel modality-aware policy shaping framework that selectively reinforces key tokens based on visual, temporal, and uncertainty attribution signals, achieving state-of-the-art results.",
        "tldr_zh": "Video-KTR通过强化学习增强VLM中的视频推理能力，采用一种新颖的模态感知策略塑造框架，基于视觉、时间和不确定性归因信号选择性地强化关键token，从而达到最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KeepLoRA: Continual Learning with Residual Gradient Adaptation",
        "summary": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.",
        "url": "http://arxiv.org/abs/2601.19659v1",
        "published_date": "2026-01-27T14:38:57+00:00",
        "updated_date": "2026-01-27T14:38:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mao-Lin Luo",
            "Zi-Hao Zhou",
            "Yi-Lin Zhang",
            "Yuanyu Wan",
            "Tong Wei",
            "Min-Ling Zhang"
        ],
        "tldr": "The paper introduces KeepLoRA, a continual learning approach for vision-language models that preserves pre-trained knowledge by updating LoRA parameters in the residual subspace orthogonal to both the pre-trained model's principal subspace and previous tasks' feature directions. This method aims to balance knowledge retention, preservation, and plasticity.",
        "tldr_zh": "该论文介绍了KeepLoRA，一种用于视觉语言模型的持续学习方法。该方法通过在与预训练模型的主子空间和先前任务的特征方向正交的残差子空间中更新LoRA参数，来保留预训练知识，旨在平衡知识保留、保存和可塑性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework",
        "summary": "Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.",
        "url": "http://arxiv.org/abs/2601.19640v1",
        "published_date": "2026-01-27T14:17:04+00:00",
        "updated_date": "2026-01-27T14:17:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Chang",
            "Zhihui Wang",
            "Lingxiang Wu",
            "Peijin Wang",
            "Wenhui Diao",
            "Jinqiao Wang"
        ],
        "tldr": "The paper introduces GovLA-10K, a new management-oriented multi-modal benchmark for low-altitude intelligence, and GovLA-Reasoner, a vision-language reasoning framework that implicitly coordinates visual and language reasoning for governance-aware aerial perception, achieving improved performance without fine-tuning.",
        "tldr_zh": "本文介绍了GovLA-10K，一个面向管理的多模态低空智能基准数据集，以及GovLA-Reasoner，一个视觉-语言推理框架，它隐式地协调视觉和语言推理，用于治理感知的空中感知，并且无需微调即可提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
        "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.",
        "url": "http://arxiv.org/abs/2601.19325v1",
        "published_date": "2026-01-27T08:12:18+00:00",
        "updated_date": "2026-01-27T08:12:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zichen Wen",
            "Boxue Yang",
            "Shuang Chen",
            "Yaojie Zhang",
            "Yuhang Han",
            "Junlong Ke",
            "Cong Wang",
            "Yicheng Fu",
            "Jiawang Zhao",
            "Jiangchao Yao",
            "Xi Fang",
            "Zhen Wang",
            "Henxing Cai",
            "Lin Yao",
            "Zhifeng Gao",
            "Yanhui Hong",
            "Nang Yuan",
            "Yixuan Li",
            "Guojiang Zhao",
            "Haoyi Tao",
            "Nan Wang",
            "Han Lyu",
            "Guolin Ke",
            "Ning Liao",
            "Xiaoxing Wang",
            "Kai Chen",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Sihan Hu",
            "Kun Chen",
            "Yanfeng Wang",
            "Weinan E",
            "Linfeng Zhang",
            "Linfeng Zhang"
        ],
        "tldr": "Innovator-VL is a data-efficient, reproducible, and high-performing scientific multimodal LLM achieved through principled training and data selection, showcasing strong generalization without relying on massive pretraining.",
        "tldr_zh": "Innovator-VL是一个数据高效、可复现且高性能的科学多模态LLM，它通过原则性的训练和数据选择实现，无需大规模预训练即可展示强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
        "summary": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.",
        "url": "http://arxiv.org/abs/2601.19247v1",
        "published_date": "2026-01-27T06:30:32+00:00",
        "updated_date": "2026-01-27T06:30:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiarun Liu",
            "Qifeng Chen",
            "Yiru Zhao",
            "Minghua Liu",
            "Baorui Ma",
            "Sheng Yang"
        ],
        "tldr": "The paper introduces TIGaussian, a novel framework that uses 3D Gaussian Splatting to improve text-image-3D alignment by disentangling 3DGS properties and employing modality-specific alignment strategies, achieving state-of-the-art performance on several tasks.",
        "tldr_zh": "该论文介绍了 TIGaussian，一种新颖的框架，它使用 3D Gaussian Splatting 通过解开 3DGS 属性并采用特定于模态的对齐策略来改进文本-图像-3D 对齐，并在多个任务上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Pixel-Level VLM Perception via Simple Points Prediction",
        "summary": "We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/",
        "url": "http://arxiv.org/abs/2601.19228v1",
        "published_date": "2026-01-27T05:50:40+00:00",
        "updated_date": "2026-01-27T05:50:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianhui Song",
            "Haoyu Lu",
            "Hao Yang",
            "Lin Sui",
            "Haoning Wu",
            "Zaida Zhou",
            "Zhiqi Huang",
            "Yiping Bao",
            "Y. Charles",
            "Xinyu Zhou",
            "Limin Wang"
        ],
        "tldr": "SimpleSeg presents a novel approach to pixel-level perception in VLMs by reframing segmentation as a point sequence generation task, achieving comparable or superior performance to task-specific architectures. It uses Reinforcement Learning to refine the point sequences.",
        "tldr_zh": "SimpleSeg提出了一种新的VLM像素级感知方法，将分割重构为点序列生成任务，实现了与特定任务架构相当或更优越的性能。它使用强化学习来优化点序列。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP",
        "summary": "Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model's inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.",
        "url": "http://arxiv.org/abs/2601.19210v1",
        "published_date": "2026-01-27T05:24:45+00:00",
        "updated_date": "2026-01-27T05:24:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sen Nie",
            "Jie Zhang",
            "Zhuo Wang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "tldr": "This paper introduces Contrastive Spectral Rectification (CSR), a test-time defense method against adversarial attacks on CLIP models, leveraging spectral analysis to realign adversarial inputs and improve robustness across various tasks.",
        "tldr_zh": "本文介绍了一种名为对比谱校正（CSR）的测试时防御方法，用于对抗CLIP模型上的对抗攻击。该方法利用谱分析来重新对齐对抗性输入，并提高各种任务的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models",
        "summary": "Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits <search> tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.",
        "url": "http://arxiv.org/abs/2601.19060v1",
        "published_date": "2026-01-27T00:46:08+00:00",
        "updated_date": "2026-01-27T00:46:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jeonghwan Kim",
            "Renjie Tao",
            "Sanat Sharma",
            "Jiaqi Wang",
            "Kai Sun",
            "Zhaojiang Lin",
            "Seungwhan Moon",
            "Lambert Mathias",
            "Anuj Kumar",
            "Heng Ji",
            "Xin Luna Dong"
        ],
        "tldr": "This paper introduces PixSearch, an end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning by generating pixel-level masks as visual queries for improved factual consistency in VQA.",
        "tldr_zh": "本文介绍了 PixSearch，这是一个端到端的分割大型多模态模型 (LMM)，它通过生成像素级掩码作为视觉查询，统一了区域级感知和检索增强推理，从而提高了 VQA 中的事实一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
        "summary": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.",
        "url": "http://arxiv.org/abs/2601.19433v1",
        "published_date": "2026-01-27T10:10:55+00:00",
        "updated_date": "2026-01-27T10:10:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jisheng Chu",
            "Wenrui Li",
            "Rui Zhao",
            "Wangmeng Zuo",
            "Shifeng Chen",
            "Xiaopeng Fan"
        ],
        "tldr": "RoamScene3D generates immersive 3D scenes from text by using a scene graph to guide adaptive camera roaming and a motion-injected inpainting model for consistent scene completion, outperforming existing methods.",
        "tldr_zh": "RoamScene3D通过使用场景图引导自适应相机漫游和运动注入的修复模型来生成沉浸式文本到3D场景，性能优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning",
        "summary": "Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.\n  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.",
        "url": "http://arxiv.org/abs/2601.19099v1",
        "published_date": "2026-01-27T02:01:56+00:00",
        "updated_date": "2026-01-27T02:01:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yosub Shin",
            "Michael Buriek",
            "Igor Molybog"
        ],
        "tldr": "The paper introduces m2sv, a new benchmark for map-to-street-view spatial reasoning, highlighting the limitations of current VLMs in aligning abstract overhead representations with egocentric views and the need for improvement in geometric alignment and reasoning consistency.",
        "tldr_zh": "该论文介绍了 m2sv，一个新的地图到街景空间推理的基准，强调了当前 VLM 在对齐抽象的俯视表示与以自我为中心的视角方面的局限性，以及在几何对齐和推理一致性方面需要改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]