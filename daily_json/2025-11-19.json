[
    {
        "title": "VLMs Guided Interpretable Decision Making for Autonomous Driving",
        "summary": "Recent advancements in autonomous driving (AD) have explored the use of vision-language models (VLMs) within visual question answering (VQA) frameworks for direct driving decision-making. However, these approaches often depend on handcrafted prompts and suffer from inconsistent performance, limiting their robustness and generalization in real-world scenarios. In this work, we evaluate state-of-the-art open-source VLMs on high-level decision-making tasks using ego-view visual inputs and identify critical limitations in their ability to deliver reliable, context-aware decisions. Motivated by these observations, we propose a new approach that shifts the role of VLMs from direct decision generators to semantic enhancers. Specifically, we leverage their strong general scene understanding to enrich existing vision-based benchmarks with structured, linguistically rich scene descriptions. Building on this enriched representation, we introduce a multi-modal interactive architecture that fuses visual and linguistic features for more accurate decision-making and interpretable textual explanations. Furthermore, we design a post-hoc refinement module that utilizes VLMs to enhance prediction reliability. Extensive experiments on two autonomous driving benchmarks demonstrate that our approach achieves state-of-the-art performance, offering a promising direction for integrating VLMs into reliable and interpretable AD systems.",
        "url": "http://arxiv.org/abs/2511.13881v1",
        "published_date": "2025-11-17T19:57:51+00:00",
        "updated_date": "2025-11-17T19:57:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Hu",
            "Taotao Jing",
            "Renran Tian",
            "Zhengming Ding"
        ],
        "tldr": "This paper proposes a new approach for autonomous driving that uses VLMs to enhance scene understanding and provide structured linguistic descriptions, improving decision-making accuracy and interpretability compared to direct VLM-based decision-making.",
        "tldr_zh": "本文提出了一种新的自动驾驶方法，该方法利用视觉语言模型来增强场景理解并提供结构化的语言描述，与直接基于视觉语言模型的决策相比，提高了决策的准确性和可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities",
        "summary": "We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent",
        "url": "http://arxiv.org/abs/2511.14631v1",
        "published_date": "2025-11-18T16:23:02+00:00",
        "updated_date": "2025-11-18T16:23:02+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Kahaan Gandhi",
            "Boris Bolliet",
            "Inigo Zubeldia"
        ],
        "tldr": "This paper demonstrates that multi-agent systems using vision-language models (VLMs) can significantly improve autonomous scientific discovery by using plots as verifiable checkpoints and dynamically generating domain-specific rubrics for self-correction, resulting in substantial performance gains on a data-driven discovery benchmark.",
        "tldr_zh": "本文展示了使用视觉语言模型 (VLM) 的多智能体系统，通过将图表作为可验证的检查点，并动态生成特定领域的评估标准进行自我纠正，可以显著提高自主科学发现能力，从而在数据驱动的发现基准测试中获得显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models",
        "summary": "Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.",
        "url": "http://arxiv.org/abs/2511.14582v1",
        "published_date": "2025-11-18T15:22:32+00:00",
        "updated_date": "2025-11-18T15:22:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keda Tao",
            "Kele Shao",
            "Bohan Yu",
            "Weiqiang Wang",
            "Jian liu",
            "Huan Wang"
        ],
        "tldr": "OmniZip is a training-free, audio-guided token compression framework for multimodal LLMs that accelerates inference and reduces memory usage by dynamically pruning video tokens based on audio information density.",
        "tldr_zh": "OmniZip是一种免训练的，音频引导的多模态LLM token压缩框架，通过基于音频信息密度动态修剪视频tokens来加速推理并减少内存使用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DIR-TIR: Dialog-Iterative Refinement for Text-to-Image Retrieval",
        "summary": "This paper addresses the task of interactive, conversational text-to-image retrieval.\n  Our DIR-TIR framework progressively refines the target image search through two specialized modules: the Dialog Refiner Module and the Image Refiner Module.\n  The Dialog Refiner actively queries users to extract essential information and generate increasingly precise descriptions of the target image.\n  Complementarily, the Image Refiner identifies perceptual gaps between generated images and user intentions, strategically reducing the visual-semantic discrepancy. By leveraging multi-turn dialogues, DIR-TIR provides superior controllability and fault tolerance compared to conventional single-query methods, significantly improving target image hit accuracy.\n  Comprehensive experiments across diverse image datasets demonstrate our dialogue-based approach substantially outperforms initial-description-only baselines, while the synergistic module integration achieves both higher retrieval precision and enhanced interactive experience.",
        "url": "http://arxiv.org/abs/2511.14449v1",
        "published_date": "2025-11-18T12:45:10+00:00",
        "updated_date": "2025-11-18T12:45:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongwei Zhen",
            "Biqing Zeng"
        ],
        "tldr": "The paper introduces DIR-TIR, a dialog-based framework for text-to-image retrieval that iteratively refines image search through dialog and image refinement modules, leading to improved accuracy and user experience.",
        "tldr_zh": "该论文介绍了DIR-TIR，一个基于对话的文本到图像检索框架，通过对话和图像细化模块迭代地改进图像搜索，从而提高准确性和用户体验。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding",
        "summary": "Video understanding requires not only visual recognition but also complex reasoning. While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement. While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training. To overcome these limitations, we propose Agentic Video Intelligence (AVI), a flexible and training-free framework that can mirror human video comprehension through system-level design and optimization. AVI introduces three key innovations: (1) a human-inspired three-phase reasoning process (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis, (2) a structured video knowledge base organized through entity graphs, along with multi-granularity integrated tools, constituting the agent's interaction environment, and (3) an open-source model ensemble combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs or RL training. Experiments on LVBench, VideoMME-Long, LongVideoBench, and Charades-STA demonstrate that AVI achieves competitive performance while offering superior interpretability.",
        "url": "http://arxiv.org/abs/2511.14446v1",
        "published_date": "2025-11-18T12:43:15+00:00",
        "updated_date": "2025-11-18T12:43:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hong Gao",
            "Yiming Bao",
            "Xuezhen Tu",
            "Yutong Xu",
            "Yue Jin",
            "Yiyang Mu",
            "Bin Zhong",
            "Linan Yue",
            "Min-Ling Zhang"
        ],
        "tldr": "The paper introduces Agentic Video Intelligence (AVI), a training-free framework that mirrors human video comprehension using a three-phase reasoning process and an open-source model ensemble, achieving competitive performance and superior interpretability on several benchmarks.",
        "tldr_zh": "该论文介绍了Agentic Video Intelligence (AVI)，一个无需训练的框架，通过三阶段推理过程和开源模型集合来模仿人类的视频理解能力，并在多个基准测试上实现了有竞争力的性能和卓越的可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model",
        "summary": "While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.",
        "url": "http://arxiv.org/abs/2511.14368v1",
        "published_date": "2025-11-18T11:18:08+00:00",
        "updated_date": "2025-11-18T11:18:08+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Rishi Gupta",
            "Mukilan Karuppasamy",
            "Shyam Marjit",
            "Aditay Tripathi",
            "Anirban Chakraborty"
        ],
        "tldr": "The paper introduces O3SLM, an open-weight LVLM, and a large-scale image-sketch-instruction dataset (SketchVCL) for improved sketch comprehension, demonstrating state-of-the-art performance in sketch-based tasks.",
        "tldr_zh": "该论文介绍了O3SLM，一个开放权重的LVLM，以及一个大规模的图像-草图-指令数据集（SketchVCL），用于提高草图理解能力，并在基于草图的任务中展示了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation",
        "summary": "Text-to-3D generation has advanced rapidly, yet state-of-the-art models, encompassing both optimization-based and feed-forward architectures, still face two fundamental limitations. First, they struggle with coarse semantic alignment, often failing to capture fine-grained prompt details. Second, they lack robust 3D spatial understanding, leading to geometric inconsistencies and catastrophic failures in part assembly and spatial relationships. To address these challenges, we propose VLM3D, a general framework that repurposes large vision-language models (VLMs) as powerful, differentiable semantic and spatial critics. Our core contribution is a dual-query critic signal derived from the VLM's Yes or No log-odds, which assesses both semantic fidelity and geometric coherence. We demonstrate the generality of this guidance signal across two distinct paradigms: (1) As a reward objective for optimization-based pipelines, VLM3D significantly outperforms existing methods on standard benchmarks. (2) As a test-time guidance module for feed-forward pipelines, it actively steers the iterative sampling process of SOTA native 3D models to correct severe spatial errors. VLM3D establishes a principled and generalizable path to inject the VLM's rich, language-grounded understanding of both semantics and space into diverse 3D generative pipelines.",
        "url": "http://arxiv.org/abs/2511.14271v1",
        "published_date": "2025-11-18T09:05:26+00:00",
        "updated_date": "2025-11-18T09:05:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weimin Bai",
            "Yubo Li",
            "Weijian Luo",
            "Zeqiang Lai",
            "Yequan Wang",
            "Wenzheng Chen",
            "He Sun"
        ],
        "tldr": "The paper introduces VLM3D, a framework that uses VLMs as differentiable semantic and spatial critics to improve text-to-3D generation by addressing semantic alignment and geometric consistency issues.",
        "tldr_zh": "该论文介绍了 VLM3D，一个利用视觉-语言模型作为可微分的语义和空间评论器的框架，通过解决语义对齐和几何一致性问题来改进文本到 3D 的生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution",
        "summary": "We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.",
        "url": "http://arxiv.org/abs/2511.14210v1",
        "published_date": "2025-11-18T07:41:02+00:00",
        "updated_date": "2025-11-18T07:41:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "N Dinesh Reddy",
            "Sudeep Pillai"
        ],
        "tldr": "Orion is a multimodal visual agent framework that utilizes tool-calling to achieve state-of-the-art results on various visual AI tasks, transitioning from passive understanding to active, tool-driven visual intelligence.",
        "tldr_zh": "Orion是一个多模态视觉代理框架，通过调用工具在各种视觉AI任务上实现了最先进的成果，从被动的理解过渡到主动的、工具驱动的视觉智能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.",
        "url": "http://arxiv.org/abs/2511.14169v1",
        "published_date": "2025-11-18T06:12:15+00:00",
        "updated_date": "2025-11-18T06:12:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinliang Zhang",
            "Lei Zhu",
            "Hangzhou He",
            "Shuang Zeng",
            "Ourui Fu",
            "Jiakui Hu",
            "Zhengjian Yao",
            "Yanye Lu"
        ],
        "tldr": "AdaTok proposes an object-level token merging strategy for adaptive token compression in MLLMs, significantly reducing the number of tokens needed while maintaining high performance, thus addressing computational and memory burdens and aligning with human vision.",
        "tldr_zh": "AdaTok提出了一种对象级别的令牌合并策略，用于MLLM中的自适应令牌压缩，在保持高性能的同时显著减少了所需令牌的数量，从而解决了计算和内存负担问题，并与人类视觉对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs",
        "summary": "Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.",
        "url": "http://arxiv.org/abs/2511.14159v1",
        "published_date": "2025-11-18T05:48:08+00:00",
        "updated_date": "2025-11-18T05:48:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huiyi Chen",
            "Jiawei Peng",
            "Dehai Min",
            "Changchang Sun",
            "Kaijie Chen",
            "Yan Yan",
            "Xu Yang",
            "Lu Cheng"
        ],
        "tldr": "The paper introduces MVI-Bench, a new benchmark for evaluating the robustness of Large Vision-Language Models (LVLMs) against misleading visual inputs, categorized by visual concepts, attributes, and relationships, and introduces a metric called MVI-Sensitivity.",
        "tldr_zh": "该论文介绍了MVI-Bench，一个新的基准，用于评估大型视觉语言模型（LVLMs）在面对误导性视觉输入时的鲁棒性，这些输入按照视觉概念、属性和关系进行分类，并引入了一个名为MVI-Sensitivity的指标。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM",
        "summary": "Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \\textit{S}hot-aware \\textit{M}ultimodal \\textit{A}udio-enhanced \\textit{R}etrieval of \\textit{T}emporal \\textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \\textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\\% increase in R1@0.5 and 2.59\\% gain in R1@0.7 on Charades-STA.",
        "url": "http://arxiv.org/abs/2511.14143v1",
        "published_date": "2025-11-18T05:03:17+00:00",
        "updated_date": "2025-11-18T05:03:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "An Yu",
            "Weiheng Lu",
            "Jian Li",
            "Zhenfei Zhang",
            "Yunhang Shen",
            "Felix X. -F. Ye",
            "Ming-Ching Chang"
        ],
        "tldr": "The paper introduces SMART, a novel MLLM-based framework for video moment retrieval that integrates audio cues and shot-level temporal structure, achieving SOTA results on Charades-STA and QVHighlights datasets.",
        "tldr_zh": "该论文介绍了SMART，一种基于MLLM的新型视频片段检索框架，它集成了音频线索和镜头级的时间结构，并在Charades-STA和QVHighlights数据集上取得了SOTA结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models",
        "summary": "Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.",
        "url": "http://arxiv.org/abs/2511.14120v1",
        "published_date": "2025-11-18T04:12:24+00:00",
        "updated_date": "2025-11-18T04:12:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hao Zhen",
            "Yunxiang Yang",
            "Jidong J. Yang"
        ],
        "tldr": "The paper introduces MP-PVIR, a framework that uses VLMs to analyze multi-view video of pedestrian-vehicle incidents, segmenting behavior into phases and generating diagnostic reports with prevention strategies. It demonstrates improved performance on the Woven Traffic Safety dataset.",
        "tldr_zh": "该论文介绍了 MP-PVIR，一个使用 VLM 分析行人-车辆事故的多视角视频的框架，将行为分割为阶段并生成包含预防策略的诊断报告。 它证明了在 Woven Traffic Safety 数据集上性能的提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Error-Driven Scene Editing for 3D Grounding in Large Language Models",
        "summary": "Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured \"Decompose, Diagnostic Evaluation, Edit, and Re-train\" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.",
        "url": "http://arxiv.org/abs/2511.14086v1",
        "published_date": "2025-11-18T03:13:29+00:00",
        "updated_date": "2025-11-18T03:13:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yue Zhang",
            "Zun Wang",
            "Han Lin",
            "Jialu Li",
            "Jianing Yang",
            "Yonatan Bitton",
            "Idan Szpektor",
            "Mohit Bansal"
        ],
        "tldr": "The paper introduces DEER-3D, an error-driven 3D scene editing framework that generates targeted visual counterfactuals for fine-tuning 3D-LLMs, addressing limitations in language grounding by focusing on predicate-level spatial understanding.",
        "tldr_zh": "该论文介绍了DEER-3D，一个误差驱动的3D场景编辑框架，通过生成有针对性的视觉反事实来微调3D-LLM，并通过关注谓词级别的空间理解来解决语言接地的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CORE: Compact Object-centric REpresentations as a New Paradigm for Token Merging in LVLMs",
        "summary": "Large Vision-Language Models (LVLMs) usually suffer from prohibitive computational and memory costs due to the quadratic growth of visual tokens with image resolution. Existing token compression methods, while varied, often lack a high-level semantic understanding, leading to suboptimal merges, information redundancy, or context loss. To address these limitations, we introduce CORE (Compact Object-centric REpresentations), a new paradigm for visual token compression. CORE leverages an efficient segmentation decoder to generate object masks, which serve as a high-level semantic prior to guide the merging of visual tokens into a compact set of object-centric representations. Furthermore, a novel centroid-guided sorting mechanism restores a coherent spatial order to the merged tokens, preserving vital positional information. Extensive experiments show that CORE not only establishes a new state-of-the-art on six authoritative benchmarks for fixed-rate compression, but also achieves dramatic efficiency gains in adaptive-rate settings. Even under extreme compression, after aggressively retaining with only 2.2% of all visual tokens, CORE still maintains 97.4% of baseline performance. Our work demonstrates the superiority of object-centric representations for efficient and effective LVLM processing.",
        "url": "http://arxiv.org/abs/2511.14072v1",
        "published_date": "2025-11-18T03:02:23+00:00",
        "updated_date": "2025-11-18T03:02:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyu Lei",
            "Gaoang Wang",
            "Der-Horng Lee"
        ],
        "tldr": "This paper introduces CORE, a new object-centric token merging paradigm for LVLMs that achieves state-of-the-art compression rates while maintaining high performance, addressing the computational cost issue of high-resolution images.",
        "tldr_zh": "该论文介绍了CORE，一种用于LVLM的新的以对象为中心的令牌合并范例，它实现了最先进的压缩率，同时保持了高性能，解决了高分辨率图像的计算成本问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models",
        "summary": "Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.",
        "url": "http://arxiv.org/abs/2511.13891v1",
        "published_date": "2025-11-17T20:29:44+00:00",
        "updated_date": "2025-11-17T20:29:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seyed Mohamad Ali Tousi",
            "John A. Lory",
            "G. N. DeSouza"
        ],
        "tldr": "This paper presents a weakly supervised pipeline using Vision Language Models (VLMs) for ephemeral gully detection in remote sensing images, addressing the challenges of limited labeled data and the temporal nature of the problem. They also release a new dataset for this task.",
        "tldr_zh": "本文提出了一种使用视觉语言模型(VLM)的弱监督管道，用于遥感图像中短暂冲沟的检测，解决了标记数据有限和问题的时间性等挑战。他们还发布了一个用于此任务的新数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "QwenCLIP: Boosting Medical Vision-Language Pretraining via LLM Embeddings and Prompt tuning",
        "summary": "Contrastive Language-Image Pretraining (CLIP) has demonstrated strong generalization for vision-language tasks in computer vision and medical domains, yet its text encoder accepts only up to 77 tokens, which limits its ability to represent long and information-rich radiology reports. Recent adaptations using domain-specific encoders, such as PubMedBERT or ClinicalBERT, mitigate this issue by leveraging medical corpora, but remain constrained by their limited input length (typically 512 tokens) and relatively shallow semantic understanding. To address these limitations, we propose QwenCLIP, a vision-language framework that replaces CLIP's text encoder with a large language model (LLM)-based embedding module (e.g., Qwen3-Embedding) and introduces learnable prompts to enhance cross-modal alignment. By leveraging the extended context window and richer representations of LLMs, QwenCLIP captures comprehensive medical semantics from long-form clinical text, substantially improving medical image-text alignment and downstream performance on radiology benchmarks. Our code is publicly available at https://github.com/Wxy-24/QwenCLIP.",
        "url": "http://arxiv.org/abs/2511.13876v1",
        "published_date": "2025-11-17T19:51:59+00:00",
        "updated_date": "2025-11-17T19:51:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyang Wei",
            "Camille Kurtz",
            "Florence Cloppet"
        ],
        "tldr": "QwenCLIP enhances medical vision-language pretraining by using LLM embeddings to overcome the input length limitations of traditional CLIP models, significantly improving performance on radiology benchmarks.",
        "tldr_zh": "QwenCLIP通过使用大型语言模型（LLM）嵌入来增强医学视觉语言预训练，克服了传统CLIP模型的输入长度限制，并显著提高了放射学基准测试的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
        "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
        "url": "http://arxiv.org/abs/2511.13719v1",
        "published_date": "2025-11-17T18:59:33+00:00",
        "updated_date": "2025-11-17T18:59:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.RO"
        ],
        "authors": [
            "Zhongang Cai",
            "Ruisi Wang",
            "Chenyang Gu",
            "Fanyi Pu",
            "Junxiang Xu",
            "Yubo Wang",
            "Wanqi Yin",
            "Zhitao Yang",
            "Chen Wei",
            "Qingping Sun",
            "Tongxi Zhou",
            "Jiaqi Li",
            "Hui En Pang",
            "Oscar Qian",
            "Yukun Wei",
            "Zhiqian Lin",
            "Xuanke Shi",
            "Kewang Deng",
            "Xiaoyang Han",
            "Zukai Chen",
            "Xiangyu Fan",
            "Hanming Deng",
            "Lewei Lu",
            "Liang Pan",
            "Bo Li",
            "Ziwei Liu",
            "Quan Wang",
            "Dahua Lin",
            "Lei Yang"
        ],
        "tldr": "This paper introduces SenseNova-SI, a family of scaled-up multimodal foundation models with enhanced spatial intelligence, trained on a curated dataset of 8 million samples and achieving state-of-the-art performance on various spatial intelligence benchmarks.",
        "tldr_zh": "本文介绍了SenseNova-SI，一个具有增强空间智能的扩展多模态基础模型系列。该模型使用一个包含800万样本的精选数据集进行训练，并在各种空间智能基准测试中取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM",
        "summary": "The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.",
        "url": "http://arxiv.org/abs/2511.14499v1",
        "published_date": "2025-11-18T13:46:18+00:00",
        "updated_date": "2025-11-18T13:46:18+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jack Qin",
            "Zhitao Wang",
            "Yinan Zheng",
            "Keyu Chen",
            "Yang Zhou",
            "Yuanxin Zhong",
            "Siyuan Cheng"
        ],
        "tldr": "This paper introduces Risk Semantic Distillation (RSD), a framework that uses VLMs to enhance end-to-end autonomous driving by distilling risk attention into BEV features, improving generalization and handling of risky situations.",
        "tldr_zh": "本文介绍了风险语义蒸馏（RSD），该框架利用视觉语言模型（VLMs）通过将风险注意力提取到鸟瞰图（BEV）特征中来增强端到端自动驾驶，从而提高泛化能力并处理风险情况。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding",
        "summary": "A structured understanding of intraoral 3D scans is essential for digital orthodontics. However, existing deep-learning approaches rely heavily on modality-specific training, large annotated datasets, and controlled scanning conditions, which limit generalization across devices and hinder deployment in real clinical workflows. Moreover, raw intraoral meshes exhibit substantial variation in arch pose, incomplete geometry caused by occlusion or tooth contact, and a lack of texture cues, making unified semantic interpretation highly challenging. To address these limitations, we propose ArchMap, a training-free and knowledge-guided framework for robust structured dental understanding. ArchMap first introduces a geometry-aware arch-flattening module that standardizes raw 3D meshes into spatially aligned, continuity-preserving multi-view projections. We then construct a Dental Knowledge Base (DKB) encoding hierarchical tooth ontology, dentition-stage policies, and clinical semantics to constrain the symbolic reasoning space. We validate ArchMap on 1060 pre-/post-orthodontic cases, demonstrating robust performance in tooth counting, anatomical partitioning, dentition-stage classification, and the identification of clinical conditions such as crowding, missing teeth, prosthetics, and caries. Compared with supervised pipelines and prompted VLM baselines, ArchMap achieves higher accuracy, reduced semantic drift, and superior stability under sparse or artifact-prone conditions. As a fully training-free system, ArchMap demonstrates that combining geometric normalization with ontology-guided multimodal reasoning offers a practical and scalable solution for the structured analysis of 3D intraoral scans in modern digital orthodontics.",
        "url": "http://arxiv.org/abs/2511.14336v1",
        "published_date": "2025-11-18T10:46:06+00:00",
        "updated_date": "2025-11-18T10:46:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bohan Zhang",
            "Yiyi Miao",
            "Taoyu Wu",
            "Tong Chen",
            "Ji Jiang",
            "Zhuoxiao Li",
            "Zhe Tang",
            "Limin Yu",
            "Jionglong Su"
        ],
        "tldr": "ArchMap is a training-free, knowledge-guided framework for structured dental understanding from 3D intraoral scans, using arch-flattening and a Dental Knowledge Base for robust performance in various clinical tasks.",
        "tldr_zh": "ArchMap是一个无需训练的、知识引导的框架，用于从3D口腔内扫描中进行结构化牙科理解，它使用弓形展平和牙科知识库，在各种临床任务中实现强大的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding",
        "summary": "Chain-of-Thought (CoT) prompting has recently shown significant promise across various NLP and computer vision tasks by explicitly generating intermediate reasoning steps. However, we find that reinforcement learning (RL)-based fine-tuned CoT reasoning can paradoxically degrade performance in Visual Grounding tasks, particularly as CoT outputs become lengthy or complex. Additionally, our analysis reveals that increased dataset size does not always enhance performance due to varying data complexities. Motivated by these findings, we propose Curriculum-based Relative Policy Optimization (CuRPO), a novel training strategy that leverages CoT length and generalized Intersection over Union (gIoU) rewards as complexity indicators to progressively structure training data from simpler to more challenging examples. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and LISA datasets demonstrate the effectiveness of our approach. CuRPO consistently outperforms existing methods, including Visual-RFT, with notable improvements of up to +12.52 mAP on RefCOCO. Moreover, CuRPO exhibits exceptional efficiency and robustness, delivering strong localization performance even in few-shot learning scenarios, particularly benefiting tasks characterized by ambiguous and intricate textual descriptions.The code is released on https://github.com/qyoung-yan/CuRPO.",
        "url": "http://arxiv.org/abs/2511.13924v1",
        "published_date": "2025-11-17T21:22:50+00:00",
        "updated_date": "2025-11-17T21:22:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingyang Yan",
            "Guangyao Chen",
            "Yixiong Zou"
        ],
        "tldr": "The paper introduces Curriculum-based Relative Policy Optimization (CuRPO), a training strategy that improves visual grounding performance by structuring training data based on complexity indicators like CoT length and gIoU rewards, achieving significant improvements over existing methods.",
        "tldr_zh": "该论文介绍了一种基于课程的相对策略优化（CuRPO）训练策略，通过基于CoT长度和gIoU奖励等复杂度指标来组织训练数据，从而提高视觉定位性能，并优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
        "summary": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
        "url": "http://arxiv.org/abs/2511.13684v1",
        "published_date": "2025-11-17T18:37:41+00:00",
        "updated_date": "2025-11-17T18:37:41+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jiangnan Ye",
            "Jiedong Zhuang",
            "Lianrui Mu",
            "Wenjie Zheng",
            "Jiaqi Hu",
            "Xingze Zou",
            "Jing Wang",
            "Haoji Hu"
        ],
        "tldr": "The paper presents GS-Light, a training-free pipeline for text-guided relighting of 3D scenes using Gaussian Splatting and diffusion models, leveraging a vision-language model to parse text prompts into lighting priors and achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了 GS-Light，一个无需训练的流程，用于使用高斯溅射和扩散模型对 3D 场景进行文本引导的重新照明。它利用视觉语言模型将文本提示解析为光照先验，并取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mind the Gap: Evaluating LLM Understanding of Human-Taught Road Safety Principles",
        "summary": "Following road safety norms is non-negotiable not only for humans but also for the AI systems that govern autonomous vehicles. In this work, we evaluate how well multi-modal large language models (LLMs) understand road safety concepts, specifically through schematic and illustrative representations. We curate a pilot dataset of images depicting traffic signs and road-safety norms sourced from school text books and use it to evaluate models capabilities in a zero-shot setting. Our preliminary results show that these models struggle with safety reasoning and reveal gaps between human learning and model interpretation. We further provide an analysis of these performance gaps for future research.",
        "url": "http://arxiv.org/abs/2511.13909v1",
        "published_date": "2025-11-17T21:01:48+00:00",
        "updated_date": "2025-11-17T21:01:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chalamalasetti Kranti"
        ],
        "tldr": "This paper evaluates how well multimodal LLMs understand road safety concepts using images of traffic signs and road-safety norms. The results show that these models struggle with safety reasoning.",
        "tldr_zh": "本文评估了多模态LLM对道路安全概念的理解程度，通过使用交通标志和道路安全规范的图像进行评估。结果表明，这些模型在安全推理方面存在困难。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
        "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
        "url": "http://arxiv.org/abs/2511.13655v1",
        "published_date": "2025-11-17T18:06:26+00:00",
        "updated_date": "2025-11-17T18:06:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Henry Herzog",
            "Favyen Bastani",
            "Yawen Zhang",
            "Gabriel Tseng",
            "Joseph Redmon",
            "Hadrien Sablon",
            "Ryan Park",
            "Jacob Morrison",
            "Alexandra Buraczynski",
            "Karen Farley",
            "Joshua Hansen",
            "Andrew Howe",
            "Patrick Alan Johnson",
            "Mark Otterlee",
            "Ted Schmitt",
            "Hunter Pitelka",
            "Stephen Daspit",
            "Rachel Ratner",
            "Christopher Wilhelm",
            "Sebastian Wood",
            "Mike Jacobi",
            "Hannah Kerner",
            "Evan Shelhamer",
            "Ali Farhadi",
            "Ranjay Krishna",
            "Patrick Beukema"
        ],
        "tldr": "OlmoEarth is a multimodal, spatio-temporal foundation model for Earth observation data, achieving state-of-the-art performance on various benchmarks and deployed as a platform for data collection, training, and inference, designed for use by non-profits and NGOs.",
        "tldr_zh": "OlmoEarth是一个用于地球观测数据的多模态时空基础模型，在各种基准测试中实现了最先进的性能，并被部署为一个数据收集、训练和推理平台，专为非营利组织和非政府组织使用而设计。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]