[
    {
        "title": "Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model",
        "summary": "Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.",
        "url": "http://arxiv.org/abs/2601.00716v1",
        "published_date": "2026-01-02T15:12:06+00:00",
        "updated_date": "2026-01-02T15:12:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hao Guan",
            "Li Zhou"
        ],
        "tldr": "This paper investigates performance degradation detection in pathology Vision-Language Models (VLMs) under data shift, proposing a framework combining input shift detection and output confidence-based indicators to improve reliability monitoring.",
        "tldr_zh": "本文研究了病理视觉语言模型（VLMs）在数据偏移下的性能退化检测问题，提出了一个结合输入偏移检测和基于输出置信度的指标的框架，以提高可靠性监控。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models",
        "summary": "Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.",
        "url": "http://arxiv.org/abs/2601.00659v1",
        "published_date": "2026-01-02T11:39:00+00:00",
        "updated_date": "2026-01-02T11:39:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Neeraj Anand",
            "Samyak Jha",
            "Udbhav Bamba",
            "Rahul Rahaman"
        ],
        "tldr": "The paper introduces CRoPS, a training-free framework to mitigate hallucinations in Vision-Language Models (LVLMs) by selectively removing key text tokens and using Generalized Contrastive Decoding with multiple hallucinated models. It outperforms existing training-free methods on multiple benchmarks.",
        "tldr_zh": "该论文介绍了一个名为CRoPS的免训练框架，通过选择性地移除关键文本标记并使用带有多个幻觉模型的广义对比解码，来缓解视觉语言模型（LVLM）中的幻觉问题。 并在多个基准测试中优于现有的免训练方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models",
        "summary": "The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\\emph{i.e.}, \\textbf{A}ssessing \\textbf{E}diting, \\textbf{G}eneration, \\textbf{I}nterpretation-Understanding for \\textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.",
        "url": "http://arxiv.org/abs/2601.00561v1",
        "published_date": "2026-01-02T04:22:18+00:00",
        "updated_date": "2026-01-02T04:22:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jintao Lin",
            "Bowen Dong",
            "Weikang Shi",
            "Chenyang Lei",
            "Suiyun Zhang",
            "Rui Liu",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces AEGIS, a new multi-task benchmark and evaluation protocol (DCE) for assessing world knowledge capabilities in Unified Multimodal Models, revealing significant deficiencies and suggesting potential improvements.",
        "tldr_zh": "该论文介绍了AEGIS，一个新的多任务基准和评估协议(DCE)，用于评估统一多模态模型中的世界知识能力，揭示了显著的不足，并提出了潜在的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
        "summary": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
        "url": "http://arxiv.org/abs/2601.00501v1",
        "published_date": "2026-01-01T22:48:26+00:00",
        "updated_date": "2026-01-01T22:48:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ahmad Rezaei",
            "Mohsen Gholami",
            "Saeed Ranjbar Alvar",
            "Kevin Cannons",
            "Mohammad Asiful Hossain",
            "Zhou Weimin",
            "Shunbo Zhou",
            "Yong Zhang",
            "Mohammad Akbari"
        ],
        "tldr": "The paper introduces CPPO, a method that uses contrastive learning to improve perception in vision-language models during reinforcement learning finetuning, achieving better performance and efficiency compared to existing perception-rewarding techniques.",
        "tldr_zh": "该论文介绍了CPPO，一种使用对比学习在强化学习微调期间提高视觉语言模型感知能力的方法，与现有的感知奖励技术相比，实现了更好的性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models",
        "summary": "Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\\approx$17% at $D_{\\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.",
        "url": "http://arxiv.org/abs/2601.00730v1",
        "published_date": "2026-01-02T16:10:08+00:00",
        "updated_date": "2026-01-02T16:10:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Janez Perš",
            "Jon Muhovič",
            "Andrej Košir",
            "Boštjan Murovec"
        ],
        "tldr": "This paper presents a multimodal LLM pipeline for grading handwritten engineering exams, using a handwritten reference solution and structured prompting for improved accuracy and reduced bias.",
        "tldr_zh": "本文提出了一种多模态LLM流程，用于批改手写工程考试试卷，利用手写参考答案和结构化提示，以提高准确率并降低偏差。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval",
        "summary": "Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.",
        "url": "http://arxiv.org/abs/2601.00584v1",
        "published_date": "2026-01-02T06:04:58+00:00",
        "updated_date": "2026-01-02T06:04:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingyu Jeon",
            "Sunjae Yoon",
            "Jonghee Kim",
            "Junyeoung Kim"
        ],
        "tldr": "The paper introduces GranAlign, a training-free framework for zero-shot video moment retrieval that addresses semantic granularity mismatch between text queries and video content using granularity-based query rewriting and query-aware caption generation, achieving state-of-the-art results on multiple benchmarks.",
        "tldr_zh": "该论文介绍了一种名为 GranAlign 的免训练框架，用于零样本视频片段检索，通过使用基于粒度的查询重写和查询感知的标题生成来解决文本查询和视频内容之间语义粒度不匹配的问题，并在多个基准测试中实现了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]