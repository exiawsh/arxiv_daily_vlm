[
    {
        "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs",
        "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
        "url": "http://arxiv.org/abs/2511.07250v1",
        "published_date": "2025-11-10T16:02:33+00:00",
        "updated_date": "2025-11-10T16:02:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tianhao Peng",
            "Haochen Wang",
            "Yuanxing Zhang",
            "Zekun Wang",
            "Zili Wang",
            "Ge Zhang",
            "Jian Yang",
            "Shihao Li",
            "Yanghai Wang",
            "Xintao Wang",
            "Houyi Li",
            "Wei Ji",
            "Pengfei Wan",
            "Wenhao Huang",
            "Zhaoxiang Zhang",
            "Jiaheng Liu"
        ],
        "tldr": "The paper introduces MVU-Eval, a new benchmark for evaluating multi-video understanding capabilities of Multimodal Large Language Models (MLLMs), revealing limitations in current models across diverse real-world tasks.",
        "tldr_zh": "该论文介绍了MVU-Eval，这是一个新的基准，用于评估多模态大型语言模型（MLLM）的多视频理解能力，揭示了当前模型在各种现实世界任务中的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation",
        "summary": "In autonomous driving and robotics, ensuring road safety and reliable\ndecision-making critically depends on out-of-distribution (OOD) segmentation.\nWhile numerous methods have been proposed to detect anomalous objects on the\nroad, leveraging the vision-language space-which provides rich linguistic\nknowledge-remains an underexplored field. We hypothesize that incorporating\nthese linguistic cues can be especially beneficial in the complex contexts\nfound in real-world autonomous driving scenarios.\n  To this end, we present a novel approach that trains a Text-Driven OOD\nSegmentation model to learn a semantically diverse set of objects in the\nvision-language space. Concretely, our approach combines a vision-language\nmodel's encoder with a transformer decoder, employs Distance-Based OOD prompts\nlocated at varying semantic distances from in-distribution (ID) classes, and\nutilizes OOD Semantic Augmentation for OOD representations. By aligning visual\nand textual information, our approach effectively generalizes to unseen objects\nand provides robust OOD segmentation in diverse driving environments.\n  We conduct extensive experiments on publicly available OOD segmentation\ndatasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets,\ndemonstrating that our approach achieves state-of-the-art performance across\nboth pixel-level and object-level evaluations. This result underscores the\npotential of vision-language-based OOD segmentation to bolster the safety and\nreliability of future autonomous driving systems.",
        "url": "http://arxiv.org/abs/2511.07238v1",
        "published_date": "2025-11-10T15:54:23+00:00",
        "updated_date": "2025-11-10T15:54:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Seungheon Song",
            "Jaekoo Lee"
        ],
        "tldr": "This paper presents a novel Text-Driven OOD Segmentation model for autonomous driving that leverages the vision-language space to detect anomalous objects, achieving state-of-the-art performance on public datasets.",
        "tldr_zh": "本文提出了一种新的文本驱动的OOD分割模型，用于自动驾驶，该模型利用视觉-语言空间来检测异常物体，并在公共数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use",
        "summary": "Deep learning-based video surveillance increasingly demands\nprivacy-preserving architectures with low computational and environmental\noverhead. Federated learning preserves privacy but deploying large\nvision-language models (VLMs) introduces major energy and sustainability\nchallenges. We compare three strategies for federated violence detection under\nrealistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference\nwith pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and\npersonalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed\n90% accuracy in binary violence detection. The 3D CNN achieves superior\ncalibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570\nWh) of federated LoRA, while VLMs provide richer multimodal reasoning.\nHierarchical category grouping (based on semantic similarity and class\nexclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime\ndataset. To our knowledge, this is the first comparative simulation study of\nLoRA-tuned VLMs and personalized CNNs for federated violence detection, with\nexplicit energy and CO2e quantification. Our results inform hybrid deployment\nstrategies that default to efficient CNNs for routine inference and selectively\nengage VLMs for complex contextual reasoning.",
        "url": "http://arxiv.org/abs/2511.07171v1",
        "published_date": "2025-11-10T15:01:51+00:00",
        "updated_date": "2025-11-10T15:01:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sébastien Thuau",
            "Siba Haidar",
            "Rachid Chelouah"
        ],
        "tldr": "This paper compares federated learning strategies for violence detection using both lightweight CNNs and VLMs, highlighting a trade-off between energy efficiency and reasoning complexity, and proposes a hybrid deployment strategy. It also quantifies energy and CO2e costs.",
        "tldr_zh": "该论文比较了使用轻量级CNN和VLM的联邦学习暴力检测策略，强调了能源效率和推理复杂性之间的权衡，并提出了混合部署策略。它还量化了能源和CO2e成本。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora",
        "summary": "Large-scale visual out-of-distribution (OOD) detection has witnessed\nremarkable progress by leveraging vision-language models such as CLIP. However,\na significant limitation of current methods is their reliance on a pre-defined\nset of in-distribution (ID) ground-truth label names (positives). These fixed\nlabel names can be unavailable, unreliable at scale, or become less relevant\ndue to in-distribution shifts after deployment. Towards truly unsupervised OOD\ndetection, we utilize widely available text corpora for positive label mining,\nbypassing the need for positives. In this paper, we utilize widely available\ntext corpora for positive label mining under a general concept mining paradigm.\nWithin this framework, we propose ClusterMine, a novel positive label mining\nmethod. ClusterMine is the first method to achieve state-of-the-art OOD\ndetection performance without access to positive labels. It extracts positive\nconcepts from a large text corpus by combining visual-only sample consistency\n(via clustering) and zero-shot image-text consistency. Our experimental study\nreveals that ClusterMine is scalable across a plethora of CLIP models and\nachieves state-of-the-art robustness to covariate in-distribution shifts. The\ncode is available at https://github.com/HHU-MMBS/clustermine_wacv_official.",
        "url": "http://arxiv.org/abs/2511.07068v1",
        "published_date": "2025-11-10T13:04:01+00:00",
        "updated_date": "2025-11-10T13:04:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nikolas Adaloglou",
            "Diana Petrusheva",
            "Mohamed Asker",
            "Felix Michels",
            "Markus Kollmann"
        ],
        "tldr": "The paper introduces ClusterMine, a novel method for unsupervised OOD detection that mines positive labels from text corpora using clustering and zero-shot image-text consistency, achieving state-of-the-art performance and robustness without relying on predefined labels.",
        "tldr_zh": "该论文介绍了ClusterMine，一种新颖的无监督OOD检测方法，它通过聚类和零样本图像-文本一致性从文本语料库中挖掘正标签，在不依赖预定义标签的情况下实现了最先进的性能和鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions",
        "summary": "Text-to-image models have rapidly evolved from casual creative tools to\nprofessional-grade systems, achieving unprecedented levels of image quality and\nrealism. Yet, most models are trained to map short prompts into detailed\nimages, creating a gap between sparse textual input and rich visual outputs.\nThis mismatch reduces controllability, as models often fill in missing details\narbitrarily, biasing toward average user preferences and limiting precision for\nprofessional use. We address this limitation by training the first open-source\ntext-to-image model on long structured captions, where every training sample is\nannotated with the same set of fine-grained attributes. This design maximizes\nexpressive coverage and enables disentangled control over visual factors. To\nprocess long captions efficiently, we propose DimFusion, a fusion mechanism\nthat integrates intermediate tokens from a lightweight LLM without increasing\ntoken length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)\nevaluation protocol. By assessing how well real images can be reconstructed\nthrough a captioning-generation loop, TaBR directly measures controllability\nand expressiveness, even for very long captions where existing evaluation\nmethods fail. Finally, we demonstrate our contributions by training the\nlarge-scale model FIBO, achieving state-of-the-art prompt alignment among\nopen-source models. Model weights are publicly available at\nhttps://huggingface.co/briaai/FIBO",
        "url": "http://arxiv.org/abs/2511.06876v1",
        "published_date": "2025-11-10T09:25:25+00:00",
        "updated_date": "2025-11-10T09:25:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Eyal Gutflaish",
            "Eliran Kachlon",
            "Hezi Zisman",
            "Tal Hacham",
            "Nimrod Sarid",
            "Alexander Visheratin",
            "Saar Huberman",
            "Gal Davidi",
            "Guy Bukchin",
            "Kfir Goldberg",
            "Ron Mokady"
        ],
        "tldr": "The paper introduces FIBO, an open-source text-to-image model trained on long, structured captions with fine-grained attributes, using a novel DimFusion mechanism and a Text-as-a-Bottleneck Reconstruction (TaBR) evaluation protocol, achieving state-of-the-art prompt alignment.",
        "tldr_zh": "该论文介绍了FIBO，一个基于长文本结构化标注训练的开源文本到图像模型，该模型采用了一种新颖的DimFusion机制和文本瓶颈重建（TaBR）评估协议，实现了最先进的prompt对齐效果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images",
        "summary": "Understanding symptom-image associations is crucial for clinical reasoning.\nHowever, existing medical multimodal models often rely on simple one-to-one\nhard labeling, oversimplifying clinical reality where symptoms relate to\nmultiple organs. In addition, they mainly use single-slice 2D features without\nincorporating 3D information, limiting their ability to capture full anatomical\ncontext. In this study, we propose Med-SORA, a framework for symptom-to-organ\nreasoning in abdominal CT images. Med-SORA introduces RAG-based dataset\nconstruction, soft labeling with learnable organ anchors to capture one-to-many\nsymptom-organ relationships, and a 2D-3D cross-attention architecture to fuse\nlocal and global image features. To our knowledge, this is the first work to\naddress symptom-to-organ reasoning in medical multimodal learning. Experimental\nresults show that Med-SORA outperforms existing medical multimodal models and\nenables accurate 3D clinical reasoning.",
        "url": "http://arxiv.org/abs/2511.06752v1",
        "published_date": "2025-11-10T06:30:51+00:00",
        "updated_date": "2025-11-10T06:30:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "You-Kyoung Na",
            "Yeong-Jun Cho"
        ],
        "tldr": "Med-SORA is a novel framework for symptom-to-organ reasoning in abdominal CT images, using RAG for dataset creation, soft labeling for symptom-organ relationships, and a 2D-3D cross-attention architecture to improve clinical reasoning.",
        "tldr_zh": "Med-SORA 是一种新颖的框架，用于腹部 CT 图像中的症状到器官的推理，使用 RAG 创建数据集，软标签处理症状-器官关系，以及 2D-3D 交叉注意力架构来提高临床推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Flexible Concept Bottleneck Model",
        "summary": "Concept bottleneck models (CBMs) improve neural network interpretability by\nintroducing an intermediate layer that maps human-understandable concepts to\npredictions. Recent work has explored the use of vision-language models (VLMs)\nto automate concept selection and annotation. However, existing VLM-based CBMs\ntypically require full model retraining when new concepts are involved, which\nlimits their adaptability and flexibility in real-world scenarios, especially\nconsidering the rapid evolution of vision-language foundation models. To\naddress these issues, we propose Flexible Concept Bottleneck Model (FCBM),\nwhich supports dynamic concept adaptation, including complete replacement of\nthe original concept set. Specifically, we design a hypernetwork that generates\nprediction weights based on concept embeddings, allowing seamless integration\nof new concepts without retraining the entire model. In addition, we introduce\na modified sparsemax module with a learnable temperature parameter that\ndynamically selects the most relevant concepts, enabling the model to focus on\nthe most informative features. Extensive experiments on five public benchmarks\ndemonstrate that our method achieves accuracy comparable to state-of-the-art\nbaselines with a similar number of effective concepts. Moreover, the model\ngeneralizes well to unseen concepts with just a single epoch of fine-tuning,\ndemonstrating its strong adaptability and flexibility.",
        "url": "http://arxiv.org/abs/2511.06678v1",
        "published_date": "2025-11-10T03:50:57+00:00",
        "updated_date": "2025-11-10T03:50:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Xingbo Du",
            "Qiantong Dou",
            "Lei Fan",
            "Rui Zhang"
        ],
        "tldr": "The paper introduces Flexible Concept Bottleneck Model (FCBM) that enables dynamic concept adaptation in CBMs using hypernetworks and a modified sparsemax, addressing the inflexibility of existing VLM-based CBMs regarding new concepts without full retraining.",
        "tldr_zh": "该论文介绍了灵活的概念瓶颈模型（FCBM），它通过使用超网络和改进的稀疏最大化方法，实现了CBM中动态的概念适应，解决了现有基于VLM的CBM在涉及新概念时需要完全重新训练的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Sim4Seg: Boosting Multimodal Multi-disease Medical Diagnosis Segmentation with Region-Aware Vision-Language Similarity Masks",
        "summary": "Despite significant progress in pixel-level medical image analysis, existing\nmedical image segmentation models rarely explore medical segmentation and\ndiagnosis tasks jointly. However, it is crucial for patients that models can\nprovide explainable diagnoses along with medical segmentation results. In this\npaper, we introduce a medical vision-language task named Medical Diagnosis\nSegmentation (MDS), which aims to understand clinical queries for medical\nimages and generate the corresponding segmentation masks as well as diagnostic\nresults. To facilitate this task, we first present the Multimodal Multi-disease\nMedical Diagnosis Segmentation (M3DS) dataset, containing diverse multimodal\nmulti-disease medical images paired with their corresponding segmentation masks\nand diagnosis chain-of-thought, created via an automated diagnosis\nchain-of-thought generation pipeline. Moreover, we propose Sim4Seg, a novel\nframework that improves the performance of diagnosis segmentation by taking\nadvantage of the Region-Aware Vision-Language Similarity to Mask (RVLS2M)\nmodule. To improve overall performance, we investigate a test-time scaling\nstrategy for MDS tasks. Experimental results demonstrate that our method\noutperforms the baselines in both segmentation and diagnosis.",
        "url": "http://arxiv.org/abs/2511.06665v1",
        "published_date": "2025-11-10T03:22:42+00:00",
        "updated_date": "2025-11-10T03:22:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lingran Song",
            "Yucheng Zhou",
            "Jianbing Shen"
        ],
        "tldr": "The paper introduces a new medical vision-language task, Medical Diagnosis Segmentation (MDS), along with a dataset (M3DS) and a novel framework (Sim4Seg) leveraging region-aware vision-language similarity to improve joint segmentation and diagnosis performance.",
        "tldr_zh": "该论文介绍了一种新的医学视觉-语言任务，即医学诊断分割（MDS），以及一个数据集（M3DS）和一个利用区域感知视觉-语言相似性的新框架（Sim4Seg），以提高联合分割和诊断性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment",
        "summary": "Contrastive vision-language models like CLIP have achieved impressive results\nin image-text retrieval by aligning image and text representations in a shared\nembedding space. However, these models often treat text as flat sequences,\nlimiting their ability to handle complex, compositional, and long-form\ndescriptions. In particular, they fail to capture two essential properties of\nlanguage: semantic hierarchy, which reflects the multi-level compositional\nstructure of text, and semantic monotonicity, where richer descriptions should\nresult in stronger alignment with visual content.To address these limitations,\nwe propose HiMo-CLIP, a representation-level framework that enhances CLIP-style\nmodels without modifying the encoder architecture. HiMo-CLIP introduces two key\ncomponents: a hierarchical decomposition (HiDe) module that extracts latent\nsemantic components from long-form text via in-batch PCA, enabling flexible,\nbatch-aware alignment across different semantic granularities, and a\nmonotonicity-aware contrastive loss (MoLo) that jointly aligns global and\ncomponent-level representations, encouraging the model to internalize semantic\nordering and alignment strength as a function of textual completeness.These\ncomponents work in concert to produce structured, cognitively-aligned\ncross-modal representations. Experiments on multiple image-text retrieval\nbenchmarks show that HiMo-CLIP consistently outperforms strong baselines,\nparticularly under long or compositional descriptions. The code is available at\nhttps://github.com/UnicomAI/HiMo-CLIP.",
        "url": "http://arxiv.org/abs/2511.06653v1",
        "published_date": "2025-11-10T03:04:36+00:00",
        "updated_date": "2025-11-10T03:04:36+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Ruijia Wu",
            "Ping Chen",
            "Fei Shen",
            "Shaoan Zhao",
            "Qiang Hui",
            "Huanlin Gao",
            "Ting Lu",
            "Zhaoxiang Liu",
            "Fang Zhao",
            "Kai Wang",
            "Shiguo Lian"
        ],
        "tldr": "HiMo-CLIP enhances CLIP models by introducing hierarchical decomposition and monotonicity-aware contrastive loss to better handle complex and long-form text descriptions in image-text retrieval.",
        "tldr_zh": "HiMo-CLIP 通过引入分层分解和单调性感知的对比损失来增强 CLIP 模型，从而更好地处理图像-文本检索中复杂和长篇文本描述。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NOVO: Bridging LLaVA and SAM with Visual-only Prompts for Reasoning Segmentation",
        "summary": "In this study, we propose NOVO (NO text, Visual-Only prompts), a novel\nframework that bridges vision-language models (VLMs) and segmentation models\nthrough visual-only prompts. Unlike prior approaches that feed text-derived SEG\ntoken embeddings into segmentation models, NOVO instead generates a coarse mask\nand point prompts from the VLM output. These visual prompts are compatible with\nthe Segment Anything Model (SAM), preserving alignment with its pretrained\ncapabilities. To further enhance boundary quality and enable instance-level\nsegmentation, we introduce a training-free refinement module that reduces\nvisual artifacts and improves the quality of segmentation masks. We also\npresent RISeg, a new benchmark comprising 918 images, 2,533 instance-level\nmasks, and diverse reasoning queries to evaluate this task. Experiments\ndemonstrate that NOVO achieves state-of-the-art performance across multiple\nmetrics and model sizes, demonstrating its effectiveness and scalability in\nreasoning segmentation.",
        "url": "http://arxiv.org/abs/2511.06651v1",
        "published_date": "2025-11-10T02:58:32+00:00",
        "updated_date": "2025-11-10T02:58:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kyung-Yoon Yoon",
            "Yeong-Jun Cho"
        ],
        "tldr": "The paper introduces NOVO, a framework that uses visual prompts generated from VLMs to perform reasoning segmentation by leveraging SAM, achieving state-of-the-art performance. They also introduce a new benchmark dataset RISeg.",
        "tldr_zh": "该论文介绍了一种名为NOVO的框架，该框架利用视觉语言模型（VLM）生成的视觉提示，通过利用SAM执行推理分割，实现了最先进的性能。 他们还引入了一个新的基准数据集RISeg。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving",
        "summary": "Vision Language Models (VLMs) are increasingly used in autonomous driving to\nhelp understand traffic scenes, but they sometimes produce hallucinations,\nwhich are false details not grounded in the visual input. Detecting and\nmitigating hallucinations is challenging when ground-truth references are\nunavailable and model internals are inaccessible. This paper proposes a novel\nself-contained low-rank approach to automatically rank multiple candidate\ncaptions generated by multiple VLMs based on their hallucination levels, using\nonly the captions themselves without requiring external references or model\naccess. By constructing a sentence-embedding matrix and decomposing it into a\nlow-rank consensus component and a sparse residual, we use the residual\nmagnitude to rank captions: selecting the one with the smallest residual as the\nmost hallucination-free. Experiments on the NuScenes dataset demonstrate that\nour approach achieves 87% selection accuracy in identifying hallucination-free\ncaptions, representing a 19% improvement over the unfiltered baseline and a\n6-10% improvement over multi-agent debate method. The sorting produced by\nsparse error magnitudes shows strong correlation with human judgments of\nhallucinations, validating our scoring mechanism. Additionally, our method,\nwhich can be easily parallelized, reduces inference time by 51-67% compared to\ndebate approaches, making it practical for real-time autonomous driving\napplications.",
        "url": "http://arxiv.org/abs/2511.06496v1",
        "published_date": "2025-11-09T18:50:30+00:00",
        "updated_date": "2025-11-09T18:50:30+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Keke Long",
            "Jiacheng Guo",
            "Tianyun Zhang",
            "Hongkai Yu",
            "Xiaopeng Li"
        ],
        "tldr": "This paper introduces a novel low-rank method to mitigate hallucination in VLMs for autonomous driving by ranking candidate captions based on their residual magnitude after low-rank decomposition, achieving improved accuracy and efficiency compared to existing methods.",
        "tldr_zh": "该论文提出了一种新颖的低秩方法，通过基于低秩分解后的残差大小对候选字幕进行排序，以减轻自动驾驶中 VLM 的幻觉，与现有方法相比，提高了准确性和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models",
        "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
        "url": "http://arxiv.org/abs/2511.07299v1",
        "published_date": "2025-11-10T16:56:11+00:00",
        "updated_date": "2025-11-10T16:56:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Cheng",
            "Yu-Ho Lin",
            "Min-Hung Chen",
            "Fu-En Yang",
            "Shang-Hong Lai"
        ],
        "tldr": "The paper introduces VADER, an LLM-driven framework for video anomaly understanding that integrates keyframe object relation features with visual cues for enhanced causal reasoning and anomaly description.",
        "tldr_zh": "该论文介绍了VADER，一个基于大型语言模型的视频异常理解框架，它整合了关键帧对象关系特征与视觉线索，以增强因果推理和异常描述。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video",
        "summary": "The prevalence of user-generated content (UGC) on platforms such as YouTube\nand TikTok has rendered no-reference (NR) perceptual video quality assessment\n(VQA) vital for optimizing video delivery. Nonetheless, the characteristics of\nnon-professional acquisition and the subsequent transcoding of UGC video on\nsharing platforms present significant challenges for NR-VQA. Although NR-VQA\nmodels attempt to infer mean opinion scores (MOS), their modeling of subjective\nscores for compressed content remains limited due to the absence of\nfine-grained perceptual annotations of artifact types. To address these\nchallenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the\nsemantic understanding capabilities of large vision-language models. Our\napproach introduces a quality-aware prompting mechanism that integrates video\nmetadata (e.g., resolution, frame rate, bitrate) with key fragments extracted\nfrom inter-frame variations to guide the BLIP-2 pretraining approach in\ngenerating fine-grained quality captions. A unified architecture has been\ndesigned to model perceptual quality across three dimensions: semantic\nalignment, temporal characteristics, and spatial characteristics. These\nmultimodal features are extracted and fused, then regressed to video quality\nscores. Extensive experiments on a wide variety of UGC datasets demonstrate\nthat our model consistently outperforms existing NR-VQA methods, achieving\nimproved accuracy without the need for costly manual fine-grained annotations.\nOur method achieves the best performance in terms of average rank and linear\ncorrelation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods.\nThe source code and trained models, along with a user-friendly demo, are\navailable at: https://github.com/xinyiW915/CAMP-VQA.",
        "url": "http://arxiv.org/abs/2511.07290v1",
        "published_date": "2025-11-10T16:37:47+00:00",
        "updated_date": "2025-11-10T16:37:47+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xinyi Wang",
            "Angeliki Katsenou",
            "Junxiao Shen",
            "David Bull"
        ],
        "tldr": "CAMP-VQA is a novel no-reference video quality assessment framework that utilizes large vision-language models and quality-aware prompting to generate fine-grained quality captions, outperforming existing methods on UGC datasets.",
        "tldr_zh": "CAMP-VQA 是一种新颖的无参考视频质量评估框架，它利用大型视觉语言模型和质量感知提示来生成细粒度的质量描述，并在用户生成内容（UGC）数据集上优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression",
        "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant\npotential in the areas of video captioning, search, and summarization. However,\ncurrent Video-LLMs still face challenges with long real-world videos. Recent\nmethods have introduced a retrieval mechanism that retrieves query-relevant KV\ncaches for question answering, enhancing the efficiency and accuracy of long\nreal-world videos. However, the compression and retrieval of KV caches are\nstill not fully explored. In this paper, we propose \\textbf{StreamKV}, a\ntraining-free framework that seamlessly equips Video-LLMs with advanced KV\ncache retrieval and compression. Compared to previous methods that used uniform\npartitioning, StreamKV dynamically partitions video streams into semantic\nsegments, which better preserves semantic information. For KV cache retrieval,\nStreamKV calculates a summary vector for each segment to retain segment-level\ninformation essential for retrieval. For KV cache compression, StreamKV\nintroduces a guidance prompt designed to capture the key semantic elements\nwithin each segment, ensuring only the most informative KV caches are retained\nfor answering questions. Moreover, StreamKV unifies KV cache retrieval and\ncompression within a single module, performing both in a layer-adaptive manner,\nthereby further improving the effectiveness of streaming video question\nanswering. Extensive experiments on public StreamingVQA benchmarks demonstrate\nthat StreamKV significantly outperforms existing Online Video-LLMs, achieving\nsuperior accuracy while substantially improving both memory efficiency and\ncomputational latency. The code has been released at\nhttps://github.com/sou1p0wer/StreamKV.",
        "url": "http://arxiv.org/abs/2511.07278v1",
        "published_date": "2025-11-10T16:25:03+00:00",
        "updated_date": "2025-11-10T16:25:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yilong Chen",
            "Xiang Bai",
            "Zhibin Wang",
            "Chengyu Bai",
            "Yuhan Dai",
            "Ming Lu",
            "Shanghang Zhang"
        ],
        "tldr": "The paper introduces StreamKV, a training-free framework for Video-LLMs that improves efficiency and accuracy in streaming video question answering through segment-based KV cache retrieval and compression, achieving superior performance on StreamingVQA benchmarks.",
        "tldr_zh": "该论文介绍了StreamKV，一个用于视频大语言模型的免训练框架，通过基于分段的KV缓存检索和压缩，提高了流视频问答的效率和准确性，并在StreamingVQA基准测试中取得了优异的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "How Bias Binds: Measuring Hidden Associations for Bias Control in Text-to-Image Compositions",
        "summary": "Text-to-image generative models often exhibit bias related to sensitive\nattributes. However, current research tends to focus narrowly on single-object\nprompts with limited contextual diversity. In reality, each object or attribute\nwithin a prompt can contribute to bias. For example, the prompt \"an assistant\nwearing a pink hat\" may reflect female-inclined biases associated with a pink\nhat. The neglected joint effects of the semantic binding in the prompts cause\nsignificant failures in current debiasing approaches. This work initiates a\npreliminary investigation on how bias manifests under semantic binding, where\ncontextual associations between objects and attributes influence generative\noutcomes. We demonstrate that the underlying bias distribution can be amplified\nbased on these associations. Therefore, we introduce a bias adherence score\nthat quantifies how specific object-attribute bindings activate bias. To delve\ndeeper, we develop a training-free context-bias control framework to explore\nhow token decoupling can facilitate the debiasing of semantic bindings. This\nframework achieves over 10% debiasing improvement in compositional generation\ntasks. Our analysis of bias scores across various attribute-object bindings and\ntoken decorrelation highlights a fundamental challenge: reducing bias without\ndisrupting essential semantic relationships. These findings expose critical\nlimitations in current debiasing approaches when applied to semantically bound\ncontexts, underscoring the need to reassess prevailing bias mitigation\nstrategies.",
        "url": "http://arxiv.org/abs/2511.07091v1",
        "published_date": "2025-11-10T13:27:05+00:00",
        "updated_date": "2025-11-10T13:27:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jeng-Lin Li",
            "Ming-Ching Chang",
            "Wei-Chao Chen"
        ],
        "tldr": "This paper investigates and quantifies bias amplification in text-to-image models due to semantic binding between objects and attributes, introducing a bias adherence score and a context-bias control framework for debiasing compositional generation.",
        "tldr_zh": "该论文研究并量化了文本到图像模型中由于对象和属性之间的语义绑定而产生的偏差放大现象，引入了偏差坚持度评分和一个上下文偏差控制框架，用于消除组合生成的偏差。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding",
        "summary": "Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D\nobjects in RGB images using text descriptions with geometric cues. However,\nexisting methods face two key limitations. Firstly, they often over-rely on\nhigh-certainty keywords that explicitly identify the target object while\nneglecting critical spatial descriptions. Secondly, generalized textual\nfeatures contain both 2D and 3D descriptive information, thereby capturing an\nadditional dimension of details compared to singular 2D or 3D visual features.\nThis characteristic leads to cross-dimensional interference when refining\nvisual features under text guidance. To overcome these challenges, we propose\nMono3DVG-EnSD, a novel framework that integrates two key components: the\nCLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled\nModule (D2M). The CLIP-LCA dynamically masks high-certainty keywords while\nretaining low-certainty implicit spatial descriptions, thereby forcing the\nmodel to develop a deeper understanding of spatial relationships in captions\nfor object localization. Meanwhile, the D2M decouples dimension-specific\n(2D/3D) textual features from generalized textual features to guide\ncorresponding visual features at same dimension, which mitigates\ncross-dimensional interference by ensuring dimensionally-consistent cross-modal\ninteractions. Through comprehensive comparisons and ablation studies on the\nMono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance\nacross all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario\nby a significant +13.54%.",
        "url": "http://arxiv.org/abs/2511.06908v1",
        "published_date": "2025-11-10T10:02:30+00:00",
        "updated_date": "2025-11-10T10:02:30+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yuzhen Li",
            "Min Liu",
            "Zhaoyang Li",
            "Yuan Bian",
            "Xueping Wang",
            "Erbo Zhai",
            "Yaonan Wang"
        ],
        "tldr": "This paper introduces Mono3DVG-EnSD, a novel framework for monocular 3D visual grounding that enhances spatial understanding by dynamically masking high-certainty keywords and decoupling textual features to mitigate cross-dimensional interference, achieving state-of-the-art results on the Mono3DRefer dataset.",
        "tldr_zh": "该论文介绍了Mono3DVG-EnSD，一种新颖的单目3D视觉定位框架。该框架通过动态屏蔽高确定性关键词来增强空间理解，并通过解耦文本特征来减轻跨维度干扰，在Mono3DRefer数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have spurred\nsignificant progress in Chain-of-Thought (CoT) reasoning. Building on the\nsuccess of Deepseek-R1, researchers extended multimodal reasoning to\npost-training paradigms based on reinforcement learning (RL), focusing\npredominantly on mathematical datasets. However, existing post-training\nparadigms tend to neglect two critical aspects: (1) The lack of quantifiable\ndifficulty metrics capable of strategically screening samples for post-training\noptimization. (2) Suboptimal post-training paradigms that fail to jointly\noptimize perception and reasoning capabilities. To address this gap, we propose\ntwo novel difficulty-aware sampling strategies: Progressive Image Semantic\nMasking (PISM) quantifies sample hardness through systematic image degradation,\nwhile Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction\ncomplexity via attention distribution analysis. Leveraging these metrics, we\ndesign a hierarchical training framework that incorporates both GRPO-only and\nSFT+GRPO hybrid training paradigms, and evaluate them across six benchmark\ndatasets. Experiments demonstrate consistent superiority of GRPO applied to\ndifficulty-stratified samples compared to conventional SFT+GRPO pipelines,\nindicating that strategic data sampling can obviate the need for supervised\nfine-tuning while improving model accuracy. Our code will be released at\nhttps://github.com/qijianyu277/DifficultySampling.",
        "url": "http://arxiv.org/abs/2511.06722v1",
        "published_date": "2025-11-10T05:31:59+00:00",
        "updated_date": "2025-11-10T05:31:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Jianyu Qi",
            "Ding Zou",
            "Wenrui Yan",
            "Rui Ma",
            "Jiaxu Li",
            "Zhijie Zheng",
            "Zhiguo Yang",
            "Rongchang Zhao"
        ],
        "tldr": "This paper introduces difficulty-aware sampling strategies (PISM and CMAB) for multimodal post-training, demonstrating improved performance over conventional methods on mathematical datasets by strategically selecting samples based on their difficulty.",
        "tldr_zh": "本文提出了一种难度感知的多模态后训练采样策略（PISM 和 CMAB），通过根据样本难度进行策略性选择，在数学数据集上展示了优于传统方法的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports",
        "summary": "Deeply understanding sports requires an intricate blend of fine-grained\nvisual perception and rule-based reasoning - a challenge that pushes the limits\nof current multimodal models. To succeed, models must master three critical\ncapabilities: perceiving nuanced visual details, applying abstract sport rule\nknowledge, and grounding that knowledge in specific visual evidence. Current\nsports benchmarks either cover single sports or lack the detailed reasoning\nchains and precise visual grounding needed to robustly evaluate these core\ncapabilities in a multi-sport context. To address this gap, we introduce\nSportR, the first multi-sports large-scale benchmark designed to train and\nevaluate MLLMs on the fundamental reasoning required for sports intelligence.\nOur benchmark provides a dataset of 5,017 images and 2,101 videos. To enable\ngranular evaluation, we structure our benchmark around a progressive hierarchy\nof question-answer (QA) pairs designed to probe reasoning at increasing depths\n- from simple infraction identification to complex penalty prediction. For the\nmost advanced tasks requiring multi-step reasoning, such as determining\npenalties or explaining tactics, we provide 7,118 high-quality, human-authored\nChain of Thought (CoT) annotations. In addition, our benchmark incorporates\nboth image and video modalities and provides manual bounding box annotations to\ntest visual grounding in the image part directly. Extensive experiments\ndemonstrate the profound difficulty of our benchmark. State-of-the-art baseline\nmodels perform poorly on our most challenging tasks. While training on our data\nvia Supervised Fine-Tuning and Reinforcement Learning improves these scores,\nthey remain relatively low, highlighting a significant gap in current model\ncapabilities. SportR presents a new challenge for the community, providing a\ncritical resource to drive future research in multimodal sports reasoning.",
        "url": "http://arxiv.org/abs/2511.06499v1",
        "published_date": "2025-11-09T18:55:20+00:00",
        "updated_date": "2025-11-09T18:55:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haotian Xia",
            "Haonan Ge",
            "Junbo Zou",
            "Hyun Woo Choi",
            "Xuebin Zhang",
            "Danny Suradja",
            "Botao Rui",
            "Ethan Tran",
            "Wendy Jin",
            "Zhen Ye",
            "Xiyang Lin",
            "Christopher Lai",
            "Shengjie Zhang",
            "Junwen Miao",
            "Shichao Chen",
            "Rhys Tracy",
            "Vicente Ordonez",
            "Weining Shen",
            "Hanjie Chen"
        ],
        "tldr": "The paper introduces SportR, a new large-scale multimodal benchmark for evaluating the reasoning capabilities of MLLMs in sports, featuring diverse tasks, CoT annotations, and visual grounding evaluations to highlight current model limitations.",
        "tldr_zh": "该论文介绍了 SportR，这是一个新的大规模多模态基准，用于评估 MLLM 在体育运动中的推理能力，具有多样化的任务、CoT 注释和视觉定位评估，旨在突出当前模型的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models",
        "summary": "Complex visual narratives, such as comics, present a significant challenge to\nVision-Language Models (VLMs). Despite excelling on natural images, VLMs often\nstruggle with stylized line art, onomatopoeia, and densely packed multi-panel\nlayouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and\ncomprehensive benchmark for VLM-based comic understanding. It spans tasks from\nfoundational recognition and detection to high-level character reasoning and\nnarrative construction, supported by dense annotations for characters, poses,\nand depth. Beyond that, we evaluate state-of-the-art proprietary models,\nincluding GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL,\nrevealing substantial performance deficits across core tasks of our benchmarks\nand underscoring that comic understanding remains an unsolved challenge. To\nenhance VLMs' capabilities in this domain, we systematically investigate\npost-training strategies, including supervised fine-tuning on solutions\n(SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and\nreinforcement learning (RL). Beyond that, inspired by the emerging \"Thinking\nwith Images\" paradigm, we propose Region-Aware Reinforcement Learning (RARL)\nfor VLMs, which trains models to dynamically attend to relevant regions through\nzoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL\nand RARL yield significant gains in low-level entity recognition and high-level\nstoryline ordering, paving the way for more accurate and efficient VLM\napplications in the comics domain.",
        "url": "http://arxiv.org/abs/2511.06490v1",
        "published_date": "2025-11-09T18:27:45+00:00",
        "updated_date": "2025-11-09T18:27:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yule Chen",
            "Yufan Ren",
            "Sabine Süsstrunk"
        ],
        "tldr": "This paper introduces a fine-grained benchmark (AI4VA-FG) for VLM comic understanding, identifies performance deficits in state-of-the-art models, and proposes Region-Aware Reinforcement Learning (RARL) to improve VLM performance on comic-related tasks.",
        "tldr_zh": "该论文介绍了一个用于VLM漫画理解的细粒度基准（AI4VA-FG），指出了最先进模型在漫画相关任务中的性能缺陷，并提出了区域感知强化学习（RARL）来提高VLM在漫画相关任务上的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation",
        "summary": "In this paper, we describe our system under the team name BLEU Monday for the\nEnglish-to-Indic Multimodal Translation Task at WAT 2025. We participate in the\ntext-only translation tasks for English-Hindi, English-Bengali,\nEnglish-Malayalam, and English-Odia language pairs. We present a two-stage\napproach that addresses quality issues in the training data through automated\nerror detection and correction, followed by parameter-efficient model\nfine-tuning.\n  Our methodology introduces a vision-augmented judge-corrector pipeline that\nleverages multimodal language models to systematically identify and correct\ntranslation errors in the training data. The judge component classifies\ntranslations into three categories: correct, visually ambiguous (requiring\nimage context), or mistranslated (poor translation quality). Identified errors\nare routed to specialized correctors: GPT-4o-mini regenerates captions\nrequiring visual disambiguation, while IndicTrans2 retranslates cases with pure\ntranslation quality issues. This automated pipeline processes 28,928 training\nexamples across four languages, correcting an average of 17.1% of captions per\nlanguage.\n  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2\nen-indic 200M distilled model on both original and corrected datasets. Training\non corrected data yields consistent improvements, with BLEU score gains of\n+1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on\nthe challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation\nset (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90\n-> 54.00).",
        "url": "http://arxiv.org/abs/2511.07010v1",
        "published_date": "2025-11-10T12:02:48+00:00",
        "updated_date": "2025-11-10T12:02:48+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Siddharth Betala",
            "Kushan Raj",
            "Vipul Betala",
            "Rohan Saswade"
        ],
        "tldr": "The paper presents a vision-guided judge-corrector system using multimodal language models to improve English-to-Indic machine translation by addressing training data quality issues and fine-tuning with LoRA, showing BLEU score improvements.",
        "tldr_zh": "本文提出了一个视觉引导的判断-纠正系统，该系统利用多模态语言模型，通过解决训练数据质量问题并通过LoRA进行微调，来改进英语到印度语言的机器翻译，并显示了BLEU分数的提高。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]