[
    {
        "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation",
        "summary": "Recent advances in generative medical models are constrained by\nmodality-specific scenarios that hinder the integration of complementary\nevidence from imaging, pathology, and clinical notes. This fragmentation limits\ntheir evolution into foundation models that can learn and reason across the\nfull spectrum of biomedical data. We propose MeDiM, the first medical discrete\ndiffusion model that learns shared distributions across modalities without\nmodality-specific components. MeDiM unifies multiple generative tasks:\ntranslating between images and text, and jointly producing image-report pairs\nacross domains in response to prompts. Built on a discrete diffusion framework,\nMeDiM bridges vision and language representations through a shared\nprobabilistic space. To enable unified and flexible medical generation, we\nemploy a multimodal large language model (MLLM) as the diffusion backbone,\nleveraging its prior knowledge and cross-modal reasoning. Two key designs are\nintroduced: (1) removing the causal attention mask for bidirectional context,\nand (2) injecting continuous timestep embeddings for diffusion awareness.\nExperiments demonstrate high-fidelity medical generation (FID 16.60 on\nMIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR\n0.2650 and 0.2580). Jointly generated image-report pairs further enhance\ndownstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,\nplus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports\ncoherent and clinically grounded multimodal outputs.",
        "url": "http://arxiv.org/abs/2510.06131v1",
        "published_date": "2025-10-07T17:06:57+00:00",
        "updated_date": "2025-10-07T17:06:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiawei Mao",
            "Yuhan Wang",
            "Lifeng Chen",
            "Can Zhao",
            "Yucheng Tang",
            "Dong Yang",
            "Liangqiong Qu",
            "Daguang Xu",
            "Yuyin Zhou"
        ],
        "tldr": "The paper introduces MeDiM, a medical discrete diffusion model using MLLMs to unify multimodal generation tasks like image-text translation and joint image-report generation, demonstrating high-fidelity and accurate results.",
        "tldr_zh": "该论文介绍了MeDiM，一种使用MLLMs的医学离散扩散模型，用于统一多模态生成任务，如图像-文本翻译和联合图像-报告生成，并展示了高保真和准确的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA",
        "summary": "CAPTCHA, originally designed to distinguish humans from robots, has evolved\ninto a real-world benchmark for assessing the spatial reasoning capabilities of\nvision-language models. In this work, we first show that step-by-step reasoning\nis crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent\nhigh-difficulty spatial reasoning tasks, and that current commercial\nvision-language models still struggle with such reasoning. In particular, we\nobserve that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to\neffectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).\nHowever, our findings indicate that requiring the model to perform step-by-step\nreasoning before generating the final coordinates can significantly enhance its\nsolving accuracy, underscoring the severity of the gap. To systematically study\nthis issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with\nreasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,\netc.) with step-by-step action solutions and grounding annotations. We further\ndefine five reasoning-oriented metrics that enable a comprehensive evaluation\nof models reasoning capabilities. To validate the effectiveness of reasoning,\nwe also propose a general agentic VLM-based framework that incorporates the\nmodels inherent reasoning abilities. Our method achieves state-of-the-art\nperformance across five high-difficulty CAPTCHA types, with an average solving\naccuracy of 83.9 percent, substantially surpassing existing baselines. These\nresults reveal the limitations of current models and highlight the importance\nof reasoning in advancing visual-spatial challenges in the future.",
        "url": "http://arxiv.org/abs/2510.06067v1",
        "published_date": "2025-10-07T15:56:21+00:00",
        "updated_date": "2025-10-07T15:56:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Python Song",
            "Luke Tenyi Chang",
            "Yun-Yun Tsai",
            "Penghui Li",
            "Junfeng Yang"
        ],
        "tldr": "This paper introduces CAPTCHA-X, a new benchmark for evaluating spatial reasoning in VLMs, and demonstrates that step-by-step reasoning significantly improves CAPTCHA-solving accuracy, proposing a VLM-based framework that achieves state-of-the-art performance.",
        "tldr_zh": "该论文介绍了CAPTCHA-X，一个新的用于评估视觉语言模型中空间推理能力的基准，并证明了逐步推理显著提高了解决验证码的准确性，并提出了一个基于视觉语言模型的框架，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Medical Vision Language Models as Policies for Robotic Surgery",
        "summary": "Vision-based Proximal Policy Optimization (PPO) struggles with visual\nobservation-based robotic laparoscopic surgical tasks due to the\nhigh-dimensional nature of visual input, the sparsity of rewards in surgical\nenvironments, and the difficulty of extracting task-relevant features from raw\nvisual data. We introduce a simple approach integrating MedFlamingo, a medical\ndomain-specific Vision-Language Model, with PPO. Our method is evaluated on\nfive diverse laparoscopic surgery task environments in LapGym, using only\nendoscopic visual observations. MedFlamingo PPO outperforms and converges\nfaster compared to both standard vision-based PPO and OpenFlamingo PPO\nbaselines, achieving task success rates exceeding 70% across all environments,\nwith improvements ranging from 66.67% to 1114.29% compared to baseline. By\nprocessing task observations and instructions once per episode to generate\nhigh-level planning tokens, our method efficiently combines medical expertise\nwith real-time visual feedback. Our results highlight the value of specialized\nmedical knowledge in robotic surgical planning and decision-making.",
        "url": "http://arxiv.org/abs/2510.06064v1",
        "published_date": "2025-10-07T15:54:34+00:00",
        "updated_date": "2025-10-07T15:54:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Akshay Muppidi",
            "Martin Radfar"
        ],
        "tldr": "This paper introduces MedFlamingo PPO, which integrates a medical domain-specific Vision-Language Model with PPO to improve robotic surgery task performance in LapGym, demonstrating superior performance compared to standard PPO and OpenFlamingo PPO.",
        "tldr_zh": "该论文介绍了MedFlamingo PPO，它将医学领域的视觉语言模型与PPO相结合，以提高LapGym中机器人手术任务的性能，并证明了与标准PPO和OpenFlamingo PPO相比，其性能更优。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization",
        "summary": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner.",
        "url": "http://arxiv.org/abs/2510.06040v1",
        "published_date": "2025-10-07T15:34:46+00:00",
        "updated_date": "2025-10-07T15:34:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinye Cao",
            "Hongcan Guo",
            "Jiawen Qian",
            "Guoshun Nan",
            "Chao Wang",
            "Yuqi Pan",
            "Tianhao Hou",
            "Xiaojuan Wang",
            "Yutong Gao"
        ],
        "tldr": "The paper introduces VideoMiner, a system for understanding long videos using MM-LLMs. It addresses the challenge of irrelevant information by iteratively segmenting, captioning, and clustering videos into a hierarchical tree structure, guided by a novel reinforcement learning method called T-GRPO.",
        "tldr_zh": "该论文介绍了VideoMiner，一个使用多模态大型语言模型理解长视频的系统。它通过迭代地分割、标注和聚类视频成一个分层树结构来解决无关信息的问题，并由一种名为T-GRPO的新型强化学习方法指导。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Data Factory with Minimal Human Effort Using VLMs",
        "summary": "Generating enough and diverse data through augmentation offers an efficient\nsolution to the time-consuming and labour-intensive process of collecting and\nannotating pixel-wise images. Traditional data augmentation techniques often\nface challenges in manipulating high-level semantic attributes, such as\nmaterials and textures. In contrast, diffusion models offer a robust\nalternative, by effectively utilizing text-to-image or image-to-image\ntransformation. However, existing diffusion-based methods are either\ncomputationally expensive or compromise on performance. To address this issue,\nwe introduce a novel training-free pipeline that integrates pretrained\nControlNet and Vision-Language Models (VLMs) to generate synthetic images\npaired with pixel-level labels. This approach eliminates the need for manual\nannotations and significantly improves downstream tasks. To improve the\nfidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and\nHigh-quality Image Selection module. Our results on PASCAL-5i and COCO-20i\npresent promising performance and outperform concurrent work for one-shot\nsemantic segmentation.",
        "url": "http://arxiv.org/abs/2510.05722v1",
        "published_date": "2025-10-07T09:43:24+00:00",
        "updated_date": "2025-10-07T09:43:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaojiao Ye",
            "Jiaxing Zhong",
            "Qian Xie",
            "Yuzhou Zhou",
            "Niki Trigoni",
            "Andrew Markham"
        ],
        "tldr": "This paper introduces a training-free pipeline using ControlNet and VLMs to generate synthetic, pixel-wise labeled images for semantic segmentation, achieving promising results on PASCAL-5i and COCO-20i.",
        "tldr_zh": "本文介绍了一种无需训练的流水线，该流水线利用 ControlNet 和 VLMs 生成合成的、像素级标记的图像，用于语义分割，并在 PASCAL-5i 和 COCO-20i 上取得了可喜的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection",
        "summary": "Recent Human-object interaction detection (HOID) methods highly require prior\nknowledge from VLMs to enhance the interaction recognition capabilities. The\ntraining strategies and model architectures for connecting the knowledge from\nVLMs to the HOI instance representations from the object detector are\nchallenging, and the whole framework is complex for further development or\napplication. On the other hand, the inherent reasoning abilities of MLLMs on\nhuman-object interaction detection are under-explored. Inspired by the recent\nsuccess of training MLLMs with reinforcement learning (RL) methods, we propose\nHOI-R1 and first explore the potential of the language model on the HOID task\nwithout any additional detection modules. We introduce an HOI reasoning process\nand HOID reward functions to solve the HOID task by pure text. The results on\nthe HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline\nwith great generalization ability. The source code is available at\nhttps://github.com/cjw2021/HOI-R1.",
        "url": "http://arxiv.org/abs/2510.05609v1",
        "published_date": "2025-10-07T06:16:02+00:00",
        "updated_date": "2025-10-07T06:16:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junwen Chen",
            "Peilin Xiong",
            "Keiji Yanai"
        ],
        "tldr": "This paper introduces HOI-R1, a novel approach leveraging MLLMs and reinforcement learning for human-object interaction detection without relying on traditional detection modules, achieving significant accuracy improvements over baselines.",
        "tldr_zh": "本文介绍了HOI-R1，一种利用多模态大语言模型和强化学习进行人-物交互检测的新方法，无需传统的检测模块，并且相比基线模型取得了显著的准确率提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval",
        "summary": "Existing Visual Language Models (VLMs) suffer structural limitations where a\nfew low contribution tokens may excessively capture global semantics,\ndominating the information aggregation process and suppressing the\ndiscriminative features in text-driven image retrieval tasks. To address this,\nwe introduce \\textbf{CalibCLIP}, a training-free method designed to calibrate\nthe suppressive effect of dominant tokens. Specifically, in the visual space,\nwe propose the Contrastive Visual Enhancer (CVE), which decouples visual\nfeatures into target and low information regions. Subsequently, it identifies\ndominant tokens and dynamically suppresses their representations.In the textual\nspace, we introduce the Discriminative Concept Calibrator (DCC), which aims to\ndifferentiate between general and discriminative concepts within the text\nquery. By mitigating the challenges posed by generic concepts and improving the\nrepresentations of discriminative concepts, DCC strengthens the differentiation\namong similar samples. Finally, extensive experiments demonstrate consistent\nimprovements across seven benchmarks spanning three image retrieval tasks,\nunderscoring the effectiveness of CalibCLIP. Code is available at:\nhttps://github.com/kangbin98/CalibCLIP",
        "url": "http://arxiv.org/abs/2510.05586v1",
        "published_date": "2025-10-07T05:16:29+00:00",
        "updated_date": "2025-10-07T05:16:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Kang",
            "Bin Chen",
            "Junjie Wang",
            "Yulin Li",
            "Junzhi Zhao",
            "Zhuotao Tian"
        ],
        "tldr": "CalibCLIP addresses the issue of dominant, low-contribution tokens in VLMs suppressing discriminative features for text-driven image retrieval via a training-free calibration method, showing improvements across various benchmarks.",
        "tldr_zh": "CalibCLIP通过一种无需训练的校准方法，解决了VLMs中占主导地位的低贡献tokens抑制文本驱动图像检索中判别特征的问题，并在多个基准测试中显示出改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Personalizing Retrieval using Joint Embeddings or \"the Return of Fluffy\"",
        "summary": "The goal of this paper is to be able to retrieve images using a compound\nquery that combines object instance information from an image, with a natural\ntext description of what that object is doing or where it is. For example, to\nretrieve an image of \"Fluffy the unicorn (specified by an image) on someone's\nhead\". To achieve this we design a mapping network that can \"translate\" from a\nlocal image embedding (of the object instance) to a text token, such that the\ncombination of the token and a natural language query is suitable for CLIP\nstyle text encoding, and image retrieval. Generating a text token in this\nmanner involves a simple training procedure, that only needs to be performed\nonce for each object instance. We show that our approach of using a trainable\nmapping network, termed pi-map, together with frozen CLIP text and image\nencoders, improves the state of the art on two benchmarks designed to assess\npersonalized retrieval.",
        "url": "http://arxiv.org/abs/2510.05411v1",
        "published_date": "2025-10-06T22:08:30+00:00",
        "updated_date": "2025-10-06T22:08:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bruno Korbar",
            "Andrew Zisserman"
        ],
        "tldr": "This paper introduces pi-map, a trainable mapping network that translates object instance image embeddings into text tokens, enabling personalized image retrieval with compound queries, and improving state-of-the-art results.",
        "tldr_zh": "本文介绍了一种名为 pi-map 的可训练映射网络，该网络将对象实例图像嵌入转换为文本标记，从而实现使用复合查询的个性化图像检索，并提高了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models",
        "summary": "Recovering the past from present observations is an intriguing challenge with\npotential applications in forensics and scene analysis. Thermal imaging,\noperating in the infrared range, provides access to otherwise invisible\ninformation. Since humans are typically warmer (37 C -98.6 F) than their\nsurroundings, interactions such as sitting, touching, or leaning leave residual\nheat traces. These fading imprints serve as passive temporal codes, allowing\nfor the inference of recent events that exceed the capabilities of RGB cameras.\nThis work proposes a time-reversed reconstruction framework that uses paired\nRGB and thermal images to recover scene states from a few seconds earlier. The\nproposed approach couples Visual-Language Models (VLMs) with a constrained\ndiffusion process, where one VLM generates scene descriptions and another\nguides image reconstruction, ensuring semantic and structural consistency. The\nmethod is evaluated in three controlled scenarios, demonstrating the\nfeasibility of reconstructing plausible past frames up to 120 seconds earlier,\nproviding a first step toward time-reversed imaging from thermal traces.",
        "url": "http://arxiv.org/abs/2510.05408v1",
        "published_date": "2025-10-06T21:57:26+00:00",
        "updated_date": "2025-10-06T21:57:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kebin Contreras",
            "Luis Toscano-Palomino",
            "Mauro Dalla Mura",
            "Jorge Bacca"
        ],
        "tldr": "This paper introduces a method for reconstructing past scene states from thermal traces using Visual Language Models (VLMs) and constrained diffusion, demonstrating the feasibility of time-reversed imaging.",
        "tldr_zh": "本文介绍了一种利用视觉语言模型（VLMs）和约束扩散从热痕迹重建过去场景状态的方法，展示了时间反转成像的可行性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment",
        "summary": "Aligning multimodal large language models (MLLMs) with human preferences\noften relies on single-signal, model-based reward methods. Such monolithic\nrewards often lack confidence calibration across domain-specific tasks, fail to\ncapture diverse aspects of human preferences, and require extensive data\nannotation and reward model training. In this work, we propose a hybrid reward\nmodeling framework that integrates complementary reward paradigms: (i)\nmodel-based rewards, where a learned reward model predicts scalar or vector\nscores from synthetic and human feedback, and (ii) rule-based rewards, where\ndomain-specific heuristics provide explicit correctness signals with\nconfidence. Beyond accuracy, we further incorporate multi-aspect rewards to\nenforce instruction adherence and introduce a generalized length-penalty reward\nto stabilize training and improve performance. The proposed framework provides\na flexible and effective approach to aligning MLLMs through reinforcement\nlearning policy optimization. Our experiments show consistent improvements\nacross different multimodal benchmarks when applying hybrid and multi-aspect\nreward modeling. Our best performing model in the 3B family achieves an overall\naverage improvement of ~9.5% across general and math reasoning tasks. Focusing\nspecifically on mathematical benchmarks, the model achieves a significant\naverage improvement of ~16%, highlighting its effectiveness in mathematical\nreasoning and problem solving.",
        "url": "http://arxiv.org/abs/2510.05283v1",
        "published_date": "2025-10-06T18:53:23+00:00",
        "updated_date": "2025-10-06T18:53:23+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Radha Gulhane",
            "Sathish Reddy Indurthi"
        ],
        "tldr": "This paper introduces a hybrid and multi-aspect reward framework for aligning MLLMs, combining model-based and rule-based rewards, and shows significant improvements in mathematical reasoning and general tasks.",
        "tldr_zh": "本文提出了一种混合和多方面的奖励框架，用于对齐MLLM，结合了基于模型和基于规则的奖励，并在数学推理和一般任务中显示出显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Detection and Measurement of Hailstones with Multimodal Large Language Models",
        "summary": "This study examines the use of social media and news images to detect and\nmeasure hailstones, utilizing pre-trained multimodal large language models. The\ndataset for this study comprises 474 crowdsourced images of hailstones from\ndocumented hail events in Austria, which occurred between January 2022 and\nSeptember 2024. These hailstones have maximum diameters ranging from 2 to 11cm.\nWe estimate the hail diameters and compare four different models utilizing\none-stage and two-stage prompting strategies. The latter utilizes additional\nsize cues from reference objects, such as human hands, within the image. Our\nresults show that pretrained models already have the potential to measure\nhailstone diameters from images with an average mean absolute error of 1.12cm\nfor the best model. In comparison to a single-stage prompt, two-stage prompting\nimproves the reliability of most models. Our study suggests that these\noff-the-shelf models, even without fine-tuning, can complement traditional hail\nsensors by extracting meaningful and spatially dense information from social\nmedia imagery, enabling faster and more detailed assessments of severe weather\nevents. The automated real-time image harvesting from social media and other\nsources remains an open task, but it will make our approach directly applicable\nto future hail events.",
        "url": "http://arxiv.org/abs/2510.06008v1",
        "published_date": "2025-10-07T15:07:29+00:00",
        "updated_date": "2025-10-07T15:07:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T07, 68T45, 86A10",
            "I.4; I.2"
        ],
        "authors": [
            "Moritz Alker",
            "David C. Schedl",
            "Andreas Stöckl"
        ],
        "tldr": "The paper explores the use of pre-trained multimodal large language models to detect and measure hailstone diameters from social media images, achieving a mean absolute error of 1.12cm without fine-tuning, showing potential for complementing traditional hail sensors.",
        "tldr_zh": "该论文探讨了使用预训练的多模态大型语言模型来检测和测量社交媒体图像中的冰雹直径，无需微调即可实现1.12厘米的平均绝对误差，显示出补充传统冰雹传感器的潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow",
        "summary": "Long-form video understanding has always been a challenging problem due to\nthe significant redundancy in both temporal and spatial contents. This\nchallenge is further exacerbated by the limited context length of Multimodal\nLarge Language Models (MLLMs). To address this issue, many previous works have\nattempted to extract key video information, where the \"key\" is typically\nsemantic-aware and heavily dependent on the CLIP model as prior. In this paper,\nwe propose Flow4Agent, a novel framework that pioneeringly incorporates motion\npriors from optical flow to facilitate LLM-based long video understanding.\nFlow4Agent mitigates the redundancy in long videos at both temporal and spatial\nlevels through two core modules: Temporal Granularity Optimization (TGO)\nadaptively refines framelevel hierarchies, which first leverages coarse flow\npriors to group similar visual contents and then applies semantic priors to\nfilter out highly irrelevant scene information. Motion Token Pruning (MTP)\nfurther refines the intra-frame visual representations, pruning high-redundancy\nvideo tokens using fine-grained optical flow information. Extensive experiments\ndemonstrate that our Flow4Agent outperforms existing methods across a wide\nrange of video MLLM benchmarks, especially for hour-level video understanding\ntasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.",
        "url": "http://arxiv.org/abs/2510.05836v1",
        "published_date": "2025-10-07T12:01:57+00:00",
        "updated_date": "2025-10-07T12:01:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruyang Liu",
            "Shangkun Sun",
            "Haoran Tang",
            "Ge Li",
            "Wei Gao"
        ],
        "tldr": "The paper introduces Flow4Agent, a framework leveraging optical flow motion priors to improve long-form video understanding with Multimodal Large Language Models by reducing temporal and spatial redundancy. It outperforms existing methods on long video benchmarks.",
        "tldr_zh": "该论文介绍了Flow4Agent，一个利用光流运动先验来改进多模态大型语言模型对长视频理解的框架，通过减少时间和空间冗余来实现。该方法在长视频基准测试中优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation",
        "summary": "Despite the prevalence of transparent object interactions in human everyday\nlife, transparent robotic manipulation research remains limited to\nshort-horizon tasks and basic grasping capabilities.Although some methods have\npartially addressed these issues, most of them have limitations in\ngeneralizability to novel objects and are insufficient for precise long-horizon\nrobot manipulation. To address this limitation, we propose DeLTa (Demonstration\nand Language-Guided Novel Transparent Object Manipulation), a novel framework\nthat integrates depth estimation, 6D pose estimation, and vision-language\nplanning for precise long-horizon manipulation of transparent objects guided by\nnatural task instructions. A key advantage of our method is its\nsingle-demonstration approach, which generalizes 6D trajectories to novel\ntransparent objects without requiring category-level priors or additional\ntraining. Additionally, we present a task planner that refines the\nVLM-generated plan to account for the constraints of a single-arm, eye-in-hand\nrobot for long-horizon object manipulation tasks. Through comprehensive\nevaluation, we demonstrate that our method significantly outperforms existing\ntransparent object manipulation approaches, particularly in long-horizon\nscenarios requiring precise manipulation capabilities. Project page:\nhttps://sites.google.com/view/DeLTa25/",
        "url": "http://arxiv.org/abs/2510.05662v1",
        "published_date": "2025-10-07T08:18:29+00:00",
        "updated_date": "2025-10-07T08:18:29+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Taeyeop Lee",
            "Gyuree Kang",
            "Bowen Wen",
            "Youngho Kim",
            "Seunghyeok Back",
            "In So Kweon",
            "David Hyunchul Shim",
            "Kuk-Jin Yoon"
        ],
        "tldr": "The paper introduces DeLTa, a framework for long-horizon transparent object manipulation using single-demonstration learning and vision-language planning, outperforming existing methods in precision and generalization.",
        "tldr_zh": "该论文介绍了DeLTa，一个使用单次演示学习和视觉-语言规划进行长时程透明物体操作的框架，在精度和泛化性方面优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work",
        "summary": "Recent advances in multimodal large language models (MLLMs) raise the\nquestion of their potential for grading, analyzing, and offering feedback on\nhandwritten student classwork. This capability would be particularly beneficial\nin elementary and middle-school mathematics education, where most work remains\nhandwritten, because seeing students' full working of a problem provides\nvaluable insights into their learning processes, but is extremely\ntime-consuming to grade. We present two experiments investigating MLLM\nperformance on handwritten student mathematics classwork. Experiment A examines\n288 handwritten responses from Ghanaian middle school students solving\narithmetic problems with objective answers. In this context, models achieved\nnear-human accuracy (95%, k = 0.90) but exhibited occasional errors that human\neducators would be unlikely to make. Experiment B evaluates 150 mathematical\nillustrations from American elementary students, where the drawings are the\nanswer to the question. These tasks lack single objective answers and require\nsophisticated visual interpretation as well as pedagogical judgment in order to\nanalyze and evaluate them. We attempted to separate MLLMs' visual capabilities\nfrom their pedagogical abilities by first asking them to grade the student\nillustrations directly, and then by augmenting the image with a detailed human\ndescription of the illustration. We found that when the models had to analyze\nthe student illustrations directly, they struggled, achieving only k = 0.20\nwith ground truth scores, but when given human descriptions, their agreement\nlevels improved dramatically to k = 0.47, which was in line with human-to-human\nagreement levels. This gap suggests MLLMs can \"see\" and interpret arithmetic\nwork relatively well, but still struggle to \"see\" student mathematical\nillustrations.",
        "url": "http://arxiv.org/abs/2510.05538v1",
        "published_date": "2025-10-07T02:59:18+00:00",
        "updated_date": "2025-10-07T02:59:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Owen Henkel",
            "Bill Roberts",
            "Doug Jaffe",
            "Laurence Holt"
        ],
        "tldr": "This paper investigates the ability of multimodal LLMs to grade handwritten student work, finding near-human accuracy on arithmetic problems but struggles with mathematical illustrations, highlighting limitations in visual understanding.",
        "tldr_zh": "本文研究了多模态LLM对学生手写作业进行评分的能力，发现其在算术问题上接近人类的准确率，但在数学插图方面表现不佳，突出了视觉理解的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]