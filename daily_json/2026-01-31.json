[
    {
        "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
        "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.",
        "url": "http://arxiv.org/abs/2601.22153v1",
        "published_date": "2026-01-29T18:59:51+00:00",
        "updated_date": "2026-01-29T18:59:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haozhe Xie",
            "Beichen Wen",
            "Jiarui Zheng",
            "Zhaoxi Chen",
            "Fangzhou Hong",
            "Haiwen Diao",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces DynamicVLA, a Vision-Language-Action model specifically designed for dynamic object manipulation, featuring temporal reasoning, continuous inference, and latent-aware action streaming, along with a new Dynamic Object Manipulation (DOM) benchmark.",
        "tldr_zh": "该论文介绍了 DynamicVLA，一种专为动态物体操作设计的视觉-语言-动作模型，具有时间推理、连续推理和潜在感知动作流等特性，以及一个新的动态物体操作 (DOM) 基准。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions",
        "summary": "Large Vision-Language Models (VLMs) often answer classic visual illusions \"correctly\" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.",
        "url": "http://arxiv.org/abs/2601.22150v1",
        "published_date": "2026-01-29T18:59:24+00:00",
        "updated_date": "2026-01-29T18:59:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoxiao Sun",
            "Mingyang Li",
            "Kun yuan",
            "Min Woo Sun",
            "Mark Endo",
            "Shengguang Wu",
            "Changlin Li",
            "Yuhui Zhang",
            "Zeyu Wang",
            "Serena Yeung-Levy"
        ],
        "tldr": "This paper introduces VI-Probe, a framework for analyzing whether VLMs' correct answers to visual illusions stem from genuine visual perception or mere memorization, finding heterogeneous causes across different VLM architectures.",
        "tldr_zh": "本文介绍了VI-Probe，一个用于分析视觉语言模型(VLM)对视觉错觉的正确答案是源于真实的视觉感知还是单纯的记忆的框架，发现不同VLM架构之间存在不同的根本原因。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence",
        "summary": "Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.",
        "url": "http://arxiv.org/abs/2601.22114v1",
        "published_date": "2026-01-29T18:41:52+00:00",
        "updated_date": "2026-01-29T18:41:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "eess.SY"
        ],
        "authors": [
            "Saoud Aldowaish",
            "Yashwanth Karumanchi",
            "Kai-Chen Chiang",
            "Soroosh Noorzad",
            "Morteza Fayazi"
        ],
        "tldr": "SINA is a novel, open-source tool that uses deep learning, CCL, OCR, and a VLM to automatically generate netlists from circuit schematic images, achieving state-of-the-art accuracy.",
        "tldr_zh": "SINA是一个新型的开源工具，它使用深度学习、CCL、OCR和VLM从电路原理图图像自动生成网表，实现了最先进的精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]