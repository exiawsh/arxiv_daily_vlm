[
    {
        "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
        "summary": "Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2512.24985v1",
        "published_date": "2025-12-31T17:31:29+00:00",
        "updated_date": "2025-12-31T17:31:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Yohan Park",
            "Hyunwoo Ha",
            "Wonjun Jo",
            "Tae-Hyun Oh"
        ],
        "tldr": "The paper introduces DarkEQA, a new benchmark for evaluating Vision-Language Models in low-light embodied question answering scenarios, highlighting performance limitations of current VLMs under such conditions.",
        "tldr_zh": "该论文介绍了 DarkEQA，一个新的基准，用于评估视觉语言模型在弱光具身问答场景中的表现，突出了当前视觉语言模型在这些条件下的性能限制。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement",
        "summary": "Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \\textbf{+22.7} pp in disease classification and \\textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.",
        "url": "http://arxiv.org/abs/2512.24947v1",
        "published_date": "2025-12-31T16:21:31+00:00",
        "updated_date": "2025-12-31T16:21:31+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Wentao Zhang",
            "Tao Fang",
            "Lina Lu",
            "Lifei Wang",
            "Weihe Zhong"
        ],
        "tldr": "The paper introduces CPJ, a training-free, few-shot framework for agricultural pest diagnosis that uses captioning and LLM-judged refinement to improve VQA performance without fine-tuning, achieving significant improvements on the CDDMBench dataset.",
        "tldr_zh": "该论文介绍了一种名为CPJ的免训练、小样本农业病虫害诊断框架，该框架利用图像描述和LLM判决的优化来提高VQA性能，无需微调，并在CDDMBench数据集上取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting",
        "summary": "Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass \"generate-then-segment\" chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a \"Generate-Evaluate-Evolve\" loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.",
        "url": "http://arxiv.org/abs/2512.24702v1",
        "published_date": "2025-12-31T08:10:03+00:00",
        "updated_date": "2025-12-31T08:10:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kai Ye",
            "Xiaotong You",
            "Jianghang Lin",
            "Jiayi Ji",
            "Pingyang Dai",
            "Liujuan Cao"
        ],
        "tldr": "The paper introduces EVOL-SAM3, a zero-shot reasoning segmentation framework that uses an evolutionary approach to iteratively refine prompts, outperforming both static baselines and supervised methods on the ReasonSeg benchmark.",
        "tldr_zh": "该论文介绍了EVOL-SAM3，一个零样本推理分割框架，它使用进化方法迭代优化提示，在ReasonSeg基准测试上优于静态基线和监督方法。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks",
        "summary": "Systematic failures of computer vision models on subsets with coherent visual patterns, known as error slices, pose a critical challenge for robust model evaluation. Existing slice discovery methods are primarily developed for image classification, limiting their applicability to multi-instance tasks such as detection, segmentation, and pose estimation. In real-world scenarios, error slices often arise from corner cases involving complex visual relationships, where existing instance-level approaches lacking fine-grained reasoning struggle to yield meaningful insights. Moreover, current benchmarks are typically tailored to specific algorithms or biased toward image classification, with artificial ground truth that fails to reflect real model failures. To address these limitations, we propose SliceLens, a hypothesis-driven framework that leverages LLMs and VLMs to generate and verify diverse failure hypotheses through grounded visual reasoning, enabling reliable identification of fine-grained and interpretable error slices. We further introduce FeSD (Fine-grained Slice Discovery), the first benchmark specifically designed for evaluating fine-grained error slice discovery across instance-level vision tasks, featuring expert-annotated and carefully refined ground-truth slices with precise grounding to local error regions. Extensive experiments on both existing benchmarks and FeSD demonstrate that SliceLens achieves state-of-the-art performance, improving Precision@10 by 0.42 (0.73 vs. 0.31) on FeSD, and identifies interpretable slices that facilitate actionable model improvements, as validated through model repair experiments.",
        "url": "http://arxiv.org/abs/2512.24592v1",
        "published_date": "2025-12-31T03:28:41+00:00",
        "updated_date": "2025-12-31T03:28:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Zhang",
            "Chaoqun Wang",
            "Zixuan Guan",
            "Sam Kao",
            "Pengfei Zhao",
            "Peng Wu",
            "Sifeng He"
        ],
        "tldr": "The paper introduces SliceLens, a framework using LLMs and VLMs for discovering fine-grained error slices in multi-instance vision tasks, along with a new benchmark (FeSD) for evaluating such methods. It claims state-of-the-art performance and actionable model improvements.",
        "tldr_zh": "该论文介绍了SliceLens，一个利用LLMs和VLMs来发现多实例视觉任务中细粒度错误切片的框架，以及一个新的用于评估此类方法的基准(FeSD)。它声称具有最先进的性能和可操作的模型改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation",
        "summary": "We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.",
        "url": "http://arxiv.org/abs/2512.24903v1",
        "published_date": "2025-12-31T15:00:03+00:00",
        "updated_date": "2025-12-31T15:00:03+00:00",
        "categories": [
            "cs.CV",
            "cs.CE"
        ],
        "authors": [
            "Zichen Tang",
            "Haihong E",
            "Rongjin Li",
            "Jiacheng Liu",
            "Linwei Jia",
            "Zhuodi Hao",
            "Zhongjun Yang",
            "Yuanze Li",
            "Haolin Tian",
            "Xinyi Hu",
            "Peizhi Zhao",
            "Yuan Liu",
            "Zhengyu Wang",
            "Xianghe Wang",
            "Yiling Huang",
            "Xueyuan Lin",
            "Ruofei Bai",
            "Zijian Xie",
            "Qian Huang",
            "Ruining Cao",
            "Haocheng Gao"
        ],
        "tldr": "The paper introduces FinMMDocR, a new challenging bilingual multimodal benchmark for financial numerical reasoning, emphasizing scenario awareness, document understanding, and multi-step computation.",
        "tldr_zh": "本文介绍 FinMMDocR，这是一个新的具有挑战性的双语多模态基准，用于金融数值推理，强调场景感知、文档理解和多步计算。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.",
        "url": "http://arxiv.org/abs/2512.24851v1",
        "published_date": "2025-12-31T13:21:21+00:00",
        "updated_date": "2025-12-31T13:21:21+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xunyi Zhao",
            "Gengze Zhou",
            "Qi Wu"
        ],
        "tldr": "The paper introduces VLN-MME, a benchmark for evaluating MLLMs in Vision-and-Language Navigation (VLN), revealing limitations in their spatial reasoning and sequential decision-making despite instruction following abilities.",
        "tldr_zh": "该论文介绍了VLN-MME，一个用于评估MLLM在视觉语言导航（VLN）中表现的基准，揭示了MLLM虽然能遵循指令，但在空间推理和序列决策方面存在局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning",
        "summary": "Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.",
        "url": "http://arxiv.org/abs/2512.24591v1",
        "published_date": "2025-12-31T03:28:17+00:00",
        "updated_date": "2025-12-31T03:28:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fuyu Dong",
            "Ke Li",
            "Di Wang",
            "Nan Luo",
            "Yiming Zhang",
            "Kaiyu Li",
            "Jianfei Yang",
            "Quan Wang"
        ],
        "tldr": "This paper introduces DARFT, a reinforcement learning fine-tuning framework to improve Change Detection Visual Question Answering (CDVQA) performance by explicitly addressing decision ambiguity, particularly in few-shot settings.",
        "tldr_zh": "本文介绍了一种名为DARFT的强化学习微调框架，通过明确解决决策模糊性来提高变化检测视觉问答（CDVQA）的性能，尤其是在少样本设置中。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
        "summary": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO",
        "url": "http://arxiv.org/abs/2512.24551v1",
        "published_date": "2025-12-31T01:19:14+00:00",
        "updated_date": "2025-12-31T01:19:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhao Cai",
            "Kunpeng Li",
            "Menglin Jia",
            "Jialiang Wang",
            "Junzhe Sun",
            "Feng Liang",
            "Weifeng Chen",
            "Felix Juefei-Xu",
            "Chu Wang",
            "Ali Thabet",
            "Xiaoliang Dai",
            "Xuan Ju",
            "Alan Yuille",
            "Ji Hou"
        ],
        "tldr": "This paper introduces PhyGDPO, a physics-aware text-to-video generation framework utilizing a physics-augmented dataset and groupwise direct preference optimization to improve physical consistency in generated videos.",
        "tldr_zh": "本文介绍了PhyGDPO，一个物理感知文本到视频生成框架，它利用物理增强数据集和分组直接偏好优化来提高生成视频中的物理一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation",
        "summary": "The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model's generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.",
        "url": "http://arxiv.org/abs/2512.24861v1",
        "published_date": "2025-12-31T13:41:16+00:00",
        "updated_date": "2025-12-31T13:41:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meng Lan",
            "Lefei Zhang",
            "Xiaomeng Li"
        ],
        "tldr": "The paper introduces OFL-SAM2, a prompt-free adaptation of SAM2 for medical image segmentation, leveraging an online few-shot learner to generate target features from limited annotated data, achieving state-of-the-art performance with minimal training data.",
        "tldr_zh": "该论文介绍了OFL-SAM2，一种无需提示的SAM2在医学图像分割中的应用，利用在线小样本学习器从有限的标注数据中生成目标特征，以最小的训练数据实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]