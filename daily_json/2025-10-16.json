[
    {
        "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models",
        "summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks\nbut exhibit sharp performance degradation when applied to novel domains with\nsubstantial distribution shifts from pretraining data. Existing domain\nadaptation approaches finetune different VLM components, but this often results\nin limited domain-specific feature learning or catastrophic forgetting of prior\ncapabilities. To address these issues, we introduce Vision Contextualized\nProbing (VisCoP), which augments the VLM's vision encoder with a compact set of\nlearnable visual probes. These probes enable efficient domain-specific\nadaptation with minimal modification to pretrained parameters. We evaluate\nVisCoP across three challenging domain adaptation settings-cross-view\n(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human\nunderstanding to robot control). Experiments show that VisCoP consistently\noutperforms existing adaptation strategies, achieving superior performance on\ntarget domains while effectively retaining source-domain knowledge.",
        "url": "http://arxiv.org/abs/2510.13808v1",
        "published_date": "2025-10-15T17:59:52+00:00",
        "updated_date": "2025-10-15T17:59:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dominick Reilly",
            "Manish Kumar Govind",
            "Le Xue",
            "Srijan Das"
        ],
        "tldr": "The paper introduces VisCoP, a visual probing method for domain adaptation in VLMs, which outperforms existing methods by using learnable visual probes to adapt to new domains while retaining source-domain knowledge.",
        "tldr_zh": "该论文介绍了一种名为VisCoP的视觉探测方法，用于视觉语言模型中的领域自适应。该方法通过使用可学习的视觉探针来适应新领域，同时保留源领域知识，优于现有的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
        "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
        "url": "http://arxiv.org/abs/2510.13804v1",
        "published_date": "2025-10-15T17:59:24+00:00",
        "updated_date": "2025-10-15T17:59:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Xinchen Zhang",
            "Xiaoying Zhang",
            "Youbin Wu",
            "Yanbin Cao",
            "Renrui Zhang",
            "Ruihang Chu",
            "Ling Yang",
            "Yujiu Yang"
        ],
        "tldr": "The paper introduces Generative Universal Verifier, a novel plugin for multimodal reasoning that enhances visual outcome reflection and refinement in VLMs. They also present a new benchmark, ViVerBench, and an omni-capable verifier, OmniVerifier-7B, showcasing improved performance in image generation and editing.",
        "tldr_zh": "该论文介绍了一种新的通用生成验证器，该插件用于多模态推理，可以增强视觉语言模型中视觉结果的反射和改进。他们还提出了一个新的基准测试ViVerBench和一个全能验证器OmniVerifier-7B，展示了在图像生成和编辑方面的改进性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reasoning in Space via Grounding in the World",
        "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2510.13800v1",
        "published_date": "2025-10-15T17:58:08+00:00",
        "updated_date": "2025-10-15T17:58:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Chen",
            "Zekun Qi",
            "Wenyao Zhang",
            "Xin Jin",
            "Li Zhang",
            "Peidong Liu"
        ],
        "tldr": "The paper introduces GS-Reasoner, a 3D LLM that integrates grounding and spatial reasoning using a unified 3D representation and a new dataset called GCoT, achieving state-of-the-art performance without external modules.",
        "tldr_zh": "该论文介绍了GS-Reasoner，一个3D LLM，它使用统一的3D表示和名为GCoT的新数据集，集成了grounding和空间推理，在没有外部模块的情况下实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs",
        "summary": "Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.",
        "url": "http://arxiv.org/abs/2510.13795v1",
        "published_date": "2025-10-15T17:52:59+00:00",
        "updated_date": "2025-10-15T17:52:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yi Zhang",
            "Bolin Ni",
            "Xin-Sheng Chen",
            "Heng-Rui Zhang",
            "Yongming Rao",
            "Houwen Peng",
            "Qinglin Lu",
            "Han Hu",
            "Meng-Hao Guo",
            "Shi-Min Hu"
        ],
        "tldr": "This paper introduces Honey-Data-15M, a high-quality SFT dataset for MLLMs, along with a data curation pipeline (HoneyPipe) and evaluation suite, resulting in the state-of-the-art fully open MLLM, Bee-8B.",
        "tldr_zh": "该论文介绍了高质量的MLLM的SFT数据集Honey-Data-15M，以及数据整理流程（HoneyPipe）和评估套件，最终产生了一个最先进的完全开放的MLLM，Bee-8B。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
        "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
        "url": "http://arxiv.org/abs/2510.13778v1",
        "published_date": "2025-10-15T17:30:05+00:00",
        "updated_date": "2025-10-15T17:30:05+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xinyi Chen",
            "Yilun Chen",
            "Yanwei Fu",
            "Ning Gao",
            "Jiaya Jia",
            "Weiyang Jin",
            "Hao Li",
            "Yao Mu",
            "Jiangmiao Pang",
            "Yu Qiao",
            "Yang Tian",
            "Bin Wang",
            "Bolun Wang",
            "Fangjing Wang",
            "Hanqing Wang",
            "Tai Wang",
            "Ziqin Wang",
            "Xueyuan Wei",
            "Chao Wu",
            "Shuai Yang",
            "Jinhui Ye",
            "Junqiu Yu",
            "Jia Zeng",
            "Jingjing Zhang",
            "Jinyu Zhang",
            "Shi Zhang",
            "Feng Zheng",
            "Bowen Zhou",
            "Yangkun Zhu"
        ],
        "tldr": "InternVLA-M1 is a vision-language-action framework using spatially guided training for generalist robot policy, demonstrating significant improvements in instruction following and spatial reasoning across various environments.",
        "tldr_zh": "InternVLA-M1是一个视觉-语言-动作框架，它采用空间引导训练来实现通用机器人策略，并在各种环境中展示了指令跟随和空间推理方面的显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
        "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.",
        "url": "http://arxiv.org/abs/2510.13759v1",
        "published_date": "2025-10-15T17:10:35+00:00",
        "updated_date": "2025-10-15T17:10:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Zou",
            "Ziqi Huang",
            "Yuhao Dong",
            "Shulin Tian",
            "Dian Zheng",
            "Hongbo Liu",
            "Jingwen He",
            "Bin Liu",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces Uni-MMMU, a new benchmark for evaluating unified multimodal models by assessing bidirectional understanding and generation capabilities across multiple disciplines, revealing performance disparities in current models.",
        "tldr_zh": "该论文介绍了Uni-MMMU，这是一个新的基准，用于评估统一的多模态模型，通过评估跨多个学科的双向理解和生成能力，揭示了当前模型的性能差异。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue",
        "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model for audio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensive omni-modal understanding and speech generation capabilities. To\nachieve this, we integrate the vision encoder, audio encoder, large language\nmodel, and speech decoder into a unified model for understanding and generation\ntasks. We design a multi-stage training strategy to ensure robust cross-modal\ncapabilities, including pre-training for omni-modal understanding, followed by\npost-training with speech conversation and audio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate a\nmulti-turn training dataset that enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and the multi-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in its long-term memory capabilities. Notably, InteractiveOmni-4B\nis comparable to the much larger model like Qwen2.5-Omni-7B on general\nbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B\nwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio, video understanding, and\nspeech generation tasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems.",
        "url": "http://arxiv.org/abs/2510.13747v1",
        "published_date": "2025-10-15T16:52:48+00:00",
        "updated_date": "2025-10-15T16:52:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenwen Tong",
            "Hewei Guo",
            "Dongchuan Ran",
            "Jiangnan Chen",
            "Jiefan Lu",
            "Kaibin Wang",
            "Keqiang Li",
            "Xiaoxu Zhu",
            "Jiakui Li",
            "Kehan Li",
            "Xueheng Li",
            "Lumin Li",
            "Chenxu Guo",
            "Jiasheng Zhou",
            "Jiandong Chen",
            "Xianye Wu",
            "Jiahao Wang",
            "Silei Wu",
            "Lei Chen",
            "Hanming Deng",
            "Yuxuan Song",
            "Dinghao Zhou",
            "Guiping Zhong",
            "Ken Zheng",
            "Shiyin Kang",
            "Lewei Lu"
        ],
        "tldr": "InteractiveOmni introduces a unified, open-source, lightweight omni-modal LLM (4B-8B parameters) for audio-visual multi-turn dialogue, achieving state-of-the-art performance against similarly sized models and demonstrating strong long-term memory capabilities.",
        "tldr_zh": "InteractiveOmni 提出了一个统一的、开源的、轻量级的全模态大型语言模型（4B-8B参数），用于音频-视频多轮对话，在同等规模的模型中实现了最先进的性能，并展示了强大的长期记忆能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching",
        "summary": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal\nretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal\nfoundation model that achieves unified modeling through discrete flow\nparadigms. By leveraging metric-induced probability paths and kinetic optimal\nvelocities, NExT-OMNI natively supports any-to-any understanding and generation\nwith enhanced response efficiency, while enabling broader application scenarios\nthrough concise unified representations rather than task-decoupled designs.\nTrained on large-scale interleaved text, image, video, and audio data,\nNExT-OMNI delivers competitive performance on multimodal generation and\nunderstanding benchmarks, while outperforming prior unified models in\nmulti-turn multimodal interaction and cross-modal retrieval, highlighting its\narchitectural advantages as a next-generation multimodal foundation model. To\nadvance further research, we release training details, data protocols, and\nopen-source both the code and model checkpoints.",
        "url": "http://arxiv.org/abs/2510.13721v1",
        "published_date": "2025-10-15T16:25:18+00:00",
        "updated_date": "2025-10-15T16:25:18+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Run Luo",
            "Xiaobo Xia",
            "Lu Wang",
            "Longze Chen",
            "Renke Shan",
            "Jing Luo",
            "Min Yang",
            "Tat-Seng Chua"
        ],
        "tldr": "NExT-OMNI is a new open-source omnimodal foundation model leveraging discrete flow matching for unified any-to-any understanding and generation, demonstrating competitive performance and architectural advantages in multimodal interaction and retrieval.",
        "tldr_zh": "NExT-OMNI是一个新的开源全模态基础模型，它利用离散流匹配实现统一的任意到任意的理解和生成，并在多模态交互和检索中表现出具有竞争力的性能和架构优势。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language Models",
        "summary": "One of the key challenges of modern AI models is ensuring that they provide\nhelpful responses to benign queries while refusing malicious ones. But often,\nthe models are vulnerable to multimodal queries with harmful intent embedded in\nimages. One approach for safety alignment is training with extensive safety\ndatasets at the significant costs in both dataset curation and training.\nInference-time alignment mitigates these costs, but introduces two drawbacks:\nexcessive refusals from misclassified benign queries and slower inference speed\ndue to iterative output adjustments. To overcome these limitations, we propose\nto reformulate queries to strengthen cross-modal attention to safety-critical\nimage regions, enabling accurate risk assessment at the query level. Using the\nassessed risk, it adaptively steers activations to generate responses that are\nsafe and helpful without overhead from iterative output adjustments. We call\nthis Risk-adaptive Activation Steering (RAS). Extensive experiments across\nmultiple benchmarks on multimodal safety and utility demonstrate that the RAS\nsignificantly reduces attack success rates, preserves general task performance,\nand improves inference speed over prior inference-time defenses.",
        "url": "http://arxiv.org/abs/2510.13698v1",
        "published_date": "2025-10-15T15:57:17+00:00",
        "updated_date": "2025-10-15T15:57:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jonghyun Park",
            "Minhyuk Seo",
            "Jonghyun Choi"
        ],
        "tldr": "The paper introduces Risk-adaptive Activation Steering (RAS), an inference-time method for multimodal LLMs that improves safety by focusing on safety-critical image regions, reducing attack success rates, preserving performance, and improving inference speed.",
        "tldr_zh": "该论文介绍了一种名为风险自适应激活引导（RAS）的推理时方法，用于多模态LLM，通过关注安全关键的图像区域来提高安全性，降低攻击成功率，保持性能，并提高推理速度。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning",
        "summary": "Open-domain visual entity recognition aims to identify and link entities\ndepicted in images to a vast and evolving set of real-world concepts, such as\nthose found in Wikidata. Unlike conventional classification tasks with fixed\nlabel sets, it operates under open-set conditions, where most target entities\nare unseen during training and exhibit long-tail distributions. This makes the\ntask inherently challenging due to limited supervision, high visual ambiguity,\nand the need for semantic disambiguation. In this work, we propose a\nKnowledge-guided Contrastive Learning (KnowCoL) framework that combines both\nimages and text descriptions into a shared semantic space grounded by\nstructured information from Wikidata. By abstracting visual and textual inputs\nto a conceptual level, the model leverages entity descriptions, type\nhierarchies, and relational context to support zero-shot entity recognition. We\nevaluate our approach on the OVEN benchmark, a large-scale open-domain visual\nrecognition dataset with Wikidata IDs as the label space. Our experiments show\nthat using visual, textual, and structured knowledge greatly improves accuracy,\nespecially for rare and unseen entities. Our smallest model improves the\naccuracy on unseen entities by 10.5% compared to the state-of-the-art, despite\nbeing 35 times smaller.",
        "url": "http://arxiv.org/abs/2510.13675v1",
        "published_date": "2025-10-15T15:33:36+00:00",
        "updated_date": "2025-10-15T15:33:36+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Hongkuan Zhou",
            "Lavdim Halilaj",
            "Sebastian Monka",
            "Stefan Schmid",
            "Yuqicheng Zhu",
            "Jingcheng Wu",
            "Nadeem Nazer",
            "Steffen Staab"
        ],
        "tldr": "This paper introduces KnowCoL, a Knowledge-guided Contrastive Learning framework for open-domain visual entity recognition, leveraging Wikidata to improve zero-shot performance, especially for rare entities.",
        "tldr_zh": "该论文介绍了KnowCoL，一种知识引导的对比学习框架，用于开放域视觉实体识别，利用Wikidata来提高零样本性能，尤其是在处理稀有实体时。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models",
        "summary": "Spatial reasoning ability is crucial for Vision Language Models (VLMs) to\nsupport real-world applications in diverse domains including robotics,\naugmented reality, and autonomous navigation. Unfortunately, existing\nbenchmarks are inadequate in assessing spatial reasoning ability, especially\nthe \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of\nhuman spatial cognition. In this paper, we propose a unified benchmark,\n\\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that\ncategorizes tasks into four fundamental quadrants:\n\\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic,\n\\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,\nto address the issue of data scarcity, we develop a scalable and automated\npipeline to generate diverse and verifiable spatial reasoning questions,\nresulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE\nBench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA\npairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals\nthat, current VLMs have a large and consistent gap to human competence,\nespecially on multi-step multi-view spatial reasoning. Spatial-DISE offers a\nrobust framework, valuable dataset, and clear direction for future research\ntoward human-like spatial intelligence. Benchmark, dataset, and code will be\npublicly released.",
        "url": "http://arxiv.org/abs/2510.13394v1",
        "published_date": "2025-10-15T10:44:01+00:00",
        "updated_date": "2025-10-15T10:44:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinmiao Huang",
            "Qisong He",
            "Zhenglin Huang",
            "Boxuan Wang",
            "Zhuoyun Li",
            "Guangliang Cheng",
            "Yi Dong",
            "Xiaowei Huang"
        ],
        "tldr": "The paper introduces Spatial-DISE, a new benchmark and dataset for evaluating spatial reasoning abilities in Vision-Language Models (VLMs), highlighting a significant performance gap compared to human competence, particularly in dynamic and multi-view spatial reasoning.",
        "tldr_zh": "该论文介绍了Spatial-DISE，一个新的基准和数据集，用于评估视觉语言模型（VLMs）中的空间推理能力，强调了与人类能力相比的显著性能差距，尤其是在动态和多视图空间推理方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning",
        "summary": "Vision-Language-Action (VLA) models have recently shown impressive\ngeneralization and language-guided manipulation capabilities. However, their\nperformance degrades on tasks requiring precise spatial reasoning due to\nlimited spatial reasoning inherited from Vision-Language Models (VLMs).\nExisting VLAs rely on extensive action-data pretraining to ground VLMs in 3D\nspace, which reduces training efficiency and is still insufficient for accurate\nspatial understanding. In this work, we present DepthVLA, a simple yet\neffective VLA architecture that explicitly incorporates spatial awareness\nthrough a pretrained depth prediction module. DepthVLA adopts a\nmixture-of-transformers design that unifies a VLM, a depth transformer, and an\naction expert with fully shared attentions, forming an end-to-end model with\nenhanced spatial reasoning. Extensive evaluations in both real-world and\nsimulated environments show that DepthVLA outperforms state-of-the-art\napproaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.\n93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.\nOur code will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.13375v1",
        "published_date": "2025-10-15T10:09:00+00:00",
        "updated_date": "2025-10-15T10:09:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyuan Yuan",
            "Yicheng Liu",
            "Chenhao Lu",
            "Zhuoguang Chen",
            "Tao Jiang",
            "Hang Zhao"
        ],
        "tldr": "The paper introduces DepthVLA, a VLA architecture that incorporates depth information using a pretrained depth prediction module to improve spatial reasoning in vision-language-action tasks, demonstrating improved performance in real-world and simulated environments.",
        "tldr_zh": "该论文介绍了DepthVLA，一种VLA架构，它利用预训练的深度预测模块整合深度信息，以提高视觉-语言-动作任务中的空间推理能力，并在真实世界和模拟环境中表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models",
        "summary": "On large-scale e-commerce platforms with tens of millions of active monthly\nusers, recommending visually similar products is essential for enabling users\nto efficiently discover items that align with their preferences. This study\npresents the application of a vision-language model (VLM) -- which has\ndemonstrated strong performance in image recognition and image-text retrieval\ntasks -- to product recommendations on Mercari, a major consumer-to-consumer\nmarketplace used by more than 20 million monthly users in Japan. Specifically,\nwe fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using\none million product image-title pairs from Mercari collected over a three-month\nperiod, and developed an image encoder for generating item embeddings used in\nthe recommendation system. Our evaluation comprised an offline analysis of\nhistorical interaction logs and an online A/B test in a production environment.\nIn offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared\nwith the baseline. In the online A/B test, the click-through rate improved by\n50% whereas the conversion rate improved by 14% compared with the existing\nmodel. These results demonstrate the effectiveness of VLM-based encoders for\ne-commerce product recommendations and provide practical insights into the\ndevelopment of visual similarity-based recommendation systems.",
        "url": "http://arxiv.org/abs/2510.13359v1",
        "published_date": "2025-10-15T09:46:27+00:00",
        "updated_date": "2025-10-15T09:46:27+00:00",
        "categories": [
            "cs.IR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yuki Yada",
            "Sho Akiyama",
            "Ryo Watanabe",
            "Yuta Ueno",
            "Yusuke Shido",
            "Andre Rusli"
        ],
        "tldr": "This paper explores applying a fine-tuned SigLIP vision-language model to product recommendations on the Mercari e-commerce platform, achieving significant improvements in nDCG, click-through rate, and conversion rate.",
        "tldr_zh": "本文探讨了将微调后的SigLIP视觉语言模型应用于Mercari电商平台的产品推荐，并在nDCG、点击率和转化率方面取得了显著提升。",
        "relevance_score": 9,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Augmented Visual Contrastive Decoding",
        "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal\ncapabilities, but they inherit the tendency to hallucinate from their\nunderlying language models. While visual contrastive decoding has been proposed\nto mitigate this issue, existing methods often apply generic visual\naugmentations that disregard the specific context provided by the text query,\nlimiting their effectiveness. This study introduces a novel training-free\ndecoding strategy that addresses these limitations, featuring two key\ncontributions. First, a self-augmentation prompting strategy that leverages the\nintrinsic knowledge of the model to dynamically align semantics between the\nquery and the visual augmentation. Second, an adaptive thresholding algorithm\nthat adaptively adjusts next token candidate size based on the output sparsity,\nutilizing full information from the logit distribution. Extensive experiments\nacross four LVLMs and seven benchmarks demonstrate that the proposed decoding\nsignificantly enhances factual consistency compared to state-of-the-art\ndecoding methods. This work highlights the importance of integrating\nquery-dependent augmentation and entropy-aware decoding for improving effective\ngeneration of LVLMs.",
        "url": "http://arxiv.org/abs/2510.13315v1",
        "published_date": "2025-10-15T09:03:34+00:00",
        "updated_date": "2025-10-15T09:03:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Eun Woo Im",
            "Muhammad Kashif Ali",
            "Vivek Gupta"
        ],
        "tldr": "This paper introduces a novel training-free decoding strategy for Large Vision-Language Models (LVLMs) that uses self-augmentation prompting and adaptive thresholding to improve factual consistency and reduce hallucination.",
        "tldr_zh": "本文提出了一种新颖的、无需训练的大型视觉语言模型（LVLMs）解码策略，该策略采用自增强提示和自适应阈值处理，以提高事实一致性并减少幻觉。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models",
        "summary": "The rapid advancement of large vision language models (LVLMs) has led to a\nsignificant expansion of their context windows. However, an extended context\nwindow does not guarantee the effective utilization of the context, posing a\ncritical challenge for real-world applications. Current evaluations of such\nlong-context faithfulness are predominantly focused on the text-only domain,\nwhile multimodal assessments remain limited to short contexts. To bridge this\ngap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate\nthe fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8\ndistinct tasks spanning 6 context length intervals and incorporates diverse\nmodalities, including text, images, and videos. Our evaluation of\nstate-of-the-art LVLMs reveals their limited faithfulness in handling long\nmultimodal contexts. Furthermore, we provide an in-depth analysis of how\ncontext length and the position of crucial content affect the faithfulness of\nthese models.",
        "url": "http://arxiv.org/abs/2510.13276v1",
        "published_date": "2025-10-15T08:22:03+00:00",
        "updated_date": "2025-10-15T08:22:03+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Keyan Zhou",
            "Zecheng Tang",
            "Lingfeng Ming",
            "Guanghao Zhou",
            "Qiguang Chen",
            "Dan Qiao",
            "Zheming Yang",
            "Libo Qin",
            "Minghui Qiu",
            "Juntao Li",
            "Min Zhang"
        ],
        "tldr": "The paper introduces MMLongCite, a new benchmark to evaluate the fidelity of long-context vision-language models (LVLMs) across multiple modalities and context lengths, revealing limitations in current state-of-the-art models.",
        "tldr_zh": "该论文介绍了MMLongCite，一个新的基准，用于评估长文本视觉语言模型（LVLMs）在多种模态和上下文长度下的保真度，揭示了当前最先进模型的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "End-to-End Multi-Modal Diffusion Mamba",
        "summary": "Current end-to-end multi-modal models utilize different encoders and decoders\nto process input and output information. This separation hinders the joint\nrepresentation learning of various modalities. To unify multi-modal processing,\nwe propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM\nutilizes a Mamba-based multi-step selection diffusion model to progressively\ngenerate and refine modality-specific information through a unified variational\nautoencoder for both encoding and decoding. This innovative approach allows MDM\nto achieve superior performance when processing high-dimensional data,\nparticularly in generating high-resolution images and extended text sequences\nsimultaneously. Our evaluations in areas such as image generation, image\ncaptioning, visual question answering, text comprehension, and reasoning tasks\ndemonstrate that MDM significantly outperforms existing end-to-end models\n(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA\nmodels like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's\neffectiveness in unifying multi-modal processes while maintaining computational\nefficiency, establishing a new direction for end-to-end multi-modal\narchitectures.",
        "url": "http://arxiv.org/abs/2510.13253v1",
        "published_date": "2025-10-15T08:03:50+00:00",
        "updated_date": "2025-10-15T08:03:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunhao Lu",
            "Qiang Lu",
            "Meichen Dong",
            "Jake Luo"
        ],
        "tldr": "The paper introduces Multi-modal Diffusion Mamba (MDM), a novel architecture unifying multi-modal processing via a Mamba-based diffusion model and a unified variational autoencoder, achieving superior performance in various multi-modal tasks compared to existing end-to-end and competitive with some SOTA models.",
        "tldr_zh": "本文介绍了一种名为多模态扩散 Mamba (MDM) 的新型架构，该架构通过基于 Mamba 的扩散模型和统一的变分自动编码器统一多模态处理，在各种多模态任务中实现了优于现有端到端模型的性能，并与一些 SOTA 模型具有竞争力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs",
        "summary": "Video Large Language Models (VideoLLMs) extend the capabilities of\nvision-language models to spatiotemporal inputs, enabling tasks such as video\nquestion answering (VideoQA). Despite recent advances in VideoLLMs, their\ninternal mechanisms on where and how they extract and propagate video and\ntextual information remain less explored. In this study, we investigate the\ninternal information flow of VideoLLMs using mechanistic interpretability\ntechniques. Our analysis reveals consistent patterns across diverse VideoQA\ntasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame\ninteractions in early-to-middle layers, (2) followed by progressive\nvideo-language integration in middle layers. This is facilitated by alignment\nbetween video representations and linguistic embeddings containing temporal\nconcepts. (3) Upon completion of this integration, the model is ready to\ngenerate correct answers in middle-to-late layers. (4) Based on our analysis,\nwe show that VideoLLMs can retain their VideoQA performance by selecting these\neffective information pathways while suppressing a substantial amount of\nattention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a\nblueprint on how VideoLLMs perform temporal reasoning and offer practical\ninsights for improving model interpretability and downstream generalization.\nOur project page with the source code is available at\nhttps://map-the-flow.github.io",
        "url": "http://arxiv.org/abs/2510.13251v1",
        "published_date": "2025-10-15T07:59:06+00:00",
        "updated_date": "2025-10-15T07:59:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minji Kim",
            "Taekyung Kim",
            "Bohyung Han"
        ],
        "tldr": "This paper uses mechanistic interpretability to analyze information flow in VideoLLMs, revealing how they perform temporal reasoning and identifying key information pathways for improved efficiency and interpretability.",
        "tldr_zh": "本文利用可解释性方法分析了视频大语言模型（VideoLLM）中的信息流动，揭示了它们如何进行时间推理，并识别出关键的信息路径，从而提高效率和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking",
        "summary": "Multimodal semantic cues, such as textual descriptions, have shown strong\npotential in enhancing target perception for tracking. However, existing\nmethods rely on static textual descriptions from large language models, which\nlack adaptability to real-time target state changes and prone to\nhallucinations. To address these challenges, we propose a unified multimodal\nvision-language tracking framework, named EPIPTrack, which leverages explicit\nand implicit prompts for dynamic target modeling and semantic alignment.\nSpecifically, explicit prompts transform spatial motion information into\nnatural language descriptions to provide spatiotemporal guidance. Implicit\nprompts combine pseudo-words with learnable descriptors to construct\nindividualized knowledge representations capturing appearance attributes. Both\nprompts undergo dynamic adjustment via the CLIP text encoder to respond to\nchanges in target state. Furthermore, we design a Discriminative Feature\nAugmentor to enhance visual and cross-modal representations. Extensive\nexperiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack\noutperforms existing trackers in diverse scenarios, exhibiting robust\nadaptability and superior performance.",
        "url": "http://arxiv.org/abs/2510.13235v1",
        "published_date": "2025-10-15T07:39:30+00:00",
        "updated_date": "2025-10-15T07:39:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yukuan Zhang",
            "Jiarui Zhao",
            "Shangqing Nie",
            "Jin Kuang",
            "Shengsheng Wang"
        ],
        "tldr": "EPIPTrack introduces a novel multi-object tracking framework using explicit and implicit prompts to dynamically model targets and align semantic information, outperforming existing trackers.",
        "tldr_zh": "EPIPTrack 引入了一种新的多目标跟踪框架，该框架使用显式和隐式提示来动态建模目标并对齐语义信息，性能优于现有的跟踪器。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "What \"Not\" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging",
        "summary": "State-of-the-art vision-language models (VLMs) suffer from a critical failure\nin understanding negation, often referred to as affirmative bias. This\nlimitation is particularly severe in described object detection (DOD) tasks. To\naddress this, we propose two primary contributions: (1) a new dataset pipeline\nand (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a\ndataset constructed with a systematic chain-of-thought (CoT) and VQA-based\npipeline to generate high-quality, instance-grounded negation data. Second, we\npropose NegToMe, a novel text token merging module that directly tackles the\narchitectural cause of affirmative bias. NegToMe fundamentally addresses the\nstructural loss of negation cues in tokenization, grouping them with attributes\ninto coherent semantic phrases. It maintains correct polarity at the input\nlevel, enabling robust negation understanding even with limited data. For\ninstance, to prevent a model from treating the fragmented tokens \"not\" and\n\"girl\" as simply \"girl\", NegToMe binds them into a single token whose meaning\nis correctly distinguished from that of \"girl\" alone. This module is integrated\nwith a parameter-efficient and strategic LoRA fine-tuning approach. Our method\nsignificantly improves performance on challenging negation benchmarks with a\nlowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval\nand demonstrating generalization to SoTA VLMs. This work marks a crucial step\nforward in addressing negation understanding for real-world detection\napplications.",
        "url": "http://arxiv.org/abs/2510.13232v1",
        "published_date": "2025-10-15T07:36:38+00:00",
        "updated_date": "2025-10-15T07:36:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Inha Kang",
            "Youngsun Lim",
            "Seonho Lee",
            "Jiho Choi",
            "Junsuk Choe",
            "Hyunjung Shim"
        ],
        "tldr": "This paper introduces CoVAND, a dataset for negation in object detection, and NegToMe, a token merging module that improves VLM performance on negation benchmarks by addressing the architectural cause of affirmative bias.",
        "tldr_zh": "本文介绍了CoVAND，一个用于目标检测中否定的数据集，以及NegToMe，一个通过解决肯定性偏差的架构原因来提高VLM在否定基准测试中性能的token合并模块。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment",
        "summary": "Text-image alignment constitutes a foundational challenge in multimedia\ncontent understanding, where effective modeling of cross-modal semantic\ncorrespondences critically enhances retrieval system performance through joint\nembedding space optimization. Given the inherent difference in information\nentropy between texts and images, conventional approaches often show an\nimbalance in the mutual retrieval of these two modalities. To address this\nparticular challenge, we propose to use the open semantic knowledge of Large\nLanguage Model (LLM) to fill for the entropy gap and reproduce the alignment\nability of humans in these tasks. Our entropy-enhancing alignment is achieved\nthrough a two-step process: 1) a new prompt template that does not rely on\nexplicit knowledge in the task domain is designed to use LLM to enhance the\npolysemy description of the text modality. By analogy, the information entropy\nof the text modality relative to the visual modality is increased; 2) A\nhypergraph adapter is used to construct multilateral connections between the\ntext and image modalities, which can correct the positive and negative matching\nerrors for synonymous semantics in the same fixed embedding space, whilst\nreducing the noise caused by open semantic entropy by mapping the reduced\ndimensions back to the original dimensions. Comprehensive evaluations on the\nFlickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic\nHypergraph Adapter (OS-HGAdapter), showcasing 16.8\\% (text-to-image) and 40.1\\%\n(image-to-text) cross-modal retrieval gains over existing methods while\nestablishing new state-of-the-art performance in semantic alignment tasks.",
        "url": "http://arxiv.org/abs/2510.13131v1",
        "published_date": "2025-10-15T04:09:00+00:00",
        "updated_date": "2025-10-15T04:09:00+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Rongjun Chen",
            "Chengsi Yao",
            "Jinchang Ren",
            "Xianxian Zeng",
            "Peixian Wang",
            "Jun Yuan",
            "Jiawen Li",
            "Huimin Zhao",
            "Xu Lu"
        ],
        "tldr": "The paper introduces OS-HGAdapter, a novel approach leveraging LLMs and hypergraph adapters to enhance image-text alignment by addressing entropy imbalance between modalities, achieving state-of-the-art retrieval performance on Flickr30K and MS-COCO.",
        "tldr_zh": "该论文介绍了OS-HGAdapter，一种利用大型语言模型和超图适配器的新方法，通过解决模态之间的熵不平衡来增强图像-文本对齐，并在Flickr30K和MS-COCO上实现了最先进的检索性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models",
        "summary": "Benchmarking autonomous driving planners to align with human judgment remains\na critical challenge, as state-of-the-art metrics like the Extended Predictive\nDriver Model Score (EPDMS) lack context awareness in nuanced scenarios. To\naddress this, we introduce DriveCritic, a novel framework featuring two key\ncontributions: the DriveCritic dataset, a curated collection of challenging\nscenarios where context is critical for correct judgment and annotated with\npairwise human preferences, and the DriveCritic model, a Vision-Language Model\n(VLM) based evaluator. Fine-tuned using a two-stage supervised and\nreinforcement learning pipeline, the DriveCritic model learns to adjudicate\nbetween trajectory pairs by integrating visual and symbolic context.\nExperiments show DriveCritic significantly outperforms existing metrics and\nbaselines in matching human preferences and demonstrates strong context\nawareness. Overall, our work provides a more reliable, human-aligned foundation\nto evaluating autonomous driving systems.",
        "url": "http://arxiv.org/abs/2510.13108v1",
        "published_date": "2025-10-15T03:00:38+00:00",
        "updated_date": "2025-10-15T03:00:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Jingyu Song",
            "Zhenxin Li",
            "Shiyi Lan",
            "Xinglong Sun",
            "Nadine Chang",
            "Maying Shen",
            "Joshua Chen",
            "Katherine A. Skinner",
            "Jose M. Alvarez"
        ],
        "tldr": "The paper introduces DriveCritic, a new framework for evaluating autonomous driving planners using Vision-Language Models, trained on a novel dataset of challenging scenarios annotated with human preferences, demonstrating improved alignment with human judgment compared to existing metrics.",
        "tldr_zh": "该论文介绍了DriveCritic，一个使用视觉-语言模型评估自动驾驶规划器的新框架。该框架基于一个包含具有人类偏好注释的挑战性场景的新数据集进行训练，与现有指标相比，展示了与人类判断的更好对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation",
        "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
        "url": "http://arxiv.org/abs/2510.13084v1",
        "published_date": "2025-10-15T01:55:32+00:00",
        "updated_date": "2025-10-15T01:55:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi Zuo",
            "Zitao Wang",
            "Lingling Li",
            "Xu Liu",
            "Fang Liu",
            "Licheng Jiao"
        ],
        "tldr": "The paper introduces Edit-Your-Interest, a text-driven video editing method that uses a spatio-temporal feature memory and feature propagation to improve efficiency and visual fidelity compared to existing diffusion-based approaches.",
        "tldr_zh": "该论文介绍了一种名为Edit-Your-Interest的文本驱动视频编辑方法，该方法使用时空特征记忆和特征传播来提高效率和视觉保真度，优于现有的基于扩散的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles",
        "summary": "Safe large-scale coordination of multiple cooperative connected autonomous\nvehicles (CAVs) hinges on communication that is both efficient and\ninterpretable. Existing approaches either rely on transmitting high-bandwidth\nraw sensor data streams or neglect perception and planning uncertainties\ninherent in shared data, resulting in systems that are neither scalable nor\nsafe. To address these limitations, we propose Uncertainty-Guided Natural\nLanguage Cooperative Autonomous Planning (UNCAP), a vision-language model-based\nplanning approach that enables CAVs to communicate via lightweight natural\nlanguage messages while explicitly accounting for perception uncertainty in\ndecision-making. UNCAP features a two-stage communication protocol: (i) an ego\nCAV first identifies the subset of vehicles most relevant for information\nexchange, and (ii) the selected CAVs then transmit messages that quantitatively\nexpress their perception uncertainty. By selectively fusing messages that\nmaximize mutual information, this strategy allows the ego vehicle to integrate\nonly the most relevant signals into its decision-making, improving both the\nscalability and reliability of cooperative planning. Experiments across diverse\ndriving scenarios show a 63% reduction in communication bandwidth with a 31%\nincrease in driving safety score, a 61% reduction in decision uncertainty, and\na four-fold increase in collision distance margin during near-miss events.\nProject website: https://uncap-project.github.io/",
        "url": "http://arxiv.org/abs/2510.12992v1",
        "published_date": "2025-10-14T21:09:09+00:00",
        "updated_date": "2025-10-14T21:09:09+00:00",
        "categories": [
            "cs.RO",
            "cs.CL",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Neel P. Bhatt",
            "Po-han Li",
            "Kushagra Gupta",
            "Rohan Siva",
            "Daniel Milan",
            "Alexander T. Hogue",
            "Sandeep P. Chinchali",
            "David Fridovich-Keil",
            "Zhangyang Wang",
            "Ufuk Topcu"
        ],
        "tldr": "The paper introduces UNCAP, a vision-language model-based cooperative autonomous planning approach that uses natural language communication to reduce communication bandwidth and improve driving safety by explicitly accounting for perception uncertainty.",
        "tldr_zh": "该论文介绍了UNCAP，一种基于视觉-语言模型的协同自主规划方法，它使用自然语言通信来减少通信带宽，并通过显式考虑感知不确定性来提高驾驶安全性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scope: Selective Cross-modal Orchestration of Visual Perception Experts",
        "summary": "Vision-language models (VLMs) benefit from multiple vision encoders, but\nnaively stacking them yields diminishing returns while multiplying inference\ncosts. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that\ndynamically selects one specialized encoder per image-text pair via\ninstance-level routing, unlike token-level routing in traditional MoE. SCOPE\nmaintains a shared encoder and a pool of routed encoders. A lightweight router\nuses cross-attention between text prompts and shared visual features to select\nthe optimal encoder from the routed encoders. To train this router, we\nintroduce dual entropy regularization with auxiliary losses to balance\ndataset-level load distribution with instance-level routing confidence.\nRemarkably, SCOPE with one shared plus one routed encoder outperforms models\nusing all four extra encoders simultaneously, while reducing compute by\n24-49\\%. This demonstrates that intelligent encoder selection beats brute-force\naggregation, challenging the prevailing paradigm in multi-encoder VLMs.",
        "url": "http://arxiv.org/abs/2510.12974v1",
        "published_date": "2025-10-14T20:33:01+00:00",
        "updated_date": "2025-10-14T20:33:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyu Zhang",
            "Suyuchen Wang",
            "Chao Wang",
            "Juan Rodriguez",
            "Ahmed Masry",
            "Xiangru Jian",
            "Yoshua Bengio",
            "Perouz Taslakian"
        ],
        "tldr": "The paper introduces SCOPE, a Mixture-of-Encoders framework for VLMs that dynamically selects specialized encoders based on image-text pairs, achieving better performance with reduced computation compared to naively stacking multiple encoders.",
        "tldr_zh": "该论文介绍了SCOPE，一个VLM的混合编码器框架，它基于图像-文本对动态选择专门的编码器，与简单堆叠多个编码器相比，以更少的计算量实现了更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation",
        "summary": "Recent medical vision-language models have shown promise on tasks such as\nVQA, report generation, and anomaly detection. However, most are adapted to\nstructured adult imaging and underperform in fetal ultrasound, which poses\nchallenges of multi-view image reasoning, numerous diseases, and image\ndiversity. To bridge this gap, we introduce FetalMind, a medical AI system\ntailored to fetal ultrasound for both report generation and diagnosis. Guided\nby clinical workflow, we propose Salient Epistemic Disentanglement (SED), which\ninjects an expert-curated bipartite graph into the model to decouple\nview-disease associations and to steer preference selection along clinically\nfaithful steps via reinforcement learning. This design mitigates variability\nacross diseases and heterogeneity across views, reducing learning bottlenecks\nwhile aligning the model's inference with obstetric practice. To train\nFetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale\nfetal ultrasound report corpus, comprising 20K reports from twelve medical\ncenters, addressing the scarcity of domain data. Extensive experiments show\nthat FetalMind outperforms open- and closed-source baselines across all\ngestational stages, achieving +14% average gains and +61.2% higher accuracy on\ncritical conditions while remaining efficient, stable, and scalable. Project\nPage: https://hexiao0275.github.io/FetalMind.",
        "url": "http://arxiv.org/abs/2510.12953v1",
        "published_date": "2025-10-14T19:57:03+00:00",
        "updated_date": "2025-10-14T19:57:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR",
            "cs.MM"
        ],
        "authors": [
            "Xiao He",
            "Huangxuan Zhao",
            "Guojia Wan",
            "Wei Zhou",
            "Yanxing Liu",
            "Juhua Liu",
            "Yongchao Xu",
            "Yong Luo",
            "Dacheng Tao",
            "Bo Du"
        ],
        "tldr": "The paper introduces FetalMind, a vision-language model tailored for fetal ultrasound interpretation, utilizing a novel Salient Epistemic Disentanglement method and a large-scale dataset FetalSigma-1M, achieving superior performance in report generation and diagnosis.",
        "tldr_zh": "该论文介绍了FetalMind，一种专为胎儿超声图像解读而设计的视觉-语言模型。它采用了一种新颖的显著性认知解耦方法，并使用大型数据集FetalSigma-1M，在报告生成和诊断方面取得了优异的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unifying Vision-Language Latents for Zero-label Image Caption Enhancement",
        "summary": "Vision-language models (VLMs) achieve remarkable performance through\nlarge-scale image-text pretraining. However, their reliance on labeled image\ndatasets limits scalability and leaves vast amounts of unlabeled image data\nunderutilized. To address this, we propose Unified Vision-Language Alignment\nfor Zero-Label Enhancement (ViZer), an enhancement training framework that\nenables zero-label learning in image captioning, providing a practical starting\npoint for broader zero-label adaptation in vision-language tasks. Unlike prior\napproaches that rely on human or synthetically annotated datasets, ViZer\nactively aligns vision and language representation features during training,\nenabling existing VLMs to generate improved captions without requiring text\nlabels or full retraining. We demonstrate ViZer's advantage in qualitative\nevaluation, as automated caption metrics such as CIDEr and BERTScore often\npenalize details that are absent in reference captions. Applying ViZer on\nSmolVLM-Base and Qwen2-VL, we observe consistent qualitative improvements,\nproducing captions that are more grounded and descriptive than their baseline.",
        "url": "http://arxiv.org/abs/2510.12931v1",
        "published_date": "2025-10-14T19:12:59+00:00",
        "updated_date": "2025-10-14T19:12:59+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Sanghyun Byun",
            "Jung Ick Guack",
            "Mohanad Odema",
            "Baisub Lee",
            "Jacob Song",
            "Woo Seong Chung"
        ],
        "tldr": "The paper introduces ViZer, a novel enhancement training framework for VLMs that enables zero-label image captioning by actively aligning vision and language features, improving caption quality without labeled data or full retraining.",
        "tldr_zh": "该论文介绍了一种名为 ViZer 的新型 VLM 增强训练框架，它通过主动对齐视觉和语言特征来实现零标签图像描述生成，无需标记数据或完全重新训练即可提高描述质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "summary": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
        "url": "http://arxiv.org/abs/2510.13515v1",
        "published_date": "2025-10-15T13:07:00+00:00",
        "updated_date": "2025-10-15T13:07:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tiancheng Gu",
            "Kaicheng Yang",
            "Kaichen Zhang",
            "Xiang An",
            "Ziyong Feng",
            "Yueyi Zhang",
            "Weidong Cai",
            "Jiankang Deng",
            "Lidong Bing"
        ],
        "tldr": "The paper introduces UniME-V2, a universal multimodal embedding model that leverages MLLMs as judges to mine hard negatives and learn more discriminative embeddings, achieving state-of-the-art results on the MMEB benchmark and retrieval tasks.",
        "tldr_zh": "该论文介绍了UniME-V2，一种通用多模态嵌入模型，利用MLLM作为判断器来挖掘困难负样本并学习更具判别性的嵌入，在MMEB基准测试和检索任务中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator",
        "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
        "url": "http://arxiv.org/abs/2510.13454v1",
        "published_date": "2025-10-15T11:55:08+00:00",
        "updated_date": "2025-10-15T11:55:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyojun Go",
            "Dominik Narnhofer",
            "Goutam Bhat",
            "Prune Truong",
            "Federico Tombari",
            "Konrad Schindler"
        ],
        "tldr": "VIST3A stitches a text-to-video generator with a 3D reconstruction network to achieve improved text-to-3D generation, using model stitching and direct reward finetuning.",
        "tldr_zh": "VIST3A 通过将文本到视频生成器与 3D 重建网络拼接起来，实现了改进的文本到 3D 生成，使用了模型拼接和直接奖励微调。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity",
        "summary": "Recent Vision-Language Models (VLMs) enable zero-shot classification by\naligning images and text in a shared space, a promising approach for\ndata-scarce conditions. However, the influence of prompt design on recognizing\nvisually similar categories, such as human postures, is not well understood.\nThis study investigates how prompt specificity affects the zero-shot\nclassification of sitting, standing, and walking/running on a small, 285-image\nCOCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,\nand SigLip, were evaluated using a three-tiered prompt design that\nsystematically increases linguistic detail. Our findings reveal a compelling,\ncounter-intuitive trend: for the highest-performing models (MetaCLIP 2 and\nOpenCLIP), the simplest, most basic prompts consistently achieve the best\nresults. Adding descriptive detail significantly degrades performance for\ninstance, MetaCLIP 2's multi-class accuracy drops from 68.8\\% to 55.1\\% a\nphenomenon we term \"prompt overfitting\". Conversely, the lower-performing\nSigLip model shows improved classification on ambiguous classes when given more\ndescriptive, body-cue-based prompts.",
        "url": "http://arxiv.org/abs/2510.13364v1",
        "published_date": "2025-10-15T09:53:46+00:00",
        "updated_date": "2025-10-15T09:53:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "MingZe Tang",
            "Jubal Chandy Jacob"
        ],
        "tldr": "This paper investigates the impact of prompt specificity on zero-shot classification of human postures using VLMs, finding that simpler prompts often outperform more detailed ones for high-performing models.",
        "tldr_zh": "本文研究了提示语具体程度对使用 VLM 进行零样本人体姿势分类的影响，发现对于高性能模型，简单的提示语通常优于更详细的提示语。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests",
        "summary": "Our daily life is highly influenced by what we consume and see. Attracting\nand holding one's attention -- the definition of (visual) interestingness -- is\nessential. The rise of Large Multimodal Models (LMMs) trained on large-scale\nvisual and textual data has demonstrated impressive capabilities. We explore\nthese models' potential to understand to what extent the concepts of visual\ninterestingness are captured and examine the alignment between human\nassessments and GPT-4o's, a leading LMM, predictions through comparative\nanalysis. Our studies reveal partial alignment between humans and GPT-4o. It\nalready captures the concept as best compared to state-of-the-art methods.\nHence, this allows for the effective labeling of image pairs according to their\n(commonly) interestingness, which are used as training data to distill the\nknowledge into a learning-to-rank model. The insights pave the way for a deeper\nunderstanding of human interest.",
        "url": "http://arxiv.org/abs/2510.13316v1",
        "published_date": "2025-10-15T09:04:48+00:00",
        "updated_date": "2025-10-15T09:04:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fitim Abdullahu",
            "Helmut Grabner"
        ],
        "tldr": "This paper explores GPT-4o's ability to predict visual interestingness compared to human assessments, finding partial alignment and using the model to label data for training a learning-to-rank model.",
        "tldr_zh": "本文探讨了GPT-4o预测视觉趣味性的能力，并将其与人类的评估进行比较，发现部分一致性，并使用该模型标记数据以训练一个学习排序模型。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception",
        "summary": "As AR/VR technologies become integral to daily life, there's a growing need\nfor AI that understands human social dynamics from an egocentric perspective.\nHowever, current LLMs often lack the social awareness to discern when to\nintervene as AI assistant. This leads to constant, socially unaware responses\nthat may disrupt natural conversation and negatively impact user focus. To\naddress these limitations, we introduce EgoSocial, a large-scale egocentric\ndataset with 13,500 social video-question pairs, specifically designed to\nbenchmark intervention in social interaction perception. We also present an\nin-depth analysis of current omnimodal LLMs (OLLMs) to assess their\neffectiveness in detecting diverse social contextual cues. Experiments show\nthat OLLMs still struggle to detect the intervention timing (14.4% for Gemini\n2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method\nfor robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD\nintegrates multimodal contextual cues (e.g., audio and visual cues) into a\nsocial thinking graph, dynamically modeling participants and interactions. Our\nmethod proactively detects intervention timing and social interactions,\nprecisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and\nGemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4\nby 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.\nWe will release the dataset and code soon.",
        "url": "http://arxiv.org/abs/2510.13105v1",
        "published_date": "2025-10-15T02:52:19+00:00",
        "updated_date": "2025-10-15T02:52:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xijun Wang",
            "Tanay Sharma",
            "Achin Kulshrestha",
            "Abhimitra Meka",
            "Aveek Purohit",
            "Dinesh Manocha"
        ],
        "tldr": "The paper introduces EgoSocial, a new dataset and method (EgoSoD) for benchmarking and improving the proactive intervention ability of omnimodal LLMs in egocentric social interactions, showing significant performance gains.",
        "tldr_zh": "该论文介绍了EgoSocial，一个新的数据集和一个方法（EgoSoD），用于评估和提升全模态LLM在以自我为中心的社交互动中主动干预能力，并展示了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]