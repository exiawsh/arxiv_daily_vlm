[
    {
        "title": "3D Aware Region Prompted Vision Language Model",
        "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
        "url": "http://arxiv.org/abs/2509.13317v1",
        "published_date": "2025-09-16T17:59:06+00:00",
        "updated_date": "2025-09-16T17:59:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "An-Chieh Cheng",
            "Yang Fu",
            "Yukang Chen",
            "Zhijian Liu",
            "Xiaolong Li",
            "Subhashree Radhakrishnan",
            "Song Han",
            "Yao Lu",
            "Jan Kautz",
            "Pavlo Molchanov",
            "Hongxu Yin",
            "Xiaolong Wang",
            "Sifei Liu"
        ],
        "tldr": "The paper introduces SR-3D, a vision-language model unifying 2D and 3D data via region prompting, achieving SOTA performance in spatial reasoning and scene understanding, even in scenarios with limited 3D data.",
        "tldr_zh": "该论文介绍了SR-3D，一种通过区域提示统一2D和3D数据的视觉-语言模型，在空间推理和场景理解方面实现了SOTA性能，即使在3D数据有限的情况下也是如此。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement",
        "summary": "Charts are a crucial visual medium for communicating and representing\ninformation. While Large Vision-Language Models (LVLMs) have made progress on\nchart question answering (CQA), the task remains challenging, particularly when\nmodels attend to irrelevant regions of the chart. In this work, we present\nChartGaze, a new eye-tracking dataset that captures human gaze patterns during\nchart reasoning tasks. Through a systematic comparison of human and model\nattention, we find that LVLMs often diverge from human gaze, leading to reduced\ninterpretability and accuracy. To address this, we propose a gaze-guided\nattention refinement that aligns image-text attention with human fixations. Our\napproach improves both answer accuracy and attention alignment, yielding gains\nof up to 2.56 percentage points across multiple models. These results\ndemonstrate the promise of incorporating human gaze to enhance both the\nreasoning quality and interpretability of chart-focused LVLMs.",
        "url": "http://arxiv.org/abs/2509.13282v1",
        "published_date": "2025-09-16T17:35:39+00:00",
        "updated_date": "2025-09-16T17:35:39+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ali Salamatian",
            "Amirhossein Abaskohi",
            "Wan-Cyuan Fan",
            "Mir Rayat Imtiaz Hossain",
            "Leonid Sigal",
            "Giuseppe Carenini"
        ],
        "tldr": "This paper introduces ChartGaze, an eye-tracking dataset for chart reasoning and a gaze-guided attention refinement method to improve LVLM accuracy and interpretability in chart question answering, showing improved performance by aligning model attention with human gaze.",
        "tldr_zh": "本文介绍了ChartGaze，一个用于图表推理的眼动追踪数据集，以及一种基于注视引导的注意力精炼方法，通过对齐模型注意力和人类注视点来提高LVLM在图表问答中的准确性和可解释性，实验显示性能有所提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era",
        "summary": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable.",
        "url": "http://arxiv.org/abs/2509.13175v1",
        "published_date": "2025-09-16T15:27:14+00:00",
        "updated_date": "2025-09-16T15:27:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingtai Li",
            "Haoran Lai",
            "Xiaoqian Zhou",
            "Shuai Ming",
            "Wenxin Ma",
            "Wei Wei",
            "Shaohua Kevin Zhou"
        ],
        "tldr": "This paper uses LLMs to extract diagnostic labels from radiology reports to create large-scale silver-standard datasets for vision-language pre-training, achieving state-of-the-art performance with a simple 3D ResNet-18 model.",
        "tldr_zh": "本文利用大型语言模型从放射学报告中提取诊断标签，创建大规模的银标准数据集用于视觉-语言预训练，并使用简单的3D ResNet-18模型实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models",
        "summary": "By cropping high-resolution images into local tiles and encoding them\nindependently, High-Resolution Large Vision-Language Models (HR-LVLMs) have\ndemonstrated remarkable fine-grained visual understanding capabilities.\nHowever, this divide-and-conquer paradigm significantly increases the number of\nvisual tokens, resulting in substantial computational and memory overhead. To\nbetter understand and address this challenge, we empirically investigate visual\ntoken utilization in HR-LVLMs and uncover three key findings: (1) the local\ntiles have varying importance, jointly determined by visual saliency and task\nrelevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage\nattention pattern across layers, with each stage attending to different types\nof visual tokens; (3) the visual tokens emphasized at different stages encode\ninformation at varying levels of granularity, playing complementary roles\nwithin LVLMs. Building on these insights, we propose HERO, a High-resolution\nvisual token early dropping framework that integrates content-adaptive token\nbudget allocation with function-aware token selection. By accurately estimating\ntile-level importance and selectively retaining visual tokens with\ncomplementary roles, HERO achieves superior efficiency-accuracy trade-offs\nacross diverse benchmarks and model scales, all in a training-free manner. This\nstudy provides both empirical insights and practical solutions toward efficient\ninference in HR-LVLMs.",
        "url": "http://arxiv.org/abs/2509.13067v1",
        "published_date": "2025-09-16T13:22:08+00:00",
        "updated_date": "2025-09-16T13:22:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Li",
            "Yuxuan Liang",
            "Xiaolei Chen",
            "Yi Zheng",
            "Haotian Chen",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "tldr": "The paper introduces HERO, a training-free framework for efficient inference in high-resolution vision-language models by adaptively dropping less important visual tokens based on content and function, leading to improved efficiency-accuracy trade-offs.",
        "tldr_zh": "该论文介绍了HERO，一个无需训练的框架，通过基于内容和功能自适应地丢弃不重要的视觉tokens，从而实现高分辨率视觉-语言模型的高效推理，并提升效率-准确性权衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models",
        "summary": "Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks.",
        "url": "http://arxiv.org/abs/2509.13031v1",
        "published_date": "2025-09-16T12:51:11+00:00",
        "updated_date": "2025-09-16T12:51:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yan Chen",
            "Long Li",
            "Teng Xi",
            "Long Zeng",
            "Jingdong Wang"
        ],
        "tldr": "This paper proposes a two-stage reinforcement learning framework (PeBR-R1) to improve both the visual perception and reasoning capabilities of Vision-Language Models, demonstrating enhanced performance on visual reasoning tasks across seven benchmark datasets.",
        "tldr_zh": "本文提出了一种两阶段强化学习框架 (PeBR-R1)，以提高视觉语言模型的视觉感知和推理能力，并在七个基准数据集上展示了增强的视觉推理任务性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluating Robustness of Vision-Language Models Under Noisy Conditions",
        "summary": "Vision-Language Models (VLMs) have attained exceptional success across\nmultimodal tasks such as image captioning and visual question answering.\nHowever, their robustness under noisy conditions remains unfamiliar. In this\nstudy, we present a comprehensive evaluation framework to evaluate the\nperformance of several state-of-the-art VLMs under controlled perturbations,\nincluding lighting variation, motion blur, and compression artifacts. We used\nboth lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based\nsimilarity measures using sentence embeddings to quantify semantic alignment.\nOur experiments span diverse datasets, revealing key insights: (1)\ndescriptiveness of ground-truth captions significantly influences model\nperformance; (2) larger models like LLaVA excel in semantic understanding but\ndo not universally outperform smaller models; and (3) certain noise types, such\nas JPEG compression and motion blur, dramatically degrade performance across\nmodels. Our findings highlight the nuanced trade-offs between model size,\ndataset characteristics, and noise resilience, offering a standardized\nbenchmark for future robust multimodal learning.",
        "url": "http://arxiv.org/abs/2509.12492v1",
        "published_date": "2025-09-15T22:31:21+00:00",
        "updated_date": "2025-09-15T22:31:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Purushoth",
            "Alireza"
        ],
        "tldr": "This paper evaluates the robustness of various Vision-Language Models (VLMs) under different noise conditions, finding that model performance is influenced by caption descriptiveness, model size isn't always indicative of better performance, and certain noise types significantly degrade performance.",
        "tldr_zh": "该论文评估了各种视觉-语言模型（VLM）在不同噪声条件下的鲁棒性，发现模型性能受标题描述性影响，模型大小并不总是代表更好的性能，并且某些噪声类型会显著降低性能。",
        "relevance_score": 9,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation",
        "summary": "Referring Image Segmentation (RIS) is a task that segments image regions\nbased on language expressions, requiring fine-grained alignment between two\nmodalities. However, existing methods often struggle with multimodal\nmisalignment and language semantic loss, especially in complex scenes\ncontaining multiple visually similar objects, where uniquely described targets\nare frequently mislocalized or incompletely segmented. To tackle these\nchallenges, this paper proposes TFANet, a Three-stage Image-Text Feature\nAlignment Network that systematically enhances multimodal alignment through a\nhierarchical framework comprising three stages: Knowledge Plus Stage (KPS),\nKnowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the\nfirst stage, we design the Multiscale Linear Cross-Attention Module (MLAM),\nwhich facilitates bidirectional semantic exchange between visual features and\ntextual representations across multiple scales. This establishes rich and\nefficient alignment between image regions and different granularities of\nlinguistic descriptions. Subsequently, the KFS further strengthens feature\nalignment through the Cross-modal Feature Scanning Module (CFSM), which applies\nmultimodal selective scanning to capture long-range dependencies and construct\na unified multimodal representation. This is essential for modeling long-range\ncross-modal dependencies and enhancing alignment accuracy in complex scenes.\nFinally, in the KIS, we propose the Word-level Linguistic Feature-guided\nSemantic Deepening Module (WFDM) to compensate for semantic degradation\nintroduced in earlier stages.",
        "url": "http://arxiv.org/abs/2509.13070v1",
        "published_date": "2025-09-16T13:26:58+00:00",
        "updated_date": "2025-09-16T13:26:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qianqi Lu",
            "Yuxiang Xie",
            "Jing Zhang",
            "Shiwei Zou",
            "Yan Chen",
            "Xidao Luan"
        ],
        "tldr": "The paper introduces TFANet, a three-stage network for referring image segmentation that focuses on enhancing multimodal alignment through knowledge plus, fusion, and intensification stages, aiming to improve performance in complex scenes with similar objects.",
        "tldr_zh": "本文提出TFANet，一个用于指代表达图像分割的三阶段网络，通过知识补充、融合和强化阶段，增强多模态对齐，旨在提高在具有相似物体的复杂场景中的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization",
        "summary": "Surgical triplet recognition, which involves identifying instrument, verb,\ntarget, and their combinations, is a complex surgical scene understanding\nchallenge plagued by long-tailed data distribution. The mainstream multi-task\nlearning paradigm benefiting from cross-task collaborative promotion has shown\npromising performance in identifying triples, but two key challenges remain: 1)\ninter-task optimization conflicts caused by entangling task-generic and\ntask-specific representations; 2) intra-task optimization conflicts due to\nclass-imbalanced training data. To overcome these difficulties, we propose the\nMLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and\nintra-task optimization for surgical triplet recognition. For inter-task\noptimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning\nscheme that decomposes representations into task-shared and task-specific\ncomponents. To enhance task-shared representations, we construct a Multimodal\nLarge Language Model (MLLM) powered probabilistic prompt pool to dynamically\naugment visual features with expert-level semantic cues. Additionally,\ncomprehensive task-specific cues are modeled via distinct task prompts covering\nthe temporal-spatial dimensions, effectively mitigating inter-task ambiguities.\nTo tackle intra-task optimization conflicts, we develop a Coordinated Gradient\nLearning (CGL) strategy, which dissects and rebalances the positive-negative\ngradients originating from head and tail classes for more coordinated learning\nbehaviors. Extensive experiments on the CholecT45 and CholecT50 datasets\ndemonstrate the superiority of our proposed framework, validating its\neffectiveness in handling optimization conflicts.",
        "url": "http://arxiv.org/abs/2509.12893v1",
        "published_date": "2025-09-16T09:48:52+00:00",
        "updated_date": "2025-09-16T09:48:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyi Zhang",
            "Yuchen Yuan",
            "Ying Zheng",
            "Jialun Pei",
            "Jinpeng Li",
            "Zheng Li",
            "Pheng-Ann Heng"
        ],
        "tldr": "The paper introduces MEJO, a framework that uses a Multimodal Large Language Model (MLLM) and gradient learning strategies to improve surgical triplet recognition by addressing inter- and intra-task optimization conflicts caused by long-tailed data distributions.",
        "tldr_zh": "该论文介绍了MEJO，一个利用多模态大型语言模型（MLLM）和梯度学习策略的框架，通过解决长尾数据分布引起的任务间和任务内优化冲突，来提高手术三重识别的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "RadGame: An AI-Powered Platform for Radiology Education",
        "summary": "We introduce RadGame, an AI-powered gamified platform for radiology education\nthat targets two core skills: localizing findings and generating reports.\nTraditional radiology training is based on passive exposure to cases or active\npractice with real-time input from supervising radiologists, limiting\nopportunities for immediate and scalable feedback. RadGame addresses this gap\nby combining gamification with large-scale public datasets and automated,\nAI-driven feedback that provides clear, structured guidance to human learners.\nIn RadGame Localize, players draw bounding boxes around abnormalities, which\nare automatically compared to radiologist-drawn annotations from public\ndatasets, and visual explanations are generated by vision-language models for\nuser missed findings. In RadGame Report, players compose findings given a chest\nX-ray, patient age and indication, and receive structured AI feedback based on\nradiology report generation metrics, highlighting errors and omissions compared\nto a radiologist's written ground truth report from public datasets, producing\na final performance and style score. In a prospective evaluation, participants\nusing RadGame achieved a 68% improvement in localization accuracy compared to\n17% with traditional passive methods and a 31% improvement in report-writing\naccuracy compared to 4% with traditional methods after seeing the same cases.\nRadGame highlights the potential of AI-driven gamification to deliver scalable,\nfeedback-rich radiology training and reimagines the application of medical AI\nresources in education.",
        "url": "http://arxiv.org/abs/2509.13270v1",
        "published_date": "2025-09-16T17:27:33+00:00",
        "updated_date": "2025-09-16T17:27:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohammed Baharoon",
            "Siavash Raissi",
            "John S. Jun",
            "Thibault Heintz",
            "Mahmoud Alabbad",
            "Ali Alburkani",
            "Sung Eun Kim",
            "Kent Kleinschmidt",
            "Abdulrahman O. Alhumaydhi",
            "Mohannad Mohammed G. Alghamdi",
            "Jeremy Francis Palacio",
            "Mohammed Bukhaytan",
            "Noah Michael Prudlo",
            "Rithvik Akula",
            "Brady Chrisler",
            "Benjamin Galligos",
            "Mohammed O. Almutairi",
            "Mazeen Mohammed Alanazi",
            "Nasser M. Alrashdi",
            "Joel Jihwan Hwang",
            "Sri Sai Dinesh Jaliparthi",
            "Luke David Nelson",
            "Nathaniel Nguyen",
            "Sathvik Suryadevara",
            "Steven Kim",
            "Mohammed F. Mohammed",
            "Yevgeniy R. Semenov",
            "Kun-Hsing Yu",
            "Abdulrhman Aljouie",
            "Hassan AlOmaish",
            "Adam Rodman",
            "Pranav Rajpurkar"
        ],
        "tldr": "RadGame is an AI-powered, gamified platform for radiology education, providing automated feedback on localization and report generation, demonstrating significant improvements over traditional training methods.",
        "tldr_zh": "RadGame是一个AI驱动的放射学教育平台，通过游戏化提供关于定位和报告生成的自动反馈，实验表明相对于传统训练方法，其效果有显著提升。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Image Realness Assessment and Localization with Multimodal Features",
        "summary": "A reliable method of quantifying the perceptual realness of AI-generated\nimages and identifying visually inconsistent regions is crucial for practical\nuse of AI-generated images and for improving photorealism of generative AI via\nrealness feedback during training. This paper introduces a framework that\naccomplishes both overall objective realness assessment and local inconsistency\nidentification of AI-generated images using textual descriptions of visual\ninconsistencies generated by vision-language models trained on large datasets\nthat serve as reliable substitutes for human annotations. Our results\ndemonstrate that the proposed multimodal approach improves objective realness\nprediction performance and produces dense realness maps that effectively\ndistinguish between realistic and unrealistic spatial regions.",
        "url": "http://arxiv.org/abs/2509.13289v1",
        "published_date": "2025-09-16T17:42:51+00:00",
        "updated_date": "2025-09-16T17:42:51+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Lovish Kaushik",
            "Agnij Biswas",
            "Somdyuti Paul"
        ]
    }
]