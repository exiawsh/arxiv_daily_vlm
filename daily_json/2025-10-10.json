[
    {
        "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models",
        "summary": "Spatial reasoning remains a fundamental challenge for Vision-Language Models\n(VLMs), with current approaches struggling to achieve robust performance\ndespite recent advances. We identify that this limitation stems from a critical\ngap: existing methods attempt to learn spatial reasoning directly without\nestablishing the hierarchical foundations of perception and understanding. To\naddress this challenge, we present a comprehensive methodology for building\nspatial intelligence progressively. We introduce SpatialLadder-26k, a\nmultimodal dataset containing 26,610 samples spanning object localization,\nsingle image, multi-view, and video spatial reasoning tasks, constructed\nthrough a standardized pipeline that ensures systematic coverage across\nmodalities. Building on this dataset, we design a three-stage progressive\ntraining framework that (1) establishes spatial perception through object\nlocalization, (2) develops spatial understanding through multi-dimensional\nspatial tasks, and (3) strengthens complex reasoning via reinforcement learning\nwith verifiable rewards. This approach yields SpatialLadder, a 3B-parameter\nmodel that achieves state-of-the-art performance on spatial reasoning\nbenchmarks, with 23.4% average improvement over the base model, surpassing\nGPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains\nstrong generalization with 7.2% improvement on out-of-domain benchmarks,\ndemonstrating that progressive training from perception to reasoning is\nessential for robust spatial intelligence.",
        "url": "http://arxiv.org/abs/2510.08531v1",
        "published_date": "2025-10-09T17:50:54+00:00",
        "updated_date": "2025-10-09T17:50:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Hongxing Li",
            "Dingming Li",
            "Zixuan Wang",
            "Yuchen Yan",
            "Hang Wu",
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "tldr": "The paper introduces SpatialLadder, a progressively trained VLM with a new dataset, achieving SOTA results in spatial reasoning by focusing on hierarchical learning from perception to reasoning.",
        "tldr_zh": "该论文介绍了SpatialLadder，一种通过渐进式训练的VLM，并提出了一个新的数据集，通过关注从感知到推理的分层学习，在空间推理方面取得了SOTA成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models",
        "summary": "Frontier AI models have achieved remarkable progress, yet recent studies\nsuggest they struggle with compositional reasoning, often performing at or\nbelow random chance on established benchmarks. We revisit this problem and show\nthat widely used evaluation metrics systematically underestimate model\ncapability. To address this, we introduce a group matching score that better\nexploits group structure and reveals substantial hidden capability in both\ncontrastive vision-language models (VLMs) and multimodal large language models\n(MLLMs). Moreover, simply overfitting to the induced group matchings at test\ntime transfers this hidden capability into higher scores under standard\nevaluation metrics, closing much of the reported gap. This adjustment enables\nSigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first\nresult surpassing estimated human performance on Winoground.\n  Building on this insight, we propose Test-Time Matching (TTM), an iterative,\nself-improving algorithm that further bootstraps model performance without any\nexternal supervision. TTM delivers additional, non-trivial improvements: for\nexample, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a\nnew state of the art. Importantly, TTM remains broadly effective even on\nbenchmarks without metric-induced effects or group structures, achieving\nrelative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16\ndataset variants spanning diverse setups, our experiments demonstrate that TTM\nconsistently improves model performance and advances the frontier of\ncompositional reasoning.",
        "url": "http://arxiv.org/abs/2510.07632v1",
        "published_date": "2025-10-09T00:00:49+00:00",
        "updated_date": "2025-10-09T00:00:49+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yinglun Zhu",
            "Jiancheng Zhang",
            "Fuzhi Tang"
        ],
        "tldr": "The paper introduces Test-Time Matching (TTM), a novel algorithm that significantly improves compositional reasoning in VLMs and MLLMs by addressing limitations in evaluation metrics and bootstrapping model performance at test time.",
        "tldr_zh": "该论文介绍了测试时匹配（TTM），一种新颖的算法，通过解决评估指标的局限性并在测试时引导模型性能，显着提高了 VLM 和 MLLM 中的组合推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning",
        "summary": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX.",
        "url": "http://arxiv.org/abs/2510.08567v1",
        "published_date": "2025-10-09T17:59:54+00:00",
        "updated_date": "2025-10-09T17:59:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Tajamul Ashraf",
            "Umair Nawaz",
            "Abdelrahman M. Shaker",
            "Rao Anwer",
            "Philip Torr",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "tldr": "The paper introduces MATRIX, a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories and generates preference pairs to train VLMs for robust tool-use reasoning, achieving state-of-the-art results on multiple benchmarks.",
        "tldr_zh": "该论文介绍了一个以视觉为中心的代理调优框架 MATRIX，该框架可以自动合成多模态轨迹并生成偏好对，以训练 VLM 实现强大的工具使用推理，并在多个基准测试中取得领先成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "How to Teach Large Multimodal Models New Skills",
        "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL",
        "url": "http://arxiv.org/abs/2510.08564v1",
        "published_date": "2025-10-09T17:59:37+00:00",
        "updated_date": "2025-10-09T17:59:37+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhen Zhu",
            "Yiming Gong",
            "Yao Xiao",
            "Yaoyao Liu",
            "Derek Hoiem"
        ],
        "tldr": "This paper investigates how to sequentially fine-tune Large Multimodal Models (LMMs) on new skills while mitigating catastrophic forgetting, proposing two tuning recipes that selectively update model layers to balance target gains and held-out performance preservation.",
        "tldr_zh": "本文研究了如何对大型多模态模型 (LMM) 进行新技能的顺序微调，同时减轻灾难性遗忘，并提出了两种选择性更新模型层的微调方法，以平衡目标增益和保留性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints",
        "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.",
        "url": "http://arxiv.org/abs/2510.08565v1",
        "published_date": "2025-10-09T17:59:37+00:00",
        "updated_date": "2025-10-09T17:59:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changyao Tian",
            "Hao Li",
            "Gen Luo",
            "Xizhou Zhu",
            "Weijie Su",
            "Hanming Deng",
            "Jinguo Zhu",
            "Jie Shao",
            "Ziran Zhu",
            "Yunpeng Liu",
            "Lewei Lu",
            "Wenhai Wang",
            "Hongsheng Li",
            "Jifeng Dai"
        ],
        "tldr": "The paper explores native (end-to-end) training of MLLMs under data constraints, proposes NaViL with a cost-effective recipe, and demonstrates competitive performance on multimodal benchmarks, providing insights for future research on native MLLMs.",
        "tldr_zh": "该论文探讨了在数据约束下 MLLM 的原生（端到端）训练，提出了具有成本效益的 NaViL，并在多模态基准测试中展示了具有竞争力的性能，为未来原生 MLLM 的研究提供了见解。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization",
        "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in reasoning tasks such as mathematics and logic, their capacity\nfor long-chain reflective reasoning, a prerequisite for solving complex\nreal-world problems, remains largely underexplored. In this work, we first\nconduct an extensive empirical investigation to evaluate this capability.\nLeveraging a carefully designed data synthesis engine, we construct MM-HELIX, a\nmultimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks\nthat require iterative thinking and backtracking. Empirical results on this\nbenchmark reveal that existing MLLMs exhibit significant performance deficits\nin long-chain reflective reasoning. To address this limitation, we generate\npost-training data and further explore learning paradigms for exploiting such\ndata. We first develop the Step-Elicited Response Generation pipeline to create\nMM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning\ntraces for instruction-tuning stage. Given that standard Reinforcement Learning\nfails on complex tasks due to sparse reward signals and catastrophic forgetting\nafter Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization\n(AHPO), a novel training strategy that dynamically unifies offline supervision\nand online optimization into a single stage. This strategy enables the model to\nlearn from expert data when rewards are sparse and conduct independent\nexploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our\nmethod achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and\ndemonstrates strong generalization with a +5.7\\% average performance gain on\ngeneral mathematic and logic tasks. Our work demonstrate that reflective\nreasoning in MLLMs can be effectively learned and generalized, paving the way\nfor developing more capable MLLMs.",
        "url": "http://arxiv.org/abs/2510.08540v1",
        "published_date": "2025-10-09T17:53:58+00:00",
        "updated_date": "2025-10-09T17:53:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Zhao",
            "Junming Lin",
            "Tianhao Liang",
            "Yifan Zhou",
            "Wenhao Chai",
            "Yuzhe Gu",
            "Weiyun Wang",
            "Kai Chen",
            "Gen Luo",
            "Wenwei Zhang",
            "Junchi Yan",
            "Hua Yang",
            "Haodong Duan",
            "Xue Yang"
        ],
        "tldr": "The paper introduces MM-HELIX, a multimodal benchmark for long-chain reflective reasoning, and proposes Adaptive Hybrid Policy Optimization (AHPO) to improve MLLMs' performance on this benchmark, demonstrating significant accuracy gains and generalization capabilities.",
        "tldr_zh": "该论文介绍了MM-HELIX，一个用于长链反思推理的多模态基准，并提出了自适应混合策略优化（AHPO）来提高MLLM在此基准上的性能，展示了显著的准确性提升和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models",
        "summary": "Large Vision Language Models (LVLMs) have recently emerged as powerful\narchitectures capable of understanding and reasoning over both visual and\ntextual information. These models typically rely on two key components: a\nVision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual\ncontent into a sequence of image tokens and serves as the perceptual front-end\n-- the eyes of the model. In contrast, the LLM interprets these tokens to\nperform high-level reasoning, generates responses, and functions as the\ncognitive core -- the brain of the model. However, it remains unclear which\nvisual tokens contribute most significantly to understanding and reasoning, and\nhow effectively these signals are propagated from ViT to the LLM. While most\nexisting works have focused on identifying attention sinks, low-semantic tokens\nreceiving disproportionately high attention, within the LLM, we shift the focus\nto the vision encoder by identifying a class of high-norm visual tokens from\nViT, referred to as ViT attention sinks -- a problem that has been rarely\nstudied but is indeed very important for LVLMs. Our findings show that these\nViT sinks encapsulate high-level semantic concepts from images, allowing the\nLLM to perform more effective understanding and reasoning. Despite their\nimportance, these sink tokens are often overlooked in existing LVLM\narchitectures. To explore their contribution, we present both qualitative and\nquantitative analyses of the information embedded in these sink tokens. We also\npropose both training-free and training-based approaches to better leverage how\nthis information is interpreted by the LLM, and to what extent. By explicitly\nutilizing these tokens, we demonstrate substantial improvements across a range\nof LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT\nattention sinks in enhancing visual reasoning.",
        "url": "http://arxiv.org/abs/2510.08510v1",
        "published_date": "2025-10-09T17:44:42+00:00",
        "updated_date": "2025-10-09T17:44:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Jiayun Luo",
            "Wan-Cyuan Fan",
            "Lyuyang Wang",
            "Xiangteng He",
            "Tanzila Rahman",
            "Purang Abolmaesumi",
            "Leonid Sigal"
        ],
        "tldr": "This paper identifies and analyzes 'ViT attention sinks,' high-norm visual tokens in Vision Transformers that encapsulate semantic information crucial for Large Vision Language Models. It proposes methods to better leverage these tokens, demonstrating improved visual reasoning performance.",
        "tldr_zh": "本文识别并分析了“ViT注意力汇”，即视觉Transformer中高范数的视觉tokens，这些tokens封装了对大型视觉语言模型至关重要的语义信息。 它提出了更好地利用这些tokens的方法，并展示了视觉推理性能的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration",
        "summary": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems.",
        "url": "http://arxiv.org/abs/2510.08508v1",
        "published_date": "2025-10-09T17:42:51+00:00",
        "updated_date": "2025-10-09T17:42:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lu Liu",
            "Chunlei Cai",
            "Shaocheng Shen",
            "Jianfeng Liang",
            "Weimin Ouyang",
            "Tianxiao Ye",
            "Jian Mao",
            "Huiyu Duan",
            "Jiangchao Yao",
            "Xiaoyun Zhang",
            "Qiang Hu",
            "Guangtao Zhai"
        ],
        "tldr": "The paper introduces MoA-VR, a Mixture-of-Agents system for all-in-one video restoration, utilizing a vision-language model driven degradation identifier, an LLM-powered router, and a VLM-based video quality assessment model to handle diverse degradations effectively.",
        "tldr_zh": "该论文介绍了MoA-VR，一个用于多合一视频修复的混合代理系统，它利用视觉语言模型驱动的退化识别器、LLM驱动的路由选择器和基于VLM的视频质量评估模型，有效地处理各种退化。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
        "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2510.08485v1",
        "published_date": "2025-10-09T17:26:09+00:00",
        "updated_date": "2025-10-09T17:26:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chong Mou",
            "Qichao Sun",
            "Yanze Wu",
            "Pengze Zhang",
            "Xinghui Li",
            "Fulong Ye",
            "Songtao Zhao",
            "Qian He"
        ],
        "tldr": "The paper introduces InstructX, a unified framework for image and video editing using MLLMs and diffusion models, demonstrating that image data training can enable video editing capabilities and achieving state-of-the-art performance across diverse tasks.",
        "tldr_zh": "该论文介绍了InstructX，一个使用MLLM和扩散模型进行图像和视频编辑的统一框架。研究表明，图像数据训练可以实现视频编辑能力，并在各种任务中达到最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping",
        "summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive\nin signed languages, offering a natural testbed for visual grounding. For\nvision-language models (VLMs), the challenge is to recover such essential\nmappings from dynamic human motion rather than static context. We introduce the\n\\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts\npsycholinguistic measures to evaluate VLMs on three tasks: (i) phonological\nsign-form prediction (e.g., handshape, location), (ii) transparency (inferring\nmeaning from visual form), and (iii) graded iconicity ratings. We assess $13$\nstate-of-the-art VLMs in zero- and few-shot settings on Sign Language of the\nNetherlands and compare them to human baselines. On \\textit{phonological form\nprediction}, VLMs recover some handshape and location detail but remain below\nhuman performance; on \\textit{transparency}, they are far from human baselines;\nand only top models correlate moderately with human \\textit{iconicity ratings}.\nInterestingly, \\textit{models with stronger phonological form prediction\ncorrelate better with human iconicity judgment}, indicating shared sensitivity\nto visually grounded structure. Our findings validate these diagnostic tasks\nand motivate human-centric signals and embodied learning methods for modelling\niconicity and improving visual grounding in multimodal models.",
        "url": "http://arxiv.org/abs/2510.08482v1",
        "published_date": "2025-10-09T17:21:59+00:00",
        "updated_date": "2025-10-09T17:21:59+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Onur Keleş",
            "Aslı Özyürek",
            "Gerardo Ortega",
            "Kadir Gökgö",
            "Esam Ghaleb"
        ],
        "tldr": "The paper introduces the Visual Iconicity Challenge, a new benchmark for evaluating vision-language models on their ability to map sign language form to meaning, and finds that current VLMs struggle with this task, especially in transparency and iconicity understanding.",
        "tldr_zh": "本文介绍了视觉标志性挑战，这是一个新的基准，用于评估视觉语言模型将手语形式映射到意义的能力。研究发现，当前的视觉语言模型在完成这项任务时存在困难，尤其是在透明度和标志性理解方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools",
        "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\npotential in bridging visual and textual reasoning, yet their reliance on\ntext-centric priors often limits their ability to disentangle semantically\nsimilar actions in open-vocabulary scenarios. To address this, we propose\nVideo-STAR, a framework that harmonizes contextual sub-motion decomposition\nwith tool-augmented reinforcement learning for open-vocabulary action\nrecognition (OVAR). Unlike prior methods that treat actions as monolithic\nentities, our approach innovatively decomposes actions into discriminative\nsub-motions for fine-grained matching while dynamically invoking\ndomain-specific tools for cross-modal interleaving, thereby enabling\ncategory-specific reasoning capacity and reducing cross-modal hallucination.\nMoreover, by designing a hierarchical reward that balances tool-usage\nefficiency, sub-motion relevance, and structural coherence in reasoning, our\nmethod autonomously leverages external tools to prioritize sub-motion patterns\nwithout explicit supervision, transmitting from text-centric reasoning to\nvisually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,\nKinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art\nperformance, outperforming existing methods in distinguishing fine-grained\nactions and handling cross-modal hallucination, validating our excellent\nrobustness and generalization.",
        "url": "http://arxiv.org/abs/2510.08480v1",
        "published_date": "2025-10-09T17:20:44+00:00",
        "updated_date": "2025-10-09T17:20:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenlong Yuan",
            "Xiangyan Qu",
            "Chengxuan Qian",
            "Rui Chen",
            "Jing Tang",
            "Lei Sun",
            "Xiangxiang Chu",
            "Dapeng Zhang",
            "Yiwei Wang",
            "Yujun Cai",
            "Shuo Li"
        ],
        "tldr": "The paper introduces Video-STAR, a tool-augmented reinforcement learning framework for open-vocabulary action recognition that decomposes actions into sub-motions and uses external tools for improved performance and reduced hallucination, achieving state-of-the-art results on several benchmark datasets.",
        "tldr_zh": "该论文介绍了Video-STAR，一个用于开放词汇动作识别的工具增强强化学习框架，它将动作分解为子动作，并使用外部工具来提高性能和减少幻觉，并在多个基准数据集上实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
        "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.",
        "url": "http://arxiv.org/abs/2510.08377v1",
        "published_date": "2025-10-09T16:01:30+00:00",
        "updated_date": "2025-10-09T16:01:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cong Wei",
            "Quande Liu",
            "Zixuan Ye",
            "Qiulin Wang",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Wenhu Chen"
        ],
        "tldr": "UniVideo is a unified framework for video understanding, generation, and editing, leveraging a dual-stream architecture of MLLM and MMDiT, demonstrating strong performance and generalization capabilities across various video tasks.",
        "tldr_zh": "UniVideo是一个统一的视频理解、生成和编辑框架，利用MLLM和MMDiT的双流架构，在各种视频任务中展示了强大的性能和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing",
        "summary": "Image editing with natural language has gained significant popularity, yet\nexisting methods struggle with intricate object intersections and fine-grained\nspatial relationships due to the lack of an explicit reasoning process. While\nChain-of-Thought (CoT) has been explored to enhance reasoning, purely textual\nCoT or CoT augmented with coordinate information is fundamentally limited in\nits ability to represent intricate visual layouts and lacks the necessary\nvisual cues to guide the generation of fine-grained, pixel-level details. To\naddress these challenges, we propose Multimodal Reasoning Edit (MURE), a novel\nframework that shifts the visual editing process from purely text-based\nreasoning to a series of interleaved textual and visual rationales. Our\nframework performs image editing using a natively multimodal, interleaved\ntext-image CoT. This approach generates a step-by-step chain of reasoning where\na textual description is followed by a corresponding visual cue, such as a\npositional mask that defined intended edited regions or a representation of new\ncontent. Furthermore, to mitigate the hallucination phenomenon of large\nlanguage models, we introduce Multimodal Deep Confidence (MMDC) reasoning\nparadigm. This paradigm explores a tree of visual reasoning paths at each step.\nBy pruning low-quality branches using a deep confidence score from a reward\nmodel, it ensures the model consistently follows a high-quality trajectory\ntowards the final edited result. The proposed method decomposes complex editing\ntasks into interdependent sub-tasks, achieving greater precision at each stage\nand yielding high-fidelity edited results. We define the formulation for\ninterleaved text-image chains and release the first CoT-Edit-14K dataset,\ncomprising 14K high-quality editing examples. Extensive experiments show that\nour method yields significant improvements across three image editing\nbenchmarks.",
        "url": "http://arxiv.org/abs/2510.08157v1",
        "published_date": "2025-10-09T12:36:51+00:00",
        "updated_date": "2025-10-09T12:36:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhentao Zou",
            "Zhengrong Yue",
            "Kunpeng Du",
            "Binlei Bao",
            "Hanting Li",
            "Haizhen Xie",
            "Guozheng Xu",
            "Yue Zhou",
            "Yali Wang",
            "Jie Hu",
            "Xue Jiang",
            "Xinghao Chen"
        ],
        "tldr": "The paper introduces MURE, a novel framework for image editing using interleaved text-image Chain-of-Thought, combined with Multimodal Deep Confidence reasoning, to improve accuracy and fidelity, and releases a new dataset.",
        "tldr_zh": "该论文提出了MURE，一种新颖的图像编辑框架，它使用交错的文本-图像链式思考，结合多模态深度置信度推理，以提高准确性和保真度，并发布了一个新的数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement",
        "summary": "Large language models (LLMs) often generate self-contradictory outputs, which\nseverely impacts their reliability and hinders their adoption in practical\napplications. In video-language models (Video-LLMs), this phenomenon recently\ndraws the attention of researchers. Specifically, these models fail to provide\nlogically consistent responses to rephrased questions based on their grounding\noutputs. However, the underlying causes of this phenomenon remain\nunderexplored. In this work, we adopt an interpretability-driven approach to\nanalyze, statistically summarize, and intervention the potential factors of the\nphenomenon. We find that one of the primary reasons for the inconsistency in\nresponses lies in the inability of cross-modal attention heads to effectively\ndistinguish video tokens across different timestamps. To address this, we\npropose an attention enhancement method called Temporally Conditioned Attention\nSharpening (TCAS), which constructs an enhancement objective based on attention\ndistinctions to enhance the model's temporal resolution capability, thereby\nimproving its temporal understanding logic consistency. Experimental results\ndemonstrate that our method significantly enhances the temporal logic\nconsistency of Video-LLMs. Further interpretability analyses reveal that our\nmethod indeed improves the temporal discriminability of attention heads,\nvalidating our conclusions. Additionally, our method achieves performance\nimprovements in general video temporal grounding tasks, highlighting that\ntemporal logic consistency is a bottleneck in temporal understanding. By\nenhancing consistency, our method drives significant progress in video temporal\nunderstanding.",
        "url": "http://arxiv.org/abs/2510.08138v1",
        "published_date": "2025-10-09T12:22:06+00:00",
        "updated_date": "2025-10-09T12:22:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Chengzhi Li",
            "Heyan Huang",
            "Ping Jian",
            "Zhen Yang",
            "Yaning Tian"
        ],
        "tldr": "The paper addresses the problem of temporal logic inconsistency in Video-LLMs by proposing a Temporally Conditioned Attention Sharpening (TCAS) method that enhances the model's ability to distinguish video tokens across timestamps, leading to improved performance in temporal understanding tasks.",
        "tldr_zh": "该论文通过提出时间条件注意力锐化（TCAS）方法来解决视频-语言模型中时间逻辑不一致的问题。该方法增强了模型区分跨时间戳的视频标记的能力，从而提高了在时间理解任务中的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning",
        "summary": "Composed Image Retrieval (CIR), which aims to find a target image from a\nreference image and a modification text, presents the core challenge of\nperforming unified reasoning across visual and semantic modalities. While\ncurrent approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more\nrecent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown\nprogress, they predominantly function as ``black boxes.\" This inherent opacity\nnot only prevents users from understanding the retrieval rationale but also\nrestricts the models' ability to follow complex, fine-grained instructions. To\novercome these limitations, we introduce CIR-CoT, the first end-to-end\nretrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)\nreasoning. By compelling the model to first generate an interpretable reasoning\nchain, CIR-CoT enhances its ability to capture crucial cross-modal\ninteractions, leading to more accurate retrieval while making its decision\nprocess transparent. Since existing datasets like FashionIQ and CIRR lack the\nnecessary reasoning data, a key contribution of our work is the creation of\nstructured CoT annotations using a three-stage process involving a caption,\nreasoning, and conclusion. Our model is then fine-tuned to produce this\nstructured output before encoding its final retrieval intent into a dedicated\nembedding. Comprehensive experiments show that CIR-CoT achieves highly\ncompetitive performance on in-domain datasets (FashionIQ, CIRR) and\ndemonstrates remarkable generalization on the out-of-domain CIRCO dataset,\nestablishing a new path toward more effective and trustworthy retrieval\nsystems.",
        "url": "http://arxiv.org/abs/2510.08003v1",
        "published_date": "2025-10-09T09:41:45+00:00",
        "updated_date": "2025-10-09T09:41:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weihuang Lin",
            "Yiwei Ma",
            "Jiayi Ji",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "tldr": "The paper introduces CIR-CoT, an end-to-end MLLM for Composed Image Retrieval that integrates Chain-of-Thought reasoning, achieving competitive performance and improved interpretability. They also create a new dataset with CoT annotations for this task.",
        "tldr_zh": "该论文介绍了 CIR-CoT，一个用于组合图像检索的端到端 MLLM，它集成了思维链推理，实现了有竞争力的性能和改进的可解释性。他们还为此任务创建了一个带有 CoT 注释的新数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding",
        "summary": "The rapid progress of large language models (LLMs) has laid the foundation\nfor multimodal models. However, visual language models (VLMs) still face heavy\ncomputational costs when extended from images to videos due to high frame rates\nand long durations. Token compression is a promising solution, yet most\nexisting training-free methods cause information loss and performance\ndegradation. To overcome this, we propose \\textbf{Memory-Augmented\nReinforcement Learning-based Token Compression (MARC)}, which integrates\nstructured retrieval and RL-based distillation. MARC adopts a\n\\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory\nRetriever (VMR)} to select key clips and a \\textbf{Compression Group Relative\nPolicy Optimization (C-GRPO)} framework to distil reasoning ability from a\nteacher to a student model. Experiments on six video benchmarks show that MARC\nachieves near-baseline accuracy using only one frame's tokens -- reducing\nvisual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by\n\\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video\nunderstanding in resource-constrained settings such as video QA, surveillance,\nand autonomous driving.",
        "url": "http://arxiv.org/abs/2510.07915v1",
        "published_date": "2025-10-09T08:07:19+00:00",
        "updated_date": "2025-10-09T08:07:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiran Wu",
            "Zhuorui Yu",
            "Yunze Liu",
            "Chi-Hao Wu",
            "Enmin Zhou",
            "Junxiao Shen"
        ],
        "tldr": "The paper introduces MARC, a Memory-Augmented RL-based token compression method for video understanding that significantly reduces computational costs by selectively compressing visual tokens using reinforcement learning and a visual memory retriever, achieving near-baseline accuracy with substantially reduced resources.",
        "tldr_zh": "该论文介绍了MARC，一种基于记忆增强的RL令牌压缩方法，用于视频理解。该方法通过使用强化学习和视觉记忆检索器选择性地压缩视觉令牌，从而显著降低计算成本，并在资源大幅减少的情况下实现接近基线的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models",
        "summary": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has\nattracted much attention due to its importance for Autonomous Driving, Embodied\nAI and General Artificial Intelligence. Existing spatial-temporal benchmarks\nmainly focus on egocentric perspective reasoning with images/video context, or\ngeographic perspective reasoning with graphics context (eg. a map), thus fail\nto assess VLMs' geographic spatial-temporal intelligence with both images/video\nand graphics context, which is important for areas like traffic management and\nemergency response. To address the gaps, we introduce Geo-Temporal Reasoning\nbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of\nmoving targets in a large-scale camera network. GTR-Bench is more challenging\nas it requires multiple perspective switches between maps and videos, joint\nreasoning across multiple videos with non-overlapping fields of view, and\ninference over spatial-temporal regions that are unobserved by any video\ncontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that\neven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags\nbehind human performance (78.61%) on geo-temporal reasoning. Moreover, our\ncomprehensive analysis on GTR-Bench reveals three primary deficiencies of\ncurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by\nan imbalanced utilization of spatial-temporal context. (2) VLMs are weak in\ntemporal forecasting, which leads to worse performance on temporal-emphasized\ntasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to\ncomprehend or align the map data with multi-view video inputs. We believe\nGTR-Bench offers valuable insights and opens up new opportunities for research\nand applications in spatial-temporal intelligence. Benchmark and code will be\nreleased at https://github.com/X-Luffy/GTR-Bench.",
        "url": "http://arxiv.org/abs/2510.07791v1",
        "published_date": "2025-10-09T05:09:27+00:00",
        "updated_date": "2025-10-09T05:09:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinghongbing Xie",
            "Zhaoyuan Xia",
            "Feng Zhu",
            "Lijun Gong",
            "Ziyue Li",
            "Rui Zhao",
            "Long Zeng"
        ],
        "tldr": "The paper introduces GTR-Bench, a new benchmark for evaluating VLMs' geo-temporal reasoning abilities using multi-view video and map data, revealing deficiencies in current models regarding spatial-temporal context utilization, temporal forecasting, and map alignment.",
        "tldr_zh": "该论文介绍了 GTR-Bench，一个新的基准测试，用于评估 VLM 使用多视角视频和地图数据进行地理时空推理的能力，揭示了当前模型在时空上下文利用、时间预测和地图对齐方面的缺陷。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction",
        "summary": "Vision-Language-Action (VLA) models leverage pretrained vision-language\nmodels (VLMs) to couple perception with robotic control, offering a promising\npath toward general-purpose embodied intelligence. However, current SOTA VLAs\nare primarily pretrained on multimodal tasks with limited relevance to embodied\nscenarios, and then finetuned to map explicit instructions to actions.\nConsequently, due to the lack of reasoning-intensive pretraining and\nreasoning-guided manipulation, these models are unable to perform implicit\nhuman intention reasoning required for complex, real-world interactions. To\novercome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework\nwith a curriculum training paradigm and an efficient inference mechanism. Our\nproposed method first leverages carefully designed reasoning data that combine\nintention inference, spatial grounding, and compact embodied reasoning,\nendowing the model with both reasoning and perception capabilities. In the\nfollowing finetuning stage, IntentionVLA employs the compact reasoning outputs\nas contextual guidance for action generation, enabling fast inference under\nindirect instructions. Experimental results show that IntentionVLA\nsubstantially outperforms $\\pi_0$, achieving 18\\% higher success rates with\ndirect instructions and 28\\% higher than ECoT under intention instructions. On\nout-of-distribution intention tasks, IntentionVLA achieves over twice the\nsuccess rate of all baselines, and further enables zero-shot human-robot\ninteraction with 40\\% success rate. These results highlight IntentionVLA as a\npromising paradigm for next-generation human-robot interaction (HRI) systems.",
        "url": "http://arxiv.org/abs/2510.07778v1",
        "published_date": "2025-10-09T04:49:46+00:00",
        "updated_date": "2025-10-09T04:49:46+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yandu Chen",
            "Kefan Gu",
            "Yuqing Wen",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Liqiang Nie"
        ],
        "tldr": "The paper introduces IntentionVLA, a new Vision-Language-Action framework that uses curriculum training and efficient inference to improve human intention reasoning in human-robot interaction, significantly outperforming existing methods, especially in out-of-distribution and zero-shot scenarios.",
        "tldr_zh": "本文介绍了IntentionVLA，一种新的视觉-语言-动作框架，它使用课程学习和高效推理来提高人机交互中对人类意图的推理能力，显著优于现有方法，尤其是在分布外和零样本场景中。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility",
        "summary": "Despite impressive visual fidelity, modern video generative models frequently\nproduce sequences that violate intuitive physical laws, such as objects\nfloating, teleporting, or morphing in ways that defy causality. While humans\ncan easily detect such implausibilities, there remains no robust method for\nquantitatively assessing physical realism in video. In this work, we explore\nwhether Video-Language Models (VLMs) can be trained to serve as reliable judges\nof physical plausibility. We find that existing VLMs struggle to identify\nphysics violations, exposing fundamental limitations in their temporal and\ncausal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe\nthat combines a balanced training dataset with a trajectory-aware attention\nmodule to improve motion encoding and discrimination in VLMs. To evaluate\nphysical reasoning more rigorously, we propose ImplausiBench, a benchmark of\n300 videos (150 real, 150 generated) that removes linguistic biases and\nisolates visual-temporal understanding. Performance is reported both with\ngold-standard human judgments and stricter LLM-as-judge metrics. Together,\nTRAVL and ImplausiBench offer a unified framework for probing and improving\nphysical plausibility in multimodal models, shedding light on a challenging and\nunderexplored aspect of visual-temporal understanding.",
        "url": "http://arxiv.org/abs/2510.07550v1",
        "published_date": "2025-10-08T21:03:46+00:00",
        "updated_date": "2025-10-08T21:03:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Saman Motamed",
            "Minghao Chen",
            "Luc Van Gool",
            "Iro Laina"
        ],
        "tldr": "The paper introduces TRAVL, a fine-tuning recipe and ImplausiBench, a benchmark, to improve and evaluate the ability of VLMs to identify physically implausible videos.",
        "tldr_zh": "该论文介绍了TRAVL（一种微调方法）和ImplausiBench（一个基准测试），以提高和评估VLM识别物理上不可信视频的能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis",
        "summary": "Effective analysis of time series data presents significant challenges due to\nthe complex temporal dependencies and cross-channel interactions in\nmultivariate data. Inspired by the way human analysts visually inspect time\nseries to uncover hidden patterns, we ask: can incorporating visual\nrepresentations enhance automated time-series analysis? Recent advances in\nmultimodal large language models have demonstrated impressive generalization\nand visual understanding capability, yet their application to time series\nremains constrained by the modality gap between continuous numerical data and\ndiscrete natural language. To bridge this gap, we introduce MLLM4TS, a novel\nframework that leverages multimodal large language models for general\ntime-series analysis by integrating a dedicated vision branch. Each time-series\nchannel is rendered as a horizontally stacked color-coded line plot in one\ncomposite image to capture spatial dependencies across channels, and a\ntemporal-aware visual patch alignment strategy then aligns visual patches with\ntheir corresponding time segments. MLLM4TS fuses fine-grained temporal details\nfrom the numerical data with global contextual information derived from the\nvisual representation, providing a unified foundation for multimodal\ntime-series analysis. Extensive experiments on standard benchmarks demonstrate\nthe effectiveness of MLLM4TS across both predictive tasks (e.g.,\nclassification) and generative tasks (e.g., anomaly detection and forecasting).\nThese results underscore the potential of integrating visual modalities with\npretrained language models to achieve robust and generalizable time-series\nanalysis.",
        "url": "http://arxiv.org/abs/2510.07513v1",
        "published_date": "2025-10-08T20:22:39+00:00",
        "updated_date": "2025-10-08T20:22:39+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DB"
        ],
        "authors": [
            "Qinghua Liu",
            "Sam Heshmati",
            "Zheda Mai",
            "Zubin Abraham",
            "John Paparrizos",
            "Liu Ren"
        ],
        "tldr": "The paper introduces MLLM4TS, a novel framework that leverages multimodal large language models and vision to enhance time-series analysis by converting time series data into visual representations and aligning them with corresponding time segments.",
        "tldr_zh": "该论文介绍了MLLM4TS，一种新颖的框架，利用多模态大型语言模型和视觉来增强时间序列分析，通过将时间序列数据转换为视觉表示并将其与相应的时间段对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception",
        "summary": "Vision-Language Models (VLMs) are becoming increasingly powerful,\ndemonstrating strong performance on a variety of tasks that require both visual\nand textual understanding. Their strong generalisation abilities make them a\npromising component for automated driving systems, which must handle unexpected\ncorner cases. However, to be trusted in such safety-critical applications, a\nmodel must first possess a reliable perception system. Moreover, since critical\nobjects and agents in traffic scenes are often at a distance, we require\nsystems that are not \"shortsighted\", i.e., systems with strong perception\ncapabilities at both close (up to 20 meters) and long (30+ meters) range. With\nthis in mind, we introduce Distance-Annotated Traffic Perception Question\nAnswering (DTPQA), the first Visual Question Answering (VQA) benchmark focused\nsolely on perception-based questions in traffic scenes, enriched with distance\nannotations. By excluding questions that require reasoning, we ensure that\nmodel performance reflects perception capabilities alone. Since automated\ndriving hardware has limited processing power and cannot support large VLMs,\nour study centers on smaller VLMs. More specifically, we evaluate several\nstate-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the\nsimplicity of the questions, these models significantly underperform compared\nto humans (~60% average accuracy for the best-performing small VLM versus ~85%\nhuman performance). However, it is important to note that the human sample size\nwas relatively small, which imposes statistical limitations. We also identify\nspecific perception tasks, such as distinguishing left from right, that remain\nparticularly challenging for these models.",
        "url": "http://arxiv.org/abs/2510.08352v1",
        "published_date": "2025-10-09T15:38:41+00:00",
        "updated_date": "2025-10-09T15:38:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nikos Theodoridis",
            "Tim Brophy",
            "Reenu Mohandas",
            "Ganesh Sistu",
            "Fiachra Collins",
            "Anthony Scanlan",
            "Ciaran Eising"
        ],
        "tldr": "The paper introduces a new VQA benchmark, DTPQA, for evaluating small VLMs' perception capabilities in traffic scenes, showing they underperform compared to humans, especially in distance-dependent perception tasks.",
        "tldr_zh": "该论文介绍了一个新的视觉问答基准DTPQA，用于评估小型视觉语言模型在交通场景中的感知能力，结果表明它们与人类相比表现不佳，尤其是在距离相关的感知任务中。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Cross-Modal Attention Guided Unlearning in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have demonstrated immense capabilities in\nmulti-modal understanding and inference tasks such as Visual Question Answering\n(VQA), which requires models to infer outputs based on visual and textual\ncontext simultaneously. Such inference abilities of large-scale pretrained\nmodels are often attributed to the massive scale of pre-training data collected\nacross several domains. However, the models may memorize private and/or\nsensitive information during training and regurgitate it in inference.\nRecently, machine unlearning has been leveraged to address the leakage of\nprivate data in LLMs. VLMs add a layer of complexity to this process, as the\nvisual context in the query may also contain sensitive information in addition\nto the text. To address this issue, we explore unlearning for vision-language\nmodels, specifically for the VQA task. We explore the role of visual tokens for\noutput generation in VLMs using cross-modal attention and utilize it to\nformulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and\nefficient VLM unlearning framework. In contrast to computationally expensive\nmodel finetuning methods, CAGUL utilizes external modules to encode unlearning\ninformation in visual tokens of low importance for relevant queries. We find\nthat the transformed visual tokens not only prevent leakage but also retain\nreference model behavior. Experimental results show that our method performs\nbetter or on par with finetuning-based baselines without altering the\npre-trained model parameters or incurring retraining costs, making it a\npractical and effective unlearning solution for VLMs.",
        "url": "http://arxiv.org/abs/2510.07567v1",
        "published_date": "2025-10-08T21:21:59+00:00",
        "updated_date": "2025-10-08T21:21:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Karuna Bhaila",
            "Aneesh Komanduri",
            "Minh-Hao Van",
            "Xintao Wu"
        ],
        "tldr": "This paper introduces Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight framework for unlearning sensitive information in VLMs by transforming low-importance visual tokens, achieving comparable or better performance than finetuning without retraining.",
        "tldr_zh": "本文介绍了一种跨模态注意力引导的卸载框架（CAGUL），通过转换低重要性的视觉标记来卸载视觉语言模型中的敏感信息，无需重新训练即可达到与微调相当或更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting",
        "summary": "Text-guided image inpainting aims at reconstructing the masked regions as per\ntext prompts, where the longstanding challenges lie in the preservation for\nunmasked regions, while achieving the semantics consistency between unmasked\nand inpainted masked regions. Previous arts failed to address both of them,\nalways with either of them to be remedied. Such facts, as we observed, stem\nfrom the entanglement of the hybrid (e.g., mid-and-low) frequency bands that\nencode varied image properties, which exhibit different robustness to text\nprompts during the denoising process. In this paper, we propose a\nnull-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for\ntext-guided image inpainting, by decomposing the semantics consistency across\nmasked and unmasked regions into the consistencies as per each frequency band,\nwhile preserving the unmasked regions, to circumvent two challenges in a row.\nBased on the diffusion process, we further divide the denoising process into\nearly (high-level noise) and late (low-level noise) stages, where the\nmid-and-low frequency bands are disentangled during the denoising process. As\nobserved, the stable mid-frequency band is progressively denoised to be\nsemantically aligned during text-guided denoising process, which, meanwhile,\nserves as the guidance to the null-text denoising process to denoise\nlow-frequency band for the masked regions, followed by a subsequent text-guided\ndenoising process at late stage, to achieve the semantics consistency for\nmid-and-low frequency bands across masked and unmasked regions, while preserve\nthe unmasked regions. Extensive experiments validate the superiority of\nNTN-Diff over the state-of-the-art diffusion models to text-guided diffusion\nmodels. Our code can be accessed from https://github.com/htyjers/NTN-Diff.",
        "url": "http://arxiv.org/abs/2510.08273v1",
        "published_date": "2025-10-09T14:30:34+00:00",
        "updated_date": "2025-10-09T14:30:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haipeng Liu",
            "Yang Wang",
            "Meng Wang"
        ],
        "tldr": "The paper proposes a novel diffusion model, NTN-Diff, for text-guided image inpainting that disentangles frequency bands during denoising to improve consistency and preservation of unmasked regions.",
        "tldr_zh": "本文提出了一种新的扩散模型 NTN-Diff，用于文本引导的图像修复，该模型在去噪过程中解耦了频率带，以提高一致性并保留未遮盖区域。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Automatic Text Box Placement for Supporting Typographic Design",
        "summary": "In layout design for advertisements and web pages, balancing visual appeal\nand communication efficiency is crucial. This study examines automated text box\nplacement in incomplete layouts, comparing a standard Transformer-based method,\na small Vision and Language Model (Phi3.5-vision), a large pretrained VLM\n(Gemini), and an extended Transformer that processes multiple images.\nEvaluations on the Crello dataset show the standard Transformer-based models\ngenerally outperform VLM-based approaches, particularly when incorporating\nricher appearance information. However, all methods face challenges with very\nsmall text or densely populated layouts. These findings highlight the benefits\nof task-specific architectures and suggest avenues for further improvement in\nautomated layout design.",
        "url": "http://arxiv.org/abs/2510.07665v1",
        "published_date": "2025-10-09T01:38:21+00:00",
        "updated_date": "2025-10-09T01:38:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jun Muraoka",
            "Daichi Haraguchi",
            "Naoto Inoue",
            "Wataru Shimoda",
            "Kota Yamaguchi",
            "Seiichi Uchida"
        ],
        "tldr": "This paper compares different models, including VLMs, for automatic text box placement in layouts, finding task-specific Transformer models perform better than VLMs on the Crello dataset but all models struggle with small text and dense layouts.",
        "tldr_zh": "本文比较了不同的模型（包括VLM）在布局中自动放置文本框的性能，发现在Crello数据集上，特定任务的Transformer模型比VLM表现更好，但所有模型在处理小文本和密集布局时都存在困难。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]