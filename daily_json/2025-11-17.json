[
    {
        "title": "ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding",
        "summary": "Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.",
        "url": "http://arxiv.org/abs/2511.12530v1",
        "published_date": "2025-11-16T09:56:57+00:00",
        "updated_date": "2025-11-16T09:56:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Zhou",
            "Litao Hua",
            "Shilong Jin",
            "Wentao Huang",
            "Haoran Duan"
        ],
        "tldr": "The paper introduces ReaSon, a reinforcement learning framework for keyframe selection in video understanding, using a Causal Information Bottleneck to optimize for both predictive sufficiency and causal necessity. Experiments show it outperforms existing methods.",
        "tldr_zh": "本文介绍了一种名为ReaSon的强化学习框架，用于视频理解中的关键帧选择。该框架使用因果信息瓶颈来优化预测充分性和因果必要性。实验表明，该方法优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics",
        "summary": "Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.",
        "url": "http://arxiv.org/abs/2511.12525v1",
        "published_date": "2025-11-16T09:43:12+00:00",
        "updated_date": "2025-11-16T09:43:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jing Li",
            "Yifan Wang",
            "Jiafeng Yan",
            "Renlong Zhang",
            "Bin Yang"
        ],
        "tldr": "The paper introduces MdaIF, a novel image fusion framework leveraging a vision-language model and a mixture-of-experts system to handle multi-degradation scenarios like haze, rain, and snow, improving fusion performance compared to SOTA methods.",
        "tldr_zh": "该论文提出了MdaIF，一种新颖的图像融合框架，利用视觉语言模型和混合专家系统来处理多种退化场景，如雾霾、雨雪，与最先进的方法相比，提高了融合性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions",
        "summary": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.",
        "url": "http://arxiv.org/abs/2511.12452v1",
        "published_date": "2025-11-16T04:46:06+00:00",
        "updated_date": "2025-11-16T04:46:06+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Xiaoyu Lin",
            "Aniket Ghorpade",
            "Hansheng Zhu",
            "Justin Qiu",
            "Dea Rrozhani",
            "Monica Lama",
            "Mick Yang",
            "Zixuan Bian",
            "Ruohan Ren",
            "Alan B. Hong",
            "Jiatao Gu",
            "Chris Callison-Burch"
        ],
        "tldr": "The paper introduces DenseAnnotate, an audio-driven platform for creating dense image and 3D scene annotations, resulting in a multilingual dataset that improves model performance in cultural alignment and 3D spatial understanding.",
        "tldr_zh": "本文介绍了一个名为DenseAnnotate的音频驱动平台，用于创建密集的图像和3D场景标注，从而生成一个多语言数据集，该数据集提高了模型在文化对齐和3D空间理解方面的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
        "summary": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
        "url": "http://arxiv.org/abs/2511.12449v1",
        "published_date": "2025-11-16T04:29:35+00:00",
        "updated_date": "2025-11-16T04:29:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Zhanheng Nie",
            "Chenghan Fu",
            "Daoze Zhang",
            "Junxian Wu",
            "Wanxian Guan",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "tldr": "The paper introduces MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding, addressing modality imbalance, underutilization of alignment relationships, and noise in e-commerce multimodal data using MoE, dual-level alignment, and MLLM-based co-augmentation.",
        "tldr_zh": "本文介绍了MOON2.0，一个用于电商产品理解的动态模态平衡多模态表征学习框架。该框架利用MoE、双层对齐和基于MLLM的协同增强来解决模态不平衡、对齐关系利用不足以及电商多模态数据中的噪声问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.",
        "url": "http://arxiv.org/abs/2511.12428v1",
        "published_date": "2025-11-16T03:11:52+00:00",
        "updated_date": "2025-11-16T03:11:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingqi Xu",
            "Jingxi Lu",
            "Chenghao Li",
            "Sreetama Sarkar",
            "Souvik Kundu",
            "Peter A. Beerel"
        ],
        "tldr": "This paper introduces RedVTP, a training-free visual token pruning method for Diffusion Vision-Language Models (DVLMs) that significantly accelerates inference by leveraging attention from masked response tokens to prune less important visual tokens after the first step, achieving significant speedups without compromising accuracy.",
        "tldr_zh": "本文介绍了一种名为RedVTP的免训练视觉Token剪枝方法，用于扩散视觉-语言模型（DVLMs）。该方法利用来自掩码响应Token的注意力来剪枝不重要的视觉Token，从而显著加速推理，并在不影响准确性的前提下实现显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving",
        "summary": "Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.",
        "url": "http://arxiv.org/abs/2511.12405v1",
        "published_date": "2025-11-16T00:55:28+00:00",
        "updated_date": "2025-11-16T00:55:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunki Seong",
            "Seongwoo Moon",
            "Hojin Ahn",
            "Jehun Kang",
            "David Hyunchul Shim"
        ],
        "tldr": "This paper introduces VLA-R, a Vision-Language Action Retrieval framework for open-world autonomous driving that utilizes a frozen vision-language model for perception and vision-action contrastive learning for transferable driving behaviors, demonstrating strong generalization in real-world settings.",
        "tldr_zh": "本文介绍了一种用于开放世界自动驾驶的视觉-语言动作检索框架VLA-R。该框架利用冻结的视觉-语言模型进行感知，并利用视觉-动作对比学习来实现可迁移的驾驶行为，在现实世界环境中表现出强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models",
        "summary": "The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).",
        "url": "http://arxiv.org/abs/2511.12371v1",
        "published_date": "2025-11-15T22:12:43+00:00",
        "updated_date": "2025-11-15T22:12:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiqing Shen",
            "Chenxiao Fan",
            "Chenjia Li",
            "Mathias Unberath"
        ],
        "tldr": "This paper introduces a new paradigm, reasoning text-to-video retrieval, which addresses implicit queries by representing videos as digital twins and leveraging large language models for reasoning, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了推理文本到视频检索的新范例，通过将视频表示为数字孪生并利用大型语言模型进行推理来解决隐式查询，实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) struggle with negation. Given a prompt like \"retrieve (or generate) a street scene without pedestrians,\" they often fail to respect the \"not.\" Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as \"A but not N,\" we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.",
        "url": "http://arxiv.org/abs/2511.12331v1",
        "published_date": "2025-11-15T19:18:40+00:00",
        "updated_date": "2025-11-15T19:18:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sepehr Kazemi Ranjbar",
            "Kumail Alhamoud",
            "Marzyeh Ghassemi"
        ],
        "tldr": "The paper introduces a training-free method called SpaceVLM to improve VLMs' understanding of negation by modeling it as a subspace in the joint embedding space, achieving significant performance gains without compromising zero-shot capabilities.",
        "tldr_zh": "该论文介绍了一种名为SpaceVLM的无需训练的方法，通过将否定建模为联合嵌入空间中的子空间来提高VLMs对否定的理解，在不损害零样本能力的情况下实现了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training",
        "summary": "Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.",
        "url": "http://arxiv.org/abs/2511.12446v1",
        "published_date": "2025-11-16T04:19:22+00:00",
        "updated_date": "2025-11-16T04:19:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahe Qian",
            "Yuhao Shen",
            "Zhangtianyi Chen",
            "Juexiao Zhou",
            "Peisong Wang"
        ],
        "tldr": "The paper introduces CoTBox-TTT, a test-time training method for medical VQA that improves accuracy and grounding by using visual chain-of-thought and localized crops, without requiring additional labels or retraining backbones.",
        "tldr_zh": "该论文介绍了CoTBox-TTT，一种用于医学视觉问答的测试时训练方法，它通过使用视觉思维链和局部裁剪来提高准确性和基础性，而无需额外的标签或重新训练骨干网络。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Explainable AI-Generated Image Detection RewardBench",
        "summary": "Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an \"MLLM as a judge\" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \\textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\\% on this benchmark (while human inter-annotator agreement reaches 98.30\\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.",
        "url": "http://arxiv.org/abs/2511.12363v1",
        "published_date": "2025-11-15T21:51:13+00:00",
        "updated_date": "2025-11-15T21:51:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Michael Yang",
            "Shijian Deng",
            "William T. Doan",
            "Kai Wang",
            "Tianyu Yang",
            "Harsh Singh",
            "Yapeng Tian"
        ],
        "tldr": "The paper introduces XAIGID-RewardBench, a new benchmark for evaluating the ability of Multimodal Large Language Models (MLLMs) to judge the quality of explanations for AI-generated image detection, revealing a gap between MLLM and human performance.",
        "tldr_zh": "该论文介绍了 XAIGID-RewardBench，一个新的基准，用于评估多模态大型语言模型 (MLLM) 判断 AI 生成图像检测解释质量的能力，揭示了 MLLM 和人类性能之间的差距。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation",
        "summary": "Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.",
        "url": "http://arxiv.org/abs/2511.12528v1",
        "published_date": "2025-11-16T09:47:45+00:00",
        "updated_date": "2025-11-16T09:47:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zheyuan Zhang",
            "Jiwei Zhang",
            "Boyu Zhou",
            "Linzhimeng Duan",
            "Hong Chen"
        ],
        "tldr": "The paper presents D²-VPR, a parameter-efficient VPR method using knowledge distillation and deformable aggregation on DINOv2 features, achieving competitive performance with significantly reduced parameters and FLOPs.",
        "tldr_zh": "该论文提出了D²-VPR，一种参数高效的视觉定位方法，它利用知识蒸馏和可变形聚合对DINOv2特征进行处理，在显著减少参数和FLOPs的同时，实现了具有竞争力的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]