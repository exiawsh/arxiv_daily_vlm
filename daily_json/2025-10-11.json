[
    {
        "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
        "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
        "url": "http://arxiv.org/abs/2510.09608v1",
        "published_date": "2025-10-10T17:59:58+00:00",
        "updated_date": "2025-10-10T17:59:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Ruyi Xu",
            "Guangxuan Xiao",
            "Yukang Chen",
            "Liuning He",
            "Kelly Peng",
            "Yao Lu",
            "Song Han"
        ],
        "tldr": "StreamingVLM enables real-time understanding of infinite video streams by maintaining a compact KV cache and using a supervised fine-tuning strategy that mimics inference-time attention patterns, achieving state-of-the-art performance on long video benchmarks.",
        "tldr_zh": "StreamingVLM通过维护紧凑的KV缓存和使用模仿推理时注意力模式的监督微调策略，实现了对无限视频流的实时理解，并在长视频基准测试中取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation",
        "summary": "Vision-Language Action (VLA) models significantly advance robotic\nmanipulation by leveraging the strong perception capabilities of pretrained\nvision-language models (VLMs). By integrating action modules into these\npretrained models, VLA methods exhibit improved generalization. However,\ntraining them from scratch is costly. In this work, we propose a simple yet\neffective distillation-based framework that equips VLMs with action-execution\ncapability by transferring knowledge from pretrained small action models. Our\narchitecture retains the original VLM structure, adding only an action token\nand a state encoder to incorporate physical inputs. To distill action\nknowledge, we adopt a two-stage training strategy. First, we perform\nlightweight alignment by mapping VLM hidden states into the action space of the\nsmall action model, enabling effective reuse of its pretrained action decoder\nand avoiding expensive pretraining. Second, we selectively fine-tune the\nlanguage model, state encoder, and action modules, enabling the system to\nintegrate multimodal inputs with precise action generation. Specifically, the\naction token provides the VLM with a direct handle for predicting future\nactions, while the state encoder allows the model to incorporate robot dynamics\nnot captured by vision alone. This design yields substantial efficiency gains\nover training large VLA models from scratch. Compared with previous\nstate-of-the-art methods, our method achieves 97.3% average success rate on\nLIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In\nreal-world experiments across five manipulation tasks, our method consistently\noutperforms the teacher model, achieving 82.0% success rate (17% improvement),\nwhich demonstrate that action distillation effectively enables VLMs to generate\nprecise actions while substantially reducing training costs.",
        "url": "http://arxiv.org/abs/2510.09607v1",
        "published_date": "2025-10-10T17:59:56+00:00",
        "updated_date": "2025-10-10T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaoqi Dong",
            "Chaoyou Fu",
            "Haihan Gao",
            "Yi-Fan Zhang",
            "Chi Yan",
            "Chu Wu",
            "Xiaoyu Liu",
            "Yunhang Shen",
            "Jing Huo",
            "Deqiang Jiang",
            "Haoyu Cao",
            "Yang Gao",
            "Xing Sun",
            "Ran He",
            "Caifeng Shan"
        ],
        "tldr": "This paper presents VITA-VLA, a distillation-based framework for efficiently training Vision-Language Models to perform robotic manipulation by transferring knowledge from small action models, achieving significant improvements in success rates and training efficiency.",
        "tldr_zh": "本文提出了VITA-VLA，一种基于知识蒸馏的框架，通过从小动作模型迁移知识，高效地训练视觉-语言模型以执行机器人操作，从而在成功率和训练效率方面取得了显著提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision Language Models: A Survey of 26K Papers",
        "summary": "We present a transparent, reproducible measurement of research trends across\n26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles\nand abstracts are normalized, phrase-protected, and matched against a\nhand-crafted lexicon to assign up to 35 topical labels and mine fine-grained\ncues about tasks, architectures, training regimes, objectives, datasets, and\nco-mentioned modalities. The analysis quantifies three macro shifts: (1) a\nsharp rise of multimodal vision-language-LLM work, which increasingly reframes\nclassic perception as instruction following and multi-step reasoning; (2)\nsteady expansion of generative methods, with diffusion research consolidating\naround controllability, distillation, and speed; and (3) resilient 3D and video\nactivity, with composition moving from NeRFs to Gaussian splatting and a\ngrowing emphasis on human- and agent-centric understanding. Within VLMs,\nparameter-efficient adaptation like prompting/adapters/LoRA and lightweight\nvision-language bridges dominate; training practice shifts from building\nencoders from scratch to instruction tuning and finetuning strong backbones;\ncontrastive objectives recede relative to cross-entropy/ranking and\ndistillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and\nICLR the highest VLM share, while reliability themes such as efficiency or\nrobustness diffuse across areas. We release the lexicon and methodology to\nenable auditing and extension. Limitations include lexicon recall and\nabstract-only scope, but the longitudinal signals are consistent across venues\nand years.",
        "url": "http://arxiv.org/abs/2510.09586v1",
        "published_date": "2025-10-10T17:43:17+00:00",
        "updated_date": "2025-10-10T17:43:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fengming Lin"
        ],
        "tldr": "This paper presents a large-scale analysis of vision-language model (VLM) research trends from 2023-2025 based on 26K papers, identifying shifts in multimodal work, generative methods, and 3D/video understanding.",
        "tldr_zh": "本文对2023-2025年间发表的2.6万篇视觉语言模型（VLM）论文进行了大规模分析，揭示了多模态研究、生成方法以及3D/视频理解方面的趋势变化。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
        "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.",
        "url": "http://arxiv.org/abs/2510.09507v1",
        "published_date": "2025-10-10T16:10:45+00:00",
        "updated_date": "2025-10-10T16:10:45+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zixin Zhang",
            "Kanghao Chen",
            "Xingwang Lin",
            "Lutao Jiang",
            "Xu Zheng",
            "Yuanhuiyi Lyu",
            "Litao Guo",
            "Yinchuan Li",
            "Ying-Cong Chen"
        ],
        "tldr": "The paper introduces PhysToolBench, a new benchmark for evaluating MLLMs' understanding of physical tools, revealing a significant deficiency in this area across a variety of models.",
        "tldr_zh": "该论文介绍了 PhysToolBench，一个新的用于评估 MLLM 对物理工具理解能力的基准，揭示了各种模型在这方面的显著缺陷。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models",
        "summary": "Test-time adaptation paradigm provides flexibility towards domain shifts by\nperforming immediate adaptation on unlabeled target data from the source model.\nVision-Language Models (VLMs) leverage their generalization capabilities for\ndiverse downstream tasks, and test-time prompt tuning has emerged as a\nprominent solution for adapting VLMs. In this work, we explore contrastive VLMs\nand identify the modality gap caused by a single dominant feature dimension\nacross modalities. We observe that the dominant dimensions in both text and\nimage modalities exhibit high predictive sensitivity, and that constraining\ntheir influence can improve calibration error. Building on this insight, we\npropose dimensional entropy maximization that regularizes the distribution of\ntextual features toward uniformity to mitigate the dependency of dominant\ndimensions. Our method alleviates the degradation of calibration performance in\ntest-time prompt tuning, offering a simple yet effective solution to enhance\nthe reliability of VLMs in real-world deployment scenarios.",
        "url": "http://arxiv.org/abs/2510.09473v1",
        "published_date": "2025-10-10T15:27:44+00:00",
        "updated_date": "2025-10-10T15:27:44+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jisu Han",
            "Wonjun Hwang"
        ],
        "tldr": "The paper proposes a dimensional entropy maximization method (D-TPT) to address the modality gap in contrastive VLMs during test-time prompt tuning, improving calibration performance by regularizing textual feature distributions.",
        "tldr_zh": "该论文提出了一种维度熵最大化方法（D-TPT），旨在解决对比视觉语言模型在测试时提示调优期间的模态差距，通过正则化文本特征分布来提高校准性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception",
        "summary": "Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,\nparticularly in enhancing their reasoning capabilities. However, existing\nreasoning benchmarks still primarily assess language-based reasoning, often\ntreating visual input as replaceable context. To address this gap, we introduce\nBLINK-Twice, a vision-centric reasoning benchmark grounded in challenging\nperceptual tasks. Instead of relying on external knowledge, our tasks require\nmodels to reason from visual content alone, shifting the focus from\nlanguage-based to image-grounded reasoning. Compared to prior perception\nbenchmarks, it moves beyond shallow perception (\"see\") and requires\nfine-grained observation and analytical reasoning (\"observe\"). BLINK-Twice\nintegrates three core components: seven types of visual challenges for testing\nvisual reasoning, natural adversarial image pairs that enforce reliance on\nvisual content, and annotated reasoning chains for fine-grained evaluation of\nthe reasoning process rather than final answers alone. We evaluate 20 leading\nMLLMs, including 12 foundation models and 8 reasoning-enhanced models.\nBLINK-Twice poses a significant challenge to current models. While existing\nreasoning strategies in the language space-such as chain-of-thought or\nself-criticism can improve performance, they often result in unstable and\nredundant reasoning. We observe that repeated image observation improves\nperformance across models, and active visual interaction, as demonstrated by\nmodels like o3, highlights the need for a new paradigm for vision reasoning.\nThe dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice",
        "url": "http://arxiv.org/abs/2510.09361v1",
        "published_date": "2025-10-10T13:14:13+00:00",
        "updated_date": "2025-10-10T13:14:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyan Ye",
            "Dongzhi Jiang",
            "Jun He",
            "Baichuan Zhou",
            "Zilong Huang",
            "Zhiyuan Yan",
            "Hongsheng Li",
            "Conghui He",
            "Weijia Li"
        ],
        "tldr": "The paper introduces BLINK-Twice, a new vision-centric reasoning benchmark designed to challenge Multimodal Large Language Models (MLLMs) with fine-grained visual perception tasks, focusing on image-grounded reasoning and analytical observation. It reveals that current MLLMs struggle with these tasks and highlights the need for new vision reasoning paradigms.",
        "tldr_zh": "该论文介绍了BLINK-Twice，一个新的以视觉为中心的推理基准，旨在通过细粒度的视觉感知任务挑战多模态大型语言模型（MLLMs），重点关注图像基础推理和分析性观察。它揭示了当前的MLLM难以应对这些任务，并强调了对新的视觉推理范式的需求。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models",
        "summary": "Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only\nmethods by incorporating multiple modalities of input information to produce a\nset of conclusive phrases. Traditional multi-modal approaches have been proven\nto have significant limitations in handling the challenging absence and unseen\nscenarios. Additionally, we identify shortcomings in existing benchmarks that\noverestimate model capability due to significant overlap in training tests. In\nthis work, we propose leveraging vision-language models (VLMs) for the MMKP\ntask. Firstly, we use two widely-used strategies, e.g., zero-shot and\nsupervised fine-tuning (SFT) to assess the lower bound performance of VLMs.\nNext, to improve the complex reasoning capabilities of VLMs, we adopt\nFine-tune-CoT, which leverages high-quality CoT reasoning data generated by a\nteacher model to finetune smaller models. Finally, to address the\n\"overthinking\" phenomenon, we propose a dynamic CoT strategy which adaptively\ninjects CoT data during training, allowing the model to flexibly leverage its\nreasoning capabilities during the inference stage. We evaluate the proposed\nstrategies on various datasets and the experimental results demonstrate the\neffectiveness of the proposed approaches. The code is available at\nhttps://github.com/bytedance/DynamicCoT.",
        "url": "http://arxiv.org/abs/2510.09358v1",
        "published_date": "2025-10-10T13:13:07+00:00",
        "updated_date": "2025-10-10T13:13:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qihang Ma",
            "Shengyu Li",
            "Jie Tang",
            "Dingkang Yang",
            "Shaodong Chen",
            "Yingyi Zhang",
            "Chao Feng",
            "Jiao Ran"
        ],
        "tldr": "This paper proposes a dynamic Chain-of-Thought (CoT) strategy to improve the performance of Vision-Language Models (VLMs) on multi-modal keyphrase prediction, addressing limitations of existing benchmarks and models.",
        "tldr_zh": "该论文提出了一种动态的思维链（CoT）策略，以提高视觉-语言模型（VLMs）在多模态关键词预测方面的性能，解决了现有基准和模型的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning",
        "summary": "Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs.",
        "url": "http://arxiv.org/abs/2510.09302v1",
        "published_date": "2025-10-10T11:47:54+00:00",
        "updated_date": "2025-10-10T11:47:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yuying Li",
            "Siyi Qian",
            "Hao Liang",
            "Leqi Zheng",
            "Ruichuan An",
            "Yongzhen Guo",
            "Wentao Zhang"
        ],
        "tldr": "The paper introduces CapGeo, a caption-assisted framework and benchmark (CapGeo-Bench) to improve geometric reasoning in MLLMs by converting visual diagrams into textual captions, leading to significant performance gains.",
        "tldr_zh": "该论文介绍了CapGeo，一个通过将视觉图转换为文本描述来提高多模态大型语言模型几何推理能力的框架和基准测试(CapGeo-Bench)，从而显著提高了性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
        "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.",
        "url": "http://arxiv.org/abs/2510.09285v1",
        "published_date": "2025-10-10T11:25:33+00:00",
        "updated_date": "2025-10-10T11:25:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyuan Huang",
            "Xiaoye Qu",
            "Yafu Li",
            "Yun Luo",
            "Zefeng He",
            "Daizong Liu",
            "Yu Cheng"
        ],
        "tldr": "This paper introduces Visually-Perceptive Policy Optimization (VPPO), a novel policy gradient algorithm that leverages token perception to enhance the multimodal reasoning capabilities of LVLMs, showing significant improvements on perception and reasoning benchmarks.",
        "tldr_zh": "本文介绍了视觉感知策略优化（VPPO），一种新的策略梯度算法，利用token感知来增强LVLM的多模态推理能力，并在感知和推理基准测试中显示出显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy",
        "summary": "To determine whether using discrete semantic entropy (DSE) to reject\nquestions likely to generate hallucinations can improve the accuracy of\nblack-box vision-language models (VLMs) in radiologic image based visual\nquestion answering (VQA). This retrospective study evaluated DSE using two\npublicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500\nimages with clinical questions and short-text answers) and (ii) a diagnostic\nradiology dataset (206 cases: 60 computed tomography scans, 60 magnetic\nresonance images, 60 radiographs, 26 angiograms) with corresponding\nground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times\nusing a temperature of 1.0. Baseline accuracy was determined using\nlow-temperature answers (temperature 0.1). Meaning-equivalent responses were\ngrouped using bidirectional entailment checks, and DSE was computed from the\nrelative frequencies of the resulting semantic clusters. Accuracy was\nrecalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and\n95% confidence intervals were obtained using bootstrap resampling and a\nBonferroni-corrected threshold of p < .004 for statistical significance. Across\n706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for\nGPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on\nthe remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and\n63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains\nwere observed across both datasets and largely remained statistically\nsignificant after Bonferroni correction. DSE enables reliable hallucination\ndetection in black-box VLMs by quantifying semantic inconsistency. This method\nsignificantly improves diagnostic answer accuracy and offers a filtering\nstrategy for clinical VLM applications.",
        "url": "http://arxiv.org/abs/2510.09256v1",
        "published_date": "2025-10-10T10:53:33+00:00",
        "updated_date": "2025-10-10T10:53:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Patrick Wienholt",
            "Sophie Caselitz",
            "Robert Siepmann",
            "Philipp Bruners",
            "Keno Bressem",
            "Christiane Kuhl",
            "Jakob Nikolas Kather",
            "Sven Nebelung",
            "Daniel Truhn"
        ],
        "tldr": "This paper introduces Discrete Semantic Entropy (DSE) to filter out questions likely to cause hallucinations in Vision-Language Models for radiology, demonstrating significant accuracy improvements in diagnostic VQA.",
        "tldr_zh": "本文介绍了一种离散语义熵（DSE）方法，用于过滤掉可能导致放射学视觉语言模型出现幻觉的问题，并在诊断性视觉问答方面实现了显著的准确性提升。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras",
        "summary": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),\nare common conditions affecting the health of people worldwide, and have a high\nincidence rate among the elderly and workers engaged in repetitive shoulder\ntasks. In regions with scarce medical resources, achieving early and accurate\ndiagnosis poses significant challenges, and there is an urgent need for\nlow-cost and easily scalable auxiliary diagnostic solutions. This research\nintroduces videos captured by consumer-grade devices as the basis for\ndiagnosis, reducing the cost for users. We focus on the innovative application\nof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of\nshoulder disorders and propose a Hybrid Motion Video Diagnosis framework\n(HMVDx). This framework divides the two tasks of action understanding and\ndisease diagnosis, which are respectively completed by two MLLMs. In addition\nto traditional evaluation indicators, this work proposes a novel metric called\nUsability Index by the logical process of medical decision-making (action\nrecognition, movement diagnosis, and final diagnosis). This index evaluates the\neffectiveness of MLLMs in the medical field from the perspective of the entire\nmedical diagnostic pathway, revealing the potential value of low-cost MLLMs in\nmedical applications for medical practitioners. In experimental comparisons,\nthe accuracy of HMVDx in diagnosing shoulder joint injuries has increased by\n79.6\\% compared with direct video diagnosis, a significant technical\ncontribution to future research on the application of MLLMs for video\nunderstanding in the medical field.",
        "url": "http://arxiv.org/abs/2510.09230v1",
        "published_date": "2025-10-10T10:17:23+00:00",
        "updated_date": "2025-10-10T10:17:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Jindong Hong",
            "Wencheng Zhang",
            "Shiqin Qiao",
            "Jianhai Chen",
            "Jianing Qiu",
            "Chuanyang Zheng",
            "Qian Xu",
            "Yun Ji",
            "Qianyue Wen",
            "Weiwei Sun",
            "Hao Li",
            "Huizhen Li",
            "Huichao Wang",
            "Kai Wu",
            "Meng Li",
            "Yijun He",
            "Lingjie Luo",
            "Jiankai Sun"
        ],
        "tldr": "The paper introduces a Hybrid Motion Video Diagnosis framework (HMVDx) using MLLMs and consumer-grade cameras for diagnosing shoulder disorders, achieving a 79.6% accuracy increase compared to direct video diagnosis and proposing a novel Usability Index.",
        "tldr_zh": "该论文介绍了一种混合运动视频诊断框架(HMVDx)，使用MLLMs和消费级相机诊断肩部疾病。与直接视频诊断相比，准确率提高了79.6%，并提出了一个新的可用性指标。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Auto-scaling Continuous Memory for GUI Agent",
        "summary": "We study how to endow GUI agents with scalable memory that help generalize\nacross unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress\npast trajectories into text tokens, which balloons context length and misses\ndecisive visual cues (e.g., exact widget size and position). We propose a\ncontinuous memory that encodes each GUI trajectory into a fixed-length sequence\nof continuous embeddings using the VLM itself as an encoder; these embeddings\nare plugged directly into the backbone's input layer, sharply reducing context\ncost while preserving fine-grained visual information. As memory size and\nretrieval depth increase, performance improves monotonically, unlike text\nmemories that degrade with long prompts. To grow memory at low cost, we\nintroduce an auto-scaling data flywheel that (i) discovers new environments via\nsearch, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out\ntrajectories with the agent, and (iv) verifies success with the same VLM. Using\nthis pipeline, we collect 100k+ trajectories for about \\$4000 and fine-tune\nonly the memory encoder (LoRA on a Q-Former, 1.2\\% parameters) with 1,500\nsamples. On real-world GUI benchmarks, our memory-augmented agent consistently\nimproves success rates under long horizons and distribution shifts. Notably,\nQwen-2.5-VL-7B + continuous memory achieves performance comparable to\nstate-of-the-art closed-source models (e.g., GPT-4o, Claude-4).",
        "url": "http://arxiv.org/abs/2510.09038v1",
        "published_date": "2025-10-10T06:16:45+00:00",
        "updated_date": "2025-10-10T06:16:45+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.CY",
            "cs.LG"
        ],
        "authors": [
            "Wenyi Wu",
            "Kun Zhou",
            "Ruoxin Yuan",
            "Vivian Yu",
            "Stephen Wang",
            "Zhiting Hu",
            "Biwei Huang"
        ],
        "tldr": "This paper introduces a continuous memory approach for GUI agents using VLMs as encoders, achieving improved performance and scalability compared to text-based memories, and demonstrating comparable results to SOTA closed-source models.",
        "tldr_zh": "该论文介绍了一种用于GUI代理的连续记忆方法，该方法使用VLM作为编码器，与基于文本的记忆相比，提高了性能和可扩展性，并展示了与SOTA闭源模型相当的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models",
        "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts.",
        "url": "http://arxiv.org/abs/2510.09008v1",
        "published_date": "2025-10-10T05:12:52+00:00",
        "updated_date": "2025-10-10T05:12:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Hoigi Seo",
            "Dong Un Kang",
            "Hyunjin Cho",
            "Joohoon Lee",
            "Se Young Chun"
        ],
        "tldr": "This paper identifies uncertain visual tokens as a key factor in object hallucination in LVLMs, proposes a method to mitigate this by masking these tokens, and demonstrates significant reduction in hallucinations through experiments.",
        "tldr_zh": "该论文指出，不确定的视觉tokens是LVLM中对象幻觉的关键因素，提出了一种通过屏蔽这些tokens来缓解幻觉的方法，并通过实验证明了幻觉的显著减少。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unleashing Perception-Time Scaling to Multimodal Reasoning Models",
        "summary": "Recent advances in inference-time scaling, particularly those leveraging\nreinforcement learning with verifiable rewards, have substantially enhanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by\nthis success, similar strategies have been applied to multimodal reasoning, yet\ntheir impact on visual perception remains unclear. To investigate this gap, we\nintroduce DisTANCE, a perception-centric benchmark for visual estimation tasks.\nEvaluation results show that LVLMs exhibit limited estimation precision, and\ninference-time scaling offers only marginal gains. We attribute this to the\nfast perception paradigm of current LVLMs, where visual understanding is\ntreated as a one-shot output without modeling the underlying perceptual\nprocess. To address this, we propose Perception-Time Scaling (PTS), a novel\nparadigm that encourages token-rich perception and decomposes complex\nperception problems into intermediate tractable sub-problems, thereby enabling\nperception to align with and benefit from inference-time scaling. Combined with\nreinforcement learning techniques, PTS significantly improves perception\naccuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%,\nand generalizes well to out-of-domain tasks. Surprisingly, even though PTS data\nare purely synthetic, combining them with math reasoning data yields consistent\ngains in both reasoning and real-world perception benchmarks. Further analysis\nreveals that PTS introduces more perception-related tokens and increases the\nmodel's attention to image tokens. Our code and data will be publicly released.",
        "url": "http://arxiv.org/abs/2510.08964v1",
        "published_date": "2025-10-10T03:17:52+00:00",
        "updated_date": "2025-10-10T03:17:52+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yifan Li",
            "Zhenghao Chen",
            "Ziheng Wu",
            "Kun Zhou",
            "Ruipu Luo",
            "Can Zhang",
            "Zhentao He",
            "Yufei Zhan",
            "Wayne Xin Zhao",
            "Minghui Qiu"
        ],
        "tldr": "The paper introduces Perception-Time Scaling (PTS), a novel paradigm that improves the perception accuracy of Large Vision-Language Models (LVLMs) by encouraging token-rich perception and decomposing complex perception problems, achieving significant performance gains on visual estimation tasks and generalizing to other benchmarks.",
        "tldr_zh": "该论文介绍了感知时间缩放（PTS），一种通过鼓励token丰富的感知并将复杂的感知问题分解为中间子问题来提高大型视觉语言模型（LVLM）感知准确性的新范式，在视觉估计任务上取得了显著的性能提升，并推广到其他基准。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos",
        "summary": "Recently, Multi-modal Large Language Models (MLLMs) have demonstrated\nsignificant performance across various video understanding tasks. However,\ntheir robustness, particularly when faced with manipulated video content,\nremains largely unexplored. In this paper, we introduce Ro-Bench, the first\nbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)\ncounterfactual video test sets. Ro-Bench incorporates high-quality, diverse and\ntemporally relevant video data, by editing Style, Object, Background and their\ncompositions. We evaluated eight recent video MLLMs and found that current\nmodels exhibit substantial performance degradation on Ro-Bench when exposed to\ncounterfactual video content. Furthermore, we demonstrate that fine-tuning\nMLLMs with counterfactual data enhances robustness, achieving a 21.73%\nperformance increase on Ro-Bench and a 12.78% improvement across 20 tasks in\nthe MVBench dataset. These findings underscore the effectiveness of\ncounterfactual data in enhancing the video understanding ability of MLLMs. The\ncode and data will be released shortly.",
        "url": "http://arxiv.org/abs/2510.08936v1",
        "published_date": "2025-10-10T02:26:48+00:00",
        "updated_date": "2025-10-10T02:26:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zixi Yang",
            "Jiapeng Li",
            "Muxi Diao",
            "Yinuo Jing",
            "Kongming Liang"
        ],
        "tldr": "The paper introduces Ro-Bench, a new benchmark to evaluate the robustness of MLLMs against counterfactual video manipulations, and demonstrates that fine-tuning with counterfactual data improves performance.",
        "tldr_zh": "该论文介绍了Ro-Bench，一个新的基准测试，用于评估MLLM在面对反事实视频操作时的鲁棒性，并表明使用反事实数据进行微调可以提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PHyCLIP: $\\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning",
        "summary": "Vision-language models have achieved remarkable success in multi-modal\nrepresentation learning from large-scale pairs of visual scenes and linguistic\ndescriptions. However, they still struggle to simultaneously express two\ndistinct types of semantic structures: the hierarchy within a concept family\n(e.g., dog $\\preceq$ mammal $\\preceq$ animal) and the compositionality across\ndifferent concept families (e.g., \"a dog in a car\" $\\preceq$ dog, car). Recent\nworks have addressed this challenge by employing hyperbolic space, which\nefficiently captures tree-like hierarchy, yet its suitability for representing\ncompositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,\nwhich employs an $\\ell_1$-Product metric on a Cartesian product of Hyperbolic\nfactors. With our design, intra-family hierarchies emerge within individual\nhyperbolic factors, and cross-family composition is captured by the\n$\\ell_1$-product metric, analogous to a Boolean algebra. Experiments on\nzero-shot classification, retrieval, hierarchical classification, and\ncompositional understanding tasks demonstrate that PHyCLIP outperforms existing\nsingle-space approaches and offers more interpretable structures in the\nembedding space.",
        "url": "http://arxiv.org/abs/2510.08919v1",
        "published_date": "2025-10-10T02:02:22+00:00",
        "updated_date": "2025-10-10T02:02:22+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Daiki Yoshikawa",
            "Takashi Matsubara"
        ],
        "tldr": "PHyCLIP introduces an \\(\\ell_1\\)-Product of Hyperbolic factors to vision-language models to better capture both hierarchical and compositional semantic structures, outperforming existing single-space approaches.",
        "tldr_zh": "PHyCLIP提出了一种视觉语言模型方法，使用双曲因子的\\(\\ell_1\\)-Product，以更好地捕捉层级和组合语义结构，优于现有的单空间方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition",
        "summary": "Video large language models (Vid-LLMs), which excel in diverse video-language\ntasks, can be effectively constructed by adapting image-pretrained\nvision-language models (VLMs). However, this adaptation remains challenging, as\nit requires processing dense and temporally extended visual inputs that exceed\nthe capacity of image-based models. This paper identifies the perception\nbottleneck and token overload as key challenges in extending image-based VLMs\nto the video domain. To address these issues, we propose D-CoDe, a\ntraining-free adaptation framework that incorporates dynamic compression and\nquestion decomposition. Specifically, dynamic compression alleviates the\nperception bottleneck through adaptive selection of representative frames and\ncontent-aware aggregation of spatial tokens, thereby reducing redundancy while\npreserving informative content. In parallel, question decomposition mitigates\ntoken overload by reformulating the original query into sub-questions, guiding\nthe model to focus on distinct aspects of the video and enabling more\ncomprehensive understanding. Experiments demonstrate that D-CoDe effectively\nimproves video understanding across various benchmarks. Furthermore, strong\nperformance on the challenging long-video benchmark highlights the potential of\nD-CoDe in handling complex video-language tasks. Code is available at\nhttps://github.com/hukcc/D-CoDe.",
        "url": "http://arxiv.org/abs/2510.08818v1",
        "published_date": "2025-10-09T21:08:32+00:00",
        "updated_date": "2025-10-09T21:08:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yiyang Huang",
            "Yizhou Wang",
            "Yun Fu"
        ],
        "tldr": "The paper introduces D-CoDe, a training-free framework that adapts image-pretrained VLMs to video by using dynamic compression and question decomposition to address the perception bottleneck and token overload challenges.",
        "tldr_zh": "该论文介绍了D-CoDe，一个免训练框架，通过动态压缩和问题分解来调整图像预训练的VLMs以适应视频，从而解决感知瓶颈和token过载的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization",
        "summary": "Video quality assessment (VQA) is a fundamental computer vision task that\naims to predict the perceptual quality of a given video in alignment with human\njudgments. Existing performant VQA models trained with direct score supervision\nsuffer from (1) poor generalization across diverse content and tasks, ranging\nfrom user-generated content (UGC), short-form videos, to AI-generated content\n(AIGC), (2) limited interpretability, and (3) lack of extensibility to novel\nuse cases or content types. We propose Q-Router, an agentic framework for\nuniversal VQA with a multi-tier model routing system. Q-Router integrates a\ndiverse set of expert models and employs vision--language models (VLMs) as\nreal-time routers that dynamically reason and then ensemble the most\nappropriate experts conditioned on the input video semantics. We build a\nmulti-tiered routing system based on the computing budget, with the heaviest\ntier involving a specific spatiotemporal artifacts localization for\ninterpretability. This agentic design enables Q-Router to combine the\ncomplementary strengths of specialized experts, achieving both flexibility and\nrobustness in delivering consistent performance across heterogeneous video\nsources and tasks. Extensive experiments demonstrate that Q-Router matches or\nsurpasses state-of-the-art VQA models on a variety of benchmarks, while\nsubstantially improving generalization and interpretability. Moreover, Q-Router\nexcels on the quality-based question answering benchmark, Q-Bench-Video,\nhighlighting its promise as a foundation for next-generation VQA systems.\nFinally, we show that Q-Router capably localizes spatiotemporal artifacts,\nshowing potential as a reward function for post-training video generation\nmodels.",
        "url": "http://arxiv.org/abs/2510.08789v2",
        "published_date": "2025-10-09T20:11:38+00:00",
        "updated_date": "2025-10-13T16:16:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuo Xing",
            "Soumik Dey",
            "Mingyang Wu",
            "Ashirbad Mishra",
            "Naveen Ravipati",
            "Binbin Li",
            "Hansi Wu",
            "Zhengzhong Tu"
        ],
        "tldr": "The paper introduces Q-Router, an agentic framework for video quality assessment (VQA) that uses a multi-tier model routing system with VLMs to dynamically select and ensemble expert models, improving generalization, interpretability, and performance across diverse video content.",
        "tldr_zh": "该论文介绍了Q-Router，一个用于视频质量评估（VQA）的agentic框架，它使用具有VLMs的多层模型路由系统来动态选择和集成专家模型，从而提高泛化性、可解释性和在各种视频内容上的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities",
        "summary": "Embodied capabilities refer to a suite of fundamental abilities for an agent\nto perceive, comprehend, and interact with the physical world. While multimodal\nlarge language models (MLLMs) show promise as embodied agents, a thorough and\nsystematic evaluation of their embodied capabilities remains underexplored, as\nexisting benchmarks primarily focus on specific domains such as planning or\nspatial understanding. To bridge this gap, we introduce BEAR, a comprehensive\nand fine-grained benchmark that evaluates MLLMs on atomic embodied\ncapabilities. BEAR comprises 4,469 interleaved image-video-text entries across\n14 domains in 6 categories, including tasks from low-level pointing, trajectory\nunderstanding, spatial reasoning, to high-level planning. Extensive evaluation\nresults of 20 representative MLLMs reveal their persistent limitations across\nall domains of embodied capabilities. To tackle the shortfall, we propose\nBEAR-Agent, a multimodal conversable agent that integrates pretrained vision\nmodels to strengthen MLLM perception, 3D understanding, and planning\ncapabilities. It substantially enhances MLLM performance across diverse\nembodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative\nimprovement of 17.5% on GPT-5. Furthermore, our experiments indicate that\nimproving MLLM embodied capabilities can benefit embodied tasks in simulated\nenvironments. Project website: https://bear-official66.github.io/",
        "url": "http://arxiv.org/abs/2510.08759v1",
        "published_date": "2025-10-09T19:18:36+00:00",
        "updated_date": "2025-10-09T19:18:36+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yu Qi",
            "Haibo Zhao",
            "Ziyu Guo",
            "Siyuan Ma",
            "Ziyan Chen",
            "Yaokun Han",
            "Renrui Zhang",
            "Zitiantao Lin",
            "Shiji Xin",
            "Yijian Huang",
            "Kai Cheng",
            "Peiheng Wang",
            "Jiazheng Liu",
            "Jiayi Zhang",
            "Yizhe Zhu",
            "Wenqing Wang",
            "Yiran Qin",
            "Xupeng Zhu",
            "Haojie Huang",
            "Lawson L. S. Wong"
        ],
        "tldr": "This paper introduces BEAR, a new benchmark for evaluating multimodal language models (MLLMs) on atomic embodied capabilities, and proposes BEAR-Agent, an enhanced MLLM architecture that achieves significant performance improvements on the benchmark.",
        "tldr_zh": "该论文介绍了BEAR，一个用于评估多模态语言模型(MLLMs)在原子具身能力方面的新基准，并提出了BEAR-Agent，一种增强的MLLM架构，在该基准上实现了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
        "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .",
        "url": "http://arxiv.org/abs/2510.09606v1",
        "published_date": "2025-10-10T17:59:46+00:00",
        "updated_date": "2025-10-10T17:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiwen Sun",
            "Shiqiang Lang",
            "Dongming Wu",
            "Yi Ding",
            "Kaituo Feng",
            "Huadai Liu",
            "Zhen Ye",
            "Rui Liu",
            "Yun-Hui Liu",
            "Jianan Wang",
            "Xiangyu Yue"
        ],
        "tldr": "This paper introduces SpaceVista, a dataset and model for all-scale visual spatial reasoning, addressing the limitations of current methods by using a structured knowledge system, scale-aware modeling, and progressive training. They demonstrate strong generalization across various spatial scales and scenarios.",
        "tldr_zh": "本文介绍了SpaceVista，一个用于全尺度视觉空间推理的数据集和模型，通过使用结构化的知识系统、尺度感知建模和渐进式训练，解决了当前方法的局限性。他们展示了在各种空间尺度和场景中的强大泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects",
        "summary": "Recent advances in vision-language-action (VLA) models have greatly improved\nembodied AI, enabling robots to follow natural language instructions and\nperform diverse tasks. However, their reliance on uncurated training datasets\nraises serious security concerns. Existing backdoor attacks on VLAs mostly\nassume white-box access and result in task failures instead of enforcing\nspecific actions. In this work, we reveal a more practical threat: attackers\ncan manipulate VLAs by simply injecting physical objects as triggers into the\ntraining dataset. We propose goal-oriented backdoor attacks (GoBA), where the\nVLA behaves normally in the absence of physical triggers but executes\npredefined and goal-oriented actions in the presence of physical triggers.\nSpecifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO\nthat incorporates diverse physical triggers and goal-oriented backdoor actions.\nIn addition, we propose a three-level evaluation that categorizes the victim\nVLA's actions under GoBA into three states: nothing to do, try to do, and\nsuccess to do. Experiments show that GoBA enables the victim VLA to\nsuccessfully achieve the backdoor goal in 97 percentage of inputs when the\nphysical trigger is present, while causing zero performance degradation on\nclean inputs. Finally, by investigating factors related to GoBA, we find that\nthe action trajectory and trigger color significantly influence attack\nperformance, while trigger size has surprisingly little effect. The code and\nBadLIBERO dataset are accessible via the project page at\nhttps://goba-attack.github.io/.",
        "url": "http://arxiv.org/abs/2510.09269v1",
        "published_date": "2025-10-10T11:09:36+00:00",
        "updated_date": "2025-10-10T11:09:36+00:00",
        "categories": [
            "cs.CR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zirun Zhou",
            "Zhengyang Xiao",
            "Haochuan Xu",
            "Jing Sun",
            "Di Wang",
            "Jingfeng Zhang"
        ],
        "tldr": "This paper introduces a goal-oriented backdoor attack (GoBA) against Vision-Language-Action (VLA) models by injecting physical objects as triggers during training, causing the VLA to execute predefined actions when the trigger is present without affecting performance on clean inputs.",
        "tldr_zh": "本文提出了一种针对视觉-语言-动作 (VLA) 模型的基于目标的后门攻击 (GoBA)，通过在训练期间注入物理对象作为触发器，使 VLA 在存在触发器时执行预定义的动作，而不会影响对干净输入的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Zero-shot image privacy classification with Vision-Language Models",
        "summary": "While specialized learning-based models have historically dominated image\nprivacy prediction, the current literature increasingly favours adopting large\nVision-Language Models (VLMs) designed for generic tasks. This trend risks\noverlooking the performance ceiling set by purpose-built models due to a lack\nof systematic evaluation. To address this problem, we establish a zero-shot\nbenchmark for image privacy classification, enabling a fair comparison. We\nevaluate the top-3 open-source VLMs, according to a privacy benchmark, using\ntask-aligned prompts and we contrast their performance, efficiency, and\nrobustness against established vision-only and multi-modal methods.\nCounter-intuitively, our results show that VLMs, despite their\nresource-intensive nature in terms of high parameter count and slower\ninference, currently lag behind specialized, smaller models in privacy\nprediction accuracy. We also find that VLMs exhibit higher robustness to image\nperturbations.",
        "url": "http://arxiv.org/abs/2510.09253v1",
        "published_date": "2025-10-10T10:50:16+00:00",
        "updated_date": "2025-10-10T10:50:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Alina Elena Baia",
            "Alessio Xompero",
            "Andrea Cavallaro"
        ],
        "tldr": "This paper benchmarks zero-shot image privacy classification using VLMs, finding they currently underperform specialized models in accuracy but exhibit higher robustness to image perturbations.",
        "tldr_zh": "本文对使用视觉语言模型进行零样本图像隐私分类进行了基准测试，发现它们在准确性方面目前不如专门模型，但在对图像扰动的鲁棒性方面表现更好。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition",
        "summary": "Cattle behaviour is a crucial indicator of an individual animal health,\nproductivity and overall well-being. Video-based monitoring, combined with deep\nlearning techniques, has become a mainstream approach in animal biometrics, and\nit can offer high accuracy in some behaviour recognition tasks. We present\nCattle-CLIP, a multimodal deep learning framework for cattle behaviour\nrecognition, using semantic cues to improve the performance of video-based\nvisual feature recognition. It is adapted from the large-scale image-language\nmodel CLIP by adding a temporal integration module. To address the domain gap\nbetween web data used for the pre-trained model and real-world cattle\nsurveillance footage, we introduce tailored data augmentation strategies and\nspecialised text prompts. Cattle-CLIP is evaluated under both fully-supervised\nand few-shot learning scenarios, with a particular focus on data-scarce\nbehaviour recognition - an important yet under-explored goal in livestock\nmonitoring. To evaluate the proposed method, we release the CattleBehaviours6\ndataset, which comprises six types of indoor behaviours: feeding, drinking,\nstanding-self-grooming, standing-ruminating, lying-self-grooming and\nlying-ruminating. The dataset consists of 1905 clips collected from our John\nOldacre Centre dairy farm research platform housing 200 Holstein-Friesian cows.\nExperiments show that Cattle-CLIP achieves 96.1% overall accuracy across six\nbehaviours in a supervised setting, with nearly 100% recall for feeding,\ndrinking and standing-ruminating behaviours, and demonstrates robust\ngeneralisation with limited data in few-shot scenarios, highlighting the\npotential of multimodal learning in agricultural and animal behaviour analysis.",
        "url": "http://arxiv.org/abs/2510.09203v1",
        "published_date": "2025-10-10T09:43:12+00:00",
        "updated_date": "2025-10-10T09:43:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huimin Liu",
            "Jing Gao",
            "Daria Baran",
            "AxelX Montout",
            "Neill W Campbell",
            "Andrew W Dowsey"
        ],
        "tldr": "The paper introduces Cattle-CLIP, a multimodal deep learning framework adapted from CLIP for cattle behavior recognition, using a new dataset (CattleBehaviours6) and achieving high accuracy in both supervised and few-shot settings.",
        "tldr_zh": "该论文介绍了Cattle-CLIP，一个基于CLIP的多模态深度学习框架，用于识别牛的行为。该框架使用了一个新的数据集 (CattleBehaviours6)，并在监督和少样本学习中都取得了较高的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images",
        "summary": "Although recent text-to-image (T2I) models have significantly improved the\noverall visual quality of generated images, they still struggle in the\ngeneration of accurate details in complex local regions, especially human\nhands. Generated hands often exhibit structural distortions and unrealistic\ntextures, which can be very noticeable even when the rest of the body is\nwell-generated. However, the quality assessment of hand regions remains largely\nneglected, limiting downstream task performance like human-centric generation\nquality optimization and AIGC detection. To address this, we propose the first\nquality assessment task targeting generated hand regions and showcase its\nabundant downstream applications. We first introduce the HandPair dataset for\ntraining hand quality assessment models. It consists of 48k images formed by\nhigh- and low-quality hand pairs, enabling low-cost, efficient supervision\nwithout manual annotation. Based on it, we develop HandEval, a carefully\ndesigned hand-specific quality assessment model. It leverages the powerful\nvisual understanding capability of Multimodal Large Language Model (MLLM) and\nincorporates prior knowledge of hand keypoints, gaining strong perception of\nhand quality. We further construct a human-annotated test set with hand images\nfrom various state-of-the-art (SOTA) T2I models to validate its quality\nevaluation capability. Results show that HandEval aligns better with human\njudgments than existing SOTA methods. Furthermore, we integrate HandEval into\nimage generation and AIGC detection pipelines, prominently enhancing generated\nhand realism and detection accuracy, respectively, confirming its universal\neffectiveness in downstream applications. Code and dataset will be available.",
        "url": "http://arxiv.org/abs/2510.08978v1",
        "published_date": "2025-10-10T03:39:10+00:00",
        "updated_date": "2025-10-10T03:39:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zichuan Wang",
            "Bo Peng",
            "Songlin Yang",
            "Zhenchen Tang",
            "Jing Dong"
        ],
        "tldr": "The paper introduces HandEval, a novel hand-specific quality assessment model for generated images, addressing the common problem of unrealistic hands in text-to-image models. It includes a new dataset (HandPair) and demonstrates improved hand realism and AIGC detection accuracy.",
        "tldr_zh": "本文介绍了HandEval，一种用于评估生成图像中手部质量的新型模型，旨在解决文本到图像模型中常见的不真实手部问题。它包含一个新的数据集 (HandPair)，并展示了改进的手部真实感和 AIGC 检测准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation",
        "summary": "Open-vocabulary 3D instance segmentation seeks to segment and classify\ninstances beyond the annotated label space. Existing methods typically map 3D\ninstances to 2D RGB-D images, and then employ vision-language models (VLMs) for\nclassification. However, such a mapping strategy usually introduces noise from\n2D occlusions and incurs substantial computational and memory costs during\ninference, slowing down the inference speed. To address the above problems, we\npropose a Fast Open-vocabulary 3D instance segmentation method via Label-guided\nKnowledge distillation (FOLK). Our core idea is to design a teacher model that\nextracts high-quality instance embeddings and distills its open-vocabulary\nknowledge into a 3D student model. In this way, during inference, the distilled\n3D model can directly classify instances from the 3D point cloud, avoiding\nnoise caused by occlusions and significantly accelerating the inference\nprocess. Specifically, we first design a teacher model to generate a 2D CLIP\nembedding for each 3D instance, incorporating both visibility and viewpoint\ndiversity, which serves as the learning target for distillation. We then\ndevelop a 3D student model that directly produces a 3D embedding for each 3D\ninstance. During training, we propose a label-guided distillation algorithm to\ndistill open-vocabulary knowledge from label-consistent 2D embeddings into the\nstudent model. FOLK conducted experiments on the ScanNet200 and Replica\ndatasets, achieving state-of-the-art performance on the ScanNet200 dataset with\nan AP50 score of 35.7, while running approximately 6.0x to 152.2x faster than\nprevious methods. All codes will be released after the paper is accepted.",
        "url": "http://arxiv.org/abs/2510.08849v1",
        "published_date": "2025-10-09T22:43:26+00:00",
        "updated_date": "2025-10-09T22:43:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongrui Wu",
            "Zhicheng Gao",
            "Jin Cao",
            "Kelu Yao",
            "Wen Shen",
            "Zhihua Wei"
        ],
        "tldr": "The paper introduces FOLK, a fast open-vocabulary 3D instance segmentation method using label-guided knowledge distillation from a 2D teacher to a 3D student model, achieving SOTA performance on ScanNet200 with significant speed improvements.",
        "tldr_zh": "该论文介绍了FOLK，一种快速的开放词汇3D实例分割方法，通过标签引导的知识蒸馏，从2D教师模型到3D学生模型，在ScanNet200上实现了SOTA性能，并显著提高了速度。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering",
        "summary": "Medical Visual Question Answering (Med-VQA) is a challenging task that\nrequires a deep understanding of both medical images and textual questions.\nAlthough recent works leveraging Medical Vision-Language Pre-training (Med-VLP)\nhave shown strong performance on the Med-VQA task, there is still no unified\nsolution for modality alignment, and the issue of hard negatives remains\nunder-explored. Additionally, commonly used knowledge fusion techniques for\nMed-VQA may introduce irrelevant information. In this work, we propose a\nframework to address these challenges through three key contributions: (1) a\nunified solution for heterogeneous modality alignments across multiple levels,\nmodalities, views, and stages, leveraging methods like contrastive learning and\noptimal transport theory; (2) a hard negative mining method that employs soft\nlabels for multi-modality alignments and enforces the hard negative pair\ndiscrimination; and (3) a Gated Cross-Attention Module for Med-VQA that\nintegrates the answer vocabulary as prior knowledge and selects relevant\ninformation from it. Our framework outperforms the previous state-of-the-art on\nwidely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019.",
        "url": "http://arxiv.org/abs/2510.08791v1",
        "published_date": "2025-10-09T20:14:49+00:00",
        "updated_date": "2025-10-09T20:14:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhao Zou",
            "Zhaozheng Yin"
        ],
        "tldr": "This paper introduces a novel framework for Medical VQA that addresses modality alignment, hard negative mining, and knowledge fusion through contrastive learning, optimal transport, and a gated cross-attention mechanism, achieving state-of-the-art results on several Med-VQA datasets.",
        "tldr_zh": "该论文提出了一种新的医学VQA框架，通过对比学习、最优传输和门控交叉注意力机制解决了模态对齐、困难负样本挖掘和知识融合问题，并在多个Med-VQA数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation",
        "summary": "Adverse weather conditions such as haze, rain, and snow significantly degrade\nthe quality of images and videos, posing serious challenges to intelligent\ntransportation systems (ITS) that rely on visual input. These degradations\naffect critical applications including autonomous driving, traffic monitoring,\nand surveillance. This survey presents a comprehensive review of image and\nvideo restoration techniques developed to mitigate weather-induced visual\nimpairments. We categorize existing approaches into traditional prior-based\nmethods and modern data-driven models, including CNNs, transformers, diffusion\nmodels, and emerging vision-language models (VLMs). Restoration strategies are\nfurther classified based on their scope: single-task models,\nmulti-task/multi-weather systems, and all-in-one frameworks capable of handling\ndiverse degradations. In addition, we discuss day and night time restoration\nchallenges, benchmark datasets, and evaluation protocols. The survey concludes\nwith an in-depth discussion on limitations in current research and outlines\nfuture directions such as mixed/compound-degradation restoration, real-time\ndeployment, and agentic AI frameworks. This work aims to serve as a valuable\nreference for advancing weather-resilient vision systems in smart\ntransportation environments. Lastly, to stay current with rapid advancements in\nthis field, we will maintain regular updates of the latest relevant papers and\ntheir open-source implementations at\nhttps://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration",
        "url": "http://arxiv.org/abs/2510.09228v1",
        "published_date": "2025-10-10T10:15:59+00:00",
        "updated_date": "2025-10-10T10:15:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Vijay M. Galshetwar",
            "Praful Hambarde",
            "Prashant W. Patil",
            "Akshay Dudhane",
            "Sachin Chaudhary",
            "Santosh Kumar Vipparathi",
            "Subrahmanyam Murala"
        ],
        "tldr": "This survey paper comprehensively reviews image and video restoration techniques for adverse weather conditions in intelligent transportation systems, covering traditional methods, deep learning models including VLMs, and future research directions.",
        "tldr_zh": "该综述论文全面回顾了智能交通系统中针对恶劣天气条件的图像和视频修复技术，涵盖了传统方法、包括VLM在内的深度学习模型以及未来的研究方向。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "SAM2-3dMed: Empowering SAM2 for 3D Medical Image Segmentation",
        "summary": "Accurate segmentation of 3D medical images is critical for clinical\napplications like disease assessment and treatment planning. While the Segment\nAnything Model 2 (SAM2) has shown remarkable success in video object\nsegmentation by leveraging temporal cues, its direct application to 3D medical\nimages faces two fundamental domain gaps: 1) the bidirectional anatomical\ncontinuity between slices contrasts sharply with the unidirectional temporal\nflow in videos, and 2) precise boundary delineation, crucial for morphological\nanalysis, is often underexplored in video tasks. To bridge these gaps, we\npropose SAM2-3dMed, an adaptation of SAM2 for 3D medical imaging. Our framework\nintroduces two key innovations: 1) a Slice Relative Position Prediction (SRPP)\nmodule explicitly models bidirectional inter-slice dependencies by guiding SAM2\nto predict the relative positions of different slices in a self-supervised\nmanner; 2) a Boundary Detection (BD) module enhances segmentation accuracy\nalong critical organ and tissue boundaries. Extensive experiments on three\ndiverse medical datasets (the Lung, Spleen, and Pancreas in the Medical\nSegmentation Decathlon (MSD) dataset) demonstrate that SAM2-3dMed significantly\noutperforms state-of-the-art methods, achieving superior performance in\nsegmentation overlap and boundary precision. Our approach not only advances 3D\nmedical image segmentation performance but also offers a general paradigm for\nadapting video-centric foundation models to spatial volumetric data.",
        "url": "http://arxiv.org/abs/2510.08967v1",
        "published_date": "2025-10-10T03:23:05+00:00",
        "updated_date": "2025-10-10T03:23:05+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yeqing Yang",
            "Le Xu",
            "Lixia Tian"
        ],
        "tldr": "The paper introduces SAM2-3dMed, an adaptation of the Segment Anything Model 2 (SAM2) for 3D medical image segmentation, addressing the differences between video and 3D medical data with Slice Relative Position Prediction and Boundary Detection modules, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了 SAM2-3dMed，这是对 Segment Anything Model 2 (SAM2) 的改进，用于 3D 医疗图像分割。它通过切片相对位置预测和边界检测模块来解决视频数据和 3D 医疗数据之间的差异，并实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]