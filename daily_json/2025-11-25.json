[
    {
        "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
        "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
        "url": "http://arxiv.org/abs/2511.19046v1",
        "published_date": "2025-11-24T12:34:38+00:00",
        "updated_date": "2025-11-24T12:34:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Anglin Liu",
            "Rundong Xue",
            "Xu R. Cao",
            "Yifan Shen",
            "Yi Lu",
            "Xiang Li",
            "Qianqian Chen",
            "Jintai Chen"
        ],
        "tldr": "MedSAM3 fine-tunes SAM with medical images and semantic labels to enable text-promptable medical image segmentation, outperforming existing models across various modalities and introducing an agent-in-the-loop workflow for complex reasoning.",
        "tldr_zh": "MedSAM3通过使用医学图像和语义标签对SAM进行微调，实现了文本提示的医学图像分割，在各种模式下优于现有模型，并引入了一个代理在环工作流程来进行复杂的推理。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Health system learning achieves generalist neuroimaging models",
        "summary": "Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.",
        "url": "http://arxiv.org/abs/2511.18640v1",
        "published_date": "2025-11-23T22:34:50+00:00",
        "updated_date": "2025-11-23T22:34:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Akhil Kondepudi",
            "Akshay Rao",
            "Chenhui Zhao",
            "Yiwei Lyu",
            "Samir Harake",
            "Soumyanil Banerjee",
            "Rushikesh Joshi",
            "Anna-Katharina Meissner",
            "Renly Hou",
            "Cheng Jiang",
            "Asadur Chowdury",
            "Ashok Srinivasan",
            "Brian Athey",
            "Vikas Gulani",
            "Aditya Pandey",
            "Honglak Lee",
            "Todd Hollon"
        ],
        "tldr": "The paper introduces NeuroVFM, a visual foundation model trained on a large neuroimaging dataset derived from health systems, demonstrating state-of-the-art performance in various clinical tasks, including report generation, and highlighting the potential of health system learning for building generalist medical AI.",
        "tldr_zh": "该论文介绍了NeuroVFM，一个基于大型医疗系统神经影像数据集训练的视觉基础模型，在包括报告生成在内的多种临床任务中表现出最先进的性能，并突出了健康系统学习在构建通用医疗人工智能方面的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning",
        "summary": "RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.",
        "url": "http://arxiv.org/abs/2511.19343v1",
        "published_date": "2025-11-24T17:42:29+00:00",
        "updated_date": "2025-11-24T17:42:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qihan Huang",
            "Haofei Zhang",
            "Rong Wei",
            "Yi Wang",
            "Rui Tang",
            "Mingli Song",
            "Jie Song"
        ],
        "tldr": "The paper introduces Syn-GRPO, a method for improving MLLM perception by using an online data generator to synthesize high-quality, diverse training data for reinforcement learning, showing significant performance improvements in visual perception tasks.",
        "tldr_zh": "该论文介绍了 Syn-GRPO，一种通过使用在线数据生成器为强化学习合成高质量、多样化的训练数据来提高 MLLM 感知能力的方法，并在视觉感知任务中表现出显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ReMatch: Boosting Representation through Matching for Multimodal Retrieval",
        "summary": "We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.",
        "url": "http://arxiv.org/abs/2511.19278v1",
        "published_date": "2025-11-24T16:28:49+00:00",
        "updated_date": "2025-11-24T16:28:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qianying Liu",
            "Xiao Liang",
            "Zhiqiang Zhang",
            "Yibo Chen",
            "Xu Tang",
            "Zhongfei Qing",
            "Fengfan Zhou",
            "Yao Hu",
            "Paul Henderson"
        ],
        "tldr": "ReMatch introduces a framework that leverages MLLMs for multimodal retrieval by using a generative matching stage for stronger supervision and fine-grained embeddings, achieving SOTA results on MMEB and showing strong zero-shot generalization.",
        "tldr_zh": "ReMatch 引入了一个利用 MLLM 进行多模态检索的框架，通过生成式匹配阶段实现更强的监督和细粒度嵌入，在 MMEB 上取得了 SOTA 结果，并显示出强大的零样本泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment",
        "summary": "Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.",
        "url": "http://arxiv.org/abs/2511.19268v1",
        "published_date": "2025-11-24T16:20:11+00:00",
        "updated_date": "2025-11-24T16:20:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dewei Zhou",
            "Mingwei Li",
            "Zongxin Yang",
            "Yu Lu",
            "Yunqiu Xu",
            "Zhizhong Wang",
            "Zeyi Huang",
            "Yi Yang"
        ],
        "tldr": "The paper introduces BideDPO, a novel framework for conditional image generation that addresses conflicts between text prompts and conditioning images by using a bidirectionally decoupled DPO and an adaptive loss balancing strategy. It significantly improves text success rates and condition adherence.",
        "tldr_zh": "该论文介绍了BideDPO，一种用于条件图像生成的新框架，通过使用双向解耦的DPO和自适应损失平衡策略来解决文本提示和条件图像之间的冲突。 它显著提高了文本成功率和条件一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models",
        "summary": "Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.",
        "url": "http://arxiv.org/abs/2511.19261v1",
        "published_date": "2025-11-24T16:13:26+00:00",
        "updated_date": "2025-11-24T16:13:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Wang",
            "Daoan Zhang",
            "Tianyi Bai",
            "Shitong Shao",
            "Jiebo Luo",
            "Jiaheng Wei"
        ],
        "tldr": "The paper introduces LAST, a method for enhancing general Vision-Language Models (VLMs) to better understand 3D space and long videos by enabling them to \"think\" in space and time using 2D images as input, demonstrating significant improvements on spatial, video, and image understanding benchmarks.",
        "tldr_zh": "该论文介绍了一种名为LAST的方法，旨在通过让通用视觉语言模型（VLM）使用2D图像输入在空间和时间中“思考”，从而增强它们对3D空间和长视频的理解，并在空间、视频和图像理解基准测试中展示了显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization",
        "summary": "Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.",
        "url": "http://arxiv.org/abs/2511.19248v1",
        "published_date": "2025-11-24T16:02:01+00:00",
        "updated_date": "2025-11-24T16:02:01+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Md Akil Raihan Iftee",
            "Syed Md. Ahnaf Hasan",
            "Amin Ahsan Ali",
            "AKM Mahbubur Rahman",
            "Sajib Mistry",
            "Aneesh Krishna"
        ],
        "tldr": "The paper introduces FedPoisonTTP, a novel grey-box attack framework targeting test-time personalization in federated learning, demonstrating the vulnerability of these systems to data poisoning during local adaptation.",
        "tldr_zh": "该论文介绍了FedPoisonTTP，一种针对联邦学习中测试时个性化的新型灰盒攻击框架，展示了这些系统在本地适应期间对数据中毒的脆弱性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
        "summary": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
        "url": "http://arxiv.org/abs/2511.19221v1",
        "published_date": "2025-11-24T15:28:25+00:00",
        "updated_date": "2025-11-24T15:28:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianhua Han",
            "Meng Tian",
            "Jiangtong Zhu",
            "Fan He",
            "Huixin Zhang",
            "Sitong Guo",
            "Dechang Zhu",
            "Hao Tang",
            "Pei Xu",
            "Yuze Guo",
            "Minzhe Niu",
            "Haojie Zhu",
            "Qichao Dong",
            "Xuechao Yan",
            "Siyuan Dong",
            "Lu Hou",
            "Qingqiu Huang",
            "Xiaosong Jia",
            "Hang Xu"
        ],
        "tldr": "Percept-WAM is a novel vision-language model that integrates 2D/3D scene understanding abilities for robust autonomous driving by unifying perception tasks into World-PV and World-BEV tokens, achieving state-of-the-art performance on downstream perception and planning benchmarks.",
        "tldr_zh": "Percept-WAM是一种新型的视觉语言模型，它通过将感知任务统一为World-PV和World-BEV tokens，集成了2D/3D场景理解能力，实现了鲁棒的自动驾驶，并在下游感知和规划基准测试中取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering",
        "summary": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.",
        "url": "http://arxiv.org/abs/2511.19220v1",
        "published_date": "2025-11-24T15:26:58+00:00",
        "updated_date": "2025-11-24T15:26:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Federico Felizzi",
            "Olivia Riccomi",
            "Michele Ferramola",
            "Francesco Andrea Causio",
            "Manuel Del Medico",
            "Vittorio De Vita",
            "Lorenzo De Mori",
            "Alessandra Piscitelli Pietro Eric Risuleo",
            "Bianca Destro Castaniti",
            "Antonio Cristiano Alessia Longo",
            "Luigi De Angelis",
            "Mariapia Vassalli",
            "Marcello Di Pumpo"
        ],
        "tldr": "This paper investigates the visual grounding capabilities of several large vision language models (VLMs) on Italian medical visual question answering, revealing significant differences in their reliance on visual information versus textual shortcuts, raising concerns about their robustness for clinical applications.",
        "tldr_zh": "本文研究了几种大型视觉语言模型（VLMs）在意大利医学视觉问答中的视觉基础能力，揭示了它们对视觉信息与文本捷径的依赖程度存在显著差异，引发了对其在临床应用中鲁棒性的担忧。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?",
        "summary": "Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired \"real\"/\"lookalike\" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.",
        "url": "http://arxiv.org/abs/2511.19200v1",
        "published_date": "2025-11-24T15:09:32+00:00",
        "updated_date": "2025-11-24T15:09:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Itay Cohen",
            "Ethan Fetaya",
            "Amir Rosenfeld"
        ],
        "tldr": "The paper investigates if vision-language models can distinguish between real objects and their look-alikes using a newly curated dataset (RoLA), and proposes a method to improve this distinction within CLIP's embedding space.",
        "tldr_zh": "该论文研究了视觉-语言模型是否能够区分真实物体和它们的仿制品，使用了一个新策划的数据集(RoLA)，并提出了一种方法来提高CLIP嵌入空间中的这种区分能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models",
        "summary": "Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.",
        "url": "http://arxiv.org/abs/2511.18978v1",
        "published_date": "2025-11-24T10:50:30+00:00",
        "updated_date": "2025-11-24T10:50:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Santiago Moreno",
            "Pablo Meseguer",
            "Rocío del Amor",
            "Valery Naranjo"
        ],
        "tldr": "This paper presents ZEUS, a zero-shot visual-language segmentation pipeline for skin tumor detection in whole-slide images, utilizing textual prompt ensembles and frozen VLMs to generate high-resolution tumor masks without pixel-level labels.",
        "tldr_zh": "本文提出了ZEUS，一个用于在全切片图像中进行皮肤肿瘤零样本分割的视觉语言分割流程，它利用文本提示集成和冻结的VLM生成高分辨率肿瘤掩模，无需像素级标签。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
        "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
        "url": "http://arxiv.org/abs/2511.18960v1",
        "published_date": "2025-11-24T10:22:28+00:00",
        "updated_date": "2025-11-24T10:22:28+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Lei Xiao",
            "Jifeng Li",
            "Juntao Gao",
            "Feiyang Ye",
            "Yan Jin",
            "Jingjing Qian",
            "Jing Zhang",
            "Yong Wu",
            "Xiaoyuan Yu"
        ],
        "tldr": "The paper introduces AVA-VLA, a novel Vision-Language-Action framework that uses Active Visual Attention (AVA) to dynamically modulate visual processing based on the agent's belief state, achieving state-of-the-art performance on robotic benchmarks and demonstrating sim-to-real transferability.",
        "tldr_zh": "该论文提出了一种新的视觉-语言-动作框架AVA-VLA，它使用主动视觉注意力（AVA）根据agent的belief state动态调节视觉处理，在机器人基准测试中实现了最先进的性能，并展示了从仿真到现实的迁移能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models",
        "summary": "Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \\textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\\% yielding over 90\\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .",
        "url": "http://arxiv.org/abs/2511.18921v1",
        "published_date": "2025-11-24T09:30:38+00:00",
        "updated_date": "2025-11-24T09:30:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Juncheng Li",
            "Yige Li",
            "Hanxun Huang",
            "Yunhao Chen",
            "Xin Wang",
            "Yixu Wang",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "tldr": "This paper introduces BackdoorVLM, a benchmark for evaluating backdoor attacks on Vision-Language Models, revealing vulnerabilities, especially through textual manipulation, that significantly impact VLM security.",
        "tldr_zh": "本文介绍了 BackdoorVLM，一个用于评估视觉语言模型后门攻击的基准，揭示了漏洞，特别是通过文本操纵，对 VLM 安全性产生重大影响。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models",
        "summary": "Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.",
        "url": "http://arxiv.org/abs/2511.18920v1",
        "published_date": "2025-11-24T09:30:02+00:00",
        "updated_date": "2025-11-24T09:30:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhao Xu",
            "Xin Dong",
            "Yue Li",
            "Haoyuan Shi",
            "Zhiwei Xiong"
        ],
        "tldr": "The paper introduces EventSTU, a training-free framework that uses event-based vision to efficiently reduce the computational cost of video large language models by keyframe sampling and adaptive token pruning, achieving significant FLOPs reduction and speedup while improving performance.",
        "tldr_zh": "该论文介绍了EventSTU，一个无需训练的框架，它利用基于事件的视觉技术，通过关键帧采样和自适应token剪枝来有效降低视频大型语言模型的计算成本，在提高性能的同时，实现了显著的FLOPs降低和加速。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference",
        "summary": "Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.",
        "url": "http://arxiv.org/abs/2511.18875v1",
        "published_date": "2025-11-24T08:29:36+00:00",
        "updated_date": "2025-11-24T08:29:36+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Wengyi Zhan",
            "Mingbao Lin",
            "Zhihang Lin",
            "Rongrong Ji"
        ],
        "tldr": "This paper introduces ParVTS, a training-free framework that selectively prunes visual tokens in MLLMs to accelerate inference by partitioning and parallel processing visual tokens, achieving significant speedup with minimal accuracy loss.",
        "tldr_zh": "本文介绍了一种名为ParVTS的无需训练的框架，该框架通过分割和并行处理视觉tokens来选择性地修剪MLLM中的视觉tokens，从而加速推理，并在最小的精度损失下实现显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models",
        "summary": "We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct \"key-information-missing\" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.",
        "url": "http://arxiv.org/abs/2511.18823v1",
        "published_date": "2025-11-24T06:57:26+00:00",
        "updated_date": "2025-11-24T06:57:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fufangchen Zhao",
            "Liao Zhang",
            "Daiqi Shi",
            "Yuanjun Gao",
            "Chen Ye",
            "Yang Cai",
            "Jian Gao",
            "Danfeng Yan"
        ],
        "tldr": "VideoPerceiver enhances fine-grained temporal perception in video LLMs using a two-stage training approach involving key frame manipulation and relative reward RL, demonstrating superior performance on fine-grained action understanding and rare event captioning.",
        "tldr_zh": "VideoPerceiver通过一个两阶段训练方法，包括关键帧操作和相对奖励强化学习，来增强视频LLM中对细粒度时间感知的理解，并在细粒度动作理解和罕见事件字幕生成方面表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring",
        "summary": "3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.",
        "url": "http://arxiv.org/abs/2511.18817v1",
        "published_date": "2025-11-24T06:51:34+00:00",
        "updated_date": "2025-11-24T06:51:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyuan Wei",
            "Chunjie Wang",
            "Xiao Liu",
            "Xiaosheng Yan",
            "Zhishan Zhou",
            "Rui Huang"
        ],
        "tldr": "The paper introduces Disc3D, an automated pipeline for generating high-quality 3D scene-dialogue data, mitigating ambiguities in object referring and viewpoint, and resulting in a large dataset used to improve 3D MLLMs.",
        "tldr_zh": "该论文介绍了Disc3D，一个自动生成高质量3D场景对话数据的流程，解决了物体指代和视点方面的歧义，并生成了一个大型数据集，用于改进3D多模态大型语言模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding Task Transfer in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.",
        "url": "http://arxiv.org/abs/2511.18787v1",
        "published_date": "2025-11-24T05:37:52+00:00",
        "updated_date": "2025-11-24T05:37:52+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Bhuvan Sachdeva",
            "Karan Uppal",
            "Abhinav Java",
            "Vineeth N. Balasubramanian"
        ],
        "tldr": "This paper systematically studies task transferability in VLMs, introducing a metric (PGF) to quantify transfer effects across perception tasks, revealing relationships and offering guidance for efficient training.",
        "tldr_zh": "本文系统研究了视觉-语言模型中的任务迁移能力，引入了一个指标 (PGF) 来量化感知任务之间的迁移效应，揭示了任务关系并为高效训练提供了指导。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion",
        "summary": "Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.",
        "url": "http://arxiv.org/abs/2511.18742v1",
        "published_date": "2025-11-24T04:10:53+00:00",
        "updated_date": "2025-11-24T04:10:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zhenghan Fang",
            "Jian Zheng",
            "Qiaozi Gao",
            "Xiaofeng Gao",
            "Jeremias Sulam"
        ],
        "tldr": "The paper introduces ProxT2I, a text-to-image diffusion model using backward discretizations and proximal operators, optimized with reinforcement learning for efficiency and human preference alignment. It also presents a new large-scale dataset, LAION-Face-T2I-15M.",
        "tldr_zh": "该论文介绍了ProxT2I，一种使用后向离散化和近端算子的文本到图像扩散模型，通过强化学习进行优化，以提高效率和对齐人类偏好。它还提出了一个新的大型数据集LAION-Face-T2I-15M。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Thinking Ahead: Foresight Intelligence in MLLMs and World Models",
        "summary": "In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.",
        "url": "http://arxiv.org/abs/2511.18735v1",
        "published_date": "2025-11-24T04:04:59+00:00",
        "updated_date": "2025-11-24T04:04:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhantao Gong",
            "Liaoyuan Fan",
            "Qing Guo",
            "Xun Xu",
            "Xulei Yang",
            "Shijie Li"
        ],
        "tldr": "The paper introduces FSU-QA, a new VQA dataset for evaluating foresight intelligence in VLMs, showing current models struggle with future reasoning and that fine-tuning on FSU-QA improves performance.",
        "tldr_zh": "该论文介绍了 FSU-QA，一个新的 VQA 数据集，用于评估 VLM 中的前瞻智能。结果表明，目前的模型在未来推理方面表现不佳，并且在 FSU-QA 上进行微调可以提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking",
        "summary": "Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.",
        "url": "http://arxiv.org/abs/2511.18692v1",
        "published_date": "2025-11-24T02:27:19+00:00",
        "updated_date": "2025-11-24T02:27:19+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.PF"
        ],
        "authors": [
            "Kichang Yang",
            "Seonjun Kim",
            "Minjae Kim",
            "Nairan Zhang",
            "Chi Zhang",
            "Youngki Lee"
        ],
        "tldr": "This paper introduces Neuron Chunking, an I/O-efficient sparsification strategy for VLMs that considers storage access costs, improving I/O efficiency on edge devices.",
        "tldr_zh": "该论文介绍了一种名为神经元分块(Neuron Chunking)的 I/O 高效 VLM 稀疏化策略，该策略考虑了存储访问成本，从而提高了边缘设备的 I/O 效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis",
        "summary": "Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., \"Is this normal or abnormal?\") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.",
        "url": "http://arxiv.org/abs/2511.18676v1",
        "published_date": "2025-11-24T01:26:07+00:00",
        "updated_date": "2025-11-24T01:26:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yongcheng Yao",
            "Yongshuo Zong",
            "Raman Dutt",
            "Yongxin Yang",
            "Sotirios A Tsaftaris",
            "Timothy Hospedales"
        ],
        "tldr": "The paper introduces MedVision, a large-scale dataset and benchmark for quantitative medical image analysis, demonstrating the limitations of current VLMs and showing performance improvements after fine-tuning.",
        "tldr_zh": "该论文介绍了MedVision，一个用于定量医学图像分析的大规模数据集和基准，展示了当前VLM的局限性，并展示了微调后性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations",
        "summary": "AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.",
        "url": "http://arxiv.org/abs/2511.18617v1",
        "published_date": "2025-11-23T21:21:10+00:00",
        "updated_date": "2025-11-23T21:21:10+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Litian Gong",
            "Fatemeh Bahrani",
            "Yutai Zhou",
            "Amin Banayeeanzade",
            "Jiachen Li",
            "Erdem Biyik"
        ],
        "tldr": "AutoFocus-IL improves visual imitation learning by using VLMs to generate saliency maps that guide policies to focus on task-relevant features, outperforming standard behavior cloning and even methods with human supervision.",
        "tldr_zh": "AutoFocus-IL 通过使用视觉语言模型（VLMs）生成显著性图，引导策略关注任务相关的特征，从而改进了视觉模仿学习，其性能优于标准的行为克隆方法，甚至优于具有人工监督的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation",
        "summary": "Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.",
        "url": "http://arxiv.org/abs/2511.18591v1",
        "published_date": "2025-11-23T19:08:45+00:00",
        "updated_date": "2025-11-23T19:08:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Dong",
            "Han Zhou",
            "Junwei Lin",
            "Jun Chen"
        ],
        "tldr": "This paper introduces a zero-reference, unsupervised generative framework for simultaneous low-light enhancement and deblurring, utilizing visual autoregressive modeling modulated by VLM-derived perceptual priors and a recursive phase-domain refinement strategy. It claims state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "本文提出了一种零参考、无监督的生成框架，用于同时进行低光增强和去模糊处理。该框架利用视觉自回归建模，并通过 VLM 导出的感知先验和递归相位域细化策略进行调制。该论文声称在基准数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Growing with the Generator: Self-paced GRPO for Video Generation",
        "summary": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.",
        "url": "http://arxiv.org/abs/2511.19356v1",
        "published_date": "2025-11-24T17:56:03+00:00",
        "updated_date": "2025-11-24T17:56:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Li",
            "Yuanzhi Liang",
            "Ziqi Ni",
            "Haibing Huang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "tldr": "This paper introduces Self-Paced GRPO, a novel reinforcement learning framework for video generation where the reward feedback co-evolves with the generator, leading to improved visual quality and semantic alignment compared to static reward approaches.",
        "tldr_zh": "本文提出了一种名为Self-Paced GRPO的视频生成新型强化学习框架，其中奖励反馈与生成器共同进化，与静态奖励方法相比，提高了视觉质量和语义对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation",
        "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.",
        "url": "http://arxiv.org/abs/2511.19147v1",
        "published_date": "2025-11-24T14:12:22+00:00",
        "updated_date": "2025-11-24T14:12:22+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Huisoo Lee",
            "Jisu Han",
            "Hyunsouk Cho",
            "Wonjun Hwang"
        ],
        "tldr": "This paper introduces a Collaborative Multi-foundation Adaptation (CoMA) framework for Source-Free Domain Adaptation (SFDA), leveraging multiple Foundation Models (FMs) and a bidirectional adaptation mechanism to improve performance across various benchmarks.",
        "tldr_zh": "本文提出了一种用于无源域适应 (SFDA) 的协作多基础模型适应 (CoMA) 框架，该框架利用多个基础模型 (FM) 和双向适应机制来提高在各种基准测试中的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay",
        "summary": "Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.",
        "url": "http://arxiv.org/abs/2511.19033v1",
        "published_date": "2025-11-24T12:13:05+00:00",
        "updated_date": "2025-11-24T12:13:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gengyuan Zhang",
            "Mingcong Ding",
            "Jingpei Wu",
            "Ruotong Liao",
            "Volker Tresp"
        ],
        "tldr": "This paper introduces ReEXplore, a training-free framework that enhances MLLM-based embodied exploration by using retrospective experience replay and hierarchical frontier selection, achieving significant performance improvements on multiple benchmarks.",
        "tldr_zh": "本文介绍了 ReEXplore，一个无需训练的框架，通过使用回顾性经验回放和分层边界选择来增强基于 MLLM 的具身探索，并在多个基准测试上实现了显著的性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric",
        "summary": "Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.",
        "url": "http://arxiv.org/abs/2511.19032v1",
        "published_date": "2025-11-24T12:07:56+00:00",
        "updated_date": "2025-11-24T12:07:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangjie Sui",
            "Songyang Li",
            "Hanwei Zhu",
            "Baoliang Chen",
            "Yuming Fang",
            "Xin Sun"
        ],
        "tldr": "This paper introduces Bench-C, a new benchmark for evaluating the robustness of LVLMs to visual corruptions, along with a novel metric, RAS, to better capture the degradation of prediction structure. The study reveals interesting model behaviors under corruption and provides a more nuanced understanding of robustness.",
        "tldr_zh": "该论文介绍了Bench-C，一个新的用于评估LVLM对视觉腐败的鲁棒性的基准，以及一个新的度量标准RAS，以更好地捕捉预测结构的退化。 该研究揭示了模型在腐败下的有趣行为，并提供了对鲁棒性的更细致的理解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery",
        "summary": "Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.",
        "url": "http://arxiv.org/abs/2511.18968v1",
        "published_date": "2025-11-24T10:34:12+00:00",
        "updated_date": "2025-11-24T10:34:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bhuvan Sachdeva",
            "Sneha Kumari",
            "Rudransh Agarwal",
            "Shalaka Kumaraswamy",
            "Niharika Singri Prasad",
            "Simon Mueller",
            "Raphael Lechtenboehmer",
            "Maximilian W. M. Wintergerst",
            "Thomas Schultz",
            "Kaushik Murali",
            "Mohit Jain"
        ],
        "tldr": "The paper introduces CataractCompDetect, a framework for detecting intraoperative complications during cataract surgery using phase-aware localization, SAM 2-based tracking, risk scoring, and vision-language reasoning. They also present a new dataset, CataComp, for validation.",
        "tldr_zh": "该论文介绍了CataractCompDetect，一个使用相位感知定位、基于SAM 2的跟踪、风险评分和视觉-语言推理来检测白内障手术期间术中并发症的框架。他们还提出了一个新的数据集CataComp用于验证。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
        "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2511.18950v1",
        "published_date": "2025-11-24T10:06:41+00:00",
        "updated_date": "2025-11-24T10:06:41+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Juntao Gao",
            "Feiyang Ye",
            "Jing Zhang",
            "Wenjing Qian"
        ],
        "tldr": "The paper introduces Compressor-VLA, a novel instruction-conditioned visual token compression framework for VLA models, aiming to reduce computational overhead in robotic manipulation by adaptively condensing task-relevant visual information, achieving significant FLOPs reduction and maintaining competitive success rates.",
        "tldr_zh": "该论文介绍了Compressor-VLA，一种新颖的指令条件视觉token压缩框架，用于VLA模型，旨在通过自适应地压缩任务相关的视觉信息来减少机器人操作中的计算开销，从而显著减少FLOPs并保持具有竞争力的成功率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration",
        "summary": "Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.",
        "url": "http://arxiv.org/abs/2511.18847v1",
        "published_date": "2025-11-24T07:40:04+00:00",
        "updated_date": "2025-11-24T07:40:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ishmam Tashdeed",
            "Md. Atiqur Rahman",
            "Sabrina Islam",
            "Md. Azam Hossain"
        ],
        "tldr": "This paper introduces FedOAP, a personalized federated learning approach for organ-agnostic tumor segmentation that leverages cross-attention to model shared features and a boundary-aware loss to improve segmentation consistency across clients.",
        "tldr_zh": "本文介绍了一种名为FedOAP的个性化联邦学习方法，用于器官不可知的肿瘤分割。该方法利用交叉注意力来建模共享特征，并使用边界感知损失来提高客户端之间分割的一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction",
        "summary": "The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\\% points using only 0.13\\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\\% of the training data-outperforming zero-shot baseline by 10.61\\%.",
        "url": "http://arxiv.org/abs/2511.18831v1",
        "published_date": "2025-11-24T07:07:58+00:00",
        "updated_date": "2025-11-24T07:07:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaobo Wang",
            "Tianle Niu",
            "Runkang Yang",
            "Deshan Liu",
            "Xu He",
            "Zichen Wen",
            "Conghui He",
            "Xuming Hu",
            "Linfeng Zhang"
        ],
        "tldr": "VideoCompressa introduces a novel video data synthesis framework that significantly improves data efficiency by jointly optimizing keyframe selection and latent code compression, achieving state-of-the-art performance with minimal data usage.",
        "tldr_zh": "VideoCompressa 提出了一种新颖的视频数据合成框架，通过联合优化关键帧选择和潜在代码压缩来显著提高数据效率，并以最小的数据用量实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache",
        "summary": "Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\\% mAP gain on rare categories and +4.39\\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.",
        "url": "http://arxiv.org/abs/2511.18811v1",
        "published_date": "2025-11-24T06:30:08+00:00",
        "updated_date": "2025-11-24T06:30:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuqiu Jiang",
            "Xiaozhen Qiao",
            "Tianyu Mei",
            "Haojian Huang",
            "Yifan Chen",
            "Ye Zheng",
            "Zhe Sun"
        ],
        "tldr": "This paper introduces a training-free, plug-and-play module called Adaptive Diversity Cache (ADC) to mitigate long-tail bias in VLM-based HOI detection, showing significant mAP improvements on rare categories and the full dataset.",
        "tldr_zh": "本文介绍了一种名为自适应多样性缓存 (ADC) 的免训练、即插即用模块，旨在减轻基于 VLM 的 HOI 检测中的长尾偏差，并在罕见类别和完整数据集上显示出显着的 mAP 改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation",
        "summary": "Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.",
        "url": "http://arxiv.org/abs/2511.18684v1",
        "published_date": "2025-11-24T01:48:44+00:00",
        "updated_date": "2025-11-24T01:48:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shristi Das Biswas",
            "Arani Roy",
            "Kaushik Roy"
        ],
        "tldr": "This paper introduces Instant Concept Erasure (ICE), a training-free method for removing concepts from text-to-image and text-to-video models, ensuring safety and robustness without retraining or inference overhead.",
        "tldr_zh": "本文介绍了即时概念擦除（ICE），这是一种无需训练的方法，用于从文本到图像和文本到视频模型中删除概念，确保安全性和鲁棒性，而无需重新训练或推理开销。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment",
        "summary": "Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.19217v1",
        "published_date": "2025-11-24T15:23:36+00:00",
        "updated_date": "2025-11-24T15:23:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wanjiang Weng",
            "Xiaofeng Tan",
            "Junbo Wang",
            "Guo-Sen Xie",
            "Pan Zhou",
            "Hongsong Wang"
        ],
        "tldr": "The paper introduces ReAlign, a reward-guided sampling method for text-to-motion generation that improves text-motion alignment and motion quality by using a step-aware reward model within a diffusion framework.",
        "tldr_zh": "该论文介绍了ReAlign，一种文本到动作生成的奖励引导采样方法，通过在扩散框架中使用步进感知奖励模型来提高文本-动作对齐和动作质量。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation",
        "summary": "The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.",
        "url": "http://arxiv.org/abs/2511.19071v1",
        "published_date": "2025-11-24T13:07:22+00:00",
        "updated_date": "2025-11-24T13:07:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fangda Chen",
            "Jintao Tang",
            "Pancheng Wang",
            "Ting Wang",
            "Shasha Li",
            "Ting Deng"
        ],
        "tldr": "The paper introduces DEAP-3DSAM, a novel approach to 3D medical image segmentation using an enhanced decoder and automatic prompting for SAM, achieving state-of-the-art performance on abdominal tumor segmentation datasets.",
        "tldr_zh": "该论文介绍了DEAP-3DSAM，一种新颖的3D医学图像分割方法，它使用增强的解码器和自动提示来实现SAM，并在腹部肿瘤分割数据集上实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]