[
    {
        "title": "Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs",
        "summary": "Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.",
        "url": "http://arxiv.org/abs/2512.18897v1",
        "published_date": "2025-12-21T22:01:29+00:00",
        "updated_date": "2025-12-21T22:01:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dmitry Demidov",
            "Zaigham Zaheer",
            "Zongyan Han",
            "Omkar Thawakar",
            "Rao Anwer"
        ],
        "tldr": "The paper introduces FiNDR, a novel reasoning-augmented LMM framework for vocabulary-free fine-grained image recognition, achieving state-of-the-art performance by generating, filtering, and ranking descriptive labels using LMMs.",
        "tldr_zh": "该论文介绍了FiNDR，一种新颖的基于推理增强的LMM框架，用于无词汇的细粒度图像识别，通过使用LMM生成、过滤和排序描述性标签，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Point What You Mean: Visually Grounded Instruction Policy",
        "summary": "Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.",
        "url": "http://arxiv.org/abs/2512.18933v1",
        "published_date": "2025-12-22T00:44:19+00:00",
        "updated_date": "2025-12-22T00:44:19+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Hang Yu",
            "Juntu Zhao",
            "Yufeng Liu",
            "Kaiyu Li",
            "Cheng Ma",
            "Di Zhang",
            "Yingdong Hu",
            "Guang Chen",
            "Junyuan Xie",
            "Junliang Guo",
            "Junqiao Zhao",
            "Yang Gao"
        ],
        "tldr": "The paper introduces Point-VLA, a plug-and-play policy that enhances Vision-Language-Action models with visual cues to improve object referring ability, particularly in challenging scenarios, and demonstrates its effectiveness through extensive evaluations.",
        "tldr_zh": "该论文介绍了Point-VLA，一种即插即用的策略，通过视觉提示增强视觉-语言-动作模型，以提高对象指代能力，尤其是在具有挑战性的场景中，并通过广泛的评估证明了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.",
        "url": "http://arxiv.org/abs/2512.18910v1",
        "published_date": "2025-12-21T23:02:56+00:00",
        "updated_date": "2025-12-21T23:02:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohamad Zamini",
            "Diksha Shukla"
        ],
        "tldr": "Delta-LLaVA introduces a token-efficient visual projector for Vision-Language Models using a base-then-specialize approach, achieving significant speedups in training and inference while maintaining performance.",
        "tldr_zh": "Delta-LLaVA 提出了一种 token 高效的视觉投影器，用于视觉-语言模型，采用了一种先基础对齐再精细化的方法，在保持性能的同时，显著提高了训练和推理速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
        "summary": "Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.",
        "url": "http://arxiv.org/abs/2512.18878v1",
        "published_date": "2025-12-21T20:39:31+00:00",
        "updated_date": "2025-12-21T20:39:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kaidi Liang",
            "Ke Li",
            "Xianbiao Hu",
            "Ruwen Qin"
        ],
        "tldr": "CrashChat, a multimodal large language model (MLLM) built upon VideoLLaMA3, is proposed for multitask traffic crash video analysis, outperforming existing methods through a novel multitask learning strategy and achieving state-of-the-art performance in crash recognition, localization, description, and reasoning.",
        "tldr_zh": "本文提出了CrashChat，一个基于VideoLLaMA3的多模态大型语言模型（MLLM），用于多任务交通碰撞视频分析。通过一种新颖的多任务学习策略，CrashChat超越了现有方法，并在碰撞识别、定位、描述和推理方面取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]