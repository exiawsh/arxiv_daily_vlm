[
    {
        "title": "Can Understanding and Generation Truly Benefit Together -- or Just Coexist?",
        "summary": "In this paper, we introduce an insightful paradigm through the Auto-Encoder\nlens-understanding as the encoder (I2T) that compresses images into text, and\ngeneration as the decoder (T2I) that reconstructs images from that text. Using\nreconstruction fidelity as the unified training objective, we enforce the\ncoherent bidirectional information flow between the understanding and\ngeneration processes, bringing mutual gains. To implement this, we propose UAE,\na novel framework for unified multimodal learning. We begin by pre-training the\ndecoder with large-scale long-context image captions to capture fine-grained\nsemantic and complex spatial relationships. We then propose Unified-GRPO via\nreinforcement learning (RL), which covers three stages: (1) A cold-start phase\nto gently initialize both encoder and decoder with a semantic reconstruction\nloss; (2) Generation for Understanding, where the encoder is trained to\ngenerate informative captions that maximize the decoder's reconstruction\nquality, enhancing its visual understanding; (3) Understanding for Generation,\nwhere the decoder is refined to reconstruct from these captions, forcing it to\nleverage every detail and improving its long-context instruction following and\ngeneration fidelity. For evaluation, we introduce Unified-Bench, the first\nbenchmark tailored to assess the degree of unification of the UMMs. A\nsurprising \"aha moment\" arises within the multimodal learning domain: as RL\nprogresses, the encoder autonomously produces more descriptive captions, while\nthe decoder simultaneously demonstrates a profound ability to understand these\nintricate descriptions, resulting in reconstructions of striking fidelity.",
        "url": "http://arxiv.org/abs/2509.09666v1",
        "published_date": "2025-09-11T17:57:59+00:00",
        "updated_date": "2025-09-11T17:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyuan Yan",
            "Kaiqing Lin",
            "Zongjian Li",
            "Junyan Ye",
            "Hui Han",
            "Zhendong Wang",
            "Hao Liu",
            "Bin Lin",
            "Hao Li",
            "Xue Xu",
            "Xinyan Xiao",
            "Jingdong Wang",
            "Haifeng Wang",
            "Li Yuan"
        ],
        "tldr": "This paper introduces a novel unified multimodal learning framework (UAE) that leverages an auto-encoder approach with reinforcement learning to improve both image understanding and generation, resulting in high-fidelity image reconstruction from captions. They also propose a new benchmark, Unified-Bench, to evaluate unified multimodal models.",
        "tldr_zh": "本文介绍了一种新的统一多模态学习框架 (UAE)，该框架利用具有强化学习的自动编码器方法来改进图像理解和生成，从而实现从标题中进行高保真图像重建。他们还提出了一个新的基准 Unified-Bench，用于评估统一的多模态模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis",
        "summary": "Recent advances in audio-driven avatar video generation have significantly\nenhanced audio-visual realism. However, existing methods treat instruction\nconditioning merely as low-level tracking driven by acoustic or visual cues,\nwithout modeling the communicative purpose conveyed by the instructions. This\nlimitation compromises their narrative coherence and character expressiveness.\nTo bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that\nunifies multimodal instruction understanding with photorealistic portrait\ngeneration. Our approach adopts a two-stage pipeline. In the first stage, we\ndesign a multimodal large language model (MLLM) director that produces a\nblueprint video conditioned on diverse instruction signals, thereby governing\nhigh-level semantics such as character motion and emotions. In the second\nstage, guided by blueprint keyframes, we generate multiple sub-clips in\nparallel using a first-last frame strategy. This global-to-local framework\npreserves fine-grained details while faithfully encoding the high-level intent\nbehind multimodal instructions. Our parallel architecture also enables fast and\nstable generation of long-duration videos, making it suitable for real-world\napplications such as digital human livestreaming and vlogging. To\ncomprehensively evaluate our method, we construct a benchmark of 375 curated\nsamples covering diverse instructions and challenging scenarios. Extensive\nexperiments demonstrate that Kling-Avatar is capable of generating vivid,\nfluent, long-duration videos at up to 1080p and 48 fps, achieving superior\nperformance in lip synchronization accuracy, emotion and dynamic\nexpressiveness, instruction controllability, identity preservation, and\ncross-domain generalization. These results establish Kling-Avatar as a new\nbenchmark for semantically grounded, high-fidelity audio-driven avatar\nsynthesis.",
        "url": "http://arxiv.org/abs/2509.09595v1",
        "published_date": "2025-09-11T16:34:57+00:00",
        "updated_date": "2025-09-11T16:34:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yikang Ding",
            "Jiwen Liu",
            "Wenyuan Zhang",
            "Zekun Wang",
            "Wentao Hu",
            "Liyuan Cui",
            "Mingming Lao",
            "Yingchao Shao",
            "Hui Liu",
            "Xiaohan Li",
            "Ming Chen",
            "Xiaoqiang Liu",
            "Yu-Shen Liu",
            "Pengfei Wan"
        ],
        "tldr": "The paper introduces Kling-Avatar, a cascaded framework that uses a multimodal large language model to generate high-fidelity, long-duration avatar videos based on multimodal instructions, improving narrative coherence and expressiveness.",
        "tldr_zh": "该论文介绍了Kling-Avatar，一个级联框架，它使用多模态大型语言模型根据多模态指令生成高保真、长时间的头像视频，从而提高叙事连贯性和表现力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Programmability: A Guide for Code-as-Thought in Chart Understanding",
        "summary": "Chart understanding presents a critical test to the reasoning capabilities of\nVision-Language Models (VLMs). Prior approaches face critical limitations: some\nrely on external tools, making them brittle and constrained by a predefined\ntoolkit, while others fine-tune specialist models that often adopt a single\nreasoning strategy, such as text-based chain-of-thought (CoT). The intermediate\nsteps of text-based reasoning are difficult to verify, which complicates the\nuse of reinforcement-learning signals that reward factual accuracy. To address\nthis, we propose a Code-as-Thought (CaT) approach to represent the visual\ninformation of a chart in a verifiable, symbolic format. Our key insight is\nthat this strategy must be adaptive: a fixed, code-only implementation\nconsistently fails on complex charts where symbolic representation is\nunsuitable. This finding leads us to introduce Visual Programmability: a\nlearnable property that determines if a chart-question pair is better solved\nwith code or direct visual analysis. We implement this concept in an adaptive\nframework where a VLM learns to choose between the CaT pathway and a direct\nvisual reasoning pathway. The selection policy of the model is trained with\nreinforcement learning using a novel dual-reward system. This system combines a\ndata-accuracy reward to ground the model in facts and prevent numerical\nhallucination, with a decision reward that teaches the model when to use each\nstrategy, preventing it from defaulting to a single reasoning mode. Experiments\ndemonstrate strong and robust performance across diverse chart-understanding\nbenchmarks. Our work shows that VLMs can be taught not only to reason but also\nhow to reason, dynamically selecting the optimal reasoning pathway for each\ntask.",
        "url": "http://arxiv.org/abs/2509.09286v1",
        "published_date": "2025-09-11T09:22:16+00:00",
        "updated_date": "2025-09-11T09:22:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bohao Tang",
            "Yan Ma",
            "Fei Zhang",
            "Jiadi Su",
            "Ethan Chern",
            "Zhulin Hu",
            "Zhixin Wang",
            "Pengfei Liu",
            "Ya Zhang"
        ],
        "tldr": "This paper introduces 'Visual Programmability,' an adaptive framework for chart understanding that allows a VLM to dynamically choose between Code-as-Thought (CaT) and direct visual reasoning, trained with a novel dual-reward reinforcement learning system. It shows strong performance on chart-understanding benchmarks.",
        "tldr_zh": "该论文提出了“视觉可编程性”，一种用于图表理解的自适应框架，允许 VLM 在代码即思想 (CaT) 和直接视觉推理之间动态选择，并使用一种新颖的双重奖励强化学习系统进行训练。它在图表理解基准测试中表现出强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis",
        "summary": "Recent advances in large vision-language models (LVLMs) have demonstrated\nstrong performance on general-purpose medical tasks. However, their\neffectiveness in specialized domains such as dentistry remains underexplored.\nIn particular, panoramic X-rays, a widely used imaging modality in oral\nradiology, pose interpretative challenges due to dense anatomical structures\nand subtle pathological cues, which are not captured by existing medical\nbenchmarks or instruction datasets. To this end, we introduce MMOral, the first\nlarge-scale multimodal instruction dataset and benchmark tailored for panoramic\nX-ray interpretation. MMOral consists of 20,563 annotated images paired with\n1.3 million instruction-following instances across diverse task types,\nincluding attribute extraction, report generation, visual question answering,\nand image-grounded dialogue. In addition, we present MMOral-Bench, a\ncomprehensive evaluation suite covering five key diagnostic dimensions in\ndentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the\nbest-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing\nsignificant limitations of current models in this domain. To promote the\nprogress of this specific domain, we also propose OralGPT, which conducts\nsupervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated\nMMOral instruction dataset. Remarkably, a single epoch of SFT yields\nsubstantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a\n24.73% improvement. Both MMOral and OralGPT hold significant potential as a\ncritical foundation for intelligent dentistry and enable more clinically\nimpactful multimodal AI systems in the dental field. The dataset, model,\nbenchmark, and evaluation suite are available at\nhttps://github.com/isbrycee/OralGPT.",
        "url": "http://arxiv.org/abs/2509.09254v1",
        "published_date": "2025-09-11T08:39:08+00:00",
        "updated_date": "2025-09-11T08:39:08+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jing Hao",
            "Yuxuan Fan",
            "Yanpeng Sun",
            "Kaixin Guo",
            "Lizhuo Lin",
            "Jinrong Yang",
            "Qi Yong H. Ai",
            "Lun M. Wong",
            "Hao Tang",
            "Kuo Feng Hung"
        ],
        "tldr": "This paper introduces MMOral, a large-scale multimodal instruction dataset and benchmark for panoramic X-ray interpretation, along with OralGPT, a fine-tuned LVLM, demonstrating significant performance improvements in dental AI tasks.",
        "tldr_zh": "本文介绍了MMOral，一个大规模多模态指令数据集和用于全景X射线图像分析的基准测试，以及OralGPT，一个微调的LVLM，展示了在牙科AI任务中显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval",
        "summary": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong\nperformance across diverse vision tasks, its application to person\nrepresentation learning faces two critical challenges: (i) the scarcity of\nlarge-scale annotated vision-language data focused on person-centric images,\nand (ii) the inherent limitations of global contrastive learning, which\nstruggles to maintain discriminative local features crucial for fine-grained\nmatching while remaining vulnerable to noisy text tokens. This work advances\nCLIP for person representation learning through synergistic improvements in\ndata curation and model architecture. First, we develop a noise-resistant data\nconstruction pipeline that leverages the in-context learning capabilities of\nMLLMs to automatically filter and caption web-sourced images. This yields\nWebPerson, a large-scale dataset of 5M high-quality person-centric image-text\npairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking\nSynergetic) framework, which improves cross-modal alignment by adaptively\nmasking noisy textual tokens based on the gradient-attention similarity score.\nAdditionally, we incorporate masked token prediction objectives that compel the\nmodel to predict informative text tokens, enhancing fine-grained semantic\nrepresentation learning. Extensive experiments show that GA-DMS achieves\nstate-of-the-art performance across multiple benchmarks.",
        "url": "http://arxiv.org/abs/2509.09118v1",
        "published_date": "2025-09-11T03:06:22+00:00",
        "updated_date": "2025-09-11T03:06:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianlu Zheng",
            "Yifan Zhang",
            "Xiang An",
            "Ziyong Feng",
            "Kaicheng Yang",
            "Qichuan Ding"
        ],
        "tldr": "The paper introduces a novel framework, GA-DMS, for robust text-based person retrieval using CLIP, addressing the limitations of existing methods by curating a large-scale person-centric dataset (WebPerson) and employing gradient-attention guided dual-masking to improve cross-modal alignment and fine-grained semantic representation learning.",
        "tldr_zh": "该论文介绍了一个名为GA-DMS的新框架，用于鲁棒的基于文本的行人检索，它利用CLIP，并通过构建大规模的以人为中心的WebPerson数据集，并采用梯度注意力引导的双重掩码来提高跨模态对齐和细粒度语义表征学习，从而解决了现有方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models",
        "summary": "Understanding 3D medical image volumes is critical in the medical field, yet\nexisting 3D medical convolution and transformer-based self-supervised learning\n(SSL) methods often lack deep semantic comprehension. Recent advancements in\nmultimodal large language models (MLLMs) provide a promising approach to\nenhance image understanding through text descriptions. To leverage these 2D\nMLLMs for improved 3D medical image understanding, we propose Med3DInsight, a\nnovel pretraining framework that integrates 3D image encoders with 2D MLLMs via\na specially designed plane-slice-aware transformer module. Additionally, our\nmodel employs a partial optimal transport based alignment, demonstrating\ngreater tolerance to noise introduced by potential noises in LLM-generated\ncontent. Med3DInsight introduces a new paradigm for scalable multimodal 3D\nmedical representation learning without requiring human annotations. Extensive\nexperiments demonstrate our state-of-the-art performance on two downstream\ntasks, i.e., segmentation and classification, across various public datasets\nwith CT and MRI modalities, outperforming current SSL methods. Med3DInsight can\nbe seamlessly integrated into existing 3D medical image understanding networks,\npotentially enhancing their performance. Our source code, generated datasets,\nand pre-trained models will be available at\nhttps://github.com/Qybc/Med3DInsight.",
        "url": "http://arxiv.org/abs/2509.09064v1",
        "published_date": "2025-09-11T00:12:59+00:00",
        "updated_date": "2025-09-11T00:12:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiuhui Chen",
            "Xuancheng Yao",
            "Huping Ye",
            "Yi Hong"
        ],
        "tldr": "The paper introduces Med3DInsight, a novel pretraining framework that leverages 2D MLLMs to enhance 3D medical image understanding, achieving state-of-the-art results on segmentation and classification tasks.",
        "tldr_zh": "该论文介绍了Med3DInsight，一种新的预训练框架，利用2D MLLM来增强3D医学图像理解，并在分割和分类任务上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can Vision-Language Models Solve Visual Math Equations?",
        "summary": "Despite strong performance in visual understanding and language-based\nreasoning, Vision-Language Models (VLMs) struggle with tasks requiring\nintegrated perception and symbolic computation. We study this limitation\nthrough visual equation solving, where mathematical equations are embedded in\nimages, variables are represented by object icons, and coefficients must be\ninferred by counting. While VLMs perform well on textual equations, they fail\non visually grounded counterparts. To understand this gap, we decompose the\ntask into coefficient counting and variable recognition, and find that counting\nis the primary bottleneck, even when recognition is accurate. We also observe\nthat composing recognition and reasoning introduces additional errors,\nhighlighting challenges in multi-step visual reasoning. Finally, as equation\ncomplexity increases, symbolic reasoning itself becomes a limiting factor.\nThese findings reveal key weaknesses in current VLMs and point toward future\nimprovements in visually grounded mathematical reasoning.",
        "url": "http://arxiv.org/abs/2509.09013v1",
        "published_date": "2025-09-10T21:16:11+00:00",
        "updated_date": "2025-09-10T21:16:11+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Monjoy Narayan Choudhury",
            "Junling Wang",
            "Yifan Hou",
            "Mrinmaya Sachan"
        ],
        "tldr": "The paper investigates the limitations of VLMs in solving visual math equations, identifying counting, multi-step reasoning, and symbolic reasoning as key bottlenecks.",
        "tldr_zh": "该论文研究了视觉语言模型在解决视觉数学方程方面的局限性，确定了计数、多步骤推理和符号推理是关键瓶颈。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM",
        "summary": "Generative AI evolves the execution of complex workflows in industry, where\nthe large multimodal model empowers fashion design in the garment industry.\nCurrent generation AI models magically transform brainstorming into fancy\ndesigns easily, but the fine-grained customization still suffers from text\nuncertainty without professional background knowledge from end-users. Thus, we\npropose the Better Understanding Generation (BUG) workflow with LMM to\nautomatically create and fine-grain customize the cloth designs from chat with\nimage-into-prompt. Our framework unleashes users' creative potential beyond\nwords and also lowers the barriers of clothing design/editing without further\nhuman involvement. To prove the effectiveness of our model, we propose a new\nFashionEdit dataset that simulates the real-world clothing design workflow,\nevaluated from generation similarity, user satisfaction, and quality. The code\nand dataset: https://github.com/detectiveli/FashionEdit.",
        "url": "http://arxiv.org/abs/2509.09324v1",
        "published_date": "2025-09-11T10:14:36+00:00",
        "updated_date": "2025-09-11T10:14:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hui Li",
            "Yi You",
            "Qiqi Chen",
            "Bingfeng Zhang",
            "George Q. Huang"
        ],
        "tldr": "This paper introduces a new workflow (BUG) using Large Multimodal Models to improve fine-grained customization in fashion design via image-into-prompt, along with a new FashionEdit dataset to evaluate its effectiveness.",
        "tldr_zh": "本文介绍了一种新的工作流程 (BUG)，该流程使用大型多模态模型通过图像到提示 (image-into-prompt) 来改进服装设计中的细粒度定制，并提供了一个新的 FashionEdit 数据集来评估其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization",
        "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.",
        "url": "http://arxiv.org/abs/2509.09307v1",
        "published_date": "2025-09-11T09:50:16+00:00",
        "updated_date": "2025-09-11T09:50:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "authors": [
            "Zhengzhao Lai",
            "Youbin Zheng",
            "Zhenyang Cai",
            "Haonan Lyu",
            "Jinpu Yang",
            "Hongqing Liang",
            "Yan Hu",
            "Benyou Wang"
        ],
        "tldr": "The paper introduces MatCha, a new benchmark for evaluating MLLMs on materials characterization image understanding, revealing a performance gap compared to human experts and limitations in adapting to real-world materials science scenarios.",
        "tldr_zh": "该论文介绍了MatCha，一个新的用于评估MLLM在材料表征图像理解方面的基准。结果表明，与人类专家相比，MLLM存在性能差距，并且在适应真实材料科学场景方面存在局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results",
        "summary": "This paper presents a summary of the VQualA 2025 Challenge on Visual Quality\nComparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025\nWorkshop on Visual Quality Assessment. The challenge aims to evaluate and\nenhance the ability of state-of-the-art LMMs to perform open-ended and detailed\nreasoning about visual quality differences across multiple images. To this end,\nthe competition introduces a novel benchmark comprising thousands of\ncoarse-to-fine grained visual quality comparison tasks, spanning single images,\npairs, and multi-image groups. Each task requires models to provide accurate\nquality judgments. The competition emphasizes holistic evaluation protocols,\nincluding 2AFC-based binary preference and multi-choice questions (MCQs).\nAround 100 participants submitted entries, with five models demonstrating the\nemerging capabilities of instruction-tuned LMMs on quality assessment. This\nchallenge marks a significant step toward open-domain visual quality reasoning\nand comparison and serves as a catalyst for future research on interpretable\nand human-aligned quality evaluation systems.",
        "url": "http://arxiv.org/abs/2509.09190v1",
        "published_date": "2025-09-11T07:00:50+00:00",
        "updated_date": "2025-09-11T07:00:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanwei Zhu",
            "Haoning Wu",
            "Zicheng Zhang",
            "Lingyu Zhu",
            "Yixuan Li",
            "Peilin Chen",
            "Shiqi Wang",
            "Chris Wei Zhou",
            "Linhan Cao",
            "Wei Sun",
            "Xiangyang Zhu",
            "Weixia Zhang",
            "Yucheng Zhu",
            "Jing Liu",
            "Dandan Zhu",
            "Guangtao Zhai",
            "Xiongkuo Min",
            "Zhichao Zhang",
            "Xinyue Li",
            "Shubo Xu",
            "Anh Dao",
            "Yifan Li",
            "Hongyuan Yu",
            "Jiaojiao Yi",
            "Yiding Tian",
            "Yupeng Wu",
            "Feiran Sun",
            "Lijuan Liao",
            "Song Jiang"
        ],
        "tldr": "The VQualA 2025 Challenge evaluated Large Multimodal Models (LMMs) on visual quality comparison using a novel benchmark of coarse-to-fine grained tasks, revealing the emerging capabilities of instruction-tuned LMMs in this domain.",
        "tldr_zh": "VQualA 2025 挑战赛评估了大型多模态模型 (LMM) 在视觉质量比较方面的能力，它使用了一个新颖的粗细粒度任务基准，揭示了指令调整 LMM 在该领域的新兴能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Discovering Divergent Representations between Text-to-Image Models",
        "summary": "In this paper, we investigate when and how visual representations learned by\ntwo different generative models diverge. Given two text-to-image models, our\ngoal is to discover visual attributes that appear in images generated by one\nmodel but not the other, along with the types of prompts that trigger these\nattribute differences. For example, \"flames\" might appear in one model's\noutputs when given prompts expressing strong emotions, while the other model\ndoes not produce this attribute given the same prompts. We introduce CompCon\n(Comparing Concepts), an evolutionary search algorithm that discovers visual\nattributes more prevalent in one model's output than the other, and uncovers\nthe prompt concepts linked to these visual differences. To evaluate CompCon's\nability to find diverging representations, we create an automated data\ngeneration pipeline to produce ID2, a dataset of 60 input-dependent\ndifferences, and compare our approach to several LLM- and VLM-powered\nbaselines. Finally, we use CompCon to compare popular text-to-image models,\nfinding divergent representations such as how PixArt depicts prompts mentioning\nloneliness with wet streets and Stable Diffusion 3.5 depicts African American\npeople in media professions. Code at: https://github.com/adobe-research/CompCon",
        "url": "http://arxiv.org/abs/2509.08940v1",
        "published_date": "2025-09-10T19:07:55+00:00",
        "updated_date": "2025-09-10T19:07:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lisa Dunlap",
            "Joseph E. Gonzalez",
            "Trevor Darrell",
            "Fabian Caba Heilbron",
            "Josef Sivic",
            "Bryan Russell"
        ],
        "tldr": "This paper introduces CompCon, an evolutionary search algorithm, to discover and compare divergent visual representations learned by different text-to-image models, identifying visual attributes and prompt concepts that trigger these differences. They compare popular text-to-image models, finding divergent representations.",
        "tldr_zh": "本文介绍了一种进化搜索算法CompCon，用于发现和比较不同文本到图像模型学习到的不同视觉表示，识别触发这些差异的视觉属性和提示概念。他们比较了流行的文本到图像模型，发现了不同的表示。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection",
        "summary": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples,\nand the difficulty of cross-domain generalization in multi-temporal and\nmulti-source remote sensing imagery, we propose PeftCD, a change detection\nframework built upon Vision Foundation Models (VFMs) with Parameter-Efficient\nFine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese\nencoder derived from a VFM, into which LoRA and Adapter modules are seamlessly\nintegrated. This design enables highly efficient task adaptation by training\nonly a minimal set of additional parameters. To fully unlock the potential of\nVFMs, we investigate two leading backbones: the Segment Anything Model v2\n(SAM2), renowned for its strong segmentation priors, and DINOv3, a\nstate-of-the-art self-supervised representation learner. The framework is\ncomplemented by a deliberately lightweight decoder, ensuring the focus remains\non the powerful feature representations from the backbones. Extensive\nexperiments demonstrate that PeftCD achieves state-of-the-art performance\nacross multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD\n(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and\nLEVIR-CD (85.62%), with notably precise boundary delineation and strong\nsuppression of pseudo-changes. In summary, PeftCD presents an optimal balance\nof accuracy, efficiency, and generalization. It offers a powerful and scalable\nparadigm for adapting large-scale VFMs to real-world remote sensing change\ndetection applications. The code and pretrained models will be released at\nhttps://github.com/dyzy41/PeftCD.",
        "url": "http://arxiv.org/abs/2509.09572v1",
        "published_date": "2025-09-11T16:08:43+00:00",
        "updated_date": "2025-09-11T16:08:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sijun Dong",
            "Yuxuan Hu",
            "LiBo Wang",
            "Geng Chen",
            "Xiaoliang Meng"
        ],
        "tldr": "The paper introduces PeftCD, a parameter-efficient change detection framework using Vision Foundation Models (VFMs) like SAM2 and DINOv3, achieving state-of-the-art results on multiple remote sensing datasets.",
        "tldr_zh": "本文介绍了 PeftCD，一个参数高效的变化检测框架，利用视觉基础模型 (VFMs)，如 SAM2 和 DINOv3，在多个遥感数据集上取得了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios",
        "summary": "With the rapid advancement of generative models, highly realistic image\nsynthesis has posed new challenges to digital security and media credibility.\nAlthough AI-generated image detection methods have partially addressed these\nconcerns, a substantial research gap remains in evaluating their performance\nunder complex real-world conditions. This paper introduces the Real-World\nRobustness Dataset (RRDataset) for comprehensive evaluation of detection models\nacross three dimensions: 1) Scenario Generalization: RRDataset encompasses\nhigh-quality images from seven major scenarios (War and Conflict, Disasters and\nAccidents, Political and Social Events, Medical and Public Health, Culture and\nReligion, Labor and Production, and everyday life), addressing existing dataset\ngaps from a content perspective. 2) Internet Transmission Robustness: examining\ndetector performance on images that have undergone multiple rounds of sharing\nacross various social media platforms. 3) Re-digitization Robustness: assessing\nmodel effectiveness on images altered through four distinct re-digitization\nmethods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on\nRRDataset and conducted a large-scale human study involving 192 participants to\ninvestigate human few-shot learning capabilities in detecting AI-generated\nimages. The benchmarking results reveal the limitations of current AI detection\nmethods under real-world conditions and underscore the importance of drawing on\nhuman adaptability to develop more robust detection algorithms.",
        "url": "http://arxiv.org/abs/2509.09172v1",
        "published_date": "2025-09-11T06:15:52+00:00",
        "updated_date": "2025-09-11T06:15:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunxiao Li",
            "Xiaoxiao Wang",
            "Meiling Li",
            "Boming Miao",
            "Peng Sun",
            "Yunjian Zhang",
            "Xiangyang Ji",
            "Yao Zhu"
        ]
    }
]