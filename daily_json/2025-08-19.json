[
    {
        "title": "Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation",
        "summary": "Despite their success, current training pipelines for reasoning VLMs focus on\na limited range of tasks, such as mathematical and logical reasoning. As a\nresult, these models face difficulties in generalizing their reasoning\ncapabilities to a wide range of domains, primarily due to the scarcity of\nreadily available and verifiable reward data beyond these narrowly defined\nareas. Moreover, integrating data from multiple domains is challenging, as the\ncompatibility between domain-specific datasets remains uncertain. To address\nthese limitations, we build a comprehensive RL-ready visual reasoning dataset\nfrom 46 data sources across 8 dimensions, covering a wide range of tasks such\nas infographic, mathematical, spatial, cross-image, graphic user interface,\nmedical, common sense and general science. We propose an influence function\nbased data selection and difficulty based filtering strategy to identify\nhigh-quality training samples from this dataset. Subsequently, we train the\nVLM, referred to as Vision-G1, using multi-round RL with a data curriculum to\niteratively improve its visual reasoning capabilities. Our model achieves\nstate-of-the-art performance across various visual reasoning benchmarks,\noutperforming similar-sized VLMs and even proprietary models like GPT-4o and\nGemini-1.5 Flash. The model, code and dataset are publicly available at\nhttps://github.com/yuh-zha/Vision-G1.",
        "url": "http://arxiv.org/abs/2508.12680v1",
        "published_date": "2025-08-18T07:24:33+00:00",
        "updated_date": "2025-08-18T07:24:33+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yuheng Zha",
            "Kun Zhou",
            "Yujia Wu",
            "Yushu Wang",
            "Jie Feng",
            "Zhi Xu",
            "Shibo Hao",
            "Zhengzhong Liu",
            "Eric P. Xing",
            "Zhiting Hu"
        ],
        "tldr": "The paper introduces Vision-G1, a VLM trained on a newly curated, comprehensive dataset spanning 46 sources and diverse tasks, achieving state-of-the-art performance compared to similar-sized and even larger proprietary models like GPT-4o and Gemini 1.5 Flash.",
        "tldr_zh": "该论文介绍了Vision-G1，一个基于新策划的综合数据集训练的VLM，该数据集涵盖46个来源和各种任务，实现了最先进的性能，甚至超过了类似大小的专有模型，如GPT-4o和Gemini 1.5 Flash。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
        "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
        "url": "http://arxiv.org/abs/2508.13142v1",
        "published_date": "2025-08-18T17:55:17+00:00",
        "updated_date": "2025-08-18T17:55:17+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG",
            "cs.MM",
            "cs.RO"
        ],
        "authors": [
            "Zhongang Cai",
            "Yubo Wang",
            "Qingping Sun",
            "Ruisi Wang",
            "Chenyang Gu",
            "Wanqi Yin",
            "Zhiqian Lin",
            "Zhitao Yang",
            "Chen Wei",
            "Xuanke Shi",
            "Kewang Deng",
            "Xiaoyang Han",
            "Zukai Chen",
            "Jiaqi Li",
            "Xiangyu Fan",
            "Hanming Deng",
            "Lewei Lu",
            "Bo Li",
            "Ziwei Liu",
            "Quan Wang",
            "Dahua Lin",
            "Lei Yang"
        ],
        "tldr": "The paper empirically evaluates GPT-5's spatial intelligence using a comprehensive taxonomy and benchmarks, finding unprecedented strength but still falling short of human performance, especially on challenging problems where proprietary models don't have a decisive advantage.",
        "tldr_zh": "该论文通过全面的分类和基准，实证评估了 GPT-5 的空间智能，发现其具有前所未有的优势，但仍然不如人类，尤其是在具有挑战性的问题上，专有模型没有明显的优势。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination",
        "summary": "Reinforcement learning (RL) with rule-based rewards has demonstrated strong\npotential in enhancing the reasoning and generalization capabilities of\nvision-language models (VLMs) and large language models (LLMs), while reducing\ncomputational overhead. However, its application in medical imaging remains\nunderexplored. Existing reinforcement fine-tuning (RFT) approaches in this\ndomain primarily target closed-ended visual question answering (VQA), limiting\ntheir applicability to real-world clinical reasoning. In contrast, open-ended\nmedical VQA better reflects clinical practice but has received limited\nattention. While some efforts have sought to unify both formats via\nsemantically guided RL, we observe that model-based semantic rewards often\nsuffer from reward collapse, where responses with significant semantic\ndifferences receive similar scores. To address this, we propose ARMed (Adaptive\nReinforcement for Medical Reasoning), a novel RL framework for open-ended\nmedical VQA. ARMed first incorporates domain knowledge through supervised\nfine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning\nwith textual correctness and adaptive semantic rewards to enhance reasoning\nquality. We evaluate ARMed on six challenging medical VQA benchmarks. Results\nshow that ARMed consistently boosts both accuracy and generalization, achieving\na 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain\nbenchmarks. These results highlight the critical role of reward\ndiscriminability in medical RL and the promise of semantically guided rewards\nfor enabling robust and clinically meaningful multimodal reasoning.",
        "url": "http://arxiv.org/abs/2508.12957v1",
        "published_date": "2025-08-18T14:31:26+00:00",
        "updated_date": "2025-08-18T14:31:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yizhou Liu",
            "Jingwei Wei",
            "Zizhi Chen",
            "Minghao Han",
            "Xukun Zhang",
            "Keliang Liu",
            "Lihua Zhang"
        ],
        "tldr": "This paper introduces ARMed, a reinforcement learning framework for open-ended medical VQA that addresses reward collapse through adaptive semantic rewards, achieving significant improvements in accuracy and generalization on medical VQA benchmarks.",
        "tldr_zh": "本文介绍了ARMed，一种用于开放式医学视觉问答的强化学习框架，通过自适应语义奖励解决奖励崩溃问题，并在医学视觉问答基准测试中实现了准确性和泛化性的显著提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning",
        "summary": "Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable\npotential in few-shot image classification and led to numerous effective\ntransfer learning strategies. These methods leverage the pretrained knowledge\nof VLMs to enable effective domain adaptation while mitigating overfitting\nthrough parameter-efficient tuning or instance-based consistency constraints.\nHowever, such regularizations often neglect the geometric structure of data\ndistribution, which may lead to distortion of the overall semantic\nrepresentation. To overcome this limitation, we propose a novel fine-tuning\nmethod, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the\ndata distribution in feature space as a semantic manifold, MPS-Tuning\nexplicitly constrains the intrinsic geometry of this manifold while further\nsculpting it to enhance class separability. Specifically, MPS-Tuning preserves\nboth macroscopic and microscopic topological structures of the original\nmanifold by aligning Gram matrices of features before and after fine-tuning.\nTheoretically, this constraint is shown to approximate an upper bound of the\nGromov-Wasserstein distance. Furthermore, features from the image and text\nmodalities are paired, and pairwise similarities are optimized to enhance the\nmanifold's class discriminability. Extensive experiments demonstrate that\nMPS-Tuning significantly improves model performance while effectively\npreserving the structure of the semantic manifold. The code will be released.",
        "url": "http://arxiv.org/abs/2508.12877v1",
        "published_date": "2025-08-18T12:28:43+00:00",
        "updated_date": "2025-08-18T12:28:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dexia Chen",
            "Qianjie Zhu",
            "Weibing Li",
            "Yue Yu",
            "Tong Zhang",
            "Ruixuan Wang"
        ],
        "tldr": "The paper introduces MPS-Tuning, a novel fine-tuning method for VLMs that preserves the geometric structure of data distribution while enhancing class separability in few-shot learning scenarios by aligning Gram matrices and optimizing pairwise similarities.",
        "tldr_zh": "该论文介绍了一种新的VLM微调方法MPS-Tuning，它通过对齐Gram矩阵和优化成对相似性，在少样本学习场景中保留数据分布的几何结构，同时增强类可分性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models",
        "summary": "Vision-language models (VLMs) pre-trained on natural image and language data,\nsuch as CLIP, have exhibited significant potential in few-shot image\nrecognition tasks, leading to development of various efficient transfer\nlearning methods. These methods exploit inherent pre-learned knowledge in VLMs\nand have achieved strong performance on standard image datasets. However, their\neffectiveness is often limited when confronted with cross-domain tasks where\nimaging domains differ from natural images. To address this limitation, we\npropose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a\nnovel fine-tuning strategy for VLMs. This strategy employs two functionally\ncomplementary expert modules to extract multi-view features, while\nincorporating prior knowledge-based consistency constraints and information\ngeometry-based consensus mechanisms to enhance the robustness of feature\nlearning. Additionally, a new cross-domain few-shot benchmark is established to\nhelp comprehensively evaluate methods on imaging domains distinct from natural\nimages. Extensive empirical evaluations on both existing and newly proposed\nbenchmarks suggest CoMuCo consistently outperforms current methods in few-shot\ntasks. The code and benchmark will be released.",
        "url": "http://arxiv.org/abs/2508.12861v1",
        "published_date": "2025-08-18T12:00:09+00:00",
        "updated_date": "2025-08-18T12:00:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dexia Chen",
            "Wentao Zhang",
            "Qianjie Zhu",
            "Ping Hu",
            "Weibing Li",
            "Tong Zhang",
            "Ruixuan Wang"
        ],
        "tldr": "This paper introduces a novel fine-tuning strategy, CoMuCo, for Vision-Language Models to improve cross-domain few-shot learning, particularly when imaging domains differ from natural images, and provides a new benchmark for evaluation.",
        "tldr_zh": "本文提出了一种新的微调策略CoMuCo，用于改进视觉-语言模型在跨域小样本学习中的性能，尤其是在成像域与自然图像不同的情况下，并提供了一个新的评估基准。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
        "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.",
        "url": "http://arxiv.org/abs/2508.12815v1",
        "published_date": "2025-08-18T10:53:20+00:00",
        "updated_date": "2025-08-18T10:53:20+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Jayneel Parekh",
            "Pegah Khayatan",
            "Mustafa Shukor",
            "Arnaud Dapogny",
            "Alasdair Newson",
            "Matthieu Cord"
        ],
        "tldr": "The paper introduces L2S, a novel method for input-dependent steering of multimodal LLMs using a learned auxiliary module to predict input-specific steering vectors, improving safety and reducing hallucinations compared to static baselines.",
        "tldr_zh": "该论文介绍了一种新颖的L2S方法，通过学习一个辅助模块来预测输入特定的steering向量，从而实现对多模态LLM的输入依赖的steering，与静态基线相比，提高了安全性和减少了幻觉。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score",
        "summary": "Large-scale text-to-image generative models have shown remarkable ability to\nsynthesize diverse and high-quality images. However, it is still challenging to\ndirectly apply these models for editing real images for two reasons. First, it\nis difficult for users to come up with a perfect text prompt that accurately\ndescribes every visual detail in the input image. Second, while existing models\ncan introduce desirable changes in certain regions, they often dramatically\nalter the input content and introduce unexpected changes in unwanted regions.\nTo address these challenges, we present Dual Contrastive Denoising Score, a\nsimple yet powerful framework that leverages the rich generative prior of\ntext-to-image diffusion models. Inspired by contrastive learning approaches for\nunpaired image-to-image translation, we introduce a straightforward dual\ncontrastive loss within the proposed framework. Our approach utilizes the\nextensive spatial information from the intermediate representations of the\nself-attention layers in latent diffusion models without depending on auxiliary\nnetworks. Our method achieves both flexible content modification and structure\npreservation between input and output images, as well as zero-shot\nimage-to-image translation. Through extensive experiments, we show that our\napproach outperforms existing methods in real image editing while maintaining\nthe capability to directly utilize pretrained text-to-image diffusion models\nwithout further training.",
        "url": "http://arxiv.org/abs/2508.12718v1",
        "published_date": "2025-08-18T08:30:07+00:00",
        "updated_date": "2025-08-18T08:30:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Syed Muhmmad Israr",
            "Feng Zhao"
        ],
        "tldr": "This paper introduces Dual Contrastive Denoising Score, a framework for text-to-image manipulation that preserves image structure while allowing flexible content modification, outperforming existing methods in real image editing without retraining.",
        "tldr_zh": "本文介绍了一种名为双对比去噪分数（Dual Contrastive Denoising Score）的文本到图像处理框架，该框架能够在保持图像结构的同时灵活地修改内容，并且在真实图像编辑方面优于现有方法，无需重新训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance in complex multimodal tasks. While MLLMs excel at visual perception\nand reasoning in third-person and egocentric videos, they are prone to\nhallucinations, generating coherent yet inaccurate responses. We present\nEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric\nvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated\nopen and closed-ended questions designed to trigger hallucinations in both\nvisual and auditory cues in egocentric videos. Evaluations across ten MLLMs\nreveal significant challenges, including powerful models like GPT-4o and\nGemini, achieving only 59% accuracy. EgoIllusion lays the foundation in\ndeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spurs\nthe development of better egocentric MLLMs with reduced hallucination rates.\nOur benchmark will be open-sourced for reproducibility.",
        "url": "http://arxiv.org/abs/2508.12687v1",
        "published_date": "2025-08-18T07:39:55+00:00",
        "updated_date": "2025-08-18T07:39:55+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ashish Seth",
            "Utkarsh Tyagi",
            "Ramaneswaran Selvakumar",
            "Nishit Anand",
            "Sonal Kumar",
            "Sreyan Ghosh",
            "Ramani Duraiswami",
            "Chirag Agarwal",
            "Dinesh Manocha"
        ],
        "tldr": "The paper introduces EgoIllusion, a new benchmark for evaluating hallucinations in Multimodal Large Language Models (MLLMs) when applied to egocentric video understanding, revealing that even state-of-the-art models struggle with accuracy.",
        "tldr_zh": "该论文介绍了EgoIllusion，一个新的基准，用于评估多模态大型语言模型（MLLM）在应用于以自我为中心的视频理解时产生的幻觉，结果表明即使是最先进的模型在准确性方面也存在困难。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer",
        "summary": "Vision-Language Models (VLMs) are increasingly deployed in real-time\napplications such as autonomous driving and human-computer interaction, which\ndemand fast and reliable responses based on accurate perception. To meet these\nrequirements, existing systems commonly employ cloud-edge collaborative\narchitectures, such as partitioned Large Vision-Language Models (LVLMs) or task\noffloading strategies between Large and Small Vision-Language Models (SVLMs).\nHowever, these methods fail to accommodate cloud latency fluctuations and\noverlook the full potential of delayed but accurate LVLM responses. In this\nwork, we propose a novel cloud-edge collaborative paradigm for VLMs, termed\nContext Transfer, which treats the delayed outputs of LVLMs as historical\ncontext to provide real-time guidance for SVLMs inference. Based on this\nparadigm, we design SpotVLM, which incorporates both context replacement and\nvisual focus modules to refine historical textual input and enhance visual\ngrounding consistency. Extensive experiments on three real-time vision tasks\nacross four datasets demonstrate the effectiveness of the proposed framework.\nThe new paradigm lays the groundwork for more effective and latency-aware\ncollaboration strategies in future VLM systems.",
        "url": "http://arxiv.org/abs/2508.12638v1",
        "published_date": "2025-08-18T05:51:41+00:00",
        "updated_date": "2025-08-18T05:51:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chen Qian",
            "Xinran Yu",
            "Zewen Huang",
            "Danyang Li",
            "Qiang Ma",
            "Fan Dang",
            "Xuan Ding",
            "Guangyong Shang",
            "Zheng Yang"
        ],
        "tldr": "The paper introduces SpotVLM, a cloud-edge collaborative framework that leverages delayed LVLM outputs as historical context to guide real-time SVLM inference, enhancing accuracy and reducing latency in VLM applications.",
        "tldr_zh": "该论文介绍了SpotVLM，一个云边协同框架，利用延迟的LVLM输出作为历史上下文来指导实时的SVLM推理，从而提高VLM应用的准确性和降低延迟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have introduced a\nparadigm shift for Image Quality Assessment (IQA) from unexplainable image\nquality scoring to explainable IQA, demonstrating practical applications like\nquality control and optimization guidance. However, current explainable IQA\nmethods not only inadequately use the same distortion criteria to evaluate both\nUser-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also\nlack detailed quality analysis for monitoring image quality and guiding image\nrestoration. In this study, we establish the first large-scale Visual\nDistortion Assessment Instruction Tuning Dataset for UGC images, termed\nViDA-UGC, which comprises 11K images with fine-grained quality grounding,\ndetailed quality perception, and reasoning quality description data. This\ndataset is constructed through a distortion-oriented pipeline, which involves\nhuman subject annotation and a Chain-of-Thought (CoT) assessment framework.\nThis framework guides GPT-4o to generate quality descriptions by identifying\nand analyzing UGC distortions, which helps capturing rich low-level visual\nfeatures that inherently correlate with distortion patterns. Moreover, we\ncarefully select 476 images with corresponding 6,149 question answer pairs from\nViDA-UGC and invite a professional team to ensure the accuracy and quality of\nGPT-generated information. The selected and revised data further contribute to\nthe first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.\nExperimental results demonstrate the effectiveness of the ViDA-UGC and CoT\nframework for consistently enhancing various image quality analysis abilities\nacross multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing\nGPT-4o.",
        "url": "http://arxiv.org/abs/2508.12605v1",
        "published_date": "2025-08-18T04:02:58+00:00",
        "updated_date": "2025-08-18T04:02:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjie Liao",
            "Jieyu Yuan",
            "Yifang Xu",
            "Chunle Guo",
            "Zilong Zhang",
            "Jihong Li",
            "Jiachen Fu",
            "Haotian Fan",
            "Tao Li",
            "Junhui Cui",
            "Chongyi Li"
        ],
        "tldr": "The paper introduces ViDA-UGC, a large-scale dataset for Visual Distortion Assessment of UGC images, along with a CoT framework that enhances MLLMs' image quality analysis capabilities, outperforming even GPT-4o on a new benchmark.",
        "tldr_zh": "该论文介绍了 ViDA-UGC，一个用于用户生成内容（UGC）图像的视觉失真评估的大规模数据集，以及一个 CoT 框架，该框架增强了 MLLM 的图像质量分析能力，并在一个新的基准测试中甚至优于 GPT-4o。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models",
        "summary": "Many reasoning techniques for large multimodal models adapt language model\napproaches, such as Chain-of-Thought (CoT) prompting, which express reasoning\nas word sequences. While effective for text, these methods are suboptimal for\nmultimodal contexts, struggling to align audio, visual, and textual information\ndynamically. To explore an alternative paradigm, we propose the Multimodal\nChain of Continuous Thought (MCOUT), which enables reasoning directly in a\njoint latent space rather than in natural language. In MCOUT, the reasoning\nstate is represented as a continuous hidden vector, iteratively refined and\naligned with visual and textual embeddings, inspired by human reflective\ncognition. We develop two variants: MCOUT-Base, which reuses the language\nmodel`s last hidden state as the continuous thought for iterative reasoning,\nand MCOUT-Multi, which integrates multimodal latent attention to strengthen\ncross-modal alignment between visual and textual features. Experiments on\nbenchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently\nimproves multimodal reasoning, yielding up to 8.23% accuracy gains over strong\nbaselines and improving BLEU scores up to 8.27% across multiple-choice and\nopen-ended tasks. These findings highlight latent continuous reasoning as a\npromising direction for advancing LMMs beyond language-bound CoT, offering a\nscalable framework for human-like reflective multimodal inference. Code is\navailable at https://github.com/Hanhpt23/OmniMod.",
        "url": "http://arxiv.org/abs/2508.12587v1",
        "published_date": "2025-08-18T02:50:20+00:00",
        "updated_date": "2025-08-18T02:50:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tan-Hanh Pham",
            "Chris Ngo"
        ],
        "tldr": "The paper introduces Multimodal Chain of Continuous Thought (MCOUT), a novel approach for reasoning in vision-language models using a joint latent space, achieving significant accuracy gains compared to language-bound Chain-of-Thought methods on several benchmarks.",
        "tldr_zh": "该论文介绍了多模态连续思维链（MCOUT），一种新的在视觉语言模型中使用联合隐空间进行推理的方法，与基于语言的思维链方法相比，在多个基准测试中取得了显著的准确性提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language",
        "summary": "The rapid advancement of generative models has intensified the challenge of\ndetecting and interpreting visual forgeries, necessitating robust frameworks\nfor image forgery detection while providing reasoning as well as localization.\nWhile existing works approach this problem using supervised training for\nspecific manipulation or anomaly detection in the embedding space,\ngeneralization across domains remains a challenge. We frame this problem of\nforgery detection as a prompt-driven visual reasoning task, leveraging the\nsemantic alignment capabilities of large vision-language models. We propose a\nframework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through\nAligned Language), that incorporates generalized guidelines. We propose two\ntangential approaches - (1) Holistic Scene-level Evaluation that relies on the\nphysics, semantics, perspective, and realism of the image as a whole and (2)\nRegion-wise anomaly detection that splits the image into multiple regions and\nanalyzes each of them. We conduct experiments over datasets from different\ndomains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language\nModels against competitive baselines and analyze the reasoning provided by\nthem.",
        "url": "http://arxiv.org/abs/2508.12543v1",
        "published_date": "2025-08-18T00:42:02+00:00",
        "updated_date": "2025-08-18T00:42:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ipsita Praharaj",
            "Yukta Butala",
            "Yash Butala"
        ],
        "tldr": "The paper introduces REVEAL, a framework leveraging large vision-language models for image forgery detection by using prompt-driven visual reasoning based on holistic scene-level evaluation and region-wise anomaly detection.",
        "tldr_zh": "该论文介绍了 REVEAL，一个利用大型视觉语言模型进行图像伪造检测的框架，通过基于整体场景评估和区域异常检测的提示驱动视觉推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models",
        "summary": "Vision Language Models (VLMs) integrate visual and text modalities to enable\nmultimodal understanding and generation. These models typically combine a\nVision Transformer (ViT) as an image encoder and a Large Language Model (LLM)\nfor text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning\nmethod to adapt pre-trained models to new tasks by introducing low-rank updates\nto their weights. While LoRA has emerged as a powerful technique for\nfine-tuning large models by introducing low-rank updates, current\nimplementations assume a fixed rank, potentially limiting flexibility and\nefficiency across diverse tasks. This paper introduces\n\\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural\nArchitecture Search (NAS) with LoRA to optimize VLMs for variable-rank\nadaptation. Our approach leverages NAS to dynamically search for the optimal\nLoRA rank configuration tailored to specific multimodal tasks, balancing\nperformance and computational efficiency. Through extensive experiments using\nthe LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates\nnotable improvement in model performance while reducing fine-tuning costs. Our\nBase and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be\nfound\n\\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\\textcolor{blue}{here}}\nand the code for LangVision-LoRA-NAS can be found\n\\href{https://github.com/krishnateja95/LangVision-NAS}{\\textcolor{blue}{here}}.",
        "url": "http://arxiv.org/abs/2508.12512v1",
        "published_date": "2025-08-17T22:19:02+00:00",
        "updated_date": "2025-08-17T22:19:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Krishna Teja Chitty-Venkata",
            "Murali Emani",
            "Venkatram Vishwanath"
        ],
        "tldr": "This paper introduces LangVision-LoRA-NAS, a framework that uses Neural Architecture Search (NAS) to optimize the LoRA rank in Vision Language Models (VLMs), improving performance and reducing fine-tuning costs.",
        "tldr_zh": "本文介绍了LangVision-LoRA-NAS，该框架使用神经架构搜索（NAS）来优化视觉语言模型（VLM）中的LoRA秩，从而提高性能并降低微调成本。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System",
        "summary": "Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a\ncritical role in sports science, rehabilitation, and clinical neurology.\nTraditional analysis of H-reflex EMG waveforms is subject to variability and\ninterpretation bias among clinicians and researchers, limiting reliability and\nstandardization. To address these challenges, we propose a Fine-Tuned\nVision-Language Model (VLM) Consortium and a reasoning Large-Language Model\n(LLM)-enabled Decision Support System for automated H-reflex waveform\ninterpretation and diagnosis. Our approach leverages multiple VLMs, each\nfine-tuned on curated datasets of H-reflex EMG waveform images annotated with\nclinical observations, recovery timelines, and athlete metadata. These models\nare capable of extracting key electrophysiological features and predicting\nneuromuscular states, including fatigue, injury, and recovery, directly from\nEMG images and contextual metadata. Diagnostic outputs from the VLM consortium\nare aggregated using a consensus-based method and refined by a specialized\nreasoning LLM, which ensures robust, transparent, and explainable decision\nsupport for clinicians and sports scientists. The end-to-end platform\norchestrates seamless communication between the VLM ensemble and the reasoning\nLLM, integrating prompt engineering strategies and automated reasoning\nworkflows using LLM Agents. Experimental results demonstrate that this hybrid\nsystem delivers highly accurate, consistent, and interpretable H-reflex\nassessments, significantly advancing the automation and standardization of\nneuromuscular diagnostics. To our knowledge, this work represents the first\nintegration of a fine-tuned VLM consortium with a reasoning LLM for image-based\nH-reflex analysis, laying the foundation for next-generation AI-assisted\nneuromuscular assessment and athlete monitoring platforms.",
        "url": "http://arxiv.org/abs/2508.12473v1",
        "published_date": "2025-08-17T19:13:27+00:00",
        "updated_date": "2025-08-17T19:13:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Eranga Bandara",
            "Ross Gore",
            "Sachin Shetty",
            "Ravi Mukkamala",
            "Christopher Rhea",
            "Atmaram Yarlagadda",
            "Shaifali Kaushik",
            "L. H. M. P. De Silva",
            "Andriy Maznychenko",
            "Inna Sokolowska",
            "Amin Hass",
            "Kasun De Zoysa"
        ],
        "tldr": "This paper presents a novel decision support system using a fine-tuned Vision-Language Model (VLM) consortium and a reasoning LLM for automated and standardized H-reflex EMG waveform interpretation, showing improved accuracy and explainability. This is the first integration of a VLM consortium with a reasoning LLM for image-based H-reflex analysis.",
        "tldr_zh": "本文提出了一种新颖的决策支持系统，该系统使用微调的视觉-语言模型（VLM）联盟和一个推理LLM，用于自动和标准化的H-反射EMG波形解释，从而提高了准确性和可解释性。 这是VLM联盟与推理LLM在基于图像的H-反射分析中的首次集成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping",
        "summary": "Traditional multimodal learning approaches require expensive alignment\npre-training to bridge vision and language modalities, typically projecting\nvisual features into discrete text token spaces. We challenge both fundamental\nassumptions underlying this paradigm by proposing Inverse-LLaVA, a novel\napproach that eliminates alignment pre-training entirely while inverting the\nconventional mapping direction. Rather than projecting visual features to text\nspace, our method maps text embeddings into continuous visual representation\nspace and performs fusion within transformer intermediate layers. Through\nselective additive components in attention mechanisms, we enable dynamic\nintegration of visual and textual representations without requiring massive\nimage-text alignment datasets. Comprehensive experiments across nine multimodal\nbenchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves\nnotable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,\nVizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing\nexpected decreases in perception tasks requiring memorized visual-text\nassociations (celebrity recognition: -49.5%, OCR: -21.3%). These results\nprovide the first empirical evidence that alignment pre-training is not\nnecessary for effective multimodal learning, particularly for complex reasoning\ntasks. Our work establishes the feasibility of a new paradigm that reduces\ncomputational requirements by 45%, challenges conventional wisdom about\nmodality fusion, and opens new research directions for efficient multimodal\narchitectures that preserve modality-specific characteristics. Our project\nwebsite with code and additional resources is available at\nhttps://inverse-llava.github.io.",
        "url": "http://arxiv.org/abs/2508.12466v1",
        "published_date": "2025-08-17T18:36:04+00:00",
        "updated_date": "2025-08-17T18:36:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xuhui Zhan",
            "Tyler Derr"
        ],
        "tldr": "Inverse-LLaVA eliminates the need for alignment pre-training in VLM by mapping text embeddings to visual space, achieving improvements in reasoning tasks while reducing computational costs.",
        "tldr_zh": "Inverse-LLaVA 通过将文本嵌入映射到视觉空间，消除了视觉语言模型中对齐预训练的需要，从而在推理任务中实现了改进，同时降低了计算成本。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning",
        "summary": "Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,\nyet its interpretation demands extensive clinical experience and suffers from\ninter-observer variability. While deep learning models offer high diagnostic\naccuracy, their black-box nature hinders clinical adoption in high-stakes\nmedical settings. To address this, we propose X-Ray-CoT (Chest X-Ray\nChain-of-Thought), a novel framework leveraging Vision-Language Large Models\n(LVLMs) for intelligent chest X-ray diagnosis and interpretable report\ngeneration. X-Ray-CoT simulates human radiologists' \"chain-of-thought\" by first\nextracting multi-modal features and visual concepts, then employing an\nLLM-based component with a structured Chain-of-Thought prompting strategy to\nreason and produce detailed natural language diagnostic reports. Evaluated on\nthe CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,\nwith a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease\ndiagnosis, slightly surpassing existing black-box models. Crucially, it\nuniquely generates high-quality, explainable reports, as validated by\npreliminary human evaluations. Our ablation studies confirm the integral role\nof each proposed component, highlighting the necessity of multi-modal fusion\nand CoT reasoning for robust and transparent medical AI. This work represents a\nsignificant step towards trustworthy and clinically actionable AI systems in\nmedical imaging.",
        "url": "http://arxiv.org/abs/2508.12455v1",
        "published_date": "2025-08-17T18:00:41+00:00",
        "updated_date": "2025-08-17T18:00:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chee Ng",
            "Liliang Sun",
            "Shaoqing Tang"
        ],
        "tldr": "The paper introduces X-Ray-CoT, a Vision-Language Large Model framework for chest X-ray diagnosis that generates interpretable reports via Chain-of-Thought prompting, achieving competitive accuracy and explainability.",
        "tldr_zh": "该论文介绍了X-Ray-CoT，一个用于胸部X射线诊断的视觉-语言大模型框架，它通过思维链提示生成可解释的报告，实现了具有竞争力的准确性和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy",
        "summary": "Vision-Language-Action (VLA) models frequently encounter challenges in\ngeneralizing to real-world environments due to inherent discrepancies between\nobservation and action spaces. Although training data are collected from\ndiverse camera perspectives, the models typically predict end-effector poses\nwithin the robot base coordinate frame, resulting in spatial inconsistencies.\nTo mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)\nframework, which grounds action predictions directly in the camera observation\nspace. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms\nend-effector poses from the robot base coordinate system into the camera\ncoordinate system, thereby unifying prediction targets across heterogeneous\nviewpoints. This lightweight, plug-and-play strategy ensures robust alignment\nbetween perception and action, substantially improving model resilience to\ncamera viewpoint variations. The proposed approach is readily compatible with\nexisting VLA architectures, requiring no substantial modifications.\nComprehensive evaluations on both simulated and real-world robotic manipulation\ntasks demonstrate that OC-VLA accelerates convergence, enhances task success\nrates, and improves cross-view generalization. The code will be publicly\navailable.",
        "url": "http://arxiv.org/abs/2508.13103v1",
        "published_date": "2025-08-18T17:10:45+00:00",
        "updated_date": "2025-08-18T17:10:45+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tianyi Zhang",
            "Haonan Duan",
            "Haoran Hao",
            "Yu Qiao",
            "Jifeng Dai",
            "Zhi Hou"
        ],
        "tldr": "The paper introduces Observation-Centric VLA (OC-VLA), a method to improve vision-language-action model generalization by grounding actions in the camera observation space, enhancing robustness to viewpoint variations.",
        "tldr_zh": "本文介绍了Observation-Centric VLA (OC-VLA)，该方法通过将动作与相机观察空间对齐来提高视觉-语言-动作模型的泛化能力，增强对视角变化的鲁棒性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art",
        "summary": "W\\\"olfflin's five principles offer a structured approach to analyzing\nstylistic variations for formal analysis. However, no existing metric\neffectively predicts all five principles in visual art. Computationally\nevaluating the visual aspects of a painting requires a metric that can\ninterpret key elements such as color, composition, and thematic choices. Recent\nadvancements in vision-language models (VLMs) have demonstrated their ability\nto evaluate abstract image attributes, making them promising candidates for\nthis task. In this work, we investigate whether CLIP, pre-trained on\nlarge-scale data, can understand and predict W\\\"olfflin's principles. Our\nfindings indicate that it does not inherently capture such nuanced stylistic\nelements. To address this, we fine-tune CLIP on annotated datasets of real art\nimages to predict a score for each principle. We evaluate our model, WP-CLIP,\non GAN-generated paintings and the Pandora-18K art dataset, demonstrating its\nability to generalize across diverse artistic styles. Our results highlight the\npotential of VLMs for automated art analysis.",
        "url": "http://arxiv.org/abs/2508.12668v1",
        "published_date": "2025-08-18T07:00:52+00:00",
        "updated_date": "2025-08-18T07:00:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abhijay Ghildyal",
            "Li-Yun Wang",
            "Feng Liu"
        ],
        "tldr": "The paper explores using CLIP to predict Wölfflin's principles in visual art, finding that pre-trained CLIP doesn't inherently capture these nuances, but fine-tuning improves performance and generalization across artistic styles.",
        "tldr_zh": "该论文探讨了使用CLIP来预测视觉艺术中沃尔夫林原则的可能性，发现预训练的CLIP本身无法捕捉到这些细微的差别，但通过微调可以提高性能并在不同的艺术风格中泛化。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning",
        "summary": "Creative image in advertising is the heart and soul of e-commerce platform.\nAn eye-catching creative image can enhance the shopping experience for users,\nboosting income for advertisers and advertising revenue for platforms. With the\nadvent of AIGC technology, advertisers can produce large quantities of creative\nimages at minimal cost. However, they struggle to assess the creative quality\nto select. Existing methods primarily focus on creative ranking, which fails to\naddress the need for explainable creative selection.\n  In this work, we propose the first paradigm for explainable creative\nassessment and selection. Powered by multimodal large language models (MLLMs),\nour approach integrates the assessment and selection of creative images into a\nnatural language generation task. To facilitate this research, we construct\nCreativePair, the first comparative reasoning-induced creative dataset\nfeaturing 8k annotated image pairs, with each sample including a label\nindicating which image is superior. Additionally, we introduce Creative4U\n(pronounced Creative for You), a MLLMs-based creative selector that takes into\naccount users' interests. Through Reason-to-Select RFT, which includes\nsupervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative\nPolicy Optimization (GRPO) based reinforcement learning, Creative4U is able to\nevaluate and select creative images accurately. Both offline and online\nexperiments demonstrate the effectiveness of our approach. Our code and dataset\nwill be made public to advance research and industrial applications.",
        "url": "http://arxiv.org/abs/2508.12628v1",
        "published_date": "2025-08-18T05:11:30+00:00",
        "updated_date": "2025-08-18T05:11:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yukang Lin",
            "Xiang Zhang",
            "Shichang Jia",
            "Bowen Wan",
            "Chenghan Fu",
            "Xudong Ren",
            "Yueran Liu",
            "Wanxian Guan",
            "Pengji Wang",
            "Jian Xu",
            "Bo Zheng",
            "Baolin Liu"
        ],
        "tldr": "The paper introduces Creative4U, a MLLM-based system for explainable advertising creative image selection, along with a new dataset CreativePair for comparative reasoning. It addresses the need for explainable creative selection and shows effectiveness through offline and online experiments.",
        "tldr_zh": "该论文介绍了Creative4U，一个基于MLLM的，可解释的广告创意图像选择系统，以及一个新的用于比较推理的数据集CreativePair。 它解决了对可解释的创意选择的需求，并通过离线和在线实验证明了其有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]