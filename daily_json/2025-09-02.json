[
    {
        "title": "Reinforced Visual Perception with Tools",
        "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
        "url": "http://arxiv.org/abs/2509.01656v1",
        "published_date": "2025-09-01T17:57:49+00:00",
        "updated_date": "2025-09-01T17:57:49+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zetong Zhou",
            "Dongping Chen",
            "Zixian Ma",
            "Zhihan Hu",
            "Mingyang Fu",
            "Sinan Wang",
            "Yao Wan",
            "Zhou Zhao",
            "Ranjay Krishna"
        ],
        "tldr": "The paper introduces ReVPT, a reinforcement learning approach to train multi-modal LLMs to effectively use visual tools for enhanced visual reasoning, achieving state-of-the-art results on perception-heavy benchmarks.",
        "tldr_zh": "该论文介绍了ReVPT，一种使用强化学习训练多模态LLM以有效利用视觉工具的方法，从而增强视觉推理能力，并在多个视觉感知基准测试中实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning",
        "summary": "This paper provides a simplification on OpenVision's architecture and loss\ndesign for enhancing its training efficiency. Following the prior\nvision-language pretraining works CapPa and AIMv2, as well as modern multimodal\ndesigns like LLaVA, our changes are straightforward: we remove the text encoder\n(and therefore the contrastive loss), retaining only the captioning loss as a\npurely generative training signal. We name this new version OpenVision 2. The\ninitial results are promising: despite this simplification, OpenVision 2\ncompetitively matches the original model's performance on a broad set of\nmultimodal benchmarks while substantially cutting both training time and memory\nconsumption. For example, with ViT-L/14, it reduces training time by about 1.5x\n(from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB,\nequivalently allowing the maximum batch size to grow from 2k to 8k). This\nsuperior training efficiency also allows us to scale far beyond the largest\nvision encoder used in OpenVision, reaching more than 1 billion parameters. We\nhold a strong belief that this lightweight, generative-only paradigm is\ncompelling for future vision encoder development in multimodal foundation\nmodels.",
        "url": "http://arxiv.org/abs/2509.01644v1",
        "published_date": "2025-09-01T17:38:21+00:00",
        "updated_date": "2025-09-01T17:38:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanqing Liu",
            "Xianhang Li",
            "Letian Zhang",
            "Zirui Wang",
            "Zeyu Zheng",
            "Yuyin Zhou",
            "Cihang Xie"
        ],
        "tldr": "OpenVision 2 simplifies the OpenVision architecture by removing the text encoder and contrastive loss, achieving comparable performance with significantly reduced training time and memory consumption, while enabling larger model scaling.",
        "tldr_zh": "OpenVision 2 通过移除文本编码器和对比损失简化了 OpenVision 架构，在显著减少训练时间和内存消耗的同时，实现了可比的性能，并能够进行更大规模的模型扩展。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Large Vision and Language Models by Learning from a Panel of Peers",
        "summary": "Traditional alignment methods for Large Vision and Language Models (LVLMs)\nprimarily rely on human-curated preference data. Human-generated preference\ndata is costly; machine-generated preference data is limited in quality; and\nself-supervised preference data often introduces hallucinations. To overcome\nthese limitations, we propose a novel Panel-of-Peers learning framework\ninspired by collaborative learning among humans. This approach leverages a\npanel of LVLMs, each evaluating and learning from their collective outputs\nthrough an iterative self-improvement process. By simulating a peer review\nsystem, our models generate, assess, and refine outputs in response to a\ncurated set of prompts, mimicking a classroom learning environment. We\ndemonstrate that this methodology enhances model performance without requiring\nextensive human-labeled datasets. Our experiments show significant improvement\nacross multiple benchmarks, demonstrating the potential of peer evaluations as\na scalable alternative to self-supervised alignment. Notably, we show that\nPanel-of-Peers increases the average score on fifteen benchmarks from 48% to\n57%",
        "url": "http://arxiv.org/abs/2509.01610v1",
        "published_date": "2025-09-01T16:43:48+00:00",
        "updated_date": "2025-09-01T16:43:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jefferson Hernandez",
            "Jing Shi",
            "Simon Jenni",
            "Vicente Ordonez",
            "Kushal Kafle"
        ],
        "tldr": "This paper introduces a 'Panel-of-Peers' learning framework for LVLMs, where multiple models collaboratively improve through iterative evaluation and refinement, achieving enhanced performance without extensive human labeling.",
        "tldr_zh": "该论文介绍了一种用于大型视觉语言模型 (LVLM) 的“同行小组”学习框架，其中多个模型通过迭代评估和改进进行协作改进，从而在无需大量人工标注的情况下提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Kwai Keye-VL 1.5 Technical Report",
        "summary": "In recent years, the development of Large Language Models (LLMs) has\nsignificantly advanced, extending their capabilities to multimodal tasks\nthrough Multimodal Large Language Models (MLLMs). However, video understanding\nremains a challenging area due to the dynamic and information-dense nature of\nvideos. Existing models struggle with the trade-off between spatial resolution\nand temporal coverage when processing video content. We present Keye-VL-1.5,\nwhich addresses fundamental challenges in video comprehension through three key\ninnovations. First, we introduce a novel Slow-Fast video encoding strategy that\ndynamically allocates computational resources based on inter-frame similarity,\nprocessing key frames with significant visual changes at higher resolution\n(Slow pathway) while handling relatively static frames with increased temporal\ncoverage at lower resolution (Fast pathway). Second, we implement a progressive\nfour-stage pre-training methodology that systematically extends the model's\ncontext length from 8K to 128K tokens, enabling processing of longer videos and\nmore complex visual content. Third, we develop a comprehensive post-training\npipeline focusing on reasoning enhancement and human preference alignment,\nincorporating a 5-step chain-of-thought data construction process, iterative\nGSPO-based reinforcement learning with progressive prompt hinting for difficult\ncases, and alignment training. Through extensive evaluation on public\nbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates\nsignificant improvements over existing models, particularly excelling in video\nunderstanding tasks while maintaining competitive performance on general\nmultimodal benchmarks.",
        "url": "http://arxiv.org/abs/2509.01563v1",
        "published_date": "2025-09-01T15:46:58+00:00",
        "updated_date": "2025-09-01T15:46:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Biao Yang",
            "Bin Wen",
            "Boyang Ding",
            "Changyi Liu",
            "Chenglong Chu",
            "Chengru Song",
            "Chongling Rao",
            "Chuan Yi",
            "Da Li",
            "Dunju Zang",
            "Fan Yang",
            "Guorui Zhou",
            "Guowang Zhang",
            "Han Shen",
            "Hao Peng",
            "Haojie Ding",
            "Hao Wang",
            "Hengrui Ju",
            "Jiaming Huang",
            "Jiangxia Cao",
            "Jiankang Chen",
            "Jingyun Hua",
            "Kaibing Chen",
            "Kaiyu Jiang",
            "Kaiyu Tang",
            "Kun Gai",
            "Muhao Wei",
            "Qiang Wang",
            "Ruitao Wang",
            "Sen Na",
            "Shengnan Zhang",
            "Siyang Mao",
            "Sui Huang",
            "Tianke Zhang",
            "Tingting Gao",
            "Wei Chen",
            "Wei Yuan",
            "Xiangyu Wu",
            "Xiao Hu",
            "Xingyu Lu",
            "Yi-Fan Zhang",
            "Yiping Yang",
            "Yulong Chen",
            "Zeyi Lu",
            "Zhenhua Wu",
            "Zhixin Ling",
            "Zhuoran Yang",
            "Ziming Li",
            "Di Xu",
            "Haixuan Gao",
            "Hang Li",
            "Jing Wang",
            "Lejian Ren",
            "Qigen Hu",
            "Qianqian Wang",
            "Shiyao Wang",
            "Xinchen Luo",
            "Yan Li",
            "Yuhang Hu",
            "Zixing Zhang"
        ],
        "tldr": "Keye-VL-1.5 introduces a novel Slow-Fast video encoding strategy, progressive pre-training for extended context, and a comprehensive post-training pipeline for enhanced video understanding, achieving significant improvements on benchmarks.",
        "tldr_zh": "Keye-VL-1.5 提出了一种新颖的 Slow-Fast 视频编码策略，用于扩展上下文的渐进式预训练，以及用于增强视频理解的综合性后训练流程，并在基准测试中取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Supervision For Vision-Language Modeling in 3D Computed Tomography",
        "summary": "General-purpose vision-language models (VLMs) have emerged as promising tools\nin radiology, offering zero-shot capabilities that mitigate the need for large\nlabeled datasets. However, in high-stakes domains like diagnostic radiology,\nthese models often lack the discriminative precision required for reliable\nclinical use. This challenge is compounded by the scarcity and heterogeneity of\npublicly available volumetric CT datasets, which vary widely in annotation\nformats and granularity. To address these limitations, we introduce Uniferum, a\nvolumetric VLM that unifies diverse supervision signals, encoded in\nclassification labels and segmentation masks, into a single training framework.\nBy harmonizing three public 3D CT datasets with distinct annotations, Uniferum\nachieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark\nby 7% compared to CLIP-based and conventional multi-label convolutional models.\nThe model demonstrates robust out-of-distribution generalization, with observed\nevidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT\ndatasets. Our results highlight the effectiveness of integrating heterogeneous\nannotations and body segmentation to enhance model performance, setting a new\ndirection for clinically reliable, data-efficient VLMs in 3D medical imaging.",
        "url": "http://arxiv.org/abs/2509.01554v1",
        "published_date": "2025-09-01T15:30:17+00:00",
        "updated_date": "2025-09-01T15:30:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hao-Chih Lee",
            "Zelong Liu",
            "Hamza Ahmed",
            "Spencer Kim",
            "Sean Huver",
            "Vishwesh Nath",
            "Zahi A. Fayad",
            "Timothy Deyer",
            "Xueyan Mei"
        ],
        "tldr": "The paper introduces Uniferum, a volumetric VLM that unifies diverse supervision signals in 3D CT data, achieving state-of-the-art performance and robust generalization in medical imaging.",
        "tldr_zh": "该论文介绍了Uniferum，一种体素VLM，它统一了3D CT数据中不同的监督信号，在医学影像中实现了最先进的性能和强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation",
        "summary": "Street-level geolocalization from images is crucial for a wide range of\nessential applications and services, such as navigation, location-based\nrecommendations, and urban planning. With the growing popularity of social\nmedia data and cameras embedded in smartphones, applying traditional computer\nvision techniques to localize images has become increasingly challenging, yet\nhighly valuable. This paper introduces a novel approach that integrates\nopen-weight and publicly accessible multimodal large language models with\nretrieval-augmented generation. The method constructs a vector database using\nthe SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query\nimages are augmented with prompts containing both similar and dissimilar\ngeolocation information retrieved from this database before being processed by\nthe multimodal large language models. Our approach has demonstrated\nstate-of-the-art performance, achieving higher accuracy compared against three\nwidely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our\nsolution eliminates the need for expensive fine-tuning or retraining and scales\nseamlessly to incorporate new data sources. The effectiveness of\nretrieval-augmented generation-based multimodal large language models in\ngeolocation estimation demonstrated by this paper suggests an alternative path\nto the traditional methods which rely on the training models from scratch,\nopening new possibilities for more accessible and scalable solutions in GeoAI.",
        "url": "http://arxiv.org/abs/2509.01341v1",
        "published_date": "2025-09-01T10:23:48+00:00",
        "updated_date": "2025-09-01T10:23:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yunus Serhat Bicakci",
            "Joseph Shingleton",
            "Anahid Basiri"
        ],
        "tldr": "This paper introduces a novel street-level geolocalization method using multimodal large language models and retrieval-augmented generation, achieving state-of-the-art performance without fine-tuning.",
        "tldr_zh": "本文提出了一种新的街景级地理定位方法，该方法使用多模态大型语言模型和检索增强生成，无需微调即可实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Novel Category Discovery with X-Agent Attention for Open-Vocabulary Semantic Segmentation",
        "summary": "Open-vocabulary semantic segmentation (OVSS) conducts pixel-level\nclassification via text-driven alignment, where the domain discrepancy between\nbase category training and open-vocabulary inference poses challenges in\ndiscriminative modeling of latent unseen category. To address this challenge,\nexisting vision-language model (VLM)-based approaches demonstrate commendable\nperformance through pre-trained multi-modal representations. However, the\nfundamental mechanisms of latent semantic comprehension remain underexplored,\nmaking the bottleneck for OVSS. In this work, we initiate a probing experiment\nto explore distribution patterns and dynamics of latent semantics in VLMs under\ninductive learning paradigms. Building on these insights, we propose X-Agent,\nan innovative OVSS framework employing latent semantic-aware ``agent'' to\norchestrate cross-modal attention mechanisms, simultaneously optimizing latent\nsemantic dynamic and amplifying its perceptibility. Extensive benchmark\nevaluations demonstrate that X-Agent achieves state-of-the-art performance\nwhile effectively enhancing the latent semantic saliency.",
        "url": "http://arxiv.org/abs/2509.01275v2",
        "published_date": "2025-09-01T09:01:58+00:00",
        "updated_date": "2025-09-03T03:02:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Li",
            "Yang Lu",
            "Yachao Zhang",
            "Fangyong Wang",
            "Yuan Xie",
            "Yanyun Qu"
        ],
        "tldr": "The paper introduces X-Agent, a novel open-vocabulary semantic segmentation (OVSS) framework that uses latent semantic-aware agents to improve cross-modal attention and enhance latent semantic saliency, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种新的开放词汇语义分割框架X-Agent，它使用潜在语义感知的agent来改进跨模态注意力并增强潜在语义显著性，从而实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation",
        "summary": "Scene Graph Generation (SGG) encodes visual relationships between objects in\nimages as graph structures. Thanks to the advances of Vision-Language Models\n(VLMs), the task of Open-Vocabulary SGG has been recently proposed where models\nare evaluated on their functionality to learn a wide and diverse range of\nrelations. Current benchmarks in SGG, however, possess a very limited\nvocabulary, making the evaluation of open-source models inefficient. In this\npaper, we propose a new reference-free metric to fairly evaluate the\nopen-vocabulary capabilities of VLMs for relation prediction. Another\nlimitation of Open-Vocabulary SGG is the reliance on weakly supervised data of\npoor quality for pre-training. We also propose a new solution for quickly\ngenerating high-quality synthetic data through region-specific prompt tuning of\nVLMs. Experimental results show that pre-training with this new data split can\nbenefit the generalization capabilities of Open-Voc SGG models.",
        "url": "http://arxiv.org/abs/2509.01209v1",
        "published_date": "2025-09-01T07:46:58+00:00",
        "updated_date": "2025-09-01T07:46:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maëlic Neau",
            "Zoe Falomir",
            "Cédric Buche",
            "Akihiro Sugimoto"
        ],
        "tldr": "This paper introduces a novel reference-free metric for evaluating open-vocabulary scene graph generation (SGG) models using Vision-Language Models (VLMs) and proposes a synthetic data generation method for pre-training to improve generalization.",
        "tldr_zh": "本文提出了一种新的无参考指标，用于评估使用视觉语言模型 (VLM) 的开放词汇场景图生成 (SGG) 模型，并提出了一种用于预训练的合成数据生成方法，以提高泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus",
        "summary": "Multi-subject personalized image generation aims to synthesize customized\nimages containing multiple specified subjects without requiring test-time\noptimization. However, achieving fine-grained independent control over multiple\nsubjects remains challenging due to difficulties in preserving subject fidelity\nand preventing cross-subject attribute leakage. We present FocusDPO, a\nframework that adaptively identifies focus regions based on dynamic semantic\ncorrespondence and supervision image complexity. During training, our method\nprogressively adjusts these focal areas across noise timesteps, implementing a\nweighted strategy that rewards information-rich patches while penalizing\nregions with low prediction confidence. The framework dynamically adjusts focus\nallocation during the DPO process according to the semantic complexity of\nreference images and establishes robust correspondence mappings between\ngenerated and reference subjects. Extensive experiments demonstrate that our\nmethod substantially enhances the performance of existing pre-trained\npersonalized generation models, achieving state-of-the-art results on both\nsingle-subject and multi-subject personalized image synthesis benchmarks. Our\nmethod effectively mitigates attribute leakage while preserving superior\nsubject fidelity across diverse generation scenarios, advancing the frontier of\ncontrollable multi-subject image synthesis.",
        "url": "http://arxiv.org/abs/2509.01181v1",
        "published_date": "2025-09-01T07:06:36+00:00",
        "updated_date": "2025-09-01T07:06:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiaoqiao Jin",
            "Siming Fu",
            "Dong She",
            "Weinan Jia",
            "Hualiang Wang",
            "Mu Liu",
            "Jidong Jiang"
        ],
        "tldr": "The paper introduces FocusDPO, a novel framework for multi-subject personalized image generation that adaptively identifies and focuses on relevant image regions, improving subject fidelity and reducing attribute leakage. It achieves state-of-the-art results on personalized image synthesis benchmarks.",
        "tldr_zh": "该论文介绍了一种名为FocusDPO的新框架，用于多主体个性化图像生成，它能够自适应地识别并聚焦于相关的图像区域，从而提高主体保真度并减少属性泄露。该方法在个性化图像合成基准测试中取得了最先进的成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models",
        "summary": "Recent advances in multimodal large language models (MLLMs) have led to much\nprogress in video understanding tasks. To avoid the heavy computational cost of\nprocessing all frames, these models typically rely on keyframe sampling methods\nguided by vision-language encoders (\\textit{e.g.,} SigLIP). However, it remains\nunclear whether such encoders can truly identify the most informative frames.\nIn this work, we provide several empirical pieces of evidence revealing that\npopular vision encoders critically suffer from their limited capability to\nidentify where the MLLM should look inside the video to handle the given\ntextual query appropriately. Our findings suggest that the development of\nbetter keyframe identification techniques may be necessary for efficient video\nMLLMs.",
        "url": "http://arxiv.org/abs/2509.01167v1",
        "published_date": "2025-09-01T06:39:08+00:00",
        "updated_date": "2025-09-01T06:39:08+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Hyunjong Ok",
            "Jaeho Lee"
        ],
        "tldr": "This paper investigates the limitations of current vision-language encoders used for keyframe sampling in video language models (VLMs), revealing their inability to accurately identify the most informative frames for a given text query. It suggests the need for better keyframe identification techniques for efficient VLMs.",
        "tldr_zh": "该论文研究了当前视频语言模型（VLM）中用于关键帧采样的视觉-语言编码器的局限性，揭示了它们无法准确识别给定文本查询的最具信息量帧。 这表明需要更好的关键帧识别技术来实现高效的VLM。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
        "summary": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.",
        "url": "http://arxiv.org/abs/2509.01106v1",
        "published_date": "2025-09-01T03:53:47+00:00",
        "updated_date": "2025-09-01T03:53:47+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Huang Fang",
            "Mengxi Zhang",
            "Heng Dong",
            "Wei Li",
            "Zixuan Wang",
            "Qifeng Zhang",
            "Xueyun Tian",
            "Yucheng Hu",
            "Hang Li"
        ],
        "tldr": "Robix is a unified vision-language model for robot interaction, reasoning, and planning that outperforms GPT-4o and Gemini 2.5 Pro in various interactive task execution scenarios, demonstrating strong generalization and novel capabilities.",
        "tldr_zh": "Robix是一个统一的视觉语言模型，用于机器人交互、推理和规划。在各种交互式任务执行场景中，Robix的性能优于GPT-4o和Gemini 2.5 Pro，展现了强大的泛化能力和创新功能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors",
        "summary": "Recent advancements in large video-language models have revolutionized video\nunderstanding tasks. However, their efficiency is significantly constrained by\nprocessing high volumes of visual tokens. Existing token compression strategies\napply a fixed compression ratio, ignoring the variability in semantic density\namong different video clips. Consequently, this lead to inadequate\nrepresentation of information-rich clips due to insufficient tokens and\nunnecessary computation on static or content-poor ones. To address this, we\npropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages a\nlightweight language model to describe video clips, converting them into soft\ncaption tokens as visual representations. Trained with our proposed semantic\ndensity-aware supervision, LangDC aims to 1) cover key visual cues necessary\nfor downstream task reasoning and 2) dynamically adjust compression ratios\nbased on scene richness, reflected by descriptions length. Our design mimics\nhow humans dynamically express what they see: complex scenes (seeing more)\nelicit more detailed language to convey nuances (saying more), whereas simpler\nscenes are described with fewer words. Experimental results show that our\nmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitive\nperformance. Furthermore, qualitative results demonstrate our approach\nadaptively adjusts the token compression ratio based on video segment richness.",
        "url": "http://arxiv.org/abs/2509.00969v1",
        "published_date": "2025-08-31T19:27:29+00:00",
        "updated_date": "2025-08-31T19:27:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangchen Wang",
            "Jinrui Zhang",
            "Teng Wang",
            "Haigang Zhang",
            "Feng Zheng"
        ],
        "tldr": "The paper introduces LangDC, a language-aware dynamic token compressor for video-language models that uses a lightweight language model to generate soft caption tokens and dynamically adjust compression ratios based on scene richness. It achieves significant FLOPs reduction while maintaining competitive performance.",
        "tldr_zh": "该论文介绍了一种名为LangDC的语言感知动态令牌压缩器，用于视频-语言模型。它利用轻量级语言模型生成软字幕令牌，并根据场景的丰富程度动态调整压缩率，在保持竞争力的同时显著降低了FLOPs。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement",
        "summary": "Identity-preserving text-to-video (IPT2V) generation creates videos faithful\nto both a reference subject image and a text prompt. While fine-tuning large\npretrained video diffusion models on ID-matched data achieves state-of-the-art\nresults on IPT2V, data scarcity and high tuning costs hinder broader\nimprovement. We thus introduce a Training-Free Prompt, Image, and Guidance\nEnhancement (TPIGE) framework that bridges the semantic gap between the video\ndescription and the reference image and design sampling guidance that enhances\nidentity preservation and video quality, achieving performance gains at minimal\ncost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o\nto enhance the text prompt with facial details derived from the reference\nimage. We then propose Prompt Aware Reference Image Enhancement, leveraging an\nidentity-preserving image generator to refine the reference image, rectifying\nconflicts with the text prompt. The above mutual refinement significantly\nimproves input quality before video generation. Finally, we propose ID-Aware\nSpatiotemporal Guidance Enhancement, utilizing unified gradients to optimize\nidentity preservation and video quality jointly during generation.Our method\noutperforms prior work and is validated by automatic and human evaluations on a\n1000 video test set, winning first place in the ACM Multimedia 2025\nIdentity-Preserving Video Generation Challenge, demonstrating state-of-the-art\nperformance and strong generality. The code is available at\nhttps://github.com/Andyplus1/IPT2V.git.",
        "url": "http://arxiv.org/abs/2509.01362v1",
        "published_date": "2025-09-01T11:03:13+00:00",
        "updated_date": "2025-09-01T11:03:13+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jiayi Gao",
            "Changcheng Hua",
            "Qingchao Chen",
            "Yuxin Peng",
            "Yang Liu"
        ],
        "tldr": "This paper introduces a training-free framework (TPIGE) for identity-preserving text-to-video generation that enhances prompts, images, and guidance to improve video quality and identity preservation, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了一种无需训练的框架 (TPIGE)，用于保持身份的文本到视频生成，该框架通过增强提示、图像和指导来提高视频质量和身份保持，实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion",
        "summary": "High-quality labeled data is essential for training accurate document\nconversion models, particularly in domains with complex formats such as tables,\nformulas, and multi-column text. However, manual annotation is both costly and\ntime-consuming, while automatic labeling using existing models often lacks\naccuracy in handling such challenging scenarios. Consequently, training student\nmodels by distilling outputs from teacher models can significantly limit their\nperformance in real-world applications. In this paper, we propose a fully\nautomated, distillation-free framework comprising two stages for constructing\nhigh-quality document extraction datasets and models capable of handling\ndiverse document formats and layouts. In the first stage, we introduce a method\nfor generating large-scale, diverse synthetic data, which enables a model to\nextract key elements in a unified format with strong initial performance. In\nthe second stage, we present a self-improvement approach that further adapts\nthe model, initially trained on synthetic data, to real-world documents.\nSpecifically, we first use the fine-tuned model to annotate real documents,\nthen apply a suite of filtering strategies to verify annotation quality, and\nfinally retrain the model on the verified dataset. By iteratively repeating\nthis process, we progressively enhance both the model's conversion capabilities\nand the quality of the generated data. We train a public POINTS-1.5 model to\nobtain POINTS-Reader, which surpasses many existing public and proprietary\nmodels of comparable or larger size. Our model is available at\nhttps://github.com/Tencent/POINTS-Reader.",
        "url": "http://arxiv.org/abs/2509.01215v1",
        "published_date": "2025-09-01T07:54:18+00:00",
        "updated_date": "2025-09-01T07:54:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Liu",
            "Zhongyin Zhao",
            "Le Tian",
            "Haicheng Wang",
            "Xubing Ye",
            "Yangxiu You",
            "Zilin Yu",
            "Chuhan Wu",
            "Xiao Zhou",
            "Yang Yu",
            "Jie Zhou"
        ],
        "tldr": "The paper introduces a distillation-free, two-stage framework (POINTS-Reader) for document conversion, using synthetic data generation and self-improvement on real documents to achieve state-of-the-art performance.",
        "tldr_zh": "该论文介绍了一个无蒸馏的两阶段文档转换框架 (POINTS-Reader)，它使用合成数据生成和真实文档上的自我改进来实现最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]