[
    {
        "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling",
        "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.",
        "url": "http://arxiv.org/abs/2511.10648v1",
        "published_date": "2025-11-13T18:59:57+00:00",
        "updated_date": "2025-11-13T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Weiye Xu",
            "Aijun Yang",
            "Wengang Zhou",
            "Lewei Lu",
            "Houqiang Li",
            "Xiaohua Wang",
            "Jinguo Zhu"
        ],
        "tldr": "The paper proposes Self-Consistency Sampling (SCS) to improve outcome-reward reinforcement learning for multimodal large language models (MLLMs) by addressing the issue of unfaithful trajectories, achieving significant accuracy gains on various benchmarks with negligible extra computation.",
        "tldr_zh": "该论文提出了自洽采样（SCS）方法，通过解决不忠实轨迹的问题，改进多模态大型语言模型（MLLM）的基于结果奖励的强化学习，并在各种基准测试中实现了显著的准确性提升，且几乎没有额外的计算负担。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
        "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
        "url": "http://arxiv.org/abs/2511.10615v1",
        "published_date": "2025-11-13T18:45:39+00:00",
        "updated_date": "2025-11-13T18:45:39+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Shruti Singh Baghel",
            "Yash Pratap Singh Rathore",
            "Sushovan Jena",
            "Anurag Pradhan",
            "Amit Shukla",
            "Arnav Bhavsar",
            "Pawan Goyal"
        ],
        "tldr": "This paper explores the performance of lightweight VLMs for generating video descriptions tailored for blind and low-vision users, introducing new evaluation frameworks and prompt strategies, and assessing their feasibility on mobile devices.",
        "tldr_zh": "本文探讨了轻量级视觉语言模型（VLMs）在生成专为盲人和低视力用户设计的视频描述方面的性能，引入了新的评估框架和提示策略，并评估了它们在移动设备上的可行性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]