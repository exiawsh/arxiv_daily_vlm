[
    {
        "title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis",
        "summary": "For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.",
        "url": "http://arxiv.org/abs/2508.20068v1",
        "published_date": "2025-08-27T17:22:34+00:00",
        "updated_date": "2025-08-27T17:22:34+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Chengzu Li",
            "Wenshan Wu",
            "Huanyu Zhang",
            "Qingtao Li",
            "Zeyu Gao",
            "Yan Xia",
            "José Hernández-Orallo",
            "Ivan Vulić",
            "Furu Wei"
        ],
        "tldr": "The paper introduces 11Plus-Bench, a benchmark for evaluating spatial reasoning in MLLMs, revealing they show early signs of spatial cognition but still significantly underperform humans with unpredictable instance-level performance.",
        "tldr_zh": "该论文介绍了11Plus-Bench，一个用于评估多模态大语言模型空间推理能力的基准。研究发现，这些模型初步展现出空间认知能力，但与人类相比仍有显著差距，且在实例层面的表现难以预测。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Segmentation Assisted Incremental Test Time Adaptation in an Open World",
        "summary": "In dynamic environments, unfamiliar objects and distribution shifts are often\nencountered, which challenge the generalization abilities of the deployed\ntrained models. This work addresses Incremental Test Time Adaptation of Vision\nLanguage Models, tackling scenarios where unseen classes and unseen domains\ncontinuously appear during testing. Unlike traditional Test Time Adaptation\napproaches, where the test stream comes only from a predefined set of classes,\nour framework allows models to adapt simultaneously to both covariate and label\nshifts, actively incorporating new classes as they emerge. Towards this goal,\nwe establish a new benchmark for ITTA, integrating single image TTA methods for\nVLMs with active labeling techniques that query an oracle for samples\npotentially representing unseen classes during test time. We propose a\nsegmentation assisted active labeling module, termed SegAssist, which is\ntraining free and repurposes the segmentation capabilities of VLMs to refine\nactive sample selection, prioritizing samples likely to belong to unseen\nclasses. Extensive experiments on several benchmark datasets demonstrate the\npotential of SegAssist to enhance the performance of VLMs in real world\nscenarios, where continuous adaptation to emerging data is essential.\nProject-page:https://manogna-s.github.io/segassist/",
        "url": "http://arxiv.org/abs/2508.20029v1",
        "published_date": "2025-08-27T16:33:32+00:00",
        "updated_date": "2025-08-27T16:33:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manogna Sreenivas",
            "Soma Biswas"
        ],
        "tldr": "The paper introduces a new benchmark and a segmentation-assisted active labeling module (SegAssist) for Incremental Test Time Adaptation (ITTA) of Vision Language Models (VLMs) in open-world scenarios with unseen classes and domain shifts.",
        "tldr_zh": "该论文介绍了一个新的基准和一个分割辅助主动标签模块(SegAssist)，用于视觉语言模型(VLM)在开放世界场景中，处理未见过的类别和领域偏移的增量测试时自适应(ITTA)。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GS: Generative Segmentation via Label Diffusion",
        "summary": "Language-driven image segmentation is a fundamental task in vision-language\nunderstanding, requiring models to segment regions of an image corresponding to\nnatural language expressions. Traditional methods approach this as a\ndiscriminative problem, assigning each pixel to foreground or background based\non semantic alignment. Recently, diffusion models have been introduced to this\ndomain, but existing approaches remain image-centric: they either (i) use image\ndiffusion models as visual feature extractors, (ii) synthesize segmentation\ndata via image generation to train discriminative models, or (iii) perform\ndiffusion inversion to extract attention cues from pre-trained image diffusion\nmodels-thereby treating segmentation as an auxiliary process. In this paper, we\npropose GS (Generative Segmentation), a novel framework that formulates\nsegmentation itself as a generative task via label diffusion. Instead of\ngenerating images conditioned on label maps and text, GS reverses the\ngenerative process: it directly generates segmentation masks from noise,\nconditioned on both the input image and the accompanying language description.\nThis paradigm makes label generation the primary modeling target, enabling\nend-to-end training with explicit control over spatial and semantic fidelity.\nTo demonstrate the effectiveness of our approach, we evaluate GS on Panoptic\nNarrative Grounding (PNG), a representative and challenging benchmark for\nmultimodal segmentation that requires panoptic-level reasoning guided by\nnarrative captions. Experimental results show that GS significantly outperforms\nexisting discriminative and diffusion-based methods, setting a new\nstate-of-the-art for language-driven segmentation.",
        "url": "http://arxiv.org/abs/2508.20020v1",
        "published_date": "2025-08-27T16:28:15+00:00",
        "updated_date": "2025-08-27T16:28:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Chen",
            "Shubin Chen",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "tldr": "The paper introduces GS, a novel generative framework for language-driven image segmentation that directly generates segmentation masks from noise conditioned on images and language, outperforming existing methods on the Panoptic Narrative Grounding benchmark.",
        "tldr_zh": "本文介绍了一种名为GS的新型生成框架，用于语言驱动的图像分割，该框架直接从噪声中生成分割掩码，并以图像和语言为条件，在全景叙事基础基准测试中优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity",
        "summary": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.",
        "url": "http://arxiv.org/abs/2508.19972v1",
        "published_date": "2025-08-27T15:30:06+00:00",
        "updated_date": "2025-08-27T15:30:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Seongheon Park",
            "Yixuan Li"
        ],
        "tldr": "The paper introduces GLSim, a training-free framework for detecting object hallucinations in LVLMs by combining global and local embedding similarity, demonstrating superior performance compared to existing methods.",
        "tldr_zh": "该论文介绍了一种名为GLSim的无需训练的框架，通过结合全局和局部嵌入相似性来检测LVLM中的对象幻觉，并展示了优于现有方法的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
        "summary": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.",
        "url": "http://arxiv.org/abs/2508.19652v1",
        "published_date": "2025-08-27T08:01:03+00:00",
        "updated_date": "2025-08-27T08:01:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongxia Li",
            "Wenhao Yu",
            "Chengsong Huang",
            "Rui Liu",
            "Zhenwen Liang",
            "Fuxiao Liu",
            "Jingxi Che",
            "Dian Yu",
            "Jordan Boyd-Graber",
            "Haitao Mi",
            "Dong Yu"
        ],
        "tldr": "The paper introduces Vision-SR1, a self-rewarding method that decomposes VLM reasoning into visual perception and language reasoning stages, improving visual reasoning and mitigating hallucinations without external visual supervision.",
        "tldr_zh": "该论文介绍了一种名为Vision-SR1的自奖励方法，它将VLM推理分解为视觉感知和语言推理阶段，从而改进了视觉推理并减轻了幻觉，而无需外部视觉监督。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scalable Object Detection in the Car Interior With Vision Foundation Models",
        "summary": "AI tasks in the car interior like identifying and localizing externally\nintroduced objects is crucial for response quality of personal assistants.\nHowever, computational resources of on-board systems remain highly constrained,\nrestricting the deployment of such solutions directly within the vehicle. To\naddress this limitation, we propose the novel Object Detection and Localization\n(ODAL) framework for interior scene understanding. Our approach leverages\nvision foundation models through a distributed architecture, splitting\ncomputational tasks between on-board and cloud. This design overcomes the\nresource constraints of running foundation models directly in the car. To\nbenchmark model performance, we introduce ODALbench, a new metric for\ncomprehensive assessment of detection and localization.Our analysis\ndemonstrates the framework's potential to establish new standards in this\ndomain. We compare the state-of-the-art GPT-4o vision foundation model with the\nlightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the\nlightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model\nachieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its\nbaseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the\nfine-tuned model maintains high detection accuracy while significantly reducing\nhallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.",
        "url": "http://arxiv.org/abs/2508.19651v1",
        "published_date": "2025-08-27T07:58:57+00:00",
        "updated_date": "2025-08-27T07:58:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bálint Mészáros",
            "Ahmet Firintepe",
            "Sebastian Schmidt",
            "Stephan Günnemann"
        ],
        "tldr": "This paper introduces ODAL, a distributed framework leveraging vision foundation models for scalable object detection in car interiors, along with ODALbench, a new evaluation metric. They demonstrate that fine-tuning a lightweight VLM can outperform GPT-4o in this specific domain.",
        "tldr_zh": "该论文介绍了ODAL，一个利用视觉基础模型进行汽车内部可扩展对象检测的分布式框架，以及新的评估指标ODALbench。他们证明，微调轻量级VLM可以在这个特定领域优于GPT-4o。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments",
        "summary": "Recent progress in large language models (LLMs) has shown strong potential\nfor multimodal reasoning beyond natural language. In this work, we explore the\nuse of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for\nclassifying neutrino interactions from pixelated detector images in high-energy\nphysics (HEP) experiments. We benchmark its performance against an established\nCNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as\nclassification accuracy, precision, recall, and AUC-ROC. Our results show that\nthe VLM not only matches or exceeds CNN performance but also enables richer\nreasoning and better integration of auxiliary textual or semantic context.\nThese findings suggest that VLMs offer a promising general-purpose backbone for\nevent classification in HEP, paving the way for multimodal approaches in\nexperimental neutrino physics.",
        "url": "http://arxiv.org/abs/2508.19376v1",
        "published_date": "2025-08-26T19:12:28+00:00",
        "updated_date": "2025-08-26T19:12:28+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "hep-ex"
        ],
        "authors": [
            "Dikshant Sagar",
            "Kaiwen Yu",
            "Alejandro Yankelevich",
            "Jianming Bian",
            "Pierre Baldi"
        ],
        "tldr": "This paper explores fine-tuning a Vision-Language Model (VLM) based on LLaMA 3.2 for neutrino event classification in high-energy physics, demonstrating comparable or superior performance to CNNs and enabling richer reasoning through multimodal integration.",
        "tldr_zh": "本文探索了基于LLaMA 3.2的视觉语言模型（VLM）在粒子物理中中微子事件分类的微调，结果表明其性能与卷积神经网络（CNN）相当甚至更优，并通过多模态集成实现更丰富的推理。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
        "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.",
        "url": "http://arxiv.org/abs/2508.20072v1",
        "published_date": "2025-08-27T17:39:11+00:00",
        "updated_date": "2025-08-27T17:39:11+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Zhixuan Liang",
            "Yizhuo Li",
            "Tianshuo Yang",
            "Chengyue Wu",
            "Sitong Mao",
            "Liuao Pei",
            "Xiaokang Yang",
            "Jiangmiao Pang",
            "Yao Mu",
            "Ping Luo"
        ],
        "tldr": "The paper introduces Discrete Diffusion VLA, a novel single-transformer policy for Vision-Language-Action tasks that utilizes discrete diffusion for action decoding, improving performance and scalability compared to autoregressive and continuous diffusion methods.",
        "tldr_zh": "该论文介绍了Discrete Diffusion VLA，一种用于视觉-语言-动作任务的新型单transformer策略，它利用离散扩散进行动作解码，与自回归和连续扩散方法相比，提高了性能和可扩展性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models",
        "summary": "Geo-localization is the task of identifying the location of an image using\nvisual cues alone. It has beneficial applications, such as improving disaster\nresponse, enhancing navigation, and geography education. Recently,\nVision-Language Models (VLMs) are increasingly demonstrating capabilities as\naccurate image geo-locators. This brings significant privacy risks, including\nthose related to stalking and surveillance, considering the widespread uses of\nAI models and sharing of photos on social media. The precision of these models\nis likely to improve in the future. Despite these risks, there is little work\non systematically evaluating the geolocation precision of Generative VLMs,\ntheir limits and potential for unintended inferences. To bridge this gap, we\nconduct a comprehensive assessment of the geolocation capabilities of 25\nstate-of-the-art VLMs on four benchmark image datasets captured in diverse\nenvironments. Our results offer insight into the internal reasoning of VLMs and\nhighlight their strengths, limitations, and potential societal risks. Our\nfindings indicate that current VLMs perform poorly on generic street-level\nimages yet achieve notably high accuracy (61\\%) on images resembling social\nmedia content, raising significant and urgent privacy concerns.",
        "url": "http://arxiv.org/abs/2508.19967v1",
        "published_date": "2025-08-27T15:21:31+00:00",
        "updated_date": "2025-08-27T15:21:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Oliver Grainge",
            "Sania Waheed",
            "Jack Stilgoe",
            "Michael Milford",
            "Shoaib Ehsan"
        ],
        "tldr": "This paper evaluates the geolocation capabilities of 25 VLMs, highlighting their accuracy, limitations, and societal risks, particularly concerning privacy on social media-like images.",
        "tldr_zh": "本文评估了25个视觉-语言模型(VLM)的地理定位能力，突出了它们的准确性、局限性和社会风险，尤其是在社交媒体图像上的隐私问题。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts",
        "summary": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.",
        "url": "http://arxiv.org/abs/2508.19944v1",
        "published_date": "2025-08-27T15:01:02+00:00",
        "updated_date": "2025-08-27T15:01:02+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Taebaek Hwang",
            "Minseo Kim",
            "Gisang Lee",
            "Seonuk Kim",
            "Hyunjun Eun"
        ],
        "tldr": "The paper introduces KRETA, a new Korean benchmark for text-rich visual question answering (VQA) designed to evaluate VLMs in diverse visual contexts, along with a semi-automated VQA generation pipeline.",
        "tldr_zh": "该论文介绍了KRETA，一个新的韩语基准，用于文本丰富的视觉问答（VQA），旨在评估多样视觉环境下的VLM，以及一个半自动化的VQA生成管道。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement",
        "summary": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.",
        "url": "http://arxiv.org/abs/2508.19887v1",
        "published_date": "2025-08-27T13:48:04+00:00",
        "updated_date": "2025-08-27T13:48:04+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Mohammed Rakibul Hasan",
            "Rafi Majid",
            "Ahanaf Tahmid"
        ],
        "tldr": "The paper introduces Bangla-Bayanno, a new 52K question-answer pair VQA dataset in Bangla, addressing the lack of high-quality, open-source resources for this low-resource language by using LLM-assisted translation refinement.",
        "tldr_zh": "该论文介绍了Bangla-Bayanno，一个新的包含52K问答对的孟加拉语视觉问答（VQA）数据集。它通过使用LLM辅助的翻译优化，解决了该低资源语言缺乏高质量开源资源的问题。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
        "summary": "In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.",
        "url": "http://arxiv.org/abs/2508.19852v1",
        "published_date": "2025-08-27T13:09:55+00:00",
        "updated_date": "2025-08-27T13:09:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Binjie Zhang",
            "Mike Zheng Shou"
        ],
        "tldr": "This paper introduces a two-stage framework for egocentric action and visual future prediction, conditioned on hand trajectories, using consecutive state modeling and causal cross-attention with a Latent Diffusion Model. It outperforms existing methods on Ego4D, BridgeData, and RLBench datasets.",
        "tldr_zh": "本文提出了一种双阶段框架，用于以手部轨迹为条件的自我中心动作和视觉未来预测，使用连续状态建模和因果交叉注意力与潜在扩散模型。在 Ego4D、BridgeData 和 RLBench 数据集上优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models",
        "summary": "Large video language models (LVLMs) have made notable progress in video\nunderstanding, spurring the development of corresponding evaluation benchmarks.\nHowever, existing benchmarks generally assess overall performance across entire\nvideo sequences, overlooking nuanced behaviors such as contextual positional\nbias, a critical yet under-explored aspect of LVLM performance. We present\nVideo-LevelGauge, a dedicated benchmark designed to systematically assess\npositional bias in LVLMs. We employ standardized probes and customized\ncontextual setups, allowing flexible control over context length, probe\nposition, and contextual types to simulate diverse real-world scenarios. In\naddition, we introduce a comprehensive analysis method that combines\nstatistical measures with morphological pattern recognition to characterize\nbias. Our benchmark comprises 438 manually curated videos spanning multiple\ntypes, yielding 1,177 high-quality multiple-choice questions and 120 open-ended\nquestions, validated for their effectiveness in exposing positional bias. Based\non these, we evaluate 27 state-of-the-art LVLMs, including both commercial and\nopen-source models. Our findings reveal significant positional biases in many\nleading open-source models, typically exhibiting head or neighbor-content\npreferences. In contrast, commercial models such as Gemini2.5-Pro show\nimpressive, consistent performance across entire video sequences. Further\nanalyses on context length, context variation, and model scale provide\nactionable insights for mitigating bias and guiding model enhancement.",
        "url": "http://arxiv.org/abs/2508.19650v1",
        "published_date": "2025-08-27T07:58:16+00:00",
        "updated_date": "2025-08-27T07:58:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hou Xia",
            "Zheren Fu",
            "Fangcan Ling",
            "Jiajun Li",
            "Yi Tu",
            "Zhendong Mao",
            "Yongdong Zhang"
        ],
        "tldr": "The paper introduces Video-LevelGauge, a benchmark to evaluate contextual positional bias in Large Video Language Models (LVLMs), revealing biases in open-source models and consistent performance in commercial models.",
        "tldr_zh": "该论文介绍了Video-LevelGauge，一个用于评估大型视频语言模型（LVLM）中上下文位置偏差的基准，揭示了开源模型中的偏差以及商业模型中的一致性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "JVLGS: Joint Vision-Language Gas Leak Segmentation",
        "summary": "Gas leaks pose serious threats to human health and contribute significantly\nto atmospheric pollution, drawing increasing public concern. However, the lack\nof effective detection methods hampers timely and accurate identification of\ngas leaks. While some vision-based techniques leverage infrared videos for leak\ndetection, the blurry and non-rigid nature of gas clouds often limits their\neffectiveness. To address these challenges, we propose a novel framework called\nJoint Vision-Language Gas leak Segmentation (JVLGS), which integrates the\ncomplementary strengths of visual and textual modalities to enhance gas leak\nrepresentation and segmentation. Recognizing that gas leaks are sporadic and\nmany video frames may contain no leak at all, our method incorporates a\npost-processing step to reduce false positives caused by noise and non-target\nobjects, an issue that affects many existing approaches. Extensive experiments\nconducted across diverse scenarios show that JVLGS significantly outperforms\nstate-of-the-art gas leak segmentation methods. We evaluate our model under\nboth supervised and few-shot learning settings, and it consistently achieves\nstrong performance in both, whereas competing methods tend to perform well in\nonly one setting or poorly in both. Code available at:\nhttps://github.com/GeekEagle/JVLGS",
        "url": "http://arxiv.org/abs/2508.19485v1",
        "published_date": "2025-08-27T00:10:43+00:00",
        "updated_date": "2025-08-27T00:10:43+00:00",
        "categories": [
            "cs.CV",
            "68T45 (Primary), 68T07 (Secondary)",
            "I.2.10; I.4.6"
        ],
        "authors": [
            "Xinlong Zhao",
            "Qixiang Pang",
            "Shan Du"
        ],
        "tldr": "The paper introduces JVLGS, a novel vision-language framework for gas leak segmentation in infrared videos, addressing the challenges of blurry and non-rigid gas clouds with a post-processing step to reduce false positives, outperforming state-of-the-art methods in supervised and few-shot settings.",
        "tldr_zh": "该论文介绍了一种新的视觉-语言框架JVLGS，用于红外视频中的气体泄漏分割。该框架通过后处理步骤减少误报，解决了气体云模糊和非刚性的问题，并在监督和少样本设置中优于最先进的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents",
        "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
        "url": "http://arxiv.org/abs/2508.19493v1",
        "published_date": "2025-08-27T00:41:28+00:00",
        "updated_date": "2025-08-27T00:41:28+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Zhixin Lin",
            "Jungang Li",
            "Shidong Pan",
            "Yibo Shi",
            "Yue Yao",
            "Dongliang Xu"
        ],
        "tldr": "This paper introduces a benchmark (SAPA-Bench) to evaluate the privacy awareness of MLLM-powered smartphone agents, finding that existing agents perform poorly in protecting user privacy and closed-source agents perform slightly better.",
        "tldr_zh": "本文介绍了一个基准测试 (SAPA-Bench)，用于评估 MLLM 驱动的智能手机代理的隐私意识，发现现有代理在保护用户隐私方面表现不佳，并且闭源代理的表现略好。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]