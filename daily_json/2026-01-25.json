[
    {
        "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
        "summary": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
        "url": "http://arxiv.org/abs/2601.16973v1",
        "published_date": "2026-01-23T18:43:34+00:00",
        "updated_date": "2026-01-23T18:43:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zirui Wang",
            "Junyi Zhang",
            "Jiaxin Ge",
            "Long Lian",
            "Letian Fu",
            "Lisa Dunlap",
            "Ken Goldberg",
            "XuDong Wang",
            "Ion Stoica",
            "David M. Chan",
            "Sewon Min",
            "Joseph E. Gonzalez"
        ],
        "tldr": "The paper introduces VisGym, a suite of 17 environments for evaluating and training Vision-Language Models (VLMs) in multi-step visual interaction tasks, revealing limitations in long-context utilization and visual reasoning but also demonstrating potential improvements through explicit goal observations and finetuning.",
        "tldr_zh": "该论文介绍了 VisGym，一个包含 17 个环境的套件，用于评估和训练视觉语言模型 (VLM) 在多步骤视觉交互任务中的表现。研究揭示了模型在长上下文利用和视觉推理方面的局限性，同时也展示了通过明确的目标观察和微调来改进的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]