[
    {
        "title": "VLM-Guided Adaptive Negative Prompting for Creative Generation",
        "summary": "Creative generation is the synthesis of new, surprising, and valuable samples\nthat reflect user intent yet cannot be envisioned in advance. This task aims to\nextend human imagination, enabling the discovery of visual concepts that exist\nin the unexplored spaces between familiar domains. While text-to-image\ndiffusion models excel at rendering photorealistic scenes that faithfully match\nuser prompts, they still struggle to generate genuinely novel content. Existing\napproaches to enhance generative creativity either rely on interpolation of\nimage features, which restricts exploration to predefined categories, or\nrequire time-intensive procedures such as embedding optimization or model\nfine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, a\ntraining-free, inference-time method that promotes creative image generation\nwhile preserving the validity of the generated object. Our approach utilizes a\nvision-language model (VLM) that analyzes intermediate outputs of the\ngeneration process and adaptively steers it away from conventional visual\nconcepts, encouraging the emergence of novel and surprising outputs. We\nevaluate creativity through both novelty and validity, using statistical\nmetrics in the CLIP embedding space. Through extensive experiments, we show\nconsistent gains in creative novelty with negligible computational overhead.\nMoreover, unlike existing methods that primarily generate single objects, our\napproach extends to complex scenarios, such as generating coherent sets of\ncreative objects and preserving creativity within elaborate compositional\nprompts. Our method integrates seamlessly into existing diffusion pipelines,\noffering a practical route to producing creative outputs that venture beyond\nthe constraints of textual descriptions.",
        "url": "http://arxiv.org/abs/2510.10715v1",
        "published_date": "2025-10-12T17:34:59+00:00",
        "updated_date": "2025-10-12T17:34:59+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Shelly Golan",
            "Yotam Nitzan",
            "Zongze Wu",
            "Or Patashnik"
        ],
        "tldr": "This paper introduces a training-free, inference-time method called VLM-Guided Adaptive Negative Prompting to enhance creative image generation by steering diffusion models away from conventional visual concepts using a vision-language model (VLM). The method improves novelty while preserving object validity and works with complex prompts.",
        "tldr_zh": "本文介绍了一种名为VLM引导的自适应负提示的训练自由、推理期方法，通过使用视觉语言模型(VLM)引导扩散模型远离传统的视觉概念，从而增强创意图像生成。该方法提高了新颖性，同时保持了对象有效性，并适用于复杂的提示。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey",
        "summary": "Image-Language Foundation Models (ILFM) have demonstrated remarkable success\nin image-text understanding/generation tasks, providing transferable multimodal\nrepresentations that generalize across diverse downstream image-based tasks.\nThe advancement of video-text research has spurred growing interest in\nextending image-based models to the video domain. This paradigm, known as\nimage-to-video transfer learning, succeeds in alleviating the substantial data\nand computational requirements associated with training video-language\nfoundation models from scratch for video-text learning. This survey provides\nthe first comprehensive review of this emerging field, which begins by\nsummarizing the widely used ILFM and their capabilities. We then systematically\nclassify existing image-to-video transfer learning strategies into two\ncategories: frozen features and modified features, depending on whether the\noriginal representations from ILFM are preserved or undergo modifications.\nBuilding upon the task-specific nature of image-to-video transfer, this survey\nmethodically elaborates these strategies and details their applications across\na spectrum of video-text learning tasks, ranging from fine-grained (e.g.,\nspatio-temporal video grounding) to coarse-grained (e.g., video question\nanswering). We further present a detailed experimental analysis to investigate\nthe efficacy of different image-to-video transfer learning paradigms on a range\nof downstream video understanding tasks. Finally, we identify prevailing\nchallenges and highlight promising directions for future research. By offering\na comprehensive and structured overview, this survey aims to establish a\nstructured roadmap for advancing video-text learning based on existing ILFM,\nand to inspire future research directions in this rapidly evolving domain.",
        "url": "http://arxiv.org/abs/2510.10671v1",
        "published_date": "2025-10-12T15:56:02+00:00",
        "updated_date": "2025-10-12T15:56:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinxuan Li",
            "Chaolei Tan",
            "Haoxuan Chen",
            "Jianxin Ma",
            "Jian-Fang Hu",
            "Wei-Shi Zheng",
            "Jianhuang Lai"
        ],
        "tldr": "This survey comprehensively reviews image-to-video transfer learning techniques using Image-Language Foundation Models (ILFM), categorizing strategies, analyzing their effectiveness, and highlighting future research directions in video-text learning.",
        "tldr_zh": "这篇综述全面回顾了基于图像-语言基础模型（ILFM）的图像到视频的迁移学习技术，对策略进行分类，分析其有效性，并强调视频-文本学习中未来的研究方向。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models",
        "summary": "Typical post-training paradigms for Large Vision-and-Language Models (LVLMs)\ninclude Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable\nRewards (RLVR). SFT leverages external guidance to inject new knowledge,\nwhereas RLVR utilizes internal reinforcement to enhance reasoning capabilities\nand overall performance. However, our analysis reveals that SFT often leads to\nsub-optimal performance, while RLVR struggles with tasks that exceed the\nmodel's internal knowledge base. To address these limitations, we propose\nViSurf (\\textbf{Vi}sual \\textbf{Su}pervised-and-\\textbf{R}einforcement\n\\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the\nstrengths of both SFT and RLVR within a single stage. We analyze the derivation\nof the SFT and RLVR objectives to establish the ViSurf objective, providing a\nunified perspective on these two paradigms. The core of ViSurf involves\ninjecting ground-truth labels into the RLVR rollouts, thereby providing\nsimultaneous external supervision and internal reinforcement. Furthermore, we\nintroduce three novel reward control strategies to stabilize and optimize the\ntraining process. Extensive experiments across several diverse benchmarks\ndemonstrate the effectiveness of ViSurf, outperforming both individual SFT,\nRLVR, and two-stage SFT \\textrightarrow RLVR. In-depth analysis corroborates\nthese findings, validating the derivation and design principles of ViSurf.",
        "url": "http://arxiv.org/abs/2510.10606v1",
        "published_date": "2025-10-12T13:42:55+00:00",
        "updated_date": "2025-10-12T13:42:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuqi Liu",
            "Liangyu Chen",
            "Jiazhen Liu",
            "Mingkang Zhu",
            "Zhisheng Zhong",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "tldr": "The paper introduces ViSurf, a novel post-training paradigm for LVLMs that unifies Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve performance by injecting ground-truth labels into RLVR rollouts.",
        "tldr_zh": "该论文介绍了 ViSurf，一种新型 LVLM 后训练范式，它统一了监督微调 (SFT) 和可验证奖励的强化学习 (RLVR)，通过将真实标签注入到 RLVR rollout 中来提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices",
        "summary": "Cross-attention transformers and other multimodal vision-language models\nexcel at grounding and generation; however, their extensive, full-precision\nbackbones make it challenging to deploy them on edge devices. Memory-augmented\narchitectures enhance the utilization of past context; however, most works\nrarely pair them with aggressive edge-oriented quantization. We introduce\nBitMar, a quantized multimodal transformer that proposes an external human-like\nepisodic memory for effective image-text generation on hardware with limited\nresources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and\none for vision (DiNOv2-based), to create compact embeddings that are combined\nand used to query a fixed-size key-value episodic memory. During vector\nretrieval, the BitNet decoder applies per-layer conditioning, which increases\nthe contextual relevance of generated content. The decoder also employs\nattention sinks with a sliding-window mechanism to process long or streaming\ninputs under tight memory budgets. The combination of per-layer conditioning\nand sliding-window attention achieves a strong quality-speed trade-off,\ndelivering competitive captioning and multimodal understanding at low latency\nwith a small model footprint. These characteristics make BitMar well-suited for\nedge deployment.",
        "url": "http://arxiv.org/abs/2510.10560v1",
        "published_date": "2025-10-12T11:59:41+00:00",
        "updated_date": "2025-10-12T11:59:41+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "68T50",
            "I.2.7"
        ],
        "authors": [
            "Euhid Aman",
            "Esteban Carlin",
            "Hsing-Kuo Pao",
            "Giovanni Beltrame",
            "Ghaluh Indah Permata Sari",
            "Yie-Tarng Chen"
        ],
        "tldr": "BitMar introduces a quantized multimodal transformer with episodic memory for efficient image-text generation on edge devices, using low-bit encoders and a sliding-window attention mechanism.",
        "tldr_zh": "BitMar 提出了一种量化的多模态 Transformer，具有情景记忆，可在边缘设备上高效生成图像文本，采用低比特编码器和滑动窗口注意力机制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Open-World Segmentation with Multi-Modal Prompts",
        "summary": "In this work, we present COSINE, a unified open-world segmentation model that\nconsolidates open-vocabulary segmentation and in-context segmentation with\nmulti-modal prompts (e.g., text and image). COSINE exploits foundation models\nto extract representations for an input image and corresponding multi-modal\nprompts, and a SegDecoder to align these representations, model their\ninteraction, and obtain masks specified by input prompts across different\ngranularities. In this way, COSINE overcomes architectural discrepancies,\ndivergent learning objectives, and distinct representation learning strategies\nof previous pipelines for open-vocabulary segmentation and in-context\nsegmentation. Comprehensive experiments demonstrate that COSINE has significant\nperformance improvements in both open-vocabulary and in-context segmentation\ntasks. Our exploratory analyses highlight that the synergistic collaboration\nbetween using visual and textual prompts leads to significantly improved\ngeneralization over single-modality approaches.",
        "url": "http://arxiv.org/abs/2510.10524v1",
        "published_date": "2025-10-12T09:45:51+00:00",
        "updated_date": "2025-10-12T09:45:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Liu",
            "Yufei Yin",
            "Chenchen Jing",
            "Muzhi Zhu",
            "Hao Chen",
            "Yuling Xi",
            "Bo Feng",
            "Hao Wang",
            "Shiyu Li",
            "Chunhua Shen"
        ],
        "tldr": "COSINE is a unified open-world segmentation model using multi-modal prompts (text and image) to improve open-vocabulary and in-context segmentation performance by leveraging foundation models and a SegDecoder.",
        "tldr_zh": "COSINE是一个统一的开放世界分割模型，它利用多模态提示（文本和图像）并通过利用基础模型和SegDecoder来提高开放词汇和上下文分割的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning",
        "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
        "url": "http://arxiv.org/abs/2510.10518v1",
        "published_date": "2025-10-12T09:29:50+00:00",
        "updated_date": "2025-10-12T09:29:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qunzhong Wang",
            "Jie Liu",
            "Jiajun Liang",
            "Yilei Jiang",
            "Yuanxing Zhang",
            "Jinyuan Chen",
            "Yaozhi Zheng",
            "Xintao Wang",
            "Pengfei Wan",
            "Xiangyu Yue",
            "Jiaheng Liu"
        ],
        "tldr": "VR-Thinker introduces a thinking-with-image framework for video reward models, addressing limitations in visual context and reasoning fidelity through visual reasoning operations and reinforcement fine-tuning, achieving state-of-the-art results on video preference benchmarks.",
        "tldr_zh": "VR-Thinker 为视频奖励模型引入了一个基于图像推理的框架，通过视觉推理操作和强化微调解决了视觉上下文和推理保真度方面的局限性，并在视频偏好基准测试中实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency",
        "summary": "Vision-Language Models (VLMs) integrate visual knowledge with the analytical\ncapabilities of Large Language Models (LLMs) through supervised visual\ninstruction tuning, using image-question-answer triplets. However, the\npotential of VLMs trained without supervised instruction remains largely\nunexplored. This study validates that VLMs possess inherent self-refinement\ncapabilities, enabling them to generate high-quality supervised data without\nexternal inputs and thereby learn autonomously. Specifically, to stimulate the\nself-refinement ability of VLMs, we propose a self-refinement framework based\non a Triangular Consistency principle: within the image-query-answer triangle,\nany masked elements should be consistently and accurately reconstructed. The\nframework involves three steps: (1) We enable the instruction generation\nability of VLMs by adding multi-task instruction tuning like\nimage$\\rightarrow$question-answer or image-answer$\\rightarrow$question. (2) We\ngenerate image-query-answer triplets from unlabeled images and use the\nTriangular Consistency principle for filtering. (3) The model is further\nupdated using the filtered synthetic data. To investigate the underlying\nmechanisms behind this self-refinement capability, we conduct a theoretical\nanalysis from a causal perspective. Using the widely recognized LLaVA-1.5 as\nour baseline, our experiments reveal that the model can autonomously achieve\nconsistent, though deliberately modest, improvements across multiple benchmarks\nwithout any external supervision, such as human annotations or environmental\nfeedback. We expect that the insights of this study on the self-refinement\nability of VLMs can inspire future research on the learning mechanism of VLMs.\nCode is available at https://github.com/dengyl20/SRF-LLaVA-1.5.",
        "url": "http://arxiv.org/abs/2510.10487v1",
        "published_date": "2025-10-12T07:37:47+00:00",
        "updated_date": "2025-10-12T07:37:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yunlong Deng",
            "Guangyi Chen",
            "Tianpei Gu",
            "Lingjing Kong",
            "Yan Li",
            "Zeyu Tang",
            "Kun Zhang"
        ],
        "tldr": "This paper introduces a self-refinement framework for VLMs based on Triangular Consistency, enabling autonomous learning and improvement without external supervision by generating and filtering image-query-answer triplets.",
        "tldr_zh": "本文提出了一种基于三角一致性的VLM自精炼框架，通过生成和过滤图像-问题-答案三元组，实现无需外部监督的自主学习和改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance",
        "summary": "Vision-Language Models (VLMs) have shown solid ability for multimodal\nunderstanding of both visual and language contexts. However, existing VLMs\noften face severe challenges of hallucinations, meaning that VLMs tend to\ngenerate responses that are only fluent in the language but irrelevant to\nimages in previous contexts. To address this issue, we analyze how language\nbias contributes to hallucinations and then introduce Cross-Modal\nGuidance(CMG), a training-free decoding method that addresses the\nhallucinations by leveraging the difference between the output distributions of\nthe original model and the one with degraded visual-language attention. In\npractice, we adaptively mask the attention weight of the most influential image\ntokens in selected transformer layers to corrupt the visual-language perception\nas a concrete type of degradation. Such a degradation-induced decoding\nemphasizes the perception of visual contexts and therefore significantly\nreduces language bias without harming the ability of VLMs. In experiment\nsections, we conduct comprehensive studies. All results demonstrate the\nsuperior advantages of CMG with neither additional conditions nor training\ncosts. We also quantitatively show CMG can improve different VLM's performance\non hallucination-specific benchmarks and generalize effectively.",
        "url": "http://arxiv.org/abs/2510.10466v1",
        "published_date": "2025-10-12T06:17:13+00:00",
        "updated_date": "2025-10-12T06:17:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinjin Cao",
            "Zhiyang Chen",
            "Zijun Wang",
            "Liyuan Ma",
            "Weijian Luo",
            "Guojun Qi"
        ],
        "tldr": "This paper introduces Cross-Modal Guidance (CMG), a training-free decoding method to mitigate language bias-induced hallucinations in VLMs by leveraging the difference between the output distributions of the original model and the one with degraded visual-language attention. CMG improves VLM performance on hallucination benchmarks without additional training costs.",
        "tldr_zh": "本文介绍了一种名为跨模态引导 (CMG) 的免训练解码方法，通过利用原始模型和视觉-语言注意力退化模型的输出分布之间的差异，来缓解 VLM 中由语言偏差引起的幻觉。CMG 提高了 VLM 在幻觉基准测试中的性能，且无需额外的训练成本。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs",
        "summary": "Multimodal large language models (MLLMs) often fail in fine-grained visual\nquestion answering, producing hallucinations about object identities,\npositions, and relations because textual queries are not explicitly anchored to\nvisual referents. Retrieval-augmented generation (RAG) alleviates some errors,\nbut it fails to align with human-like processing at both the retrieval and\naugmentation levels. Specifically, it focuses only on global-level image\ninformation but lacks local detail and limits reasoning about fine-grained\ninteractions. To overcome this limitation, we present Human-Like\nRetrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal\nreasoning as a ``what--where--reweight'' cascade. Queries are first anchored to\ncandidate referents via open-vocabulary detection (what), then spatially\nresolved with SAM-derived masks to recover fine-grained precision (where), and\nadaptively prioritized through the trade-off between local and global alignment\n(reweight). Mask-guided fine-tuning further injects spatial evidence into the\ngeneration process, transforming grounding from a passive bias into an explicit\nconstraint on answer formulation. Extensive experiments demonstrate that this\nhuman-like cascade improves grounding fidelity and factual consistency while\nreducing hallucinations, advancing multimodal question answering toward\ntrustworthy reasoning.",
        "url": "http://arxiv.org/abs/2510.10426v1",
        "published_date": "2025-10-12T03:22:33+00:00",
        "updated_date": "2025-10-12T03:22:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Suyang Xi",
            "Chenxi Yang",
            "Hong Ding",
            "Yiqing Ni",
            "Catherine C. Liu",
            "Yunhao Liu",
            "Chengqi Zhang"
        ],
        "tldr": "The paper introduces HuLiRAG, a novel retrieval-augmented generation framework for MLLMs that mimics human-like image reading through a 'what-where-reweight' cascade to improve grounding fidelity and reduce hallucinations in visual question answering.",
        "tldr_zh": "该论文介绍了一种名为HuLiRAG的新型检索增强生成框架，用于多模态大语言模型（MLLMs），它通过“是什么-在哪里-重新加权”的级联方式模拟人类的图像阅读方式，以提高视觉问答的 grounding 保真度并减少幻觉。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Simple and Better Baseline for Visual Grounding",
        "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
        "url": "http://arxiv.org/abs/2510.10587v1",
        "published_date": "2025-10-12T13:06:59+00:00",
        "updated_date": "2025-10-12T13:06:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingchao Wang",
            "Wenlong Zhang",
            "Dingjiang Huang",
            "Hong Wang",
            "Yefeng Zheng"
        ],
        "tldr": "The paper introduces FSVG, a simple and efficient visual grounding baseline that uses feature selection to improve accuracy and reduce computational cost compared to iterative methods.",
        "tldr_zh": "该论文介绍了FSVG，一个简单而高效的视觉定位基线，通过使用特征选择来提高准确性并降低计算成本，优于迭代方法。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Ordinal Scale Traffic Congestion Classification with Multi-Modal Vision-Language and Motion Analysis",
        "summary": "Accurate traffic congestion classification is essential for intelligent\ntransportation systems and real-time urban traffic management. This paper\npresents a multimodal framework combining open-vocabulary visual-language\nreasoning (CLIP), object detection (YOLO-World), and motion analysis via\nMOG2-based background subtraction. The system predicts congestion levels on an\nordinal scale from 1 (free flow) to 5 (severe congestion), enabling\nsemantically aligned and temporally consistent classification. To enhance\ninterpretability, we incorporate motion-based confidence weighting and generate\nannotated visual outputs. Experimental results show the model achieves 76.7\npercent accuracy, an F1 score of 0.752, and a Quadratic Weighted Kappa (QWK) of\n0.684, significantly outperforming unimodal baselines. These results\ndemonstrate the framework's effectiveness in preserving ordinal structure and\nleveraging visual-language and motion modalities. Future enhancements include\nincorporating vehicle sizing and refined density metrics.",
        "url": "http://arxiv.org/abs/2510.10342v1",
        "published_date": "2025-10-11T20:59:59+00:00",
        "updated_date": "2025-10-11T20:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu-Hsuan Lin"
        ],
        "tldr": "This paper presents a multimodal framework combining vision-language reasoning, object detection, and motion analysis for ordinal scale traffic congestion classification, achieving improved accuracy compared to unimodal baselines.",
        "tldr_zh": "本文提出了一种多模态框架，结合视觉-语言推理、目标检测和运动分析，用于有序尺度上的交通拥堵分类，与单模态基线相比，实现了更高的准确率。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vision4PPG: Emergent PPG Analysis Capability of Vision Foundation Models for Vital Signs like Blood Pressure",
        "summary": "Photoplethysmography (PPG) sensor in wearable and clinical devices provides\nvaluable physiological insights in a non-invasive and real-time fashion.\nSpecialized Foundation Models (FM) or repurposed time-series FMs are used to\nbenchmark physiological tasks. Our experiments with fine-tuning FMs reveal that\nVision FM (VFM) can also be utilized for this purpose and, in fact,\nsurprisingly leads to state-of-the-art (SOTA) performance on many tasks,\nnotably blood pressure estimation. We leverage VFMs by simply transforming\none-dimensional PPG signals into image-like two-dimensional representations,\nsuch as the Short-Time Fourier transform (STFT). Using the latest VFMs, such as\nDINOv3 and SIGLIP-2, we achieve promising performance on other vital signs and\nblood lab measurement tasks as well. Our proposal, Vision4PPG, unlocks a new\nclass of FMs to achieve SOTA performance with notable generalization to other\n2D input representations, including STFT phase and recurrence plots. Our work\nimproves upon prior investigations of vision models for PPG by conducting a\ncomprehensive study, comparing them to state-of-the-art time-series FMs, and\ndemonstrating the general PPG processing ability by reporting results on six\nadditional tasks. Thus, we provide clinician-scientists with a new set of\npowerful tools that is also computationally efficient, thanks to\nParameter-Efficient Fine-Tuning (PEFT) techniques.",
        "url": "http://arxiv.org/abs/2510.10366v1",
        "published_date": "2025-10-11T23:13:30+00:00",
        "updated_date": "2025-10-11T23:13:30+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Saurabh Kataria",
            "Ayca Ermis",
            "Lovely Yeswanth Panchumarthi",
            "Minxiao Wang",
            "Xiao Hu"
        ],
        "tldr": "The paper proposes using Vision Foundation Models (VFMs) to analyze PPG signals for vital signs estimation, achieving state-of-the-art results by converting 1D PPG signals into 2D representations.",
        "tldr_zh": "该论文提出使用视觉基础模型（VFMs）分析PPG信号以进行生命体征估计，通过将一维PPG信号转换为二维表示来实现最先进的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection",
        "summary": "With abundant, unlabeled real faces, how can we learn robust and transferable\nfacial representations to boost generalization across various face security\ntasks? We make the first attempt and propose FS-VFM, a scalable self-supervised\npre-training framework, to learn fundamental representations of real face\nimages. We introduce three learning objectives, namely 3C, that synergize\nmasked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM\nto encode both local patterns and global semantics of real faces. Specifically,\nwe formulate various facial masking strategies for MIM and devise a simple yet\neffective CRFR-P masking, which explicitly prompts the model to pursue\nmeaningful intra-region Consistency and challenging inter-region Coherency. We\npresent a reliable self-distillation mechanism that seamlessly couples MIM with\nID to establish underlying local-to-global Correspondence. After pre-training,\nvanilla vision transformers (ViTs) serve as universal Vision Foundation Models\nfor downstream Face Security tasks: cross-dataset deepfake detection,\ncross-domain face anti-spoofing, and unseen diffusion facial forensics. To\nefficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a\nlightweight plug-and-play bottleneck atop the frozen backbone with a novel\nreal-anchor contrastive objective. Extensive experiments on 11 public\nbenchmarks demonstrate that our FS-VFM consistently generalizes better than\ndiverse VFMs, spanning natural and facial domains, fully, weakly, and\nself-supervised paradigms, small, base, and large ViT scales, and even\noutperforms SOTA task-specific methods, while FS-Adapter offers an excellent\nefficiency-performance trade-off. The code and models are available on\nhttps://fsfm-3c.github.io/fsvfm.html.",
        "url": "http://arxiv.org/abs/2510.10663v1",
        "published_date": "2025-10-12T15:38:03+00:00",
        "updated_date": "2025-10-12T15:38:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4.10; I.2.10; I.5.0"
        ],
        "authors": [
            "Gaojian Wang",
            "Feng Lin",
            "Tong Wu",
            "Zhisheng Yan",
            "Kui Ren"
        ],
        "tldr": "The paper introduces FS-VFM, a self-supervised pre-training framework for learning robust facial representations using masked image modeling and instance discrimination, demonstrating improved generalization across various face security tasks compared to other vision foundation models.",
        "tldr_zh": "该论文介绍了一种自监督预训练框架FS-VFM，它使用掩码图像建模和实例判别来学习鲁棒的人脸表示，并展示了与其他视觉基础模型相比，在各种人脸安全任务中的泛化能力。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]