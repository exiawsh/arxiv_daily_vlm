[
    {
        "title": "Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation",
        "summary": "We introduce VocAlign, a novel source-free domain adaptation framework\nspecifically designed for VLMs in open-vocabulary semantic segmentation. Our\nmethod adopts a student-teacher paradigm enhanced with a vocabulary alignment\nstrategy, which improves pseudo-label generation by incorporating additional\nclass concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to\nfine-tune the model, preserving its original capabilities while minimizing\ncomputational overhead. In addition, we propose a Top-K class selection\nmechanism for the student model, which significantly reduces memory\nrequirements while further improving adaptation performance. Our approach\nachieves a notable 6.11 mIoU improvement on the CityScapes dataset and\ndemonstrates superior performance on zero-shot segmentation benchmarks, setting\na new standard for source-free adaptation in the open-vocabulary setting.",
        "url": "http://arxiv.org/abs/2509.15225v1",
        "published_date": "2025-09-18T17:59:58+00:00",
        "updated_date": "2025-09-18T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Silvio Mazzucco",
            "Carl Persson",
            "Mattia Segu",
            "Pier Luigi Dovesi",
            "Federico Tombari",
            "Luc Van Gool",
            "Matteo Poggi"
        ],
        "tldr": "The paper introduces VocAlign, a source-free domain adaptation framework for VLMs in open-vocabulary semantic segmentation that uses vocabulary alignment, LoRA for efficiency, and Top-K class selection to improve performance and reduce memory usage. It achieves a significant mIoU improvement on CityScapes and demonstrates superior zero-shot performance.",
        "tldr_zh": "该论文介绍了一种名为VocAlign的源自由领域自适应框架，用于开放词汇语义分割中的视觉语言模型。该框架利用词汇对齐、LoRA技术提高效率，以及Top-K类别选择来提升性能并减少内存占用。在CityScapes数据集上取得了显著的mIoU提升，并在零样本分割基准测试中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Calibration-Aware Prompt Learning for Medical Vision-Language Models",
        "summary": "Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable\nperformance across diverse medical imaging tasks by leveraging large-scale\nimage-text pretraining. However, their confidence calibration is largely\nunexplored, and so remains a significant challenge. As such, miscalibrated\npredictions can lead to overconfident errors, undermining clinical trust and\ndecision-making reliability. To address this, we introduce CalibPrompt, the\nfirst framework to calibrate Med-VLMs during prompt tuning. CalibPrompt\noptimizes a small set of learnable prompts with carefully designed calibration\nobjectives under scarce labeled data regime. First, we study a regularizer that\nattempts to align the smoothed accuracy with the predicted model confidences.\nSecond, we introduce an angular separation loss to maximize textual feature\nproximity toward improving the reliability in confidence estimates of\nmultimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs\nand five diverse medical imaging datasets reveal that CalibPrompt consistently\nimproves calibration without drastically affecting clean accuracy. Our code is\navailable at https://github.com/iabh1shekbasu/CalibPrompt.",
        "url": "http://arxiv.org/abs/2509.15226v1",
        "published_date": "2025-09-18T17:59:58+00:00",
        "updated_date": "2025-09-18T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abhishek Basu",
            "Fahad Shamshad",
            "Ashshak Sharifdeen",
            "Karthik Nandakumar",
            "Muhammad Haris Khan"
        ],
        "tldr": "The paper introduces CalibPrompt, a novel framework for calibrating Medical Vision-Language Models (Med-VLMs) during prompt tuning using calibration objectives and scarce labeled data, improving confidence calibration without significantly impacting accuracy.",
        "tldr_zh": "该论文介绍了CalibPrompt，一种新颖的框架，用于在使用校准目标和稀缺的标记数据进行提示调整期间校准医学视觉-语言模型 (Med-VLM)，从而提高置信度校准，而不会显着影响准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data",
        "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
        "url": "http://arxiv.org/abs/2509.15221v1",
        "published_date": "2025-09-18T17:59:22+00:00",
        "updated_date": "2025-09-18T17:59:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoyang Liu",
            "JingJing Xie",
            "Zichen Ding",
            "Zehao Li",
            "Bowen Yang",
            "Zhenyu Wu",
            "Xuehui Wang",
            "Qiushi Sun",
            "Shi Liu",
            "Weiyun Wang",
            "Shenglong Ye",
            "Qingyun Li",
            "Zeyue Tian",
            "Gen Luo",
            "Xiangyu Yue",
            "Biqing Qi",
            "Kai Chen",
            "Bowen Zhou",
            "Yu Qiao",
            "Qifeng Chen",
            "Wenhai Wang"
        ],
        "tldr": "ScaleCUA introduces a large-scale, open-source dataset for computer use agents (CUAs) across multiple operating systems, leading to significant performance improvements and new state-of-the-art results. The data, models, and code will be released.",
        "tldr_zh": "ScaleCUA 提出了一个大规模、开源的计算机使用代理（CUA）数据集，涵盖多种操作系统，从而显著提高了性能并取得了新的最先进成果。数据、模型和代码将被发布。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalizable Geometric Image Caption Synthesis",
        "summary": "Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU.",
        "url": "http://arxiv.org/abs/2509.15217v1",
        "published_date": "2025-09-18T17:59:11+00:00",
        "updated_date": "2025-09-18T17:59:11+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yue Xin",
            "Wenyuan Wang",
            "Rui Pan",
            "Ruida Wang",
            "Howard Meng",
            "Renjie Pi",
            "Shizhe Diao",
            "Tong Zhang"
        ],
        "tldr": "The paper introduces a Reinforcement Learning with Verifiable Rewards (RLVR) approach to generate high-quality image-text pairs for geometric images, improving the geometric reasoning abilities of multimodal large language models and enhancing their general reasoning across various tasks.",
        "tldr_zh": "该论文介绍了一种使用可验证奖励的强化学习 (RLVR) 方法来生成高质量的几何图像图像-文本对，从而提高多模态大型语言模型在几何推理方面的能力，并增强其在各种任务中的通用推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding",
        "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as \\textit{grounding tokens},\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
        "url": "http://arxiv.org/abs/2509.15178v1",
        "published_date": "2025-09-18T17:35:50+00:00",
        "updated_date": "2025-09-18T17:35:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zaiquan Yang",
            "Yuhao Liu",
            "Gerhard Hancke",
            "Rynson W. H. Lau"
        ],
        "tldr": "The paper proposes a zero-shot spatio-temporal video grounding framework using MLLMs, addressing their limitations in integrating textual cues by introducing decomposed spatio-temporal highlighting and temporal-augmented assembling strategies, achieving SOTA results.",
        "tldr_zh": "该论文提出了一种基于多模态大语言模型(MLLM)的零样本时空视频定位框架，通过引入分解时空高亮和时间增强组装策略，解决了MLLM在整合文本线索方面的局限性，并取得了SOTA的效果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of Redlining with a Multimodal LLM",
        "summary": "This paper shows how a multimodal large language model (MLLM) can expand\nurban measurement capacity and support tracking of place-based policy\ninterventions. Using a structured, reason-then-estimate pipeline on street-view\nimagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in\na quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o\nrecovers the expected adverse socio-environmental legacy effects of redlining,\nwith estimates statistically indistinguishable from authoritative sources, and\nit outperforms a conventional pixel-based segmentation baseline-consistent with\nthe idea that holistic scene reasoning extracts higher-order information beyond\nobject counts alone. These results position MLLMs as policy-grade instruments\nfor neighborhood measurement and motivate broader validation across\npolicy-evaluation settings.",
        "url": "http://arxiv.org/abs/2509.15132v1",
        "published_date": "2025-09-18T16:42:01+00:00",
        "updated_date": "2025-09-18T16:42:01+00:00",
        "categories": [
            "cs.CY",
            "cs.CV"
        ],
        "authors": [
            "Anthony Howell",
            "Nancy Wu",
            "Sharmistha Bagchi",
            "Yushim Kim",
            "Chayn Sun"
        ],
        "tldr": "This paper uses a multimodal LLM (GPT-4o) to assess neighborhood poverty and tree canopy from street-view imagery, successfully recovering known effects of historical redlining policies, thus positioning MLLMs as valuable tools for policy evaluation.",
        "tldr_zh": "本文利用多模态大语言模型（GPT-4o）从街景图像中评估社区贫困和树冠覆盖率，成功恢复了历史上“红线政策”的已知影响，从而将MLLM定位为政策评估的宝贵工具。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models",
        "summary": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms.",
        "url": "http://arxiv.org/abs/2509.15076v1",
        "published_date": "2025-09-18T15:36:38+00:00",
        "updated_date": "2025-09-18T15:36:38+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Mohammad Saleh Vahdatpour",
            "Maryam Eyvazi",
            "Yanqing Zhang"
        ],
        "tldr": "This paper presents an AI system using VLMs to predict air pollution levels from sky images and generate visualizations of pollution scenarios, aiming to improve public awareness and decision-making.",
        "tldr_zh": "本文提出了一种人工智能系统，该系统使用视觉语言模型（VLM）从天空图像预测空气污染水平，并生成污染场景的可视化效果，旨在提高公众意识和决策水平。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection",
        "summary": "With the increasing prevalence of synthetic images, evaluating image\nauthenticity and locating forgeries accurately while maintaining human\ninterpretability remains a challenging task. Existing detection models\nprimarily focus on simple authenticity classification, ultimately providing\nonly a forgery probability or binary judgment, which offers limited explanatory\ninsights into image authenticity. Moreover, while MLLM-based detection methods\ncan provide more interpretable results, they still lag behind expert models in\nterms of pure authenticity classification accuracy. To address this, we propose\nDF-LLaVA, a simple yet effective framework that unlocks the intrinsic\ndiscrimination potential of MLLMs. Our approach first extracts latent knowledge\nfrom MLLMs and then injects it into training via prompts. This framework allows\nLLaVA to achieve outstanding detection accuracy exceeding expert models while\nstill maintaining the interpretability offered by MLLMs. Extensive experiments\nconfirm the superiority of our DF-LLaVA, achieving both high accuracy and\nexplainability in synthetic image detection. Code is available online at:\nhttps://github.com/Eliot-Shen/DF-LLaVA.",
        "url": "http://arxiv.org/abs/2509.14957v1",
        "published_date": "2025-09-18T13:43:42+00:00",
        "updated_date": "2025-09-18T13:43:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuokang Shen",
            "Kaisen Zhang",
            "Bohan Jia",
            "Yuan Fang",
            "Zhou Yu",
            "Shaohui Lin"
        ],
        "tldr": "The paper introduces DF-LLaVA, a framework that enhances LLaVA's synthetic image detection accuracy by injecting latent knowledge extracted from MLLMs via prompt engineering, achieving superior performance with interpretability.",
        "tldr_zh": "该论文介绍了DF-LLaVA，一个通过注入从MLLM提取的潜在知识（通过提示工程）来提高LLaVA在合成图像检测方面的准确性的框架，实现了卓越的性能和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MARIC: Multi-Agent Reasoning for Image Classification",
        "summary": "Image classification has traditionally relied on parameter-intensive model\ntraining, requiring large-scale annotated datasets and extensive fine tuning to\nachieve competitive performance. While recent vision language models (VLMs)\nalleviate some of these constraints, they remain limited by their reliance on\nsingle pass representations, often failing to capture complementary aspects of\nvisual content. In this paper, we introduce Multi Agent based Reasoning for\nImage Classification (MARIC), a multi agent framework that reformulates image\nclassification as a collaborative reasoning process. MARIC first utilizes an\nOutliner Agent to analyze the global theme of the image and generate targeted\nprompts. Based on these prompts, three Aspect Agents extract fine grained\ndescriptions along distinct visual dimensions. Finally, a Reasoning Agent\nsynthesizes these complementary outputs through integrated reflection step,\nproducing a unified representation for classification. By explicitly\ndecomposing the task into multiple perspectives and encouraging reflective\nsynthesis, MARIC mitigates the shortcomings of both parameter-heavy training\nand monolithic VLM reasoning. Experiments on 4 diverse image classification\nbenchmark datasets demonstrate that MARIC significantly outperforms baselines,\nhighlighting the effectiveness of multi-agent visual reasoning for robust and\ninterpretable image classification.",
        "url": "http://arxiv.org/abs/2509.14860v1",
        "published_date": "2025-09-18T11:27:00+00:00",
        "updated_date": "2025-09-18T11:27:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "authors": [
            "Wonduk Seo",
            "Minhyeong Yu",
            "Hyunjin An",
            "Seunghyun Lee"
        ],
        "tldr": "The paper introduces MARIC, a multi-agent framework for image classification that decomposes the task into collaborative reasoning steps, outperforming baselines on diverse datasets.",
        "tldr_zh": "该论文介绍了MARIC，一个用于图像分类的多智能体框架，它将任务分解为协同推理步骤，并在各种数据集上优于基线。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model",
        "summary": "Text to image latent diffusion models have recently advanced medical image\nsynthesis, but applications to 3D CT generation remain limited. Existing\napproaches rely on simplified prompts, neglecting the rich semantic detail in\nfull radiology reports, which reduces text image alignment and clinical\nfidelity. We propose Report2CT, a radiology report conditional latent diffusion\nframework for synthesizing 3D chest CT volumes directly from free text\nradiology reports, incorporating both findings and impression sections using\nmultiple text encoder. Report2CT integrates three pretrained medical text\nencoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced\nclinical context. Radiology reports and voxel spacing information condition a\n3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset.\nModel performance was evaluated using Frechet Inception Distance (FID) for real\nsynthetic distributional similarity and CLIP based metrics for semantic\nalignment, with additional qualitative and quantitative comparisons against\nGenerateCT model. Report2CT generated anatomically consistent CT volumes with\nexcellent visual quality and text image alignment. Multi encoder conditioning\nimproved CLIP scores, indicating stronger preservation of fine grained clinical\ndetails in the free text radiology reports. Classifier free guidance further\nenhanced alignment with only a minor trade off in FID. We ranked first in the\nVLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved\nstate of the art performance across all evaluation metrics. By leveraging\ncomplete radiology reports and multi encoder text conditioning, Report2CT\nadvances 3D CT synthesis, producing clinically faithful and high quality\nsynthetic data.",
        "url": "http://arxiv.org/abs/2509.14780v1",
        "published_date": "2025-09-18T09:32:23+00:00",
        "updated_date": "2025-09-18T09:32:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sina Amirrajab",
            "Zohaib Salahuddin",
            "Sheng Kuang",
            "Henry C. Woodruff",
            "Philippe Lambin"
        ],
        "tldr": "The paper introduces Report2CT, a novel framework for generating 3D chest CT volumes from radiology reports using a multi-encoder latent diffusion model, achieving state-of-the-art performance in text-conditional CT generation.",
        "tldr_zh": "该论文介绍了Report2CT，一种新的框架，使用多编码器潜在扩散模型从放射学报告生成3D胸部CT图像，并在文本条件CT生成方面实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Frame Sampling Strategies Matter: A Benchmark for small vision language models",
        "summary": "Comparing vision language models on videos is particularly complex, as the\nperformances is jointly determined by the model's visual representation\ncapacity and the frame-sampling strategy used to construct the input. Current\nvideo benchmarks are suspected to suffer from substantial frame-sampling bias,\nas models are evaluated with different frame selection strategies. In this\nwork, we propose the first frame-accurate benchmark of state-of-the-art small\nVLMs for video question-answering, evaluated under controlled frame-sampling\nstrategies. Our results confirm the suspected bias and highlight both\ndata-specific and task-specific behaviors of SVLMs under different\nframe-sampling techniques. By open-sourcing our benchmarking code, we provide\nthe community with a reproducible and unbiased protocol for evaluating video\nVLMs and emphasize the need for standardized frame-sampling strategies tailored\nto each benchmarking dataset in future research.",
        "url": "http://arxiv.org/abs/2509.14769v1",
        "published_date": "2025-09-18T09:18:42+00:00",
        "updated_date": "2025-09-18T09:18:42+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Marija Brkic",
            "Anas Filali Razzouki",
            "Yannis Tevissen",
            "Khalil Guetari",
            "Mounim A. El Yacoubi"
        ],
        "tldr": "This paper introduces a frame-accurate benchmark for evaluating small Vision Language Models (VLMs) in video question-answering, highlighting the impact of frame-sampling strategies and advocating for standardized protocols.",
        "tldr_zh": "本文介绍了一个精确到帧的基准测试，用于评估小型视觉语言模型 (VLM) 在视频问答中的表现，强调了帧采样策略的影响，并提倡采用标准化的协议。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Chain-of-Thought Re-ranking for Image Retrieval Tasks",
        "summary": "Image retrieval remains a fundamental yet challenging problem in computer\nvision. While recent advances in Multimodal Large Language Models (MLLMs) have\ndemonstrated strong reasoning capabilities, existing methods typically employ\nthem only for evaluation, without involving them directly in the ranking\nprocess. As a result, their rich multimodal reasoning abilities remain\nunderutilized, leading to suboptimal performance. In this paper, we propose a\nnovel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.\nSpecifically, we design a listwise ranking prompt that enables MLLM to directly\nparticipate in re-ranking candidate images. This ranking process is grounded in\nan image evaluation prompt, which assesses how well each candidate aligns with\nusers query. By allowing MLLM to perform listwise reasoning, our method\nsupports global comparison, consistent reasoning, and interpretable\ndecision-making - all of which are essential for accurate image retrieval. To\nenable structured and fine-grained analysis, we further introduce a query\ndeconstruction prompt, which breaks down the original query into multiple\nsemantic components. Extensive experiments on five datasets demonstrate the\neffectiveness of our CoTRR method, which achieves state-of-the-art performance\nacross three image retrieval tasks, including text-to-image retrieval (TIR),\ncomposed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our\ncode is available at https://github.com/freshfish15/CoTRR .",
        "url": "http://arxiv.org/abs/2509.14746v1",
        "published_date": "2025-09-18T08:48:46+00:00",
        "updated_date": "2025-09-18T08:48:46+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Shangrong Wu",
            "Yanghong Zhou",
            "Yang Chen",
            "Feng Zhang",
            "P. Y. Mok"
        ],
        "tldr": "This paper introduces a Chain-of-Thought Re-Ranking (CoTRR) method for image retrieval that leverages Multimodal Large Language Models (MLLMs) to directly participate in the ranking process, achieving state-of-the-art performance on multiple image retrieval tasks.",
        "tldr_zh": "本文提出了一种用于图像检索的思维链重排序 (CoTRR) 方法，该方法利用多模态大型语言模型 (MLLM) 直接参与排序过程，并在多个图像检索任务上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation",
        "summary": "Ensuring factual consistency and reliable reasoning remains a critical\nchallenge for medical vision-language models. We introduce MEDFACT-R1, a\ntwo-stage framework that integrates external knowledge grounding with\nreinforcement learning to improve the factual medical reasoning. The first\nstage uses pseudo-label supervised fine-tuning (SFT) to incorporate external\nfactual expertise; while the second stage applies Group Relative Policy\nOptimization (GRPO) with four tailored factual reward signals to encourage\nself-consistent reasoning. Across three public medical QA benchmarks,\nMEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over\nprevious state-of-the-art methods. Ablation studies highlight the necessity of\npseudo-label SFT cold start and validate the contribution of each GRPO reward,\nunderscoring the synergy between knowledge grounding and RL-driven reasoning\nfor trustworthy medical AI. Codes are released at\nhttps://github.com/Garfieldgengliang/MEDFACT-R1.",
        "url": "http://arxiv.org/abs/2509.15154v1",
        "published_date": "2025-09-18T16:59:59+00:00",
        "updated_date": "2025-09-18T16:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gengliang Li",
            "Rongyu Chen",
            "Bin Li",
            "Linlin Yang",
            "Guodong Ding"
        ],
        "tldr": "The paper introduces MEDFACT-R1, a two-stage framework using pseudo-label supervised fine-tuning and reinforcement learning to improve factual accuracy in medical vision-language models, achieving substantial improvements on medical QA benchmarks.",
        "tldr_zh": "该论文介绍了MEDFACT-R1，一个两阶段框架，使用伪标签监督微调和强化学习来提高医学视觉语言模型的事实准确性，并在医学QA基准测试中取得了显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "QuizRank: Picking Images by Quizzing VLMs",
        "summary": "Images play a vital role in improving the readability and comprehension of\nWikipedia articles by serving as `illustrative aids.' However, not all images\nare equally effective and not all Wikipedia editors are trained in their\nselection. We propose QuizRank, a novel method of image selection that\nleverages large language models (LLMs) and vision language models (VLMs) to\nrank images as learning interventions. Our approach transforms textual\ndescriptions of the article's subject into multiple-choice questions about\nimportant visual characteristics of the concept. We utilize these questions to\nquiz the VLM: the better an image can help answer questions, the higher it is\nranked. To further improve discrimination between visually similar items, we\nintroduce a Contrastive QuizRank that leverages differences in the features of\ntarget (e.g., a Western Bluebird) and distractor concepts (e.g., Mountain\nBluebird) to generate questions. We demonstrate the potential of VLMs as\neffective visual evaluators by showing a high congruence with human quiz-takers\nand an effective discriminative ranking of images.",
        "url": "http://arxiv.org/abs/2509.15059v1",
        "published_date": "2025-09-18T15:22:33+00:00",
        "updated_date": "2025-09-18T15:22:33+00:00",
        "categories": [
            "cs.HC",
            "cs.CV"
        ],
        "authors": [
            "Tenghao Ji",
            "Eytan Adar"
        ],
        "tldr": "The paper introduces QuizRank, a method to rank images for Wikipedia articles by quizzing Vision-Language Models (VLMs) on their ability to answer multiple-choice questions derived from the article's text, with a contrastive approach to improve discrimination.",
        "tldr_zh": "该论文介绍了一种名为QuizRank的方法，通过利用视觉语言模型（VLM）回答从文章文本中提取的多项选择题，对维基百科文章的图像进行排序，并采用对比方法来提高区分度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Communication Efficient Split Learning of ViTs with Attention-based Double Compression",
        "summary": "This paper proposes a novel communication-efficient Split Learning (SL)\nframework, named Attention-based Double Compression (ADC), which reduces the\ncommunication overhead required for transmitting intermediate Vision\nTransformers activations during the SL training process. ADC incorporates two\nparallel compression strategies. The first one merges samples' activations that\nare similar, based on the average attention score calculated in the last client\nlayer; this strategy is class-agnostic, meaning that it can also merge samples\nhaving different classes, without losing generalization ability nor decreasing\nfinal results. The second strategy follows the first and discards the least\nmeaningful tokens, further reducing the communication cost. Combining these\nstrategies not only allows for sending less during the forward pass, but also\nthe gradients are naturally compressed, allowing the whole model to be trained\nwithout additional tuning or approximations of the gradients. Simulation\nresults demonstrate that Attention-based Double Compression outperforms\nstate-of-the-art SL frameworks by significantly reducing communication\noverheads while maintaining high accuracy.",
        "url": "http://arxiv.org/abs/2509.15058v1",
        "published_date": "2025-09-18T15:22:24+00:00",
        "updated_date": "2025-09-18T15:22:24+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Federico Alvetreti",
            "Jary Pomponi",
            "Paolo Di Lorenzo",
            "Simone Scardapane"
        ],
        "tldr": "This paper introduces an Attention-based Double Compression (ADC) method for communication-efficient split learning of Vision Transformers, significantly reducing communication overhead while maintaining high accuracy by merging similar activations and discarding less meaningful tokens.",
        "tldr_zh": "本文提出了一种基于注意力机制的双重压缩（ADC）方法，用于高效通信的Vision Transformer分割学习，通过合并相似激活并丢弃不太重要的tokens，显著降低了通信开销，同时保持了较高的准确率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PRISM: Product Retrieval In Shopping Carts using Hybrid Matching",
        "summary": "Compared to traditional image retrieval tasks, product retrieval in retail\nsettings is even more challenging. Products of the same type from different\nbrands may have highly similar visual appearances, and the query image may be\ntaken from an angle that differs significantly from view angles of the stored\ncatalog images. Foundational models, such as CLIP and SigLIP, often struggle to\ndistinguish these subtle but important local differences. Pixel-wise matching\nmethods, on the other hand, are computationally expensive and incur\nprohibitively high matching times. In this paper, we propose a new, hybrid\nmethod, called PRISM, for product retrieval in retail settings by leveraging\nthe advantages of both vision-language model-based and pixel-wise matching\napproaches. To provide both efficiency/speed and finegrained retrieval\naccuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)\nis employed first to retrieve the top 35 most semantically similar products\nfrom a fixed gallery, thereby narrowing the search space significantly; 2) a\nsegmentation model (YOLO-E) is applied to eliminate background clutter; 3)\nfine-grained pixel-level matching is performed using LightGlue across the\nfiltered candidates. This framework enables more accurate discrimination\nbetween products with high inter-class similarity by focusing on subtle visual\ncues often missed by global models. Experiments performed on the ABV dataset\nshow that our proposed PRISM outperforms the state-of-the-art image retrieval\nmethods by 4.21% in top-1 accuracy while still remaining within the bounds of\nreal-time processing for practical retail deployments.",
        "url": "http://arxiv.org/abs/2509.14985v1",
        "published_date": "2025-09-18T14:15:37+00:00",
        "updated_date": "2025-09-18T14:15:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Arda Kabadayi",
            "Senem Velipasalar",
            "Jiajing Chen"
        ],
        "tldr": "The paper introduces PRISM, a hybrid vision-language and pixel-wise matching method for product retrieval in retail, achieving improved accuracy and real-time processing on the ABV dataset.",
        "tldr_zh": "该论文介绍了一种名为PRISM的混合视觉语言和像素级匹配方法，用于零售环境中的产品检索，在ABV数据集上实现了更高的准确性和实时处理。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark",
        "summary": "Understanding how people read city scenes can inform design and planning. We\nintroduce a small benchmark for testing vision-language models (VLMs) on urban\nperception using 100 Montreal street images, evenly split between photographs\nand photorealistic synthetic scenes. Twelve participants from seven community\ngroups supplied 230 annotation forms across 30 dimensions mixing physical\nattributes and subjective impressions. French responses were normalized to\nEnglish. We evaluated seven VLMs in a zero-shot setup with a structured prompt\nand deterministic parser. We use accuracy for single-choice items and Jaccard\noverlap for multi-label items; human agreement uses Krippendorff's alpha and\npairwise Jaccard. Results suggest stronger model alignment on visible,\nobjective properties than subjective appraisals. The top system (claude-sonnet)\nreaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human\nagreement coincides with better model scores. Synthetic images slightly lower\nscores. We release the benchmark, prompts, and harness for reproducible,\nuncertainty-aware evaluation in participatory urban analysis.",
        "url": "http://arxiv.org/abs/2509.14574v1",
        "published_date": "2025-09-18T03:21:10+00:00",
        "updated_date": "2025-09-18T03:21:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rashid Mushkani"
        ],
        "tldr": "The paper introduces a new benchmark for evaluating vision-language models (VLMs) on urban perception, comparing model performance with human annotations on a dataset of Montreal street images.",
        "tldr_zh": "该论文介绍了一个新的基准，用于评估视觉语言模型（VLMs）在城市感知方面的能力，通过比较模型在蒙特利尔街道图像数据集上的表现与人类的标注。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications",
        "summary": "Foundation models such as CLIP have demonstrated exceptional zero- and\nfew-shot transfer capabilities across diverse vision tasks. However, when\nfine-tuned for highly specialized biometric tasks, face recognition (FR),\nmorphing attack detection (MAD), and presentation attack detection (PAD), these\nmodels may suffer from over-specialization. Thus, they may lose one of their\nfoundational strengths, cross-domain generalization. In this work, we\nsystematically quantify these trade-offs by evaluating three instances of CLIP\nfine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the\noriginal CLIP baseline on 14 general vision datasets under zero-shot and\nlinear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our\nresults indicate that fine-tuned models suffer from over-specialization,\nespecially when fine-tuned for complex tasks of FR. Also, our results pointed\nout that task complexity and classification head design, multi-class (FR) vs.\nbinary (MAD and PAD), correlate with the degree of catastrophic forgetting. The\nFRoundation model with the ViT-L backbone outperforms other approaches on the\nlarge-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%.\nHowever, it experiences a substantial performance drop on ImageNetV2, reaching\nonly 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover,\nthe larger CLIP architecture consistently preserves more of the model's\noriginal generalization ability than the smaller variant, indicating that\nincreased model capacity may help mitigate over-specialization.",
        "url": "http://arxiv.org/abs/2509.14921v1",
        "published_date": "2025-09-18T12:58:18+00:00",
        "updated_date": "2025-09-18T12:58:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tahar Chettaoui",
            "Naser Damer",
            "Fadi Boutros"
        ],
        "tldr": "This paper investigates the trade-off between fine-tuning CLIP for specific biometric tasks (face recognition, morphing attack detection, presentation attack detection) and maintaining its general vision capabilities, finding that fine-tuning leads to over-specialization and performance degradation on general vision datasets.",
        "tldr_zh": "本文研究了为特定生物识别任务（人脸识别、变形攻击检测、演示攻击检测）微调CLIP与其保持通用视觉能力之间的权衡，发现微调会导致过度专业化以及在通用视觉数据集上的性能下降。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    }
]