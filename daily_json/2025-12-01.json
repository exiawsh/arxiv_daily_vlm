[
    {
        "title": "Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation",
        "summary": "Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.",
        "url": "http://arxiv.org/abs/2512.00706v1",
        "published_date": "2025-11-30T02:55:20+00:00",
        "updated_date": "2025-11-30T02:55:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chengzhi Yu",
            "Yifan Xu",
            "Yifan Chen",
            "Wenyi Zhang"
        ],
        "tldr": "This paper introduces a method for mitigating hallucinations in LVLMs by using on-policy data and a robust iterative DPO algorithm with a hallucination classifier, achieving significant improvements on benchmark datasets.",
        "tldr_zh": "本文提出了一种通过使用on-policy数据和带有幻觉分类器的稳健迭代DPO算法来减轻LVLM中幻觉的方法，并在基准数据集上取得了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MM-ACT: Learn from Multimodal Parallel Generation to Act",
        "summary": "A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.",
        "url": "http://arxiv.org/abs/2512.00975v1",
        "published_date": "2025-11-30T16:46:35+00:00",
        "updated_date": "2025-11-30T16:46:35+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Haotian Liang",
            "Xinyi Chen",
            "Bin Wang",
            "Mingkang Chen",
            "Yitian Liu",
            "Yuhao Zhang",
            "Zanxin Chen",
            "Tianshuo Yang",
            "Yilun Chen",
            "Jiangmiao Pang",
            "Dong Liu",
            "Xiaokang Yang",
            "Yao Mu",
            "Wenqi Shao",
            "Ping Luo"
        ],
        "tldr": "MM-ACT is a Vision-Language-Action model that uses parallel decoding and context-shared multimodal learning to improve robotic policy generation across text, image, and action modalities, showing promising results in simulation and real-world experiments.",
        "tldr_zh": "MM-ACT是一个视觉-语言-动作模型，它使用并行解码和上下文共享的多模态学习来改进机器人策略生成，涵盖文本、图像和动作模态，并在模拟和现实世界实验中显示出令人鼓舞的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
        "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
        "url": "http://arxiv.org/abs/2512.00903v1",
        "published_date": "2025-11-30T14:10:28+00:00",
        "updated_date": "2025-11-30T14:10:28+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Chaojun Ni",
            "Cheng Chen",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Wenzhao Zheng",
            "Boyuan Wang",
            "Tianrun Chen",
            "Guosheng Zhao",
            "Haoyun Li",
            "Zhehao Dong",
            "Qiang Zhang",
            "Yun Ye",
            "Yang Wang",
            "Guan Huang",
            "Wenjun Mei"
        ],
        "tldr": "SwiftVLA introduces a lightweight VLA model that enhances spatiotemporal reasoning using a 4D visual geometry transformer and fusion tokens, achieving comparable performance to larger VLAs with significantly reduced computational costs and memory footprint.",
        "tldr_zh": "SwiftVLA 提出了一种轻量级的 VLA 模型，它通过使用 4D 视觉几何变换器和融合令牌来增强时空推理能力，在显著降低计算成本和内存占用的情况下，实现了与更大规模 VLA 相当的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
        "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}.",
        "url": "http://arxiv.org/abs/2512.00891v1",
        "published_date": "2025-11-30T13:44:28+00:00",
        "updated_date": "2025-11-30T13:44:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyu Wang",
            "Xuyang Liu",
            "Xiyan Gui",
            "Xinying Lin",
            "Boxue Yang",
            "Chenfei Liao",
            "Tailai Chen",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces STC, a hierarchical token compression framework for streaming VideoLLMs that accelerates processing by caching and pruning visual tokens, achieving significant latency reduction with minimal accuracy loss.",
        "tldr_zh": "该论文介绍了一种名为STC的分层令牌压缩框架，用于加速流式视频大型语言模型（VideoLLM）的处理，通过缓存和剪枝视觉令牌，在几乎不损失精度的前提下显著降低延迟。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multilingual Training-Free Remote Sensing Image Captioning",
        "summary": "Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.",
        "url": "http://arxiv.org/abs/2512.00887v1",
        "published_date": "2025-11-30T13:16:42+00:00",
        "updated_date": "2025-11-30T13:16:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Carlos Rebelo",
            "Gil Rocha",
            "João Daniel Silva",
            "Bruno Martins"
        ],
        "tldr": "This paper introduces a training-free, multilingual remote sensing image captioning approach using retrieval-augmented prompting, achieving competitive results with supervised English-only systems and demonstrating good generalization to other languages.",
        "tldr_zh": "本文介绍了一种无需训练的多语言遥感图像字幕方法，该方法使用检索增强提示，在性能上与有监督的英语系统竞争，并展示了对其他语言的良好泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints",
        "summary": "Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to \"Reasoning-Driven Hallucination\" where linguistic priors override visual perception. A key bottleneck is the \"Modality Gap\": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose \"Look, Recite, Then Answer,\" a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.6% over Qwen-VL and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval",
        "url": "http://arxiv.org/abs/2512.00882v1",
        "published_date": "2025-11-30T13:04:43+00:00",
        "updated_date": "2025-11-30T13:04:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xisheng Feng"
        ],
        "tldr": "The paper introduces \"Look, Recite, Then Answer,\" a parameter-efficient framework for enhancing VLMs in specialized domains by using self-generated knowledge hints to mitigate reasoning-driven hallucinations, achieving state-of-the-art results on AgroBench.",
        "tldr_zh": "该论文提出了一个名为“看、背诵、然后回答”的参数高效框架，通过使用自生成的知识提示来增强VLM在特定领域的性能，以减轻推理驱动的幻觉，并在AgroBench上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent",
        "summary": "There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.",
        "url": "http://arxiv.org/abs/2512.00846v1",
        "published_date": "2025-11-30T11:32:54+00:00",
        "updated_date": "2025-11-30T11:32:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Neeraj Anand",
            "Rishabh Jain",
            "Sohan Patnaik",
            "Balaji Krishnamurthy",
            "Mausoom Sarkar"
        ],
        "tldr": "The paper introduces AFRAgent, a smaller, more efficient GUI automation agent based on Instruct-BLIP that uses adaptive feature renormalization to enhance image embeddings for improved performance on smartphone automation tasks.",
        "tldr_zh": "该论文介绍了AFRAgent，一个基于Instruct-BLIP的更小、更高效的GUI自动化代理，它使用自适应特征重归一化来增强图像嵌入，从而提高智能手机自动化任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning",
        "summary": "MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.",
        "url": "http://arxiv.org/abs/2512.00818v1",
        "published_date": "2025-11-30T09:56:50+00:00",
        "updated_date": "2025-11-30T09:56:50+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Haozhen Gong",
            "Xiaozhong Ji",
            "Yuansen Liu",
            "Wenbin Wu",
            "Xiaoxiao Yan",
            "Jingjing Liu",
            "Kai Wu",
            "Jiazhen Pan",
            "Bailiang Jian",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Hongwei Bran Li"
        ],
        "tldr": "The paper introduces Med-CMR, a new benchmark for evaluating the complex multimodal reasoning capabilities of MLLMs in the medical domain, highlighting challenges in visual-reasoning integration and rare-case robustness.",
        "tldr_zh": "该论文介绍了Med-CMR，这是一个新的基准，用于评估医学领域中MLLM的复杂多模态推理能力，强调了视觉推理集成和罕见案例鲁棒性方面的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering",
        "summary": "This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling.",
        "url": "http://arxiv.org/abs/2512.00773v1",
        "published_date": "2025-11-30T08:09:43+00:00",
        "updated_date": "2025-11-30T08:09:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Toshiki Katsube",
            "Taiga Fukuhara",
            "Kenichiro Ando",
            "Yusuke Mukuta",
            "Kohei Uehara",
            "Tatsuya Harada"
        ],
        "tldr": "The paper introduces DEJIMA, a new large-scale Japanese image captioning and VQA dataset created using a novel pipeline involving web collection, LLM refinement, and object detection. It outperforms existing Japanese V&L datasets in size and quality, showing improved performance on multimodal benchmarks.",
        "tldr_zh": "该论文介绍了DEJIMA，一个新型的大规模日语图像描述和VQA数据集，它使用了一种新颖的流水线，包括网络收集、LLM优化和对象检测。在规模和质量上，它优于现有的日语V&L数据集，并在多模态基准测试中表现出改进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models",
        "summary": "Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\\% to 68.9\\% (+7.6 pp), accuracy from 67.2\\% to 73.6\\% (+6.4 pp), and macro-F1 from 32.1\\% to 36.9\\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.",
        "url": "http://arxiv.org/abs/2512.00597v1",
        "published_date": "2025-11-29T19:03:25+00:00",
        "updated_date": "2025-11-29T19:03:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thuraya Alzubaidi",
            "Farhad R. Nezami",
            "Muzammil Behzad"
        ],
        "tldr": "The paper introduces MedCT-VLM, a parameter-efficient vision-language model adapting CT-CLIP for multi-label pathology classification using LoRA, demonstrating improved zero-shot performance on thoracic pathologies.",
        "tldr_zh": "该论文介绍了MedCT-VLM，一种参数高效的视觉-语言模型，使用LoRA来调整CT-CLIP模型，用于多标签病理分类，并在胸部病理的零样本性能上有所提高。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PhotoFramer: Multi-modal Image Composition Instruction",
        "summary": "Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.",
        "url": "http://arxiv.org/abs/2512.00993v1",
        "published_date": "2025-11-30T17:26:47+00:00",
        "updated_date": "2025-11-30T17:26:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyuan You",
            "Ke Wang",
            "He Zhang",
            "Xin Cai",
            "Jinjin Gu",
            "Tianfan Xue",
            "Chao Dong",
            "Zhoutong Zhang"
        ],
        "tldr": "PhotoFramer introduces a multi-modal framework for image composition guidance, providing textual instructions and example images to improve poorly composed photos, and a large-scale dataset for training.",
        "tldr_zh": "PhotoFramer 提出了一个多模态图像构图指导框架，提供文本指导和示例图像来改善构图不良的照片，并提供了一个大规模数据集用于训练。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding",
        "summary": "Grounding complex, compositional visual queries with multiple objects and relationships is a fundamental challenge for vision-language models. While standard phrase grounding methods excel at localizing single objects, they lack the structural inductive bias to parse intricate relational descriptions, often failing as queries become more descriptive. To address this structural deficit, we focus on scene-graph grounding, a powerful but less-explored formulation where the query is an explicit graph of objects and their relationships. However, existing methods for this task also struggle, paradoxically showing decreased performance as the query graph grows -- failing to leverage the very information that should make grounding easier. We introduce SceneProp, a novel method that resolves this issue by reformulating scene-graph grounding as a Maximum a Posteriori (MAP) inference problem in a Markov Random Field (MRF). By performing global inference over the entire query graph, SceneProp finds the optimal assignment of image regions to nodes that jointly satisfies all constraints. This is achieved within an end-to-end framework via a differentiable implementation of the Belief Propagation algorithm. Experiments on four benchmarks show that our dedicated focus on the scene-graph grounding formulation allows SceneProp to significantly outperform prior work. Critically, its accuracy consistently improves with the size and complexity of the query graph, demonstrating for the first time that more relational context can, and should, lead to better grounding. Codes are available at https://github.com/keitaotani/SceneProp.",
        "url": "http://arxiv.org/abs/2512.00936v1",
        "published_date": "2025-11-30T15:35:38+00:00",
        "updated_date": "2025-11-30T15:35:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keita Otani",
            "Tatsuya Harada"
        ],
        "tldr": "SceneProp addresses the challenge of grounding complex visual queries by reformulating scene-graph grounding as a MAP inference problem in a Markov Random Field, achieving state-of-the-art results and improved performance with increasing query complexity.",
        "tldr_zh": "SceneProp通过将场景图的grounding问题重新定义为马尔可夫随机场中的最大后验概率推断问题，解决了复杂视觉查询的grounding挑战，实现了最先进的结果，并且随着查询复杂性的增加，性能也得到了提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards",
        "summary": "Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \\textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \\textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \\textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \\emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \\emph{reward-based grouping} to compute advantages for each reward function \\textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \\textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \\textit{PickScore-25k} and multi-objective \\textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \\href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.",
        "url": "http://arxiv.org/abs/2512.00743v1",
        "published_date": "2025-11-30T05:44:35+00:00",
        "updated_date": "2025-11-30T05:44:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiang Lyu",
            "Zicong Chen",
            "Chongxiao Wang",
            "Haolin Shi",
            "Shibo Gao",
            "Ran Piao",
            "Youwei Zeng",
            "Jianlou Si",
            "Fei Ding",
            "Jing Li",
            "Chun Pong Lau",
            "Weiqiang Wang"
        ],
        "tldr": "The paper introduces Multi-GRPO, a novel framework addressing limitations in GRPO-based text-to-image alignment by improving credit assignment via tree-based trajectories and disentangling multi-objective rewards through reward-based grouping, demonstrating superior performance on new and existing benchmarks.",
        "tldr_zh": "本文介绍了 Multi-GRPO，一种新颖的框架，通过树状轨迹改善信用分配并分离基于奖励的分组，从而解决了基于 GRPO 的文本到图像对齐的局限性，并在新的和现有的基准测试中展示了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension",
        "summary": "Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.",
        "url": "http://arxiv.org/abs/2512.00582v1",
        "published_date": "2025-11-29T18:27:50+00:00",
        "updated_date": "2025-11-29T18:27:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Jiang",
            "Haiwei Xue",
            "Minghao Han",
            "Mingcheng Li",
            "Xiaolu Hou",
            "Dingkang Yang",
            "Lihua Zhang",
            "Xu Zheng"
        ],
        "tldr": "The paper introduces SatireDecoder, a training-free framework employing visual cascaded decoupling and chain-of-thought reasoning to improve satirical image comprehension by better integrating local and global semantic representations and reducing hallucinations in vision-language models.",
        "tldr_zh": "该论文介绍了一种名为SatireDecoder的免训练框架，该框架采用视觉级联解耦和思维链推理，通过更好地整合局部和全局语义表示并减少视觉-语言模型中的幻觉，从而提高对讽刺图像的理解。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba",
        "summary": "Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \\textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.",
        "url": "http://arxiv.org/abs/2512.00647v1",
        "published_date": "2025-11-29T21:58:08+00:00",
        "updated_date": "2025-11-29T21:58:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shanhui Liu",
            "Rui Xu",
            "Yunke Wang"
        ],
        "tldr": "The paper introduces Coarse-to-Fine Vision Mamba (CF-ViM), an adaptive inference framework that dynamically adjusts the resolution of image processing based on image complexity, improving efficiency without sacrificing accuracy compared to standard Vision Mamba and other token reduction methods.",
        "tldr_zh": "该论文介绍了Coarse-to-Fine Vision Mamba (CF-ViM)，一种自适应推理框架，它根据图像的复杂性动态调整图像处理的分辨率，与标准的Vision Mamba和其他token减少方法相比，提高了效率且不牺牲精度。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]