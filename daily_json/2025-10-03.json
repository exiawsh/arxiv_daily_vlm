[
    {
        "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories",
        "summary": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.",
        "url": "http://arxiv.org/abs/2510.01454v1",
        "published_date": "2025-10-01T20:47:29+00:00",
        "updated_date": "2025-10-01T20:47:29+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nilay Naharas",
            "Dang Nguyen",
            "Nesihan Bulut",
            "Mohammadhossein Bateni",
            "Vahab Mirrokni",
            "Baharan Mirzasoleiman"
        ],
        "tldr": "This paper introduces XMAS, a novel data selection method for efficient fine-tuning of Large Vision-Language Models (LVLMs) by clustering examples based on cross-modal attention trajectories, achieving significant data reduction while maintaining performance.",
        "tldr_zh": "本文介绍了XMAS，一种新颖的数据选择方法，通过基于跨模态注意力轨迹对样本进行聚类，从而实现大型视觉语言模型（LVLM）的高效微调，并在保持性能的同时显著减少数据量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
        "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
        "url": "http://arxiv.org/abs/2510.02295v1",
        "published_date": "2025-10-02T17:58:54+00:00",
        "updated_date": "2025-10-02T17:58:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Enxin Song",
            "Wenhao Chai",
            "Shusheng Yang",
            "Ethan Armand",
            "Xiaojun Shan",
            "Haiyang Xu",
            "Jianwen Xie",
            "Zhuowen Tu"
        ],
        "tldr": "The paper introduces VideoNSA, an adaptation of Native Sparse Attention for video-language models, improving long-video understanding and temporal reasoning by selectively applying sparse attention to video and dense attention to text.",
        "tldr_zh": "该论文介绍了VideoNSA，一种针对视频语言模型的原生稀疏注意力机制的改进版本。通过对视频应用稀疏注意力，对文本应用密集注意力，提高了模型对长视频的理解和时间推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens",
        "summary": "We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,\nanalysis, and interpretation of vision-language models (VLMs) by supporting the\nextraction of intermediate outputs from any layer during the forward pass of\nopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that\nabstracts away model-specific complexities and supports user-friendly operation\nacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs and\ntheir over 30 variants, and is extensible to accommodate new models without\nchanging the core logic.\n  The toolkit integrates easily with various interpretability and analysis\nmethods. We demonstrate its usage with two simple analytical experiments,\nrevealing systematic differences in the hidden representations of VLMs across\nlayers and target concepts. VLM-Lens is released as an open-sourced project to\naccelerate community efforts in understanding and improving VLMs.",
        "url": "http://arxiv.org/abs/2510.02292v1",
        "published_date": "2025-10-02T17:58:41+00:00",
        "updated_date": "2025-10-02T17:58:41+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Hala Sheta",
            "Eric Huang",
            "Shuyu Wu",
            "Ilia Alenabi",
            "Jiajun Hong",
            "Ryker Lin",
            "Ruoxi Ning",
            "Daniel Wei",
            "Jialin Yang",
            "Jiawei Zhou",
            "Ziqiao Ma",
            "Freda Shi"
        ],
        "tldr": "The paper introduces VLM-Lens, an open-source toolkit for benchmarking, analyzing, and interpreting vision-language models by extracting intermediate layer outputs. It supports multiple VLMs and facilitates interpretability research.",
        "tldr_zh": "该论文介绍了一个名为VLM-Lens的开源工具包，用于通过提取中间层输出，对视觉语言模型进行基准测试、分析和解释。它支持多种视觉语言模型，并促进可解释性研究。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification",
        "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.",
        "url": "http://arxiv.org/abs/2510.02270v1",
        "published_date": "2025-10-02T17:47:39+00:00",
        "updated_date": "2025-10-02T17:47:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sathira Silva",
            "Eman Ali",
            "Chetan Arora",
            "Muhammad Haris Khan"
        ],
        "tldr": "The paper introduces microCLIP, a self-training framework for unsupervised adaptation of CLIP for fine-grained image classification, using a novel TokenFusion module and dynamic knowledge aggregation for improved accuracy.",
        "tldr_zh": "该论文介绍了 microCLIP，一个用于 CLIP 无监督自适应微粒度图像分类的自训练框架，它使用了一种新颖的 TokenFusion 模块和动态知识聚合，以提高准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding",
        "summary": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c .",
        "url": "http://arxiv.org/abs/2510.02262v1",
        "published_date": "2025-10-02T17:43:01+00:00",
        "updated_date": "2025-10-02T17:43:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangyu Sun",
            "Archit Singhal",
            "Burak Uzkent",
            "Mubarak Shah",
            "Chen Chen",
            "Garin Kessler"
        ],
        "tldr": "The paper introduces F2C, a training-free method that selects key clips instead of key frames for long-form video understanding, improving performance by preserving temporal coherence within a fixed computational budget by adaptively adjusting spatial resolution and clip length.",
        "tldr_zh": "该论文介绍了F2C，一种无需训练的方法，通过选择关键片段而非关键帧来进行长视频理解，通过自适应调整空间分辨率和片段长度，在固定的计算预算内，提高了性能，同时保留了时间连贯性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting",
        "summary": "Prompting has emerged as a practical way to adapt frozen vision-language\nmodels (VLMs) for video anomaly detection (VAD). Yet, existing prompts are\noften overly abstract, overlooking the fine-grained human-object interactions\nor action semantics that define complex anomalies in surveillance videos. We\npropose ASK-Hint, a structured prompting framework that leverages\naction-centric knowledge to elicit more accurate and interpretable reasoning\nfrom frozen VLMs. Our approach organizes prompts into semantically coherent\ngroups (e.g. violence, property crimes, public safety) and formulates\nfine-grained guiding questions that align model predictions with discriminative\nvisual cues. Extensive experiments on UCF-Crime and XD-Violence show that\nASK-Hint consistently improves AUC over prior baselines, achieving\nstate-of-the-art performance compared to both fine-tuned and training-free\nmethods. Beyond accuracy, our framework provides interpretable reasoning traces\ntowards anomaly and demonstrates strong generalization across datasets and VLM\nbackbones. These results highlight the critical role of prompt granularity and\nestablish ASK-Hint as a new training-free and generalizable solution for\nexplainable video anomaly detection.",
        "url": "http://arxiv.org/abs/2510.02155v1",
        "published_date": "2025-10-02T16:06:31+00:00",
        "updated_date": "2025-10-02T16:06:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shu Zou",
            "Xinyu Tian",
            "Lukas Wesemann",
            "Fabian Waschkowski",
            "Zhaoyuan Yang",
            "Jing Zhang"
        ],
        "tldr": "This paper introduces ASK-Hint, a structured prompting framework that uses action-centric knowledge and fine-grained guiding questions to improve video anomaly detection using frozen vision-language models. It achieves state-of-the-art performance and offers interpretable reasoning.",
        "tldr_zh": "本文介绍了一种名为ASK-Hint的结构化提示框架，该框架利用以动作为中心的知识和细粒度的引导性问题，来改进使用冻结的视觉语言模型进行视频异常检测。它实现了最先进的性能，并提供可解释的推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation",
        "summary": "Federeated Learning (FL) offers a privacy-preserving solution for Semantic\nSegmentation (SS) tasks to adapt to new domains, but faces significant\nchallenges from these domain shifts, particularly when client data is\nunlabeled. However, most existing FL methods unrealistically assume access to\nlabeled data on remote clients or fail to leverage the power of modern Vision\nFoundation Models (VFMs). Here, we propose a novel and challenging task,\nFFREEDG, in which a model is pretrained on a server's labeled source dataset\nand subsequently trained across clients using only their unlabeled data,\nwithout ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a\nframework that leverages the knowledge of a VFM by integrating vision and\nlanguage modalities. Our approach employs a Vision-Language decoder guided by\nCLIP-based text embeddings to improve semantic disambiguation and uses a\nweak-to-strong consistency learning strategy for robust local training on\npseudo-labels. Our experiments on synthetic-to-real and\nclear-to-adverse-weather benchmarks demonstrate that our framework effectively\ntackles this new task, achieving competitive performance against established\ndomain generalization and adaptation methods and setting a strong baseline for\nfuture research.",
        "url": "http://arxiv.org/abs/2510.02114v1",
        "published_date": "2025-10-02T15:21:49+00:00",
        "updated_date": "2025-10-02T15:21:49+00:00",
        "categories": [
            "cs.CV",
            "68T10"
        ],
        "authors": [
            "Ding-Ruei Shen"
        ],
        "tldr": "The paper introduces FRIEREN, a Federated Learning framework for semantic segmentation that uses Vision-Language regularization to address domain shifts in unlabeled client data by leveraging Vision Foundation Models. It proposes a new federated learning task, FFREEDG.",
        "tldr_zh": "该论文介绍了一种用于语义分割的联邦学习框架 FRIEREN，该框架利用视觉语言正则化来解决未标记客户端数据中的领域转移问题，并利用了视觉基础模型。它提出了一种新的联邦学习任务 FFREEDG。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs",
        "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
        "url": "http://arxiv.org/abs/2510.01954v1",
        "published_date": "2025-10-02T12:23:57+00:00",
        "updated_date": "2025-10-02T12:23:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongyi Su",
            "Haojie Zhang",
            "Shijie Li",
            "Nanqing Liu",
            "Jingyi Liao",
            "Junyi Pan",
            "Yuan Liu",
            "Xiaofen Xing",
            "Chong Sun",
            "Chen Li",
            "Nancy F. Chen",
            "Shuicheng Yan",
            "Xulei Yang",
            "Xun Xu"
        ],
        "tldr": "The paper introduces Patch-as-Decodable-Token (PaDT), a novel approach for unified multi-modal vision tasks in MLLMs by directly generating both textual and visual outputs using Visual Reference Tokens, achieving state-of-the-art performance across various visual tasks.",
        "tldr_zh": "该论文介绍了Patch-as-Decodable-Token (PaDT)，一种用于多模态大型语言模型中统一多模态视觉任务的新方法，通过使用视觉参考令牌直接生成文本和视觉输出，并在各种视觉任务中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VaPR -- Vision-language Preference alignment for Reasoning",
        "summary": "Preference finetuning methods like Direct Preference Optimization (DPO) with\nAI-generated feedback have shown promise in aligning Large Vision-Language\nModels (LVLMs) with human preferences. However, existing techniques overlook\nthe prevalence of noise in synthetic preference annotations in the form of\nstylistic and length biases. To this end, we introduce a hard-negative response\ngeneration framework based on LLM-guided response editing, that produces\nrejected responses with targeted errors, maintaining stylistic and length\nsimilarity to the accepted ones. Using this framework, we develop the VaPR\ndataset, comprising 30K high-quality samples, to finetune three LVLM families:\nLLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver\nsignificant performance improvements across ten benchmarks, achieving average\ngains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable\nimprovements on reasoning tasks. A scaling analysis shows that performance\nconsistently improves with data size, with LLaVA models benefiting even at\nsmaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary\nquestions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we\nshow that the framework generalizes to open-source LLMs as editors, with models\ntrained on VaPR-OS achieving ~99% of the performance of models trained on\n\\name, which is synthesized using GPT-4o. Our data, models, and code can be\nfound on the project page https://vap-r.github.io",
        "url": "http://arxiv.org/abs/2510.01700v1",
        "published_date": "2025-10-02T06:10:43+00:00",
        "updated_date": "2025-10-02T06:10:43+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Rohan Wadhawan",
            "Fabrice Y Harel-Canada",
            "Zi-Yi Dou",
            "Suhaila Shakiah",
            "Robinson Piramuthu",
            "Nanyun Peng"
        ],
        "tldr": "The paper introduces VaPR, a dataset and method for preference alignment in LVLMs using LLM-guided hard-negative response generation, showing significant performance improvements and bias reduction in reasoning tasks.",
        "tldr_zh": "该论文介绍了VaPR，一种用于LVLM中偏好对齐的数据集和方法，它使用LLM引导的硬负例响应生成，在推理任务中显示出显著的性能改进和偏差减少。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs",
        "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.",
        "url": "http://arxiv.org/abs/2510.01691v1",
        "published_date": "2025-10-02T05:42:00+00:00",
        "updated_date": "2025-10-02T05:42:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiyao Liu",
            "Jinjie Wei",
            "Wanying Qu",
            "Chenglong Ma",
            "Junzhi Ning",
            "Yunheng Li",
            "Ying Chen",
            "Xinzhe Luo",
            "Pengcheng Chen",
            "Xin Gao",
            "Ming Hu",
            "Huihui Xu",
            "Xin Wang",
            "Shujian Gao",
            "Dingkang Yang",
            "Zhongying Deng",
            "Jin Ye",
            "Lihao Liu",
            "Junjun He",
            "Ningsheng Xu"
        ],
        "tldr": "The paper introduces MedQ-Bench, a new benchmark for evaluating medical image quality assessment capabilities of MLLMs using language-based evaluation with perceptual and reasoning tasks, finding current models insufficient for clinical use.",
        "tldr_zh": "该论文介绍了MedQ-Bench，一个新的基准，用于评估MLLM在医学图像质量评估方面的能力，该基准使用基于语言的评估方法，包含感知和推理任务。研究发现，目前的模型不足以在临床中使用。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning",
        "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they\nfrequently struggle with tasks requiring precise understanding and handling of\nfine-grained visual elements. This is mainly due to information loss during\nimage encoding or insufficient attention to critical regions. Recent work has\nshown promise by incorporating pixel-level visual information into the\nreasoning process, enabling VLMs to access high-resolution visual details\nduring their thought process. However, this pixel-level information is often\noverused, leading to inefficiency and distraction from irrelevant visual\ndetails. To address these challenges, we propose the first framework for\nadaptive pixel reasoning that dynamically determines necessary pixel-level\noperations based on the input query. Specifically, we first apply\noperation-aware supervised fine-tuning to establish baseline competence in\ntextual reasoning and visual operations, then design a novel rollout-guided\nreinforcement learning framework relying on feedback of the model's own\nresponses, which enables the VLM to determine when pixel operations should be\ninvoked based on query difficulty. Experiments on extensive multimodal\nreasoning benchmarks show that our model achieves superior performance while\nsignificantly reducing unnecessary visual operations. Impressively, our model\nachieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of\nonly 20.1\\%, improving accuracy and simultaneously reducing tool usage by\n66.5\\% compared to the previous methods.",
        "url": "http://arxiv.org/abs/2510.01681v1",
        "published_date": "2025-10-02T05:14:52+00:00",
        "updated_date": "2025-10-02T05:14:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xuchen Li",
            "Xuzhao Li",
            "Jiahui Gao",
            "Renjie Pi",
            "Shiyu Hu",
            "Wentao Zhang"
        ],
        "tldr": "This paper introduces an adaptive pixel reasoning framework for VLMs that uses rollout-guided reinforcement learning to selectively use pixel-level visual information, improving performance and efficiency on multimodal reasoning tasks.",
        "tldr_zh": "本文介绍了一种用于视觉语言模型（VLMs）的自适应像素推理框架，该框架使用rollout引导的强化学习来选择性地使用像素级视觉信息，从而提高多模态推理任务的性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.",
        "url": "http://arxiv.org/abs/2510.01623v1",
        "published_date": "2025-10-02T02:54:03+00:00",
        "updated_date": "2025-10-02T02:54:03+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Angen Ye",
            "Zeyu Zhang",
            "Boyuan Wang",
            "Xiaofeng Wang",
            "Dapeng Zhang",
            "Zheng Zhu"
        ],
        "tldr": "The paper introduces VLA-R1, a reasoning-enhanced Vision-Language-Action model that integrates reinforcement learning from verifiable rewards (RLVR) with group relative policy optimization (GRPO), along with a new dataset VLA-CoT-13K, to improve reasoning and execution in VLA models.",
        "tldr_zh": "该论文介绍了VLA-R1，一种增强推理的视觉-语言-动作模型，它集成了来自可验证奖励的强化学习（RLVR）和群体相对策略优化（GRPO），以及一个新的数据集VLA-CoT-13K，以提高VLA模型中的推理和执行能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models",
        "summary": "We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the\ndevelopment of Vision Language Models (VLMs) with explicit reasoning\ncapabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,\nproviding structured thinking tokens and corresponding answers. Our synthetic\ndataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and\nKimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of\nthinking-answer sequences, creating a resource for training and evaluating\nmultimodal reasoning models. We capture the step-by-step reasoning process of\nVLMs and the final descriptive answers. Our goal with this dataset is to enable\nthe development of more robust VLMs while contributing to the broader\nunderstanding of multimodal reasoning mechanisms. The dataset and evaluation\nbenchmarks will be publicly available to aid research in reasoning/thinking\nmultimodal VLMs.",
        "url": "http://arxiv.org/abs/2510.01582v1",
        "published_date": "2025-10-02T02:02:45+00:00",
        "updated_date": "2025-10-02T02:02:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Krishna Teja Chitty-Venkata",
            "Murali Emani"
        ],
        "tldr": "The paper introduces ImageNet-Think-250K, a large-scale synthetic dataset for training and evaluating multimodal reasoning in Vision Language Models (VLMs), generated using two state-of-the-art VLMs.",
        "tldr_zh": "该论文介绍了ImageNet-Think-250K，一个大规模合成数据集，用于训练和评估视觉语言模型（VLM）中的多模态推理，该数据集由两个最先进的VLM生成。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations",
        "summary": "Multimodal large language models (MLLMs) have been integrated into visual\ninterpretation applications to support Blind and Low Vision (BLV) users because\nof their accuracy and ability to provide rich, human-like interpretations.\nHowever, these applications often default to comprehensive, lengthy\ndescriptions regardless of context. This leads to inefficient exchanges, as\nusers must go through irrelevant details rather than receiving the specific\ninformation they are likely to seek. To deliver more contextually-relevant\ninformation, we developed a system that draws on historical BLV users\nquestions. When given an image, our system identifies similar past visual\ncontexts from the VizWiz-LF dataset and uses the associated questions to guide\nthe MLLM generate descriptions more relevant to BLV users. An evaluation with\nthree human labelers who revised 92 context-aware and context-free descriptions\nshowed that context-aware descriptions anticipated and answered users'\nquestions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of\ncomparisons (50 out of 92). Our paper reviews, and data analysis are publicly\navailable in a Github repository at\nhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .",
        "url": "http://arxiv.org/abs/2510.01576v1",
        "published_date": "2025-10-02T01:48:51+00:00",
        "updated_date": "2025-10-02T01:48:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "I.2.m; H.5.2"
        ],
        "authors": [
            "Ricardo Gonzalez Penuela",
            "Felipe Arias-Russi",
            "Victor Capriles"
        ],
        "tldr": "This paper enhances MLLM-based visual interpretation for BLV users by guiding the model with historical questions from the VizWiz-LF dataset, leading to more relevant and preferred descriptions.",
        "tldr_zh": "本文通过使用VizWiz-LF数据集中历史问题引导MLLM，从而增强了基于MLLM的BLV用户视觉解释，从而产生更相关和更受青睐的描述。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs",
        "summary": "Multimodal large language models (MLLMs) extend the success of language\nmodels to visual understanding, and recent efforts have sought to build unified\nMLLMs that support both understanding and generation. However, constructing\nsuch models remains challenging: hybrid approaches combine continuous\nembeddings with diffusion or flow-based objectives, producing high-quality\nimages but breaking the autoregressive paradigm, while pure autoregressive\napproaches unify text and image prediction over discrete visual tokens but\noften face trade-offs between semantic alignment and pixel-level fidelity. In\nthis work, we present Bridge, a pure autoregressive unified MLLM that augments\npre-trained visual understanding models with generative ability through a\nMixture-of-Transformers architecture, enabling both image understanding and\ngeneration within a single next-token prediction framework. To further improve\nvisual generation fidelity, we propose a semantic-to-pixel discrete\nrepresentation that integrates compact semantic tokens with fine-grained pixel\ntokens, achieving strong language alignment and precise description of visual\ndetails with only a 7.9% increase in sequence length. Extensive experiments\nacross diverse multimodal benchmarks demonstrate that Bridge achieves\ncompetitive or superior results in both understanding and generation\nbenchmarks, while requiring less training data and reduced training time\ncompared to prior unified MLLMs.",
        "url": "http://arxiv.org/abs/2510.01546v1",
        "published_date": "2025-10-02T00:40:02+00:00",
        "updated_date": "2025-10-02T00:40:02+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Hanyu Wang",
            "Jiaming Han",
            "Ziyan Yang",
            "Qi Zhao",
            "Shanchuan Lin",
            "Xiangyu Yue",
            "Abhinav Shrivastava",
            "Zhenheng Yang",
            "Hao Chen"
        ],
        "tldr": "The paper introduces Bridge, a pure autoregressive unified MLLM using a Mixture-of-Transformers architecture, augmenting pre-trained visual understanding models with generative ability through a semantic-to-pixel discrete representation, achieving strong performance in both understanding and generation tasks with less data and training time.",
        "tldr_zh": "该论文介绍了Bridge，一种纯自回归统一MLLM，采用混合Transformer架构，通过语义到像素的离散表示，增强预训练的视觉理解模型，使其具备生成能力，在理解和生成任务中均表现出色，并且所需数据和训练时间更少。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation",
        "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io",
        "url": "http://arxiv.org/abs/2510.01388v1",
        "published_date": "2025-10-01T19:21:28+00:00",
        "updated_date": "2025-10-01T19:21:28+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Arthur Zhang",
            "Xiangyun Meng",
            "Luca Calliari",
            "Dong-Ki Kim",
            "Shayegan Omidshafiei",
            "Joydeep Biswas",
            "Ali Agha",
            "Amirreza Shaban"
        ],
        "tldr": "VENTURA finetunes image diffusion models for vision-language navigation by generating path masks as visual plans, achieving improved performance in real-world robot navigation tasks compared to state-of-the-art baselines.",
        "tldr_zh": "VENTURA通过微调图像扩散模型，生成路径掩码作为视觉规划，用于视觉语言导航，在真实机器人导航任务中，与最先进的基线相比，实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models",
        "summary": "State-of-the-art vision-and-language models consist of many parameters and\nlearn from enormous datasets, surpassing the amounts of linguistic data that\nchildren are exposed to as they acquire a language. This paper presents our\napproach to the multimodal track of the BabyLM challenge addressing this\ndiscrepancy. We develop language-only and multimodal models in low-resource\nsettings using developmentally plausible datasets, with our multimodal models\noutperforming previous BabyLM baselines. One finding in the multimodal language\nmodel literature is that these models tend to underperform in\n\\textit{language-only} tasks. Therefore, we focus on maintaining language-only\nabilities in multimodal models. To this end, we experiment with \\textit{model\nmerging}, where we fuse the parameters of multimodal models with those of\nlanguage-only models using weighted linear interpolation. Our results\ncorroborate the findings that multimodal models underperform in language-only\nbenchmarks that focus on grammar, and model merging with text-only models can\nhelp alleviate this problem to some extent, while maintaining multimodal\nperformance.",
        "url": "http://arxiv.org/abs/2510.01845v1",
        "published_date": "2025-10-02T09:38:25+00:00",
        "updated_date": "2025-10-02T09:38:25+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Ece Takmaz",
            "Lisa Bylinina",
            "Jakub Dotlacil"
        ],
        "tldr": "The paper explores model merging to improve the language-only performance of multimodal models trained on developmentally plausible datasets, addressing the tendency of such models to underperform in language-focused tasks.",
        "tldr_zh": "该论文探索了模型合并方法，旨在提高在发育上合理的的数据集上训练的多模态模型在纯语言任务中的性能，解决这些模型在语言相关任务中表现不佳的问题。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation",
        "summary": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).",
        "url": "http://arxiv.org/abs/2510.02186v1",
        "published_date": "2025-10-02T16:37:56+00:00",
        "updated_date": "2025-10-02T16:37:56+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Weijia Dou",
            "Xu Zhang",
            "Yi Bin",
            "Jian Liu",
            "Bo Peng",
            "Guoqing Wang",
            "Yang Yang",
            "Heng Tao Shen"
        ],
        "tldr": "GeoPurify addresses the trade-off between noisy 2D-to-3D feature transfer in open-vocabulary 3D segmentation and the need for large labeled 3D datasets by using geometric priors distilled from a self-supervised teacher, achieving SOTA performance with significantly less data.",
        "tldr_zh": "GeoPurify 通过利用自监督教师模型提取的几何先验，解决了开放词汇 3D 分割中嘈杂的 2D 到 3D 特征迁移与大型标记 3D 数据集需求之间的矛盾，以显著减少的数据量实现了 SOTA 性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework",
        "summary": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to\nautomatically generate jaw cyst findings on dental panoramic radiographs. To\nimprove accuracy, we constructed a Self-correction Loop with Structured Output\n(SLSO) framework and verified its effectiveness. A 10-step process was\nimplemented for 22 cases of jaw cysts, including image input and analysis,\nstructured data generation, tooth number extraction and consistency checking,\niterative regeneration when inconsistencies were detected, and finding\ngeneration with subsequent restructuring and consistency verification. A\ncomparative experiment was conducted using the conventional Chain-of-Thought\n(CoT) method across seven evaluation items: transparency, internal structure,\nborders, root resorption, tooth movement, relationships with other structures,\nand tooth number. The results showed that the proposed SLSO framework improved\noutput accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates\nfor tooth number, tooth movement, and root resorption, respectively. In the\nsuccessful cases, a consistently structured output was achieved after up to\nfive regenerations. Although statistical significance was not reached because\nof the small size of the dataset, the overall SLSO framework enforced negative\nfinding descriptions, suppressed hallucinations, and improved tooth number\nidentification accuracy. However, the accurate identification of extensive\nlesions spanning multiple teeth is limited. Nevertheless, further refinement is\nrequired to enhance overall performance and move toward a practical finding\ngeneration system.",
        "url": "http://arxiv.org/abs/2510.02001v1",
        "published_date": "2025-10-02T13:22:13+00:00",
        "updated_date": "2025-10-02T13:22:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nanaka Hosokawa",
            "Ryo Takahashi",
            "Tomoya Kitano",
            "Yukihiro Iida",
            "Chisako Muramatsu",
            "Tatsuro Hayashi",
            "Yuta Seino",
            "Xiangrong Zhou",
            "Takeshi Hara",
            "Akitoshi Katsumata",
            "Hiroshi Fujita"
        ],
        "tldr": "This paper explores using GPT-4o with a Self-correction Loop with Structured Output (SLSO) framework to automatically generate findings for jaw cysts in dental panoramic radiographs, showing improvements in some areas compared to Chain-of-Thought (CoT) but with limitations.",
        "tldr_zh": "本文探讨了使用GPT-4o和一个自我修正循环结构化输出(SLSO)框架，自动生成牙科全景X光片中颌骨囊肿的发现，与思维链(CoT)相比，在某些方面有所改进，但存在局限性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    }
]