[
    {
        "title": "dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model",
        "summary": "Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.",
        "url": "http://arxiv.org/abs/2512.02498v1",
        "published_date": "2025-12-02T07:42:38+00:00",
        "updated_date": "2025-12-02T07:42:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yumeng Li",
            "Guang Yang",
            "Hao Liu",
            "Bowen Wang",
            "Colin Zhang"
        ],
        "tldr": "The paper introduces dots.ocr, a novel end-to-end Vision-Language Model for multilingual document layout parsing, achieving state-of-the-art performance and establishing a new benchmark for multilingual document intelligence.",
        "tldr_zh": "该论文介绍了dots.ocr，一种用于多语言文档布局解析的新型端到端视觉-语言模型，实现了最先进的性能，并为多语言文档智能建立了一个新的基准。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration",
        "summary": "Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.",
        "url": "http://arxiv.org/abs/2512.02981v1",
        "published_date": "2025-12-02T17:59:52+00:00",
        "updated_date": "2025-12-02T17:59:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongyu Yang",
            "Yingfang Yuan",
            "Xuanming Jiang",
            "Baoyi An",
            "Wei Pang"
        ],
        "tldr": "The paper proposes InEx, a training-free, multi-agent framework for mitigating hallucination in multimodal LLMs by using internal introspection and external cross-modal collaboration, achieving significant performance gains over existing methods.",
        "tldr_zh": "该论文提出了InEx，一个无需训练的多智能体框架，通过内部自省和外部跨模态协作来缓解多模态LLM中的幻觉问题，并在现有方法上实现了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
        "summary": "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2512.02906v1",
        "published_date": "2025-12-02T16:22:01+00:00",
        "updated_date": "2025-12-02T16:22:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Fan Yang",
            "Kaihao Zhang"
        ],
        "tldr": "The paper introduces MRD, a training-free framework that uses multi-resolution semantic fusion and open-vocabulary object detection to improve high-resolution image understanding in multimodal large language models.",
        "tldr_zh": "该论文介绍了一种名为MRD的免训练框架，它使用多分辨率语义融合和开放词汇对象检测来提升多模态大型语言模型中高分辨率图像的理解能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm",
        "summary": "We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.",
        "url": "http://arxiv.org/abs/2512.02895v1",
        "published_date": "2025-12-02T16:04:11+00:00",
        "updated_date": "2025-12-02T16:04:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Chen",
            "Chaoqun Du",
            "Feng Gu",
            "Wei He",
            "Qizhen Li",
            "Zide Liu",
            "Xuhao Pan",
            "Chang Ren",
            "Xudong Rao",
            "Chenfeng Wang",
            "Tao Wei",
            "Chengjun Yu",
            "Pengfei Yu",
            "Yufei Zheng",
            "Chunpeng Zhou",
            "Pan Zhou",
            "Xuhan Zhu"
        ],
        "tldr": "MindGPT-4ov presents a post-training paradigm for multimodal large language models (MLLMs) with innovations in data generation, fine-tuning, and reinforcement learning, achieving state-of-the-art performance and efficient deployment, with code and data to be open-sourced.",
        "tldr_zh": "MindGPT-4ov 提出了一个用于多模态大型语言模型（MLLM）的后训练范式，其在数据生成、微调和强化学习方面进行了创新，实现了最先进的性能和高效部署，并将开源代码和数据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?",
        "summary": "Anticipating actions before they occur is a core challenge in action understanding research. While conventional methods rely on extracting and aggregating temporal information from videos, as humans we can often predict upcoming actions by observing a single moment from a scene, when given sufficient context. Can a model achieve this competence? The short answer is yes, although its effectiveness depends on the complexity of the task. In this work, we investigate to what extent video aggregation can be replaced with alternative modalities. To this end, based on recent advances in visual feature extraction and language-based reasoning, we introduce AAG, a method for Action Anticipation at a Glimpse. AAG combines RGB features with depth cues from a single frame for enhanced spatial reasoning, and incorporates prior action information to provide long-term context. This context is obtained either through textual summaries from Vision-Language Models, or from predictions generated by a single-frame action recognizer. Our results demonstrate that multimodal single-frame action anticipation using AAG can perform competitively compared to both temporally aggregated video baselines and state-of-the-art methods across three instructional activity datasets: IKEA-ASM, Meccano, and Assembly101.",
        "url": "http://arxiv.org/abs/2512.02846v1",
        "published_date": "2025-12-02T14:57:17+00:00",
        "updated_date": "2025-12-02T14:57:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manuel Benavent-Lledo",
            "Konstantinos Bacharidis",
            "Victoria Manousaki",
            "Konstantinos Papoutsakis",
            "Antonis Argyros",
            "Jose Garcia-Rodriguez"
        ],
        "tldr": "This paper explores action anticipation using multimodal cues from a single frame, demonstrating competitive performance against video-based methods on instructional activity datasets. It introduces AAG, a method that combines RGB, depth, and prior action information for action anticipation at a glimpse.",
        "tldr_zh": "本文研究了使用单帧多模态线索进行动作预测，并在教学活动数据集上展示了与基于视频的方法相比具有竞争力的性能。它介绍了AAG，一种结合RGB、深度和先前动作信息的方法，用于快速动作预测。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
        "summary": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
        "url": "http://arxiv.org/abs/2512.02835v1",
        "published_date": "2025-12-02T14:44:12+00:00",
        "updated_date": "2025-12-02T14:44:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yifan Li",
            "Yingda Yin",
            "Lingting Zhu",
            "Weikai Chen",
            "Shengju Qian",
            "Xin Wang",
            "Yanwei Fu"
        ],
        "tldr": "ReVSeg introduces a reinforcement learning approach to video object segmentation, using explicit reasoning steps with pretrained VLMs to improve performance and interpretability.",
        "tldr_zh": "ReVSeg 提出了一种用于视频对象分割的强化学习方法，通过使用预训练的 VLM 进行显式推理步骤来提高性能和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols",
        "summary": "Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/",
        "url": "http://arxiv.org/abs/2512.02787v1",
        "published_date": "2025-12-02T14:02:42+00:00",
        "updated_date": "2025-12-02T14:02:42+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xianchao Zeng",
            "Xinyu Zhou",
            "Youcheng Li",
            "Jiayou Shi",
            "Tianle Li",
            "Liangming Chen",
            "Lei Ren",
            "Yong-Lu Li"
        ],
        "tldr": "The paper introduces ViFailback, a framework and dataset for diagnosing and correcting robotic manipulation failures using visual symbols and VLMs, demonstrating improved failure recovery in real-world robotic experiments.",
        "tldr_zh": "该论文介绍了 ViFailback，一个用于诊断和纠正机器人操作失败的框架和数据集，它使用视觉符号和 VLM，并在现实世界的机器人实验中展示了改进的失败恢复能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm",
        "summary": "Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup.",
        "url": "http://arxiv.org/abs/2512.02700v1",
        "published_date": "2025-12-02T12:30:05+00:00",
        "updated_date": "2025-12-02T12:30:05+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhenkai Wu",
            "Xiaowen Ma",
            "Zhenliang Ni",
            "Dengming Zhang",
            "Han Shu",
            "Xin Jiang",
            "Xinghao Chen"
        ],
        "tldr": "VLM-Pruner introduces a training-free token pruning method for VLMs that balances redundancy and spatial sparsity through centrifugal pruning and a buffering criterion, achieving significant pruning rates and speedups.",
        "tldr_zh": "VLM-Pruner 提出了一种针对 VLM 的免训练 token 剪枝方法，通过离心剪枝和缓冲准则来平衡冗余和空间稀疏性，从而实现显著的剪枝率和加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation",
        "summary": "Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.",
        "url": "http://arxiv.org/abs/2512.02660v1",
        "published_date": "2025-12-02T11:29:54+00:00",
        "updated_date": "2025-12-02T11:29:54+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Agathoklis Georgiou"
        ],
        "tldr": "This paper introduces Snappy, a hybrid architecture that combines the strengths of VLMs (ColPali) and OCR systems for spatially-grounded document retrieval, enabling more precise context retrieval for RAG applications without additional training.",
        "tldr_zh": "该论文介绍了一种名为Snappy的混合架构，它结合了视觉语言模型（ColPali）和OCR系统的优势，用于空间定位的文档检索，从而为RAG应用提供更精确的上下文检索，且无需额外训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature",
        "summary": "There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.",
        "url": "http://arxiv.org/abs/2512.02566v1",
        "published_date": "2025-12-02T09:37:51+00:00",
        "updated_date": "2025-12-02T09:37:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kun Yuan",
            "Min Woo Sun",
            "Zhen Chen",
            "Alejandro Lozano",
            "Xiangteng He",
            "Shi Li",
            "Nassir Navab",
            "Xiaoxiao Sun",
            "Nicolas Padoy",
            "Serena Yeung-Levy"
        ],
        "tldr": "The paper introduces Panel2Patch, a novel data pipeline for biomedical vision-language pretraining that leverages hierarchical structure in scientific figures and text to create multi-granular supervision, resulting in improved performance with less pretraining data.",
        "tldr_zh": "本文介绍了一种名为 Panel2Patch 的新型数据管道，用于生物医学视觉语言预训练，该管道利用科学图表和文本中的分层结构来创建多粒度监督，从而在更少预训练数据的情况下提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens",
        "summary": "Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.",
        "url": "http://arxiv.org/abs/2512.02536v1",
        "published_date": "2025-12-02T09:02:20+00:00",
        "updated_date": "2025-12-02T09:02:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Yang",
            "Dacheng Yin",
            "Xiaoxuan He",
            "Yong Li",
            "Fengyun Rao",
            "Jing Lyu",
            "Wei Zhai",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "tldr": "This paper introduces Noisy Query Tokens and a VAE branch to improve the generalization and continual learning capabilities of Vision-Language Models when bridging them with Diffusion Models, addressing the task generalization collapse issue.",
        "tldr_zh": "该论文介绍了噪声查询令牌和VAE分支，以提高视觉语言模型在与扩散模型桥接时的泛化能力和持续学习能力，解决了任务泛化崩溃问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts",
        "summary": "The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.",
        "url": "http://arxiv.org/abs/2512.02517v1",
        "published_date": "2025-12-02T08:17:16+00:00",
        "updated_date": "2025-12-02T08:17:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Liu",
            "Ronghao Fu",
            "Lang Sun",
            "Haoran Liu",
            "Xiao Yang",
            "Weipeng Zhang",
            "Xu Na",
            "Zhuoran Duan",
            "Bo Yang"
        ],
        "tldr": "SkyMoE, a Mixture-of-Experts VLM, is introduced for enhanced geospatial interpretation, using task-aware routing and context-disentangled augmentation to achieve state-of-the-art performance across various remote sensing tasks.",
        "tldr_zh": "SkyMoE是一种混合专家（MoE）VLM，用于增强地理空间解释，它采用任务感知的路由和上下文解耦的增强方法，在各种遥感任务中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding",
        "summary": "Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.",
        "url": "http://arxiv.org/abs/2512.02505v1",
        "published_date": "2025-12-02T07:59:46+00:00",
        "updated_date": "2025-12-02T07:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Liu",
            "Ronghao Fu",
            "Haoran Liu",
            "Lang Sun",
            "Bo Yang"
        ],
        "tldr": "GeoDiT, a diffusion-based vision-language model, is introduced to address limitations of autoregressive models in geospatial understanding by reframing generation as a parallel refinement process, achieving state-of-the-art results in structured output tasks.",
        "tldr_zh": "GeoDiT是一个基于扩散的视觉语言模型，旨在解决自回归模型在地理空间理解中的局限性，通过将生成过程重新定义为并行细化过程，在结构化输出任务中取得了最先进的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding",
        "summary": "Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.",
        "url": "http://arxiv.org/abs/2512.02487v1",
        "published_date": "2025-12-02T07:22:36+00:00",
        "updated_date": "2025-12-02T07:22:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yerim Jeon",
            "Miso Lee",
            "WonJun Moon",
            "Jae-Pil Heo"
        ],
        "tldr": "The paper introduces 3D-SLIM, a novel masking strategy for LLMs that improves spatial reasoning in 3D scene-language understanding by addressing the limitations of causal attention masks. It uses geometry-adaptive and instruction-aware masks without architectural changes or added parameters, leading to significant performance gains.",
        "tldr_zh": "本文介绍了一种名为3D-SLIM的新型掩码策略，通过解决因果注意力掩码的局限性，从而提升LLM在3D场景-语言理解中的空间推理能力。它采用了几何自适应和指令感知的掩码，无需架构修改或添加参数，并显著提高了性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making",
        "summary": "Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.",
        "url": "http://arxiv.org/abs/2512.02485v1",
        "published_date": "2025-12-02T07:20:21+00:00",
        "updated_date": "2025-12-02T07:20:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qianhan Feng",
            "Zhongzhen Huang",
            "Yakun Zhu",
            "Xiaofan Zhang",
            "Qi Dou"
        ],
        "tldr": "The paper introduces UCAgents, a hierarchical multi-agent framework that enforces unidirectional convergence through structured evidence auditing to improve the accuracy and efficiency of visual-language models in medical diagnosis by anchoring reasoning to visual evidence and suppressing textual noise.",
        "tldr_zh": "本文介绍了UCAgents，一种分层多智能体框架，通过结构化的证据审计来强制单向收敛，从而通过将推理锚定到视觉证据并抑制文本噪声，来提高视觉语言模型在医学诊断中的准确性和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration",
        "summary": "Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.",
        "url": "http://arxiv.org/abs/2512.02458v1",
        "published_date": "2025-12-02T06:35:30+00:00",
        "updated_date": "2025-12-02T06:35:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongyi Cai",
            "Yi Du",
            "Chen Wang",
            "Yu Kong"
        ],
        "tldr": "This paper introduces SEER-Bench for sequential embodied tasks and proposes 3DSPMR, a novel approach leveraging geometric information to enhance MLLMs for reasoning and exploration in such tasks, achieving improved performance on EQA and EMN.",
        "tldr_zh": "本文介绍了用于序列具身任务的SEER-Bench，并提出了3DSPMR，一种利用几何信息来增强MLLM在序列具身任务中进行推理和探索的新方法，在EQA和EMN上取得了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "See, Think, Learn: A Self-Taught Multimodal Reasoner",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.",
        "url": "http://arxiv.org/abs/2512.02456v1",
        "published_date": "2025-12-02T06:30:10+00:00",
        "updated_date": "2025-12-02T06:30:10+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Sourabh Sharma",
            "Sonam Gupta",
            "Sadbhawna"
        ],
        "tldr": "The paper introduces See-Think-Learn (STL), a self-training framework that enhances multimodal reasoning in Vision-Language Models (VLMs) by using structured rationales generated by the model itself, including negative rationales for improved discriminative learning.",
        "tldr_zh": "该论文介绍了一种名为See-Think-Learn (STL)的自训练框架，通过使用模型自身生成的结构化理由（包括负面理由）来增强视觉语言模型（VLMs）中的多模态推理能力，从而提升判别学习。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources",
        "summary": "In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .",
        "url": "http://arxiv.org/abs/2512.02438v1",
        "published_date": "2025-12-02T05:53:51+00:00",
        "updated_date": "2025-12-02T05:53:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Phuc Pham",
            "Nhu Pham",
            "Ngoc Quoc Ly"
        ],
        "tldr": "This paper introduces a momentum self-distillation approach for training medical Vision-Language Models (VLMs) with limited computing resources, achieving competitive performance in zero-shot classification and significant improvements in few-shot adaptation and retrieval tasks.",
        "tldr_zh": "本文介绍了一种动量自蒸馏方法，用于在有限计算资源下训练医学视觉-语言模型（VLM），在零样本分类中实现了有竞争力的性能，并在少样本适应和检索任务中取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalizing Vision-Language Models with Dedicated Prompt Guidance",
        "summary": "Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.",
        "url": "http://arxiv.org/abs/2512.02421v1",
        "published_date": "2025-12-02T05:06:17+00:00",
        "updated_date": "2025-12-02T05:06:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyao Li",
            "Yinjie Min",
            "Hongbo Chen",
            "Zhekai Du",
            "Fengling Li",
            "Jingjing Li"
        ],
        "tldr": "The paper introduces GuiDG, a two-step framework that improves the domain generalization ability of VLMs by training domain-expert models with prompt tuning and integrating them using a cross-modal attention module during vision encoder fine-tuning.",
        "tldr_zh": "该论文介绍了GuiDG，一个两步框架，通过训练具有prompt tuning的领域专家模型，并使用跨模态注意力模块在视觉编码器微调期间集成它们，从而提高VLMs的领域泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate",
        "summary": "Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.",
        "url": "http://arxiv.org/abs/2512.02405v1",
        "published_date": "2025-12-02T04:31:52+00:00",
        "updated_date": "2025-12-02T04:31:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Anoop Cherian",
            "River Doyle",
            "Eyal Ben-Dov",
            "Suhas Lohit",
            "Kuan-Chuan Peng"
        ],
        "tldr": "The paper introduces WISE, a novel multi-agent debate framework leveraging LLMs with heterogeneous multimodal capabilities for solving vision-and-language reasoning problems. It demonstrates improved accuracy over existing methods on several multimodal datasets.",
        "tldr_zh": "该论文介绍了WISE，一种新的多智能体辩论框架，利用具有异构多模态能力的大型语言模型来解决视觉和语言推理问题。它在多个多模态数据集上展示了比现有方法更高的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
        "summary": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
        "url": "http://arxiv.org/abs/2512.02395v1",
        "published_date": "2025-12-02T04:12:57+00:00",
        "updated_date": "2025-12-02T04:12:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Zhang",
            "Liang Hu",
            "Haofeng Sun",
            "Peiyu Wang",
            "Yichen Wei",
            "Shukang Yin",
            "Jiangbo Pei",
            "Wei Shen",
            "Peng Xia",
            "Yi Peng",
            "Tianyidan Xie",
            "Eric Li",
            "Yang Liu",
            "Xuchen Song",
            "Yahui Zhou"
        ],
        "tldr": "Skywork-R1V4, a 30B parameter multimodal agent, achieves state-of-the-art results in multimodal search and perception by unifying planning, image manipulation, deep multimodal search, and interleaved reasoning through supervised fine-tuning, eliminating the need for reinforcement learning.",
        "tldr_zh": "Skywork-R1V4是一个30B参数的多模态智能体，通过有监督微调统一了规划、图像操作、深度多模态搜索和交错推理，从而在多模态搜索和感知方面实现了最先进的结果，无需强化学习。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VACoT: Rethinking Visual Data Augmentation with VLMs",
        "summary": "While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.",
        "url": "http://arxiv.org/abs/2512.02361v1",
        "published_date": "2025-12-02T03:11:32+00:00",
        "updated_date": "2025-12-02T03:11:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhengzhuo Xu",
            "Chong Sun",
            "SiNan Du",
            "Chen Li",
            "Jing Lyu",
            "Chun Yuan"
        ],
        "tldr": "The paper introduces VACoT, a framework that dynamically applies visual augmentations during VLM inference to improve robustness, particularly in challenging and adversarial OCR scenarios, using reinforcement learning for efficient augmentation selection.",
        "tldr_zh": "该论文提出了VACoT，一个在VLM推理过程中动态应用视觉增强的框架，旨在提高鲁棒性，尤其是在具有挑战性和对抗性的OCR场景中，并使用强化学习来高效选择增强方式。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
        "summary": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \\href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.",
        "url": "http://arxiv.org/abs/2512.02351v1",
        "published_date": "2025-12-02T02:47:29+00:00",
        "updated_date": "2025-12-02T02:47:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shwai He",
            "Chaorui Deng",
            "Ang Li",
            "Shen Yan"
        ],
        "tldr": "This paper analyzes the compressibility of unified multimodal models, finding that understanding components are more compressible than generation components. They propose a Mixture-of-Experts adaptation to improve the efficiency of generation while maintaining performance.",
        "tldr_zh": "本文分析了统一多模态模型的可压缩性，发现理解组件比生成组件更易压缩。他们提出了一种混合专家适配方法，以提高生成效率，同时保持性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
        "url": "http://arxiv.org/abs/2512.02231v1",
        "published_date": "2025-12-01T21:57:26+00:00",
        "updated_date": "2025-12-01T21:57:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Le Thien Phuc Nguyen",
            "Zhuoran Yu",
            "Samuel Low Yu Hang",
            "Subin An",
            "Jeongik Lee",
            "Yohan Ban",
            "SeungEun Chung",
            "Thanh-Huy Nguyen",
            "JuWan Maeng",
            "Soochahn Lee",
            "Yong Jae Lee"
        ],
        "tldr": "The paper introduces AV-SpeakerBench, a new benchmark for evaluating multimodal large language models' ability to understand speaker-centric audiovisual speech, and benchmarks several models demonstrating performance gaps.",
        "tldr_zh": "该论文介绍了AV-SpeakerBench，这是一个新的基准，用于评估多模态大型语言模型理解以说话者为中心的视听语音的能力，并对多个模型进行了基准测试，展示了性能差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges",
        "summary": "Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.",
        "url": "http://arxiv.org/abs/2512.02161v1",
        "published_date": "2025-12-01T19:46:03+00:00",
        "updated_date": "2025-12-01T19:46:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kevin David Hayes",
            "Micah Goldblum",
            "Vikash Sehwag",
            "Gowthami Somepalli",
            "Ashwinee Panda",
            "Tom Goldstein"
        ],
        "tldr": "The paper introduces a structured methodology (FineGRAIN) and dataset for evaluating failure modes of text-to-image models using vision language models as judges, revealing systematic errors in attribute fidelity and object representation and highlighting the need for better metrics.",
        "tldr_zh": "该论文介绍了一种结构化的方法（FineGRAIN）和数据集，用于评估文本到图像模型的失败模式，使用视觉语言模型作为判断者，揭示了属性保真度和对象表示方面的系统性错误，并强调了对更好指标的需求。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "summary": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
        "url": "http://arxiv.org/abs/2512.02014v1",
        "published_date": "2025-12-01T18:59:51+00:00",
        "updated_date": "2025-12-01T18:59:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiheng Liu",
            "Weiming Ren",
            "Haozhe Liu",
            "Zijian Zhou",
            "Shoufa Chen",
            "Haonan Qiu",
            "Xiaoke Huang",
            "Zhaochong An",
            "Fanny Yang",
            "Aditya Patel",
            "Viktar Atliha",
            "Tony Ng",
            "Xiao Han",
            "Chuyan Zhu",
            "Chenyang Zhang",
            "Ding Liu",
            "Juan-Manuel Perez-Rua",
            "Sen He",
            "Jürgen Schmidhuber",
            "Wenhu Chen",
            "Ping Luo",
            "Wei Liu",
            "Tao Xiang",
            "Jonas Schult",
            "Yuren Cong"
        ],
        "tldr": "TUNA is a native unified multimodal model (UMM) that uses a cascaded VAE and representation encoder to create a unified visual representation space, achieving state-of-the-art results in both understanding and generation tasks.",
        "tldr_zh": "TUNA 是一种原生统一多模态模型 (UMM)，它使用级联的 VAE 和表示编码器来创建统一的视觉表示空间，在理解和生成任务中均取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
        "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
        "url": "http://arxiv.org/abs/2512.01988v1",
        "published_date": "2025-12-01T18:45:30+00:00",
        "updated_date": "2025-12-01T18:45:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Tang",
            "Yanpeng Sun",
            "Shan Zhang",
            "Xiaofan Li",
            "Piotr Koniusz",
            "Wei Li",
            "Na Zhao",
            "Zechao Li"
        ],
        "tldr": "The paper introduces Artemis, a perception-policy learning framework that uses structured proposal-based reasoning with (label, bounding-box) pairs to improve visual reasoning and general perception policies, outperforming language-based reasoning methods.",
        "tldr_zh": "该论文介绍了Artemis，一个感知策略学习框架，它使用基于结构化提议的推理，利用(标签，边界框)对来改善视觉推理和通用感知策略，优于基于语言的推理方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback",
        "summary": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.",
        "url": "http://arxiv.org/abs/2512.01979v1",
        "published_date": "2025-12-01T18:37:19+00:00",
        "updated_date": "2025-12-01T18:37:19+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Aiden Yiliu Li",
            "Bizhi Yu",
            "Daoan Lei",
            "Tianhe Ren",
            "Shilong Liu"
        ],
        "tldr": "The paper introduces Chain-of-Ground (CoG), a training-free, multi-step grounding framework that improves GUI grounding accuracy by iteratively refining its hypotheses using multimodal large language models, demonstrating significant improvements on existing and novel datasets.",
        "tldr_zh": "该论文介绍了一种名为Chain-of-Ground (CoG) 的免训练多步 grounding 框架，通过使用多模态大型语言模型迭代地优化其假设，从而提高 GUI grounding 的准确性，并在现有和新的数据集上展示了显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities",
        "summary": "While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\\% and 91.07\\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.",
        "url": "http://arxiv.org/abs/2512.02973v1",
        "published_date": "2025-12-02T17:51:02+00:00",
        "updated_date": "2025-12-02T17:51:02+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.CR"
        ],
        "authors": [
            "Yuan Xiong",
            "Ziqi Miao",
            "Lijun Li",
            "Chen Qian",
            "Jie Li",
            "Jing Shao"
        ],
        "tldr": "The paper introduces Contextual Image Attack (CIA), a novel method that leverages visual context to jailbreak MLLMs by subtly embedding harmful queries within images, achieving high attack success rates against advanced models like GPT-4o and Qwen2.5-VL-72B.",
        "tldr_zh": "该论文介绍了一种名为上下文图像攻击 (CIA) 的新方法，该方法利用视觉上下文通过在图像中巧妙地嵌入有害查询来破解 MLLM，从而针对 GPT-4o 和 Qwen2.5-VL-72B 等高级模型实现了高攻击成功率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reasoning-Aware Multimodal Fusion for Hateful Video Detection",
        "summary": "Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.",
        "url": "http://arxiv.org/abs/2512.02743v1",
        "published_date": "2025-12-02T13:24:17+00:00",
        "updated_date": "2025-12-02T13:24:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shuonan Yang",
            "Tailin Chen",
            "Jiangbei Yue",
            "Guangliang Cheng",
            "Jianbo Jiao",
            "Zeyu Fu"
        ],
        "tldr": "The paper introduces a Reasoning-Aware Multimodal Fusion (RAMF) framework for hateful video detection, using Local-Global Context Fusion, Semantic Cross Attention, and adversarial reasoning to improve performance on real-world datasets.",
        "tldr_zh": "该论文介绍了一个用于仇恨视频检测的推理感知多模态融合(RAMF)框架，它使用局部-全局上下文融合、语义交叉注意和对抗推理来提高在真实世界数据集上的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization",
        "summary": "Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.",
        "url": "http://arxiv.org/abs/2512.02697v1",
        "published_date": "2025-12-02T12:28:22+00:00",
        "updated_date": "2025-12-02T12:28:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixuan Song",
            "Jing Zhang",
            "Di Wang",
            "Zidie Zhou",
            "Wenbin Liu",
            "Haonan Guo",
            "En Wang",
            "Bo Du"
        ],
        "tldr": "The paper introduces GeoBridge, a multi-view foundation model for geo-localization that uses textual descriptions as semantic anchors to bridge different image views (drone, satellite, street). They also present a new large-scale dataset, GeoLoc, for training and evaluation.",
        "tldr_zh": "该论文介绍了GeoBridge，一种用于地理定位的多视图基础模型，它使用文本描述作为语义锚点来桥接不同的图像视图（无人机、卫星、街道）。他们还提出了一个新的大规模数据集GeoLoc，用于训练和评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective",
        "summary": "Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.",
        "url": "http://arxiv.org/abs/2512.02340v1",
        "published_date": "2025-12-02T02:21:29+00:00",
        "updated_date": "2025-12-02T02:21:29+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Qiyao Xue",
            "Weichen Liu",
            "Shiqi Wang",
            "Haoming Wang",
            "Yuyang Wu",
            "Wei Gao"
        ],
        "tldr": "The paper introduces ReMindView-Bench, a novel benchmark for evaluating multi-view spatial reasoning in VLMs, and identifies significant performance gaps in cross-view alignment and perspective-taking through detailed analysis of reasoning paths and latent states.",
        "tldr_zh": "该论文介绍了一个新的基准测试ReMindView-Bench，用于评估视觉语言模型中的多视角空间推理能力。通过对推理路径和潜在状态的详细分析，揭示了模型在跨视角对齐和视角转换方面存在的显著性能差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]