[
    {
        "title": "XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis",
        "summary": "Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating\nearly and accurate diagnosis. While deep learning models have shown promise in\nDR detection, their black-box nature often hinders clinical adoption due to a\nlack of transparency and interpretability. To address this, we propose XDR-LVLM\n(eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that\nleverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis\ncoupled with natural language-based explanations. XDR-LVLM integrates a\nspecialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt\nEngineering and Multi-stage Fine-tuning to deeply understand pathological\nfeatures within fundus images and generate comprehensive diagnostic reports.\nThese reports explicitly include DR severity grading, identification of key\npathological concepts (e.g., hemorrhages, exudates, microaneurysms), and\ndetailed explanations linking observed features to the diagnosis. Extensive\nexperiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM\nachieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and\nan F1 Score of 79.92% for disease diagnosis, and superior results for concept\ndetection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the\nhigh fluency, accuracy, and clinical utility of the generated explanations,\nshowcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and\nclinical needs by providing robust and interpretable insights.",
        "url": "http://arxiv.org/abs/2508.15168v1",
        "published_date": "2025-08-21T02:14:46+00:00",
        "updated_date": "2025-08-21T02:14:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masato Ito",
            "Kaito Tanaka",
            "Keisuke Matsuda",
            "Aya Nakayama"
        ],
        "tldr": "The paper introduces XDR-LVLM, a novel Vision-Language Large Model framework for diabetic retinopathy diagnosis, featuring explainable diagnostic reports and state-of-the-art performance on the DDR dataset.",
        "tldr_zh": "该论文介绍了 XDR-LVLM，一种用于糖尿病视网膜病变诊断的新型视觉-语言大型模型框架，具有可解释的诊断报告，并在 DDR 数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding",
        "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
        "url": "http://arxiv.org/abs/2508.15717v1",
        "published_date": "2025-08-21T16:56:29+00:00",
        "updated_date": "2025-08-21T16:56:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yanlai Yang",
            "Zhuokai Zhao",
            "Satya Narayan Shukla",
            "Aashu Singh",
            "Shlok Kumar Mishra",
            "Lizhu Zhang",
            "Mengye Ren"
        ],
        "tldr": "The paper introduces StreamMem, a query-agnostic KV cache compression method for efficient long video understanding in memory-constrained scenarios using MLLMs, showing SOTA performance on several benchmarks.",
        "tldr_zh": "该论文介绍了StreamMem，一种与查询无关的KV缓存压缩方法，用于在内存受限的场景下，利用多模态大语言模型高效地进行长视频理解。实验表明，该方法在多个基准测试中表现出最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions",
        "summary": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nimpressive capability in visual tasks, but their fine-tuning often suffers from\nbias in class-imbalanced scene. Recent works have introduced large language\nmodels (LLMs) to enhance VLM fine-tuning with supplementing semantic\ninformation. However, they often overlook inherent class imbalance in VLMs'\npre-training, which may lead to bias accumulation in downstream tasks. To\naddress this problem, this paper proposes a Multi-dimensional Dynamic Prompt\nRouting (MDPR) framework. MDPR constructs a comprehensive knowledge base for\nclasses, spanning five visual-semantic dimensions. During fine-tuning, the\ndynamic routing mechanism aligns global visual classes, retrieves optimal\nprompts, and balances fine-grained semantics, yielding stable predictions\nthrough logits fusion. Extensive experiments on long-tailed benchmarks,\nincluding CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achieves\ncomparable results with current SOTA methods. Ablation studies further confirm\nthe effectiveness of our semantic library for tail classes, and show that our\ndynamic routing incurs minimal computational overhead, making MDPR a flexible\nand efficient enhancement for VLM fine-tuning under data imbalance.",
        "url": "http://arxiv.org/abs/2508.15688v1",
        "published_date": "2025-08-21T16:12:06+00:00",
        "updated_date": "2025-08-21T16:12:06+00:00",
        "categories": [
            "cs.CV",
            "I.4.10"
        ],
        "authors": [
            "Yongju Jia",
            "Jiarui Ma",
            "Xiangxian Li",
            "Baiqiao Zhang",
            "Xianhui Cao",
            "Juan Liu",
            "Yulong Bian"
        ],
        "tldr": "The paper introduces a Multi-dimensional Dynamic Prompt Routing (MDPR) framework to address the class imbalance problem in VLM fine-tuning under long-tailed distributions, using LLMs to supplement semantic information and dynamic prompt routing for balanced fine-grained semantics.",
        "tldr_zh": "该论文介绍了一种多维动态提示路由（MDPR）框架，旨在解决长尾分布下VLM微调中的类别不平衡问题，利用LLM补充语义信息，并采用动态提示路由来实现细粒度语义的平衡。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding",
        "summary": "Understanding videos requires more than answering open ended questions, it\ndemands the ability to pinpoint when events occur and how entities interact\nacross time. While recent Video LLMs have achieved remarkable progress in\nholistic reasoning, they remain coarse in temporal perception: timestamps are\nencoded only implicitly, frame level features are weak in capturing continuity,\nand language vision alignment often drifts from the entities of interest. In\nthis paper, we present Grounded VideoDiT, a Video LLM designed to overcome\nthese limitations by introducing three key innovations. First, a Diffusion\nTemporal Latent (DTL) encoder enhances boundary sensitivity and maintains\ntemporal consistency. Second, object grounded representations explicitly bind\nquery entities to localized visual evidence, strengthening alignment. Third, a\nmixed token scheme with discrete temporal tokens provides explicit timestamp\nmodeling, enabling fine grained temporal reasoning. Together, these designs\nequip Grounded VideoDiT with robust grounding capabilities, as validated by\nstate of the art results on Charades STA, NExT GQA, and multiple VideoQA\nbenchmarks.",
        "url": "http://arxiv.org/abs/2508.15641v1",
        "published_date": "2025-08-21T15:12:14+00:00",
        "updated_date": "2025-08-21T15:12:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pengcheng Fang",
            "Yuxia Chen",
            "Rui Guo"
        ],
        "tldr": "The paper introduces Grounded VideoDiT, a Video LLM that enhances temporal perception and language-vision alignment by using a Diffusion Temporal Latent encoder, object-grounded representations, and a mixed token scheme for explicit timestamp modeling, achieving SOTA results on several benchmarks.",
        "tldr_zh": "该论文介绍了一种名为Grounded VideoDiT的视频语言模型，它通过使用扩散时间潜在编码器、对象定位表示和混合令牌方案来增强时间感知和语言-视觉对齐，从而在多个基准测试中实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Empirical Study on How Video-LLMs Answer Video Questions",
        "summary": "Taking advantage of large-scale data and pretrained language models, Video\nLarge Language Models (Video-LLMs) have shown strong capabilities in answering\nvideo questions. However, most existing efforts focus on improving performance,\nwith limited attention to understanding their internal mechanisms. This paper\naims to bridge this gap through a systematic empirical study. To interpret\nexisting VideoLLMs, we adopt attention knockouts as our primary analytical tool\nand design three variants: Video Temporal Knockout, Video Spatial Knockout, and\nLanguage-to-Video Knockout. Then, we apply these three knockouts on different\nnumbers of layers (window of layers). By carefully controlling the window of\nlayers and types of knockouts, we provide two settings: a global setting and a\nfine-grained setting. Our study reveals three key findings: (1) Global setting\nindicates Video information extraction primarily occurs in early layers,\nforming a clear two-stage process -- lower layers focus on perceptual encoding,\nwhile higher layers handle abstract reasoning; (2) In the fine-grained setting,\ncertain intermediate layers exert an outsized impact on video question\nanswering, acting as critical outliers, whereas most other layers contribute\nminimally; (3) In both settings, we observe that spatial-temporal modeling\nrelies more on language-guided retrieval than on intra- and inter-frame\nself-attention among video tokens, despite the latter's high computational\ncost. Finally, we demonstrate that these insights can be leveraged to reduce\nattention computation in Video-LLMs. To our knowledge, this is the first work\nto systematically uncover how Video-LLMs internally process and understand\nvideo content, offering interpretability and efficiency perspectives for future\nresearch.",
        "url": "http://arxiv.org/abs/2508.15360v1",
        "published_date": "2025-08-21T08:42:35+00:00",
        "updated_date": "2025-08-21T08:42:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenhui Gou",
            "Ziyu Ma",
            "Zicheng Duan",
            "Haoyu He",
            "Feng Chen",
            "Akide Liu",
            "Bohan Zhuang",
            "Jianfei Cai",
            "Hamid Rezatofighi"
        ],
        "tldr": "This paper presents an empirical study on Video-LLMs, using attention knockouts to analyze how they process video content and answer questions, revealing the roles of different layers and the importance of language-guided retrieval.",
        "tldr_zh": "本文通过注意力剔除法对视频大语言模型(Video-LLM)进行了实证研究，分析了它们如何处理视频内容并回答问题，揭示了不同层的作用以及语言引导检索的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images",
        "summary": "Anomaly detection in computational pathology aims to identify rare and scarce\nanomalies where disease-related data are often limited or missing. Existing\nanomaly detection methods, primarily designed for industrial settings, face\nlimitations in pathology due to computational constraints, diverse tissue\nstructures, and lack of interpretability. To address these challenges, we\npropose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented\nVision-Language model for Anomaly detection in pathology images. Ano-NAViLa is\nbuilt on a pre-trained vision-language model with a lightweight trainable MLP.\nBy incorporating both normal and abnormal pathology knowledge, Ano-NAViLa\nenhances accuracy and robustness to variability in pathology images and\nprovides interpretability through image-text associations. Evaluated on two\nlymph node datasets from different organs, Ano-NAViLa achieves the\nstate-of-the-art performance in anomaly detection and localization,\noutperforming competing models.",
        "url": "http://arxiv.org/abs/2508.15256v1",
        "published_date": "2025-08-21T05:40:23+00:00",
        "updated_date": "2025-08-21T05:40:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinsol Song",
            "Jiamu Wang",
            "Anh Tien Nguyen",
            "Keunho Byeon",
            "Sangjeong Ahn",
            "Sung Hak Lee",
            "Jin Tae Kwak"
        ],
        "tldr": "The paper introduces Ano-NAViLa, a novel vision-language model augmented with pathology knowledge for anomaly detection in pathology images, achieving state-of-the-art results on lymph node datasets.",
        "tldr_zh": "该论文介绍了一种名为Ano-NAViLa的新型视觉-语言模型，该模型通过病理学知识增强，用于病理图像中的异常检测，并在淋巴结数据集上取得了最先进的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation",
        "summary": "Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables\nUnmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural\nlanguage instructions and visual cues. However, due to the extended\ntrajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN\nperformance is challenging and often requires human intervention or overly\ndetailed instructions. To harness the advantages of UAVs' high mobility, which\ncould provide multi-grained perspectives, while maintaining a manageable motion\nspace for learning, we introduce a novel task called Dual-Altitude UAV\nCollaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct\naltitudes: a high-altitude UAV responsible for broad environmental reasoning,\nand a low-altitude UAV tasked with precise navigation. To support the training\nand evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising\n13,838 collaborative high-low UAV demonstration trajectories, each paired with\ntarget-oriented language instructions. This dataset includes both unseen maps\nand an unseen object validation set to systematically evaluate the model's\ngeneralization capabilities across novel environments and unfamiliar targets.\nTo consolidate their complementary strengths, we propose a dual-UAV\ncollaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a\nmultimodal large language model (Pilot-LLM) for target reasoning, while the\nlow-altitude UAV employs a lightweight multi-stage policy for navigation and\ntarget grounding. The two UAVs work collaboratively and only exchange minimal\ncoordinate information to ensure efficiency.",
        "url": "http://arxiv.org/abs/2508.15232v1",
        "published_date": "2025-08-21T04:43:35+00:00",
        "updated_date": "2025-08-21T04:43:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruipu Wu",
            "Yige Zhang",
            "Jinyu Chen",
            "Linjiang Huang",
            "Shifeng Zhang",
            "Xu Zhou",
            "Liang Wang",
            "Si Liu"
        ],
        "tldr": "The paper introduces a novel Dual-Altitude UAV Collaborative VLN task and dataset (HaL-13k), along with a dual-UAV framework (AeroDuo) leveraging a multimodal LLM for high-altitude reasoning and a multi-stage policy for low-altitude navigation.",
        "tldr_zh": "该论文介绍了一种新颖的双高度无人机协同视觉语言导航（VLN）任务和数据集（HaL-13k），以及一个双无人机框架（AeroDuo），利用多模态LLM进行高空推理，并利用多阶段策略进行低空导航。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "See it. Say it. Sorted: Agentic System for Compositional Diagram Generation",
        "summary": "We study sketch-to-diagram generation: converting rough hand sketches into\nprecise, compositional diagrams. Diffusion models excel at photorealism but\nstruggle with the spatial precision, alignment, and symbolic structure required\nfor flowcharts. We introduce See it. Say it. Sorted., a training-free agentic\nsystem that couples a Vision-Language Model (VLM) with Large Language Models\n(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system\nruns an iterative loop in which a Critic VLM proposes a small set of\nqualitative, relational edits; multiple candidate LLMs synthesize SVG updates\nwith diverse strategies (conservative->aggressive, alternative, focused); and a\nJudge VLM selects the best candidate, ensuring stable improvement. This design\nprioritizes qualitative reasoning over brittle numerical estimates, preserves\nglobal constraints (e.g., alignment, connectivity), and naturally supports\nhuman-in-the-loop corrections. On 10 sketches derived from flowcharts in\npublished papers, our method more faithfully reconstructs layout and structure\nthan two frontier closed-source image generation LLMs (GPT-5 and\nGemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)\nwithout inserting unwanted text. Because outputs are programmatic SVGs, the\napproach is readily extensible to presentation tools (e.g., PowerPoint) via\nAPIs and can be specialized with improved prompts and task-specific tools. The\ncodebase is open-sourced at\nhttps://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.",
        "url": "http://arxiv.org/abs/2508.15222v1",
        "published_date": "2025-08-21T04:20:36+00:00",
        "updated_date": "2025-08-21T04:20:36+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Hantao Zhang",
            "Jingyang Liu",
            "Ed Li"
        ],
        "tldr": "The paper introduces \"See it. Say it. Sorted.\", a training-free agentic system that uses VLMs and LLMs to convert hand sketches into precise, editable SVG diagrams, outperforming existing image generation models in flowchart reconstruction.",
        "tldr_zh": "该论文介绍了“See it. Say it. Sorted”，一种无需训练的agentic系统，利用VLM和LLM将手绘草图转换为精确的、可编辑的SVG图表，在流程图重建方面优于现有的图像生成模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis",
        "summary": "Surgical site infection (SSI) is one of the most common and costly\nhealthcare-associated infections and and surgical wound care remains a\nsignificant clinical challenge in preventing SSIs and improving patient\noutcomes. While recent studies have explored the use of deep learning for\npreliminary surgical wound screening, progress has been hindered by concerns\nover data privacy and the high costs associated with expert annotation.\nCurrently, no publicly available dataset or benchmark encompasses various types\nof surgical wounds, resulting in the absence of an open-source Surgical-Wound\nscreening tool. To address this gap: (1) we present SurgWound, the first\nopen-source dataset featuring a diverse array of surgical wound types. It\ncontains 697 surgical wound images annotated by 3 professional surgeons with\neight fine-grained clinical attributes. (2) Based on SurgWound, we introduce\nthe first benchmark for surgical wound diagnosis, which includes visual\nquestion answering (VQA) and report generation tasks to comprehensively\nevaluate model performance. (3) Furthermore, we propose a three-stage learning\nframework, WoundQwen, for surgical wound diagnosis. In the first stage, we\nemploy five independent MLLMs to accurately predict specific surgical wound\ncharacteristics. In the second stage, these predictions serve as additional\nknowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess\ninfection risk and guide subsequent interventions. In the third stage, we train\na MLLM that integrates the diagnostic results from the previous two stages to\nproduce a comprehensive report. This three-stage framework can analyze detailed\nsurgical wound characteristics and provide subsequent instructions to patients\nbased on surgical images, paving the way for personalized wound care, timely\nintervention, and improved patient outcomes.",
        "url": "http://arxiv.org/abs/2508.15189v1",
        "published_date": "2025-08-21T03:00:17+00:00",
        "updated_date": "2025-08-21T03:00:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiahao Xu",
            "Changchang Yin",
            "Odysseas Chatzipanagiotou",
            "Diamantis Tsilimigras",
            "Kevin Clear",
            "Bingsheng Yao",
            "Dakuo Wang",
            "Timothy Pawlik",
            "Ping Zhang"
        ],
        "tldr": "The paper introduces SurgWound-Bench, a new open-source surgical wound dataset and benchmark with VQA and report generation tasks, along with a three-stage MLLM framework (WoundQwen) for surgical wound diagnosis.",
        "tldr_zh": "该论文介绍了SurgWound-Bench，这是一个新的开源手术伤口数据集和基准，包含视觉问答和报告生成任务，以及一个用于手术伤口诊断的三阶段MLLM框架（WoundQwen）。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Waver: Wave Your Way to Lifelike Video Generation",
        "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
        "url": "http://arxiv.org/abs/2508.15761v1",
        "published_date": "2025-08-21T17:56:10+00:00",
        "updated_date": "2025-08-21T17:56:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifu Zhang",
            "Hao Yang",
            "Yuqi Zhang",
            "Yifei Hu",
            "Fengda Zhu",
            "Chuang Lin",
            "Xiaofeng Mei",
            "Yi Jiang",
            "Zehuan Yuan",
            "Bingyue Peng"
        ],
        "tldr": "Waver is a high-performance foundation model for unified image and video generation, supporting T2V, I2V, and T2I with superior motion capture and temporal consistency, ranking among the top models on leaderboards.",
        "tldr_zh": "Waver是一个高性能的统一图像和视频生成的基础模型，支持T2V、I2V和T2I，具有卓越的运动捕捉和时间一致性，并在排行榜上名列前茅。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation",
        "summary": "The logistics and construction industries face persistent challenges in\nautomating pallet handling, especially in outdoor environments with variable\npayloads, inconsistencies in pallet quality and dimensions, and unstructured\nsurroundings. In this paper, we tackle automation of a critical step in pallet\ntransport: the pallet pick-up operation. Our work is motivated by labor\nshortages, safety concerns, and inefficiencies in manually locating and\nretrieving pallets under such conditions. We present Lang2Lift, a framework\nthat leverages foundation models for natural language-guided pallet detection\nand 6D pose estimation, enabling operators to specify targets through intuitive\ncommands such as \"pick up the steel beam pallet near the crane.\" The perception\npipeline integrates Florence-2 and SAM-2 for language-grounded segmentation\nwith FoundationPose for robust pose estimation in cluttered, multi-pallet\noutdoor scenes under variable lighting. The resulting poses feed into a motion\nplanning module for fully autonomous forklift operation. We validate Lang2Lift\non the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet\nsegmentation accuracy on a real-world test dataset. Timing and error analysis\ndemonstrate the system's robustness and confirm its feasibility for deployment\nin operational logistics and construction environments. Video demonstrations\nare available at https://eric-nguyen1402.github.io/lang2lift.github.io/",
        "url": "http://arxiv.org/abs/2508.15427v1",
        "published_date": "2025-08-21T10:28:39+00:00",
        "updated_date": "2025-08-21T10:28:39+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Huy Hoang Nguyen",
            "Johannes Huemer",
            "Markus Murschitz",
            "Tobias Glueck",
            "Minh Nhat Vu",
            "Andreas Kugi"
        ],
        "tldr": "The paper introduces Lang2Lift, a framework for language-guided pallet detection and pose estimation using VLMs, enabling autonomous forklift operation in outdoor environments. It integrates Florence-2, SAM-2, and FoundationPose, demonstrating robustness and feasibility on a real-world dataset.",
        "tldr_zh": "该论文介绍了Lang2Lift，一个使用视觉语言模型（VLM）进行语言引导的托盘检测和姿态估计的框架，从而实现户外环境中的自主叉车操作。它集成了Florence-2、SAM-2和FoundationPose，并在真实世界的数据集上展示了其鲁棒性和可行性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model",
        "summary": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use of cell-specific SAM models for\ndirect segmentation. These approaches enable effective segmentation across a\nrange of nuclei and cells. However, general vision foundation models often face\nchallenges with fine-grained semantic segmentation, such as identifying\nspecific nuclei subtypes or particular cells. Approach: In this paper, we\npropose the molecular-empowered All-in-SAM Model to advance computational\npathology by leveraging the capabilities of vision foundation models. This\nmodel incorporates a full-stack approach, focusing on: (1) annotation-engaging\nlay annotators through molecular-empowered learning to reduce the need for\ndetailed pixel-level annotations, (2) learning-adapting the SAM model to\nemphasize specific semantics, which utilizes its strong generalizability with\nSAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating\nMolecular-Oriented Corrective Learning (MOCL). Results: Experimental results\nfrom both in-house and public datasets show that the All-in-SAM model\nsignificantly improves cell classification performance, even when faced with\nvarying annotation quality. Conclusions: Our approach not only reduces the\nworkload for annotators but also extends the accessibility of precise\nbiomedical image analysis to resource-limited settings, thereby advancing\nmedical diagnostics and automating pathology image analysis.",
        "url": "http://arxiv.org/abs/2508.15751v1",
        "published_date": "2025-08-21T17:49:21+00:00",
        "updated_date": "2025-08-21T17:49:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xueyuan Li",
            "Can Cui",
            "Ruining Deng",
            "Yucheng Tang",
            "Quan Liu",
            "Tianyuan Yao",
            "Shunxing Bao",
            "Naweed Chowdhury",
            "Haichun Yang",
            "Yuankai Huo"
        ],
        "tldr": "The paper introduces a molecular-empowered All-in-SAM model for fine-grained multi-class nuclei segmentation, aiming to improve cell classification performance with reduced annotation effort and enhanced accessibility in resource-limited settings.",
        "tldr_zh": "本文介绍了一种分子赋能的 All-in-SAM 模型，用于细粒度的多类细胞核分割，旨在提高细胞分类性能，减少标注工作量，并增强资源有限环境下的可访问性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]