[
    {
        "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization",
        "summary": "We introduce ImageGem, a dataset for studying generative models that\nunderstand fine-grained individual preferences. We posit that a key challenge\nhindering the development of such a generative model is the lack of in-the-wild\nand fine-grained user preference annotations. Our dataset features real-world\ninteraction data from 57K users, who collectively have built 242K customized\nLoRAs, written 3M text prompts, and created 5M generated images. With user\npreference annotations from our dataset, we were able to train better\npreference alignment models. In addition, leveraging individual user\npreference, we investigated the performance of retrieval models and a\nvision-language model on personalized image retrieval and generative model\nrecommendation. Finally, we propose an end-to-end framework for editing\ncustomized diffusion models in a latent weight space to align with individual\nuser preferences. Our results demonstrate that the ImageGem dataset enables,\nfor the first time, a new paradigm for generative model personalization.",
        "url": "http://arxiv.org/abs/2510.18433v1",
        "published_date": "2025-10-21T09:08:01+00:00",
        "updated_date": "2025-10-21T09:08:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR"
        ],
        "authors": [
            "Yuanhe Guo",
            "Linxi Xie",
            "Zhuoran Chen",
            "Kangrui Yu",
            "Ryan Po",
            "Guandao Yang",
            "Gordon Wetztein",
            "Hongyi Wen"
        ],
        "tldr": "The paper introduces ImageGem, a new dataset of real-world user interactions with generative models, enabling the study and development of personalized generative models based on fine-grained user preferences.",
        "tldr_zh": "该论文介绍了 ImageGem，一个包含真实用户与生成模型交互的新数据集，它支持研究和开发基于细粒度用户偏好的个性化生成模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos.",
        "url": "http://arxiv.org/abs/2510.18876v1",
        "published_date": "2025-10-21T17:59:59+00:00",
        "updated_date": "2025-10-21T17:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haochen Wang",
            "Yuhao Wang",
            "Tao Zhang",
            "Yikang Zhou",
            "Yanwei Li",
            "Jiacong Wang",
            "Ye Tian",
            "Jiahao Meng",
            "Zilong Huang",
            "Guangcan Mai",
            "Anran Wang",
            "Yunhai Tong",
            "Zhuochen Wang",
            "Xiangtai Li",
            "Zhaoxiang Zhang"
        ],
        "tldr": "The paper introduces Grasp Any Region (GAR), a region-level MLLM that leverages RoI-aligned feature replay for precise perception with global context and improved compositional reasoning, achieving state-of-the-art performance on several benchmarks.",
        "tldr_zh": "本文介绍了一种名为Grasp Any Region (GAR) 的区域级多模态大语言模型，该模型利用RoI对齐的特征重放技术，以实现带有全局上下文的精确感知和改进的组合推理，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent",
        "summary": "Computer-using agents (CUAs) enable task completion through natural\ninteraction with operating systems and software interfaces. While script-based\nverifiers are widely adopted for evaluation, they suffer from limited\nscalability and inability to provide step-wise assessment. Reward models offer\npromising alternatives, but their effectiveness on CUA evaluation remains\nlargely underexplored. To address this gap, we present CUARewardBench,\ncomprising four key contributions: (1) First-ever Comprehensive CUA Reward\nBenchmark: We introduce the first benchmark for evaluating both outcome reward\nmodels (ORM) and process reward models (PRM) on CUA tasks, enabling systematic\nassessment across trajectory-level and step-level evaluation. (2) Diverse,\nPractical and Reliable Dataset: CUARewardBench encompasses trajectories from 10\nsoftware categories and 7 agent architectures with varying performance levels\n(25.9%-50.8% success rates). All trajectories are expertly annotated through\ncarefully designed protocols, with rigorous quality control to ensure\nreliability and practical applicability. (3) Comprehensive Analysis and\nInsights: Through extensive experiments across 7 vision-language models and 3\nprompt templates, we reveal critical limitations of current CUA RMs, including\ninsufficient visual reasoning capabilities, knowledge deficiencies, and the\nsuperiority of general VLMs over specialized CUA models for reward evaluation.\n(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our\ncomprehensive analysis, we propose UPE, a novel ensemble method that\nsignificantly enhances reward model reliability through strict unanimous voting\nand strategic prompt-template configurations. UPE achieves 89.8% precision and\n93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially\noutperforming single VLMs and traditional ensemble approaches.",
        "url": "http://arxiv.org/abs/2510.18596v1",
        "published_date": "2025-10-21T12:53:40+00:00",
        "updated_date": "2025-10-21T12:53:40+00:00",
        "categories": [
            "cs.SE",
            "cs.CV"
        ],
        "authors": [
            "Haojia Lin",
            "Xiaoyu Tan",
            "Yulei Qin",
            "Zihan Xu",
            "Yuchen Shi",
            "Zongyi Li",
            "Gang Li",
            "Shaofei Cai",
            "Siqi Cai",
            "Chaoyou Fu",
            "Ke Li",
            "Xing Sun"
        ],
        "tldr": "This paper introduces CUARewardBench, a benchmark for evaluating reward models for computer-using agents, and proposes a novel ensemble method (UPE) to improve reward model reliability, demonstrating significant performance gains.",
        "tldr_zh": "该论文介绍了CUARewardBench，这是一个用于评估计算机使用代理的奖励模型的基准。它还提出了一种新的集成方法（UPE），通过严格的一致投票和战略性提示模板配置来提高奖励模型的可靠性，并展示了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder",
        "summary": "Multimodal dataset distillation aims to synthesize a small set of image-text\npairs that enables efficient training of large-scale vision-language models.\nWhile dataset distillation has shown promise in unimodal tasks, extending it to\nmultimodal contrastive learning presents key challenges: learning cross-modal\nalignment and managing the high computational cost of large encoders. Prior\napproaches address scalability by freezing the text encoder and update only the\nimage encoder and text projection layer. However, we find this severely limits\nsemantic alignment and becomes a bottleneck for performance scaling. We propose\nCovMatch, a scalable dataset distillation framework that aligns the\ncross-covariance of real and synthetic features while regularizing feature\ndistributions within each modality. Unlike prior approaches, CovMatch enables\njoint optimization of both encoders, leading to stronger cross-modal alignment\nand improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms\nstate-of-the-art multimodal distillation methods and achieves up to 6.8%\nabsolute gains in retrieval accuracy using only 500 synthetic pairs.",
        "url": "http://arxiv.org/abs/2510.18583v1",
        "published_date": "2025-10-21T12:36:25+00:00",
        "updated_date": "2025-10-21T12:36:25+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yongmin Lee",
            "Hye Won Chung"
        ],
        "tldr": "The paper introduces CovMatch, a novel multimodal dataset distillation framework that enables joint optimization of image and text encoders, achieving superior cross-modal alignment and retrieval accuracy compared to existing methods.",
        "tldr_zh": "该论文介绍了一种名为CovMatch的新型多模态数据集蒸馏框架，它能够联合优化图像和文本编码器，与现有方法相比，实现了更好的跨模态对齐和检索精度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation",
        "summary": "Vehicle make and model recognition (VMMR) is an important task in intelligent\ntransportation systems, but existing approaches struggle to adapt to newly\nreleased models. Contrastive Language-Image Pretraining (CLIP) provides strong\nvisual-text alignment, yet its fixed pretrained weights limit performance\nwithout costly image-specific finetuning. We propose a pipeline that integrates\nvision language models (VLMs) with Retrieval-Augmented Generation (RAG) to\nsupport zero-shot recognition through text-based reasoning. A VLM converts\nvehicle images into descriptive attributes, which are compared against a\ndatabase of textual features. Relevant entries are retrieved and combined with\nthe description to form a prompt, and a language model (LM) infers the make and\nmodel. This design avoids large-scale retraining and enables rapid updates by\nadding textual descriptions of new vehicles. Experiments show that the proposed\nmethod improves recognition by nearly 20% over the CLIP baseline, demonstrating\nthe potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city\napplications.",
        "url": "http://arxiv.org/abs/2510.18502v1",
        "published_date": "2025-10-21T10:39:39+00:00",
        "updated_date": "2025-10-21T10:39:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Wei-Chia Chang",
            "Yan-Ann Chen"
        ],
        "tldr": "This paper introduces a zero-shot vehicle make and model recognition pipeline using VLM and RAG, which avoids finetuning and achieves better accuracy compared to CLIP baseline by leveraging textual descriptions for reasoning.",
        "tldr_zh": "本文提出了一种基于 VLM 和 RAG 的零样本车辆品牌和型号识别流水线，该方法避免了微调，并通过利用文本描述进行推理，实现了比 CLIP 基线更高的准确率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "The Impact of Image Resolution on Biomedical Multimodal Large Language Models",
        "summary": "Imaging technologies are fundamental to biomedical research and modern\nmedicine, requiring analysis of high-resolution images across various\nmodalities. While multimodal large language models (MLLMs) show promise for\nbiomedical image analysis, most are designed for low-resolution images from\ngeneral-purpose datasets, risking critical information loss. We investigate how\nimage resolution affects MLLM performance in biomedical applications and\ndemonstrate that: (1) native-resolution training and inference significantly\nimprove performance across multiple tasks, (2) misalignment between training\nand inference resolutions severely degrades performance, and (3)\nmixed-resolution training effectively mitigates misalignment and balances\ncomputational constraints with performance requirements. Based on these\nfindings, we recommend prioritizing native-resolution inference and\nmixed-resolution datasets to optimize biomedical MLLMs for transformative\nimpact in scientific research and clinical applications.",
        "url": "http://arxiv.org/abs/2510.18304v1",
        "published_date": "2025-10-21T05:19:43+00:00",
        "updated_date": "2025-10-21T05:19:43+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Liangyu Chen",
            "James Burgess",
            "Jeffrey J Nirschl",
            "Orr Zohar",
            "Serena Yeung-Levy"
        ],
        "tldr": "This paper investigates the impact of image resolution on biomedical MLLMs, showing that native-resolution training/inference improves performance and mixed-resolution training balances performance and computational cost, advocating for native-resolution inference and mixed-resolution datasets.",
        "tldr_zh": "本文研究了图像分辨率对生物医学多模态大语言模型的影响，表明原生分辨率的训练/推理可以提高性能，混合分辨率训练可以平衡性能和计算成本，并提倡使用原生分辨率推理和混合分辨率数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models",
        "summary": "Incentivizing the reasoning ability of Multimodal Large Language Models\n(MLLMs) is essential for medical applications to transparently analyze medical\nscans and provide reliable diagnosis. However, existing medical MLLMs rely\nsolely on internal knowledge during reasoning, leading to hallucinated\nreasoning and factual inaccuracies when encountering cases beyond their\ntraining scope. Although recent Agentic Retrieval-Augmented Generation (RAG)\nmethods elicit the medical model's proactive retrieval ability during\nreasoning, they are confined to unimodal LLMs, neglecting the crucial visual\ninformation during reasoning and retrieval. Consequently, we propose the first\nMultimodal Medical Reasoning-with-Retrieval framework, Med-RwR, which actively\nretrieves external knowledge by querying observed symptoms or domain-specific\nmedical concepts during reasoning. Specifically, we design a two-stage\nreinforcement learning strategy with tailored rewards that stimulate the model\nto leverage both visual diagnostic findings and textual clinical information\nfor effective retrieval. Building on this foundation, we further propose a\nConfidence-Driven Image Re-retrieval (CDIR) method for test-time scaling when\nlow prediction confidence is detected. Evaluation on various public medical\nbenchmarks demonstrates Med-RwR's significant improvements over baseline\nmodels, proving the effectiveness of enhancing reasoning capabilities with\nexternal knowledge integration. Furthermore, Med-RwR demonstrates remarkable\ngeneralizability to unfamiliar domains, evidenced by 8.8% performance gain on\nour proposed EchoCardiography Benchmark (ECBench), despite the scarcity of\nechocardiography data in the training corpus. Our data, model, and codes will\nbe made publicly available at https://github.com/xmed-lab/Med-RwR.",
        "url": "http://arxiv.org/abs/2510.18303v1",
        "published_date": "2025-10-21T05:18:18+00:00",
        "updated_date": "2025-10-21T05:18:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lehan Wang",
            "Yi Qin",
            "Honglong Yang",
            "Xiaomeng Li"
        ],
        "tldr": "The paper introduces Med-RwR, a novel multimodal medical reasoning framework that leverages retrieval-augmented generation with reinforcement learning to improve diagnostic accuracy and generalizability of MLLMs, especially in data-scarce scenarios.",
        "tldr_zh": "本文介绍了 Med-RwR，一种新颖的多模态医学推理框架，它利用检索增强生成和强化学习来提高 MLLM 的诊断准确性和泛化能力，尤其是在数据稀缺的情况下。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StreamingTOM: Streaming Token Compression for Efficient Video Understanding",
        "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
        "url": "http://arxiv.org/abs/2510.18269v1",
        "published_date": "2025-10-21T03:39:41+00:00",
        "updated_date": "2025-10-21T03:39:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xueyi Chen",
            "Keda Tao",
            "Kele Shao",
            "Huan Wang"
        ],
        "tldr": "The paper introduces StreamingTOM, a training-free framework for efficient streaming video understanding that addresses pre-LLM and post-LLM bottlenecks by compressing visual tokens and quantizing memory, achieving significant performance improvements.",
        "tldr_zh": "该论文介绍了StreamingTOM，一个无需训练的框架，用于高效的流式视频理解，通过压缩视觉tokens和量化内存来解决pre-LLM和post-LLM瓶颈，实现了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining",
        "summary": "Zero-shot 3D object classification is crucial for real-world applications\nlike autonomous driving, however it is often hindered by a significant domain\ngap between the synthetic data used for training and the sparse, noisy LiDAR\nscans encountered in the real-world. Current methods trained solely on\nsynthetic data fail to generalize to outdoor scenes, while those trained only\non real data lack the semantic diversity to recognize rare or unseen objects.\n  We introduce BlendCLIP, a multimodal pretraining framework that bridges this\nsynthetic-to-real gap by strategically combining the strengths of both domains.\nWe first propose a pipeline to generate a large-scale dataset of object-level\ntriplets -- consisting of a point cloud, image, and text description -- mined\ndirectly from real-world driving data and human annotated 3D boxes. Our core\ncontribution is a curriculum-based data mixing strategy that first grounds the\nmodel in the semantically rich synthetic CAD data before progressively adapting\nit to the specific characteristics of real-world scans.\n  Our experiments show that our approach is highly label-efficient: introducing\nas few as 1.5\\% real-world samples per batch into training boosts zero-shot\naccuracy on the nuScenes benchmark by 27\\%. Consequently, our final model\nachieves state-of-the-art performance on challenging outdoor datasets like\nnuScenes and TruckScenes, improving over the best prior method by 19.3\\% on\nnuScenes, while maintaining strong generalization on diverse synthetic\nbenchmarks. Our findings demonstrate that effective domain adaptation, not\nfull-scale real-world annotation, is the key to unlocking robust\nopen-vocabulary 3D perception. Our code and dataset will be released upon\nacceptance on https://github.com/kesu1/BlendCLIP.",
        "url": "http://arxiv.org/abs/2510.18244v1",
        "published_date": "2025-10-21T03:08:27+00:00",
        "updated_date": "2025-10-21T03:08:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ajinkya Khoche",
            "Gergő László Nagy",
            "Maciej Wozniak",
            "Thomas Gustafsson",
            "Patric Jensfelt"
        ],
        "tldr": "BlendCLIP addresses the domain gap in zero-shot 3D object classification by using a curriculum-based data mixing strategy with synthetic and real-world data, achieving state-of-the-art performance on outdoor datasets.",
        "tldr_zh": "BlendCLIP通过结合合成数据和真实世界数据，并采用基于课程的数据混合策略，解决了零样本3D对象分类中的领域差距，并在户外数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeepSeek-OCR: Contexts Optical Compression",
        "summary": "We present DeepSeek-OCR as an initial investigation into the feasibility of\ncompressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two\ncomponents: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically,\nDeepEncoder serves as the core engine, designed to maintain low activations\nunder high-resolution input while achieving high compression ratios to ensure\nan optimal and manageable number of vision tokens. Experiments show that when\nthe number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10x), the model can achieve decoding (OCR) precision of\n97%. Even at a compression ratio of 20x, the OCR accuracy still remains at\nabout 60%. This shows considerable promise for research areas such as\nhistorical long-context compression and memory forgetting mechanisms in LLMs.\nBeyond this, DeepSeek-OCR also demonstrates high practical value. On\nOmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision\ntokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while\nutilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can\ngenerate training data for LLMs/VLMs at a scale of 200k+ pages per day (a\nsingle A100-40G). Codes and model weights are publicly accessible at\nhttp://github.com/deepseek-ai/DeepSeek-OCR.",
        "url": "http://arxiv.org/abs/2510.18234v1",
        "published_date": "2025-10-21T02:41:44+00:00",
        "updated_date": "2025-10-21T02:41:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Wei",
            "Yaofeng Sun",
            "Yukun Li"
        ],
        "tldr": "DeepSeek-OCR uses a novel DeepEncoder to compress long text contexts into a small number of vision tokens for efficient OCR, achieving high accuracy and surpassing existing methods on benchmark datasets.",
        "tldr_zh": "DeepSeek-OCR 采用了一种新颖的 DeepEncoder 将长文本上下文压缩成少量视觉 tokens，从而实现高效的 OCR，在基准数据集上实现了高精度并超越了现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety",
        "summary": "Safety evaluation of multimodal foundation models often treats vision and\nlanguage inputs separately, missing risks from joint interpretation where\nbenign content becomes harmful in combination. Existing approaches also fail to\ndistinguish clearly unsafe content from borderline cases, leading to\nproblematic over-blocking or under-refusal of genuinely harmful content. We\npresent Vision Language Safety Understanding (VLSU), a comprehensive framework\nto systematically evaluate multimodal safety through fine-grained severity\nclassification and combinatorial analysis across 17 distinct safety patterns.\nUsing a multi-stage pipeline with real-world images and human annotation, we\nconstruct a large-scale benchmark of 8,187 samples spanning 15 harm categories.\nOur evaluation of eleven state-of-the-art models reveals systematic joint\nunderstanding failures: while models achieve 90%-plus accuracy on clear\nunimodal safety signals, performance degrades substantially to 20-55% when\njoint image-text reasoning is required to determine the safety label. Most\ncritically, 34% of errors in joint image-text safety classification occur\ndespite correct classification of the individual modalities, further\ndemonstrating absent compositional reasoning capabilities. Additionally, we\nfind that models struggle to balance refusing unsafe content while still\nresponding to borderline cases that deserve engagement. For example, we find\nthat instruction framing can reduce the over-blocking rate on borderline\ncontent from 62.4% to 10.4% in Gemini-1.5, but only at the cost of\nunder-refusing on unsafe content with refusal rate dropping from 90.8% to\n53.9%. Overall, our framework exposes weaknesses in joint image-text\nunderstanding and alignment gaps in current models, and provides a critical\ntest bed to enable the next milestones in research on robust vision-language\nsafety.",
        "url": "http://arxiv.org/abs/2510.18214v1",
        "published_date": "2025-10-21T01:30:31+00:00",
        "updated_date": "2025-10-21T01:30:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Shruti Palaskar",
            "Leon Gatys",
            "Mona Abdelrahman",
            "Mar Jacobo",
            "Larry Lindsey",
            "Rutika Moharir",
            "Gunnar Lund",
            "Yang Xu",
            "Navid Shiee",
            "Jeffrey Bigham",
            "Charles Maalouf",
            "Joseph Yitan Cheng"
        ],
        "tldr": "The paper introduces VLSU, a benchmark for evaluating multimodal AI safety, revealing significant failures in current models' joint image-text understanding, especially in compositional reasoning and balancing safety refusals with appropriate engagement.",
        "tldr_zh": "该论文介绍了VLSU，一个用于评估多模态人工智能安全性的基准，揭示了当前模型在联合图像-文本理解方面的重大失败，尤其是在组合推理以及平衡安全拒绝和适当参与方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology",
        "summary": "Most current medical vision language models struggle to jointly generate\ndiagnostic text and pixel-level segmentation masks in response to complex\nvisual questions. This represents a major limitation towards clinical\napplication, as assistive systems that fail to provide both modalities\nsimultaneously offer limited value to medical practitioners. To alleviate this\nlimitation, we first introduce RadDiagSeg-D, a dataset combining abnormality\ndetection, diagnosis, and multi-target segmentation into a unified and\nhierarchical task. RadDiagSeg-D covers multiple imaging modalities and is\nprecisely designed to support the development of models that produce\ndescriptive text and corresponding segmentation masks in tandem. Subsequently,\nwe leverage the dataset to propose a novel vision-language model, RadDiagSeg-M,\ncapable of joint abnormality detection, diagnosis, and flexible segmentation.\nRadDiagSeg-M provides highly informative and clinically useful outputs,\neffectively addressing the need to enrich contextual information for assistive\ndiagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong\nperformance across all components involved in the task of multi-target\ntext-and-mask generation, establishing a robust and competitive baseline.",
        "url": "http://arxiv.org/abs/2510.18188v1",
        "published_date": "2025-10-21T00:28:13+00:00",
        "updated_date": "2025-10-21T00:28:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68",
            "I.4.6"
        ],
        "authors": [
            "Chengrun Li",
            "Corentin Royer",
            "Haozhe Luo",
            "Bastian Wittmann",
            "Xia Li",
            "Ibrahim Hamamci",
            "Sezgin Er",
            "Anjany Sekuboyina",
            "Bjoern Menze"
        ],
        "tldr": "The paper introduces RadDiagSeg-D, a new dataset for joint diagnosis and multi-target segmentation in radiology, and RadDiagSeg-M, a vision-language model that leverages the dataset to generate diagnostic text and segmentation masks simultaneously.",
        "tldr_zh": "本文介绍了一个新的数据集RadDiagSeg-D，用于放射学中的联合诊断和多目标分割，以及一个视觉语言模型RadDiagSeg-M，该模型利用该数据集同时生成诊断文本和分割掩码。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Online In-Context Distillation for Low-Resource Vision Language Models",
        "summary": "As the field continues its push for ever more resources, this work turns the\nspotlight on a critical question: how can vision-language models (VLMs) be\nadapted to thrive in low-resource, budget-constrained settings? While large\nVLMs offer strong performance, they are impractical to deploy in such settings.\nSmall VLMs, on the other hand, are efficient but typically require costly\nfine-tuning to close the performance gap with larger models in the deployment\ndomain. Inspired by the in-context learning framework, we propose an online\nIn-Context Distillation (ICD) method, in which a small VLM collaborates with a\nstronger teacher model at inference time, distilling its knowledge via sparse\ndemonstrations to efficiently bridge the gap between them. Our method is built\non an in-depth analysis that identifies the scale and the choice of models for\nwhich vision-language ICL is currently feasible, and demonstrates the advantage\nof ICL over fine-tuning under constrained compute budgets. We enhance our\nmethod with a novel cross-modal demonstration selection strategy, teacher\ntest-time scaling to reduce noise, and student uncertainty conditioning to\ndynamically populate a demonstration pool and minimize teacher queries. Our ICD\nmethod significantly boosts the performance of small models (up to 33%) using\nscarce teacher annotations (as low as 4%), and competes with the teacher's\nzero-shot performance.",
        "url": "http://arxiv.org/abs/2510.18117v1",
        "published_date": "2025-10-20T21:35:17+00:00",
        "updated_date": "2025-10-20T21:35:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiqi Kang",
            "Rahaf Aljundi",
            "Vaggelis Dorovatas",
            "Karteek Alahari"
        ],
        "tldr": "The paper proposes an online In-Context Distillation (ICD) method to adapt vision-language models (VLMs) to low-resource settings by distilling knowledge from a stronger teacher model during inference, achieving significant performance boosts with limited teacher annotations.",
        "tldr_zh": "该论文提出了一种在线上下文蒸馏 (ICD) 方法，通过在推理过程中从更强大的教师模型中提取知识，使视觉语言模型 (VLM) 适应低资源环境，并在有限的教师注释下实现显着的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection",
        "summary": "Autonomous driving systems remain critically vulnerable to the long-tail of\nrare, out-of-distribution scenarios with semantic anomalies. While Vision\nLanguage Models (VLMs) offer promising reasoning capabilities, naive prompting\napproaches yield unreliable performance and depend on expensive proprietary\nmodels, limiting practical deployment. We introduce SAVANT (Semantic Analysis\nwith Vision-Augmented Anomaly deTection), a structured reasoning framework that\nachieves high accuracy and recall in detecting anomalous driving scenarios from\ninput images through layered scene analysis and a two-phase pipeline:\nstructured scene description extraction followed by multi-modal evaluation. Our\napproach transforms VLM reasoning from ad-hoc prompting to systematic analysis\nacross four semantic layers: Street, Infrastructure, Movable Objects, and\nEnvironment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world\ndriving scenarios, significantly outperforming unstructured baselines. More\nimportantly, we demonstrate that our structured framework enables a fine-tuned\n7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8%\naccuracy - surpassing all models evaluated while enabling local deployment at\nnear-zero cost. By automatically labeling over 9,640 real-world images with\nhigh accuracy, SAVANT addresses the critical data scarcity problem in anomaly\ndetection and provides a practical path toward reliable, accessible semantic\nmonitoring for autonomous systems.",
        "url": "http://arxiv.org/abs/2510.18034v1",
        "published_date": "2025-10-20T19:14:29+00:00",
        "updated_date": "2025-10-20T19:14:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO",
            "I.2.9; I.4.8"
        ],
        "authors": [
            "Roberto Brusnicki",
            "David Pop",
            "Yuan Gao",
            "Mattia Piccinini",
            "Johannes Betz"
        ],
        "tldr": "SAVANT is a framework for detecting anomalous driving scenarios using vision-augmented anomaly detection and structured reasoning with VLMs, achieving high accuracy and recall using a fine-tuned open-source model and addressing data scarcity through automatic labeling.",
        "tldr_zh": "SAVANT是一个用于检测异常驾驶场景的框架，它使用视觉增强的异常检测和VLM的结构化推理，通过微调的开源模型实现了高精度和召回率，并通过自动标记解决了数据稀缺问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation",
        "summary": "Climate change is intensifying the occurrence of harmful algal bloom (HAB),\nparticularly cyanobacteria, which threaten aquatic ecosystems and human health\nthrough oxygen depletion, toxin release, and disruption of marine biodiversity.\nTraditional monitoring approaches, such as manual water sampling, remain\nlabor-intensive and limited in spatial and temporal coverage. Recent advances\nin vision-language models (VLMs) for remote sensing have shown potential for\nscalable AI-driven solutions, yet challenges remain in reasoning over imagery\nand quantifying bloom severity. In this work, we introduce ALGae Observation\nand Segmentation (ALGOS), a segmentation-and-reasoning system for HAB\nmonitoring that combines remote sensing image understanding with severity\nestimation. Our approach integrates GeoSAM-assisted human evaluation for\nhigh-quality segmentation mask curation and fine-tunes vision language model on\nseverity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)\nfrom NASA. Experiments demonstrate that ALGOS achieves robust performance on\nboth segmentation and severity-level estimation, paving the way toward\npractical and automated cyanobacterial monitoring systems.",
        "url": "http://arxiv.org/abs/2510.18751v1",
        "published_date": "2025-10-21T15:59:00+00:00",
        "updated_date": "2025-10-21T15:59:00+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Patterson Hsieh",
            "Jerry Yeh",
            "Mao-Chi He",
            "Wen-Han Hsieh",
            "Elvis Hsieh"
        ],
        "tldr": "This paper introduces ALGOS, a system combining GeoSAM-assisted segmentation and fine-tuned VLMs for automated cyanobacterial bloom monitoring and severity estimation using remote sensing imagery.",
        "tldr_zh": "本文介绍了ALGOS，一个结合GeoSAM辅助分割和微调VLM的系统，用于使用遥感图像自动监测和估计蓝藻水华的严重程度。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "HouseTour: A Virtual Real Estate A(I)gent",
        "summary": "We introduce HouseTour, a method for spatially-aware 3D camera trajectory and\nnatural language summary generation from a collection of images depicting an\nexisting 3D space. Unlike existing vision-language models (VLMs), which\nstruggle with geometric reasoning, our approach generates smooth video\ntrajectories via a diffusion process constrained by known camera poses and\nintegrates this information into the VLM for 3D-grounded descriptions. We\nsynthesize the final video using 3D Gaussian splatting to render novel views\nalong the trajectory. To support this task, we present the HouseTour dataset,\nwhich includes over 1,200 house-tour videos with camera poses, 3D\nreconstructions, and real estate descriptions. Experiments demonstrate that\nincorporating 3D camera trajectories into the text generation process improves\nperformance over methods handling each task independently. We evaluate both\nindividual and end-to-end performance, introducing a new joint metric. Our work\nenables automated, professional-quality video creation for real estate and\ntouristic applications without requiring specialized expertise or equipment.",
        "url": "http://arxiv.org/abs/2510.18054v1",
        "published_date": "2025-10-20T19:47:35+00:00",
        "updated_date": "2025-10-20T19:47:35+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Ata Çelen",
            "Marc Pollefeys",
            "Daniel Barath",
            "Iro Armeni"
        ],
        "tldr": "The paper introduces HouseTour, a method and dataset for generating spatially-aware 3D camera trajectories and natural language summaries for real estate, leveraging diffusion processes and 3D Gaussian splatting to improve vision-language model performance in geometric reasoning.",
        "tldr_zh": "该论文介绍了HouseTour，一种用于生成具有空间感知能力的3D相机轨迹和房地产自然语言摘要的方法和数据集，利用扩散过程和3D高斯溅射来提高视觉语言模型在几何推理方面的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation",
        "summary": "Papillary thyroid microcarcinoma (PTMC) is increasingly managed with\nradio-frequency ablation (RFA), yet accurate lesion segmentation in ultrasound\nvideos remains difficult due to low contrast, probe-induced motion, and\nheat-related artifacts. The recent Segment Anything Model 2 (SAM-2) generalizes\nwell to static images, but its frame-independent design yields unstable\npredictions and temporal drift in interventional ultrasound. We introduce\n\\textbf{EMA-SAM}, a lightweight extension of SAM-2 that incorporates a\nconfidence-weighted exponential moving average pointer into the memory bank,\nproviding a stable latent prototype of the tumour across frames. This design\npreserves temporal coherence through probe pressure and bubble occlusion while\nrapidly adapting once clear evidence reappears. On our curated PTMC-RFA dataset\n(124 minutes, 13 patients), EMA-SAM improves \\emph{maxDice} from 0.82 (SAM-2)\nto 0.86 and \\emph{maxIoU} from 0.72 to 0.76, while reducing false positives by\n29\\%. On external benchmarks, including VTUS and colonoscopy video polyp\ndatasets, EMA-SAM achieves consistent gains of 2--5 Dice points over SAM-2.\nImportantly, the EMA pointer adds \\textless0.1\\% FLOPs, preserving real-time\nthroughput of $\\sim$30\\,FPS on a single A100 GPU. These results establish\nEMA-SAM as a robust and efficient framework for stable tumour tracking,\nbridging the gap between foundation models and the stringent demands of\ninterventional ultrasound. Codes are available here \\hyperref[code\n{https://github.com/mdialameh/EMA-SAM}.",
        "url": "http://arxiv.org/abs/2510.18213v1",
        "published_date": "2025-10-21T01:30:27+00:00",
        "updated_date": "2025-10-21T01:30:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maryam Dialameh",
            "Hossein Rajabzadeh",
            "Jung Suk Sim",
            "Hyock Ju Kwon"
        ],
        "tldr": "The paper introduces EMA-SAM, a lightweight extension to SAM-2 that incorporates a confidence-weighted exponential moving average pointer for stable tumour tracking in interventional ultrasound videos, improving segmentation accuracy and temporal coherence.",
        "tldr_zh": "该论文介绍了EMA-SAM，SAM-2的一个轻量级扩展，它结合了一个置信度加权的指数移动平均指针，用于在介入性超声视频中进行稳定的肿瘤跟踪，从而提高分割精度和时间连贯性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
        "summary": "Though recent advances in vision-language models (VLMs) have achieved\nremarkable progress across a wide range of multimodal tasks, understanding 3D\nspatial relationships from limited views remains a significant challenge.\nPrevious reasoning methods typically rely on pure text (e.g., topological\ncognitive maps) or on 2D visual cues. However, their limited representational\ncapacity hinders performance in specific tasks that require 3D spatial\nimagination. To address this limitation, we propose 3DThinker, a framework that\ncan effectively exploits the rich geometric information embedded within images\nwhile reasoning, like humans do. Our framework is the first to enable 3D\nmentaling during reasoning without any 3D prior input, and it does not rely on\nexplicitly labeled 3D data for training. Specifically, our training consists of\ntwo stages. First, we perform supervised training to align the 3D latent\ngenerated by VLM while reasoning with that of a 3D foundation model (e.g.,\nVGGT). Then, we optimize the entire reasoning trajectory solely based on\noutcome signals, thereby refining the underlying 3D mentaling. Extensive\nexperiments across multiple benchmarks show that 3DThinker consistently\noutperforms strong baselines and offers a new perspective toward unifying 3D\nrepresentations into multimodal reasoning. Our code will be available at\nhttps://github.com/zhangquanchen/3DThinker.",
        "url": "http://arxiv.org/abs/2510.18632v1",
        "published_date": "2025-10-21T13:36:58+00:00",
        "updated_date": "2025-10-21T13:36:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.2.10"
        ],
        "authors": [
            "Zhangquan Chen",
            "Manyuan Zhang",
            "Xinlei Yu",
            "Xufang Luo",
            "Mingze Sun",
            "Zihao Pan",
            "Yan Feng",
            "Peng Pei",
            "Xunliang Cai",
            "Ruqi Huang"
        ]
    }
]