[
    {
        "title": "LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance on\ngeneral visual benchmarks but struggle with out-of-distribution (OOD) tasks in\nspecialized domains such as medical imaging, where labeled data is limited and\nexpensive. We introduce LEAML, a label-efficient adaptation framework that\nleverages both scarce labeled VQA samples and abundant unlabeled images. Our\napproach generates domain-relevant pseudo question-answer pairs for unlabeled\ndata using a QA generator regularized by caption distillation. Importantly, we\nselectively update only those neurons most relevant to question-answering,\nenabling the QA Generator to efficiently acquire domain-specific knowledge\nduring distillation. Experiments on gastrointestinal endoscopy and sports VQA\ndemonstrate that LEAML consistently outperforms standard fine-tuning under\nminimal supervision, highlighting the effectiveness of our proposed LEAML\nframework.",
        "url": "http://arxiv.org/abs/2510.03232v1",
        "published_date": "2025-10-03T17:59:56+00:00",
        "updated_date": "2025-10-03T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ci-Siang Lin",
            "Min-Hung Chen",
            "Yu-Yang Sheng",
            "Yu-Chiang Frank Wang"
        ],
        "tldr": "LEAML is a label-efficient adaptation framework for MLLMs that uses pseudo question-answer pairs generated from unlabeled data and selective neuron updating to improve performance on out-of-distribution visual tasks with limited labeled data.",
        "tldr_zh": "LEAML是一种标签高效的MLLM适配框架，它利用从无标签数据生成的伪问答对和选择性神经元更新，以提高在标记数据有限的情况下，对分布外视觉任务的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
        "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
        "url": "http://arxiv.org/abs/2510.03230v1",
        "published_date": "2025-10-03T17:59:34+00:00",
        "updated_date": "2025-10-03T17:59:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Suyuchen Wang",
            "Tianyu Zhang",
            "Ahmed Masry",
            "Christopher Pal",
            "Spandana Gella",
            "Bang Liu",
            "Perouz Taslakian"
        ],
        "tldr": "This paper introduces RULER tokens and Interleaved MRoPE (I-MRoPE) to improve GUI grounding accuracy, especially on high-resolution displays, by providing explicit spatial guidance for VLMs.",
        "tldr_zh": "本文提出了RULER tokens和交错式MRoPE (I-MRoPE) 来提高GUI接地的准确性，尤其是在高分辨率显示器上，通过为VLM提供显式的空间指导。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus",
        "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
        "url": "http://arxiv.org/abs/2510.03160v1",
        "published_date": "2025-10-03T16:32:02+00:00",
        "updated_date": "2025-10-03T16:32:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ming Zhao",
            "Wenhui Dong",
            "Yang Zhang",
            "Xiang Zheng",
            "Zhonghao Zhang",
            "Zian Zhou",
            "Yunzhi Guan",
            "Liukun Xu",
            "Wei Peng",
            "Zhaoyang Gong",
            "Zhicheng Zhang",
            "Dachuan Li",
            "Xiaosheng Ma",
            "Yuli Ma",
            "Jianing Ni",
            "Changjiang Jiang",
            "Lixia Tian",
            "Qixin Chen",
            "Kaishun Xia",
            "Pingping Liu",
            "Tongshun Zhang",
            "Zhiqiang Liu",
            "Zhongan Bi",
            "Chenyang Si",
            "Tiansheng Sun",
            "Caifeng Shan"
        ],
        "tldr": "The paper introduces SpineMed, a large-scale, level-aware dataset and benchmark (SpineBench) for AI-assisted diagnosis of spine disorders, highlighting weaknesses in existing LVLMs and demonstrating improvements with a fine-tuned model.",
        "tldr_zh": "该论文介绍了 SpineMed，一个大规模、水平感知的脊柱疾病 AI 辅助诊断数据集和基准 (SpineBench)，强调了现有 LVLM 的弱点，并展示了微调模型的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
        "summary": "Visual navigation policy is widely regarded as a promising direction, as it\nmimics humans by using egocentric visual observations for navigation. However,\noptical information of visual observations is difficult to be explicitly\nmodeled like LiDAR point clouds or depth maps, which subsequently requires\nintelligent models and large-scale data. To this end, we propose to leverage\nthe intelligence of the Vision-Language-Action (VLA) model to learn diverse\nnavigation capabilities from synthetic expert data in a teacher-student manner.\nSpecifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360\nobservations) based on pretrained large language models and visual foundation\nmodels. For large-scale navigation data, we collect expert data from three\nreinforcement learning (RL) experts trained with privileged depth information\nin three challenging tailor-made environments for different navigation\ncapabilities: reaching, squeezing, and avoiding. We iteratively train our VLA\nmodel using data collected online from RL experts, where the training ratio is\ndynamically balanced based on performance on individual capabilities. Through\nextensive experiments in synthetic environments, we demonstrate that our model\nachieves strong generalization capability. Moreover, we find that our student\nVLA model outperforms the RL teachers, demonstrating the synergistic effect of\nintegrating multiple capabilities. Extensive real-world experiments further\nconfirm the effectiveness of our method.",
        "url": "http://arxiv.org/abs/2510.03142v1",
        "published_date": "2025-10-03T16:15:09+00:00",
        "updated_date": "2025-10-03T16:15:09+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tianyu Xu",
            "Jiawei Chen",
            "Jiazhao Zhang",
            "Wenyao Zhang",
            "Zekun Qi",
            "Minghan Li",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "tldr": "The paper introduces MM-Nav, a multi-view Vision-Language-Action model for visual navigation, trained using expert data from reinforcement learning agents with diverse navigation capabilities, showing strong generalization and outperforming the teachers in synthetic and real-world environments.",
        "tldr_zh": "该论文介绍了一种多视角视觉-语言-动作模型 MM-Nav，用于视觉导航。该模型通过强化学习代理的专家数据进行训练，这些代理具有不同的导航能力。结果表明，该模型具有很强的泛化能力，并在合成和现实环境中优于教师模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights",
        "summary": "Reliable risk assessment for carotid atheromatous disease remains a major\nclinical challenge, as it requires integrating diverse clinical and imaging\ninformation in a manner that is transparent and interpretable to clinicians.\nThis study investigates the potential of state-of-the-art and recent large\nvision-language models (LVLMs) for multimodal carotid plaque assessment by\nintegrating ultrasound imaging (USI) with structured clinical, demographic,\nlaboratory, and protein biomarker data. A framework that simulates realistic\ndiagnostic scenarios through interview-style question sequences is proposed,\ncomparing a range of open-source LVLMs, including both general-purpose and\nmedically tuned models. Zero-shot experiments reveal that even if they are very\npowerful, not all LVLMs can accurately identify imaging modality and anatomy,\nwhile all of them perform poorly in accurate risk classification. To address\nthis limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using\nlow-rank adaptation (LoRA), resulting in substantial improvements in stroke\nrisk stratification. The integration of multimodal tabular data in the form of\ntext further enhances specificity and balanced accuracy, yielding competitive\nperformance compared to prior convolutional neural network (CNN) baselines\ntrained on the same dataset. Our findings highlight both the promise and\nlimitations of LVLMs in ultrasound-based cardiovascular risk prediction,\nunderscoring the importance of multimodal integration, model calibration, and\ndomain adaptation for clinical translation.",
        "url": "http://arxiv.org/abs/2510.02922v1",
        "published_date": "2025-10-03T11:48:12+00:00",
        "updated_date": "2025-10-03T11:48:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Daphne Tsolissou",
            "Theofanis Ganitidis",
            "Konstantinos Mitsis",
            "Stergios CHristodoulidis",
            "Maria Vakalopoulou",
            "Konstantina Nikita"
        ],
        "tldr": "This paper explores the use of large vision-language models (LVLMs) for carotid plaque assessment, integrating ultrasound imaging with clinical data, and demonstrates the need for domain adaptation and multimodal integration to improve risk stratification accuracy.",
        "tldr_zh": "本文探讨了使用大型视觉语言模型（LVLMs）进行颈动脉斑块评估，将超声成像与临床数据相结合，并证明了领域自适应和多模态整合对于提高风险分层准确性的必要性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting",
        "summary": "Vision-language models like CLIP demonstrate impressive zero-shot\ngeneralization but remain highly vulnerable to adversarial attacks. In this\nwork, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot\nrobustness in vision-language models. CAW consists of two components: (1) a\nConfidence-Aware loss that prioritizes uncertain adversarial examples by\nscaling the KL divergence between clean and adversarial predictions, and (2) a\nfeature alignment regularization that preserves semantic consistency by\nminimizing the distance between frozen and fine-tuned image encoder features on\nadversarial inputs. These components work jointly to improve both clean and\nrobust accuracy without sacrificing generalization. Extensive experiments on\nTinyImageNet and 14 additional datasets show that CAW outperforms recent\nmethods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while\nusing less memory.",
        "url": "http://arxiv.org/abs/2510.02913v1",
        "published_date": "2025-10-03T11:36:02+00:00",
        "updated_date": "2025-10-03T11:36:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikoo Naghavian",
            "Mostafa Tavassolipour"
        ],
        "tldr": "This paper introduces Confidence-Aware Weighting (CAW), a method to enhance the zero-shot robustness of vision-language models against adversarial attacks by prioritizing uncertain adversarial examples and preserving semantic consistency. Experiments show CAW outperforms existing methods with less memory usage.",
        "tldr_zh": "本文提出了一种名为 Confidence-Aware Weighting (CAW) 的方法，通过优先考虑不确定的对抗样本并保持语义一致性，来增强视觉语言模型对对抗攻击的零样本鲁棒性。实验表明，CAW 在使用更少内存的情况下优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Don't Just Chase \"Highlighted Tokens\" in MLLMs: Revisiting Visual Holistic Context Retention",
        "summary": "Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)\nsuffer from considerable computational overhead due to their reliance on\nmassive visual tokens. Recent studies have explored token pruning to alleviate\nthis problem, which typically uses text-vision cross-attention or\n[\\texttt{CLS}] attention to assess and discard redundant visual tokens. In this\nwork, we identify a critical limitation of such attention-first pruning\napproaches, i.e., they tend to preserve semantically similar tokens, resulting\nin pronounced performance drops under high pruning ratios. To this end, we\npropose {HoloV}, a simple yet effective, plug-and-play visual token pruning\nframework for efficient inference. Distinct from previous attention-first\nschemes, HoloV rethinks token retention from a holistic perspective. By\nadaptively distributing the pruning budget across different spatial crops,\nHoloV ensures that the retained tokens capture the global visual context rather\nthan isolated salient features. This strategy minimizes representational\ncollapse and maintains task-relevant information even under aggressive pruning.\nExperimental results demonstrate that our HoloV achieves superior performance\nacross various tasks, MLLM architectures, and pruning ratios compared to SOTA\nmethods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\\% of the\noriginal performance after pruning 88.9\\% of visual tokens, achieving superior\nefficiency-accuracy trade-offs.",
        "url": "http://arxiv.org/abs/2510.02912v1",
        "published_date": "2025-10-03T11:33:40+00:00",
        "updated_date": "2025-10-03T11:33:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Zou",
            "Di Lu",
            "Yizhou Wang",
            "Yibo Yan",
            "Yuanhuiyi Lyu",
            "Xu Zheng",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "tldr": "This paper introduces HoloV, a novel visual token pruning method for MLLMs that adaptively distributes pruning across spatial crops to retain global visual context, achieving better performance under high pruning ratios compared to attention-first methods.",
        "tldr_zh": "本文介绍了一种名为HoloV的新型多模态大语言模型(MLLM)视觉token剪枝方法，该方法通过自适应地在空间区域上分配剪枝预算，以保留全局视觉上下文，与基于注意力机制的方法相比，在高剪枝率下实现了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
        "summary": "Zero-shot captioners are recently proposed models that utilize common-space\nvision-language representations to caption images without relying on paired\nimage-text data. To caption an image, they proceed by textually decoding a\ntext-aligned image feature, but they limit their scope to global\nrepresentations and whole-image captions. We present \\frameworkName{}, a\nunified framework for zero-shot captioning that shifts from an image-centric to\na patch-centric paradigm, enabling the captioning of arbitrary regions without\nthe need of region-level supervision. Instead of relying on global image\nrepresentations, we treat individual patches as atomic captioning units and\naggregate them to describe arbitrary regions, from single patches to\nnon-contiguous areas and entire images. We analyze the key ingredients that\nenable current latent captioners to work in our novel proposed framework.\nExperiments demonstrate that backbones producing meaningful, dense visual\nfeatures, such as DINO, are key to achieving state-of-the-art performance in\nmultiple region-based captioning tasks. Compared to other baselines and\nstate-of-the-art competitors, our models achieve better performance on\nzero-shot dense, region-set, and a newly introduced trace captioning task,\nhighlighting the effectiveness of patch-wise semantic representations for\nscalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
        "url": "http://arxiv.org/abs/2510.02898v1",
        "published_date": "2025-10-03T11:05:56+00:00",
        "updated_date": "2025-10-03T11:05:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lorenzo Bianchi",
            "Giacomo Pacini",
            "Fabio Carrara",
            "Nicola Messina",
            "Giuseppe Amato",
            "Fabrizio Falchi"
        ],
        "tldr": "This paper introduces a unified zero-shot captioning framework that shifts from image-centric to patch-centric, enabling the captioning of arbitrary regions without region-level supervision, achieving state-of-the-art performance in region-based captioning tasks.",
        "tldr_zh": "本文介绍了一种统一的零样本图像描述框架，该框架从以图像为中心转向以补丁为中心，无需区域级监督即可描述任意区域，并在基于区域的图像描述任务中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving",
        "summary": "Visual Language Models (VLMs), with powerful multimodal reasoning\ncapabilities, are gradually integrated into autonomous driving by several\nautomobile manufacturers to enhance planning capability in challenging\nenvironments. However, the trajectory planning capability of VLMs in work\nzones, which often include irregular layouts, temporary traffic control, and\ndynamically changing geometric structures, is still unexplored. To bridge this\ngap, we conduct the \\textit{first} systematic study of VLMs for work zone\ntrajectory planning, revealing that mainstream VLMs fail to generate correct\ntrajectories in $68.0%$ of cases. To better understand these failures, we first\nidentify candidate patterns via subgraph mining and clustering analysis, and\nthen confirm the validity of $8$ common failure patterns through human\nverification. Building on these findings, we propose REACT-Drive, a trajectory\nplanning framework that integrates VLMs with Retrieval-Augmented Generation\n(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases\ninto constraint rules and executable trajectory planning code, while RAG\nretrieves similar patterns in new scenarios to guide trajectory generation.\nExperimental results on the ROADWork dataset show that REACT-Drive yields a\nreduction of around $3\\times$ in average displacement error relative to VLM\nbaselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the\nlowest inference time ($0.58$s) compared with other methods such as fine-tuning\n($17.90$s). We further conduct experiments using a real vehicle in 15 work zone\nscenarios in the physical world, demonstrating the strong practicality of\nREACT-Drive.",
        "url": "http://arxiv.org/abs/2510.02803v1",
        "published_date": "2025-10-03T08:21:15+00:00",
        "updated_date": "2025-10-03T08:21:15+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yifan Liao",
            "Zhen Sun",
            "Xiaoyun Qiu",
            "Zixiao Zhao",
            "Wenbing Tang",
            "Xinlei He",
            "Xinhu Zheng",
            "Tianwei Zhang",
            "Xinyi Huang",
            "Xingshuo Han"
        ],
        "tldr": "This paper identifies failure patterns of VLMs in autonomous driving within work zones and proposes REACT-Drive, a RAG-enhanced framework to improve trajectory planning, demonstrating significant error reduction and practicality in real-world scenarios.",
        "tldr_zh": "该论文识别了视觉语言模型在工作区自动驾驶中轨迹规划的失败模式，并提出了 REACT-Drive，这是一个 RAG 增强的框架，以改善轨迹规划。实验结果表明，该框架显著降低了误差，并在实际场景中具有很强的实用性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding",
        "summary": "Large vision-language models (LVLMs) have shown remarkable performance in\nvisual-language understanding for downstream multimodal tasks. While their\ncapabilities are improving, problems emerge simultaneously. Among those\nproblems, the hallucinations have attracted much attention, which stands for\nthe phenomenon where LVLMs generate contradictory content to their input visual\nand text contents. Many approaches have been proposed to deal with this issue,\nsuch as contrastive decoding and attention manipulation. However, contrastive\ndecoding methods struggle in constructing appropriate contrastive samples, and\nattention manipulation methods are highly sensitive, lacking stability. In this\nwork, we propose image head Masked Contrastive Decoding (MaskCD). Our approach\nutilizes the \"image heads\" in LVLMs, masking them to construct contrastive\nsamples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and\nQwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The\nresults demonstrate that MaskCD effectively alleviates the phenomenon of\nhallucinations and retains the general capabilities of LVLMs. Corresponding\nresources could be found at: https://github.com/Deng-Jingyuan/MaskCD .",
        "url": "http://arxiv.org/abs/2510.02790v1",
        "published_date": "2025-10-03T07:59:16+00:00",
        "updated_date": "2025-10-03T07:59:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "authors": [
            "Jingyuan Deng",
            "Yujiu Yang"
        ],
        "tldr": "This paper introduces MaskCD, a novel contrastive decoding method that masks image heads in LVLMs to mitigate hallucinations, showing effectiveness on LLaVA-1.5-7b and Qwen-VL-7b across various benchmarks.",
        "tldr_zh": "本文介绍了MaskCD，一种新颖的对比解码方法，通过屏蔽LVLM中的图像头部来减轻幻觉现象。实验结果表明，该方法在LLaVA-1.5-7b和Qwen-VL-7b等模型上有效，并在多个基准测试中表现良好。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models",
        "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
        "url": "http://arxiv.org/abs/2510.02750v1",
        "published_date": "2025-10-03T06:27:33+00:00",
        "updated_date": "2025-10-03T06:27:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lihua Zhou",
            "Mao Ye",
            "Shuaifeng Li",
            "Nianxin Li",
            "Jinlin Wu",
            "Xiatian Zhu",
            "Lei Deng",
            "Hongbin Liu",
            "Jiebo Luo",
            "Zhen Lei"
        ],
        "tldr": "The paper introduces BCA+, a training-free Bayesian test-time adaptation framework for both object recognition and detection in VLMs, achieving state-of-the-art performance by dynamically updating class embeddings, spatial scales, and class priors.",
        "tldr_zh": "该论文介绍了BCA+，一个无需训练的贝叶斯测试时自适应框架，用于视觉语言模型中的物体识别和检测。该框架通过动态更新类别嵌入、空间尺度和类别先验，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval",
        "summary": "The success of DeepSeek-R1 demonstrates the immense potential of using\nreinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper\nintroduces Retrv-R1, the first R1-style MLLM specifically designed for\nmultimodal universal retrieval, achieving higher performance by employing\nstep-by-step reasoning to produce more accurate retrieval results. We find that\ndirectly applying the methods of DeepSeek-R1 to retrieval tasks is not\nfeasible, mainly due to (1) the high computational cost caused by the large\ntoken consumption required for multiple candidates with reasoning processes,\nand (2) the instability and suboptimal results when directly applying RL to\ntrain for retrieval tasks. To address these issues, Retrv-R1 introduces an\ninformation compression module with a details inspection mechanism, which\nenhances computational efficiency by reducing the number of tokens while\nensuring that critical information for challenging candidates is preserved.\nFurthermore, a new training paradigm is proposed, including an activation stage\nusing a retrieval-tailored synthetic CoT dataset for more effective\noptimization, followed by RL with a novel curriculum reward to improve both\nperformance and efficiency. Incorporating these novel designs, Retrv-R1\nachieves SOTA performance, high efficiency, and strong generalization ability,\nas demonstrated by experiments across multiple benchmarks and tasks.",
        "url": "http://arxiv.org/abs/2510.02745v1",
        "published_date": "2025-10-03T06:16:58+00:00",
        "updated_date": "2025-10-03T06:16:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lanyun Zhu",
            "Deyi Ji",
            "Tianrun Chen",
            "Haiyang Wu",
            "Shiqi Wang"
        ],
        "tldr": "Retrv-R1 is a new MLLM framework designed for multimodal universal retrieval, leveraging reinforcement learning with a novel information compression module and training paradigm to achieve SOTA performance and efficiency.",
        "tldr_zh": "Retrv-R1是一个新的MLLM框架，专为多模态通用检索设计，利用强化学习、一种新型信息压缩模块和训练范式，以实现SOTA的性能和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback",
        "summary": "Recent advances in large video-language models (VLMs) rely on extensive\nfine-tuning techniques that strengthen alignment between textual and visual\ncomprehension. Leading pipelines typically pair supervised fine-tuning (SFT)\nwith reinforcement learning from preference data to enhance video\ncomprehension. However, as VLMs scale in parameter size, so does the cost of\ngathering enough human feedback. To make fine-tuning more cost-effective,\nrecent frameworks explore reinforcement learning with AI feedback (RLAIF),\nwhich replace human preference with AI as a judge. Current RLAIF frameworks\nrely on a specialized reward model trained with video narratives to create\ncalibrated scalar rewards-- an expensive and restrictive pipeline. We propose\nOracle-RLAIF, a novel framework that replaces the trained reward model with a\nmore general Oracle ranker which acts as a drop-in model ranking candidate\nmodel responses rather than scoring them. Alongside Oracle-RLAIF, we introduce\n$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy\nOptimization (GRPO) that directly optimizes ordinal feedback with rank-aware\nadvantages. Empirically, we demonstrate that Oracle-RLAIF consistently\noutperforms leading VLMs using existing fine-tuning methods when evaluated\nacross various video comprehension benchmarks. Oracle-RLAIF paves the path to\ncreating flexible and data-efficient frameworks for aligning large multi-modal\nvideo models with reinforcement learning from rank rather than score.",
        "url": "http://arxiv.org/abs/2510.02561v1",
        "published_date": "2025-10-02T20:57:10+00:00",
        "updated_date": "2025-10-02T20:57:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Derek Shi",
            "Ruben Glatt",
            "Christine Klymko",
            "Shubham Mohole",
            "Hongjun Choi",
            "Shashank Kushwaha",
            "Sam Sakla",
            "Felipe Leno da Silva"
        ],
        "tldr": "The paper introduces Oracle-RLAIF, a novel reinforcement learning framework for fine-tuning video-language models using a general Oracle ranker instead of a trained reward model, coupled with a new rank-based loss function, achieving improved performance on video comprehension benchmarks.",
        "tldr_zh": "该论文介绍了Oracle-RLAIF，一种新颖的强化学习框架，用于微调视频-语言模型，使用通用的Oracle排序器代替训练的奖励模型，并结合了一种新的基于排序的损失函数，在视频理解基准测试中实现了更高的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring OCR-augmented Generation for Bilingual VQA",
        "summary": "We investigate OCR-augmented generation with Vision Language Models (VLMs),\nexploring tasks in Korean and English toward multilingualism. To support\nresearch in this domain, we train and release KLOCR, a strong bilingual OCR\nbaseline trained on 100M instances to augment VLMs with OCR ability. To\ncomplement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and\nanalyze different prompting methods. Extensive experiments show that\nOCR-extracted text significantly boosts performance across open source and\ncommercial models. Our work offers new insights into OCR-augmented generation\nfor bilingual VQA. Model, code, and data are available at\nhttps://github.com/JHLee0513/KLOCR.",
        "url": "http://arxiv.org/abs/2510.02543v1",
        "published_date": "2025-10-02T20:19:31+00:00",
        "updated_date": "2025-10-02T20:19:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "JoonHo Lee",
            "Sunho Park"
        ],
        "tldr": "This paper introduces KLOCR, a bilingual OCR model for Korean and English, and KOCRBench, a Korean VQA benchmark. Experiments demonstrate that OCR-extracted text improves VQA performance across various models.",
        "tldr_zh": "本文介绍KLOCR，一个用于韩语和英语的双语OCR模型，以及KOCRBench，一个韩语VQA基准。实验表明，提取的OCR文本可以提高各种模型的VQA性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency",
        "summary": "With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.02987v1",
        "published_date": "2025-10-03T13:25:16+00:00",
        "updated_date": "2025-10-03T13:25:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Juntong Wang",
            "Huiyu Duan",
            "Jiarui Wang",
            "Ziheng Jia",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "tldr": "This paper introduces LPG-Bench, a new benchmark for evaluating text-to-image models with long prompts, and TIT-Score, a novel metric based on text-to-image-to-text consistency for assessing long-prompt alignment, showing improved correlation with human judgment.",
        "tldr_zh": "本文介绍了LPG-Bench，一个用于评估长文本提示下文本到图像模型的基准，以及TIT-Score，一种基于文本到图像到文本一致性的新型指标，用于评估长文本提示对齐，表明与人类判断的相关性有所提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet their\ncognitive processes remain opaque on complex lateral thinking challenges like\nrebus puzzles. While recent work has demonstrated these models struggle\nsignificantly with rebus puzzle solving, the underlying reasoning processes and\nfailure patterns remain largely unexplored. We address this gap through a\ncomprehensive explainability analysis that moves beyond performance metrics to\nunderstand how VLMs approach these complex lateral thinking challenges. Our\nstudy contributes a systematically annotated dataset of 221 rebus puzzles\nacross six cognitive categories, paired with an evaluation framework that\nseparates reasoning quality from answer correctness. We investigate three\nprompting strategies designed to elicit different types of explanatory\nprocesses and reveal critical insights into VLM cognitive processes. Our\nfindings demonstrate that reasoning quality varies dramatically across puzzle\ncategories, with models showing systematic strengths in visual composition\nwhile exhibiting fundamental limitations in absence interpretation and cultural\nsymbolism. We also discover that prompting strategy substantially influences\nboth cognitive approach and problem-solving effectiveness, establishing\nexplainability as an integral component of model performance rather than a\npost-hoc consideration.",
        "url": "http://arxiv.org/abs/2510.02780v1",
        "published_date": "2025-10-03T07:27:47+00:00",
        "updated_date": "2025-10-03T07:27:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Prahitha Movva"
        ],
        "tldr": "This paper analyzes the reasoning processes of Vision-Language Models (VLMs) on rebus puzzles using explainability techniques, revealing cognitive strengths and limitations related to visual composition, absence interpretation, and cultural symbolism.",
        "tldr_zh": "该论文通过可解释性技术分析了视觉语言模型 (VLM) 在字谜上的推理过程，揭示了与视觉组成、缺失解释和文化象征相关的认知优势和局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding",
        "summary": "Understanding long-form videos remains a significant challenge for\nvision--language models (VLMs) due to their extensive temporal length and high\ninformation density. Most current multimodal large language models (MLLMs) rely\non uniform sampling, which often overlooks critical moments, leading to\nincorrect responses to queries. In parallel, many keyframe selection approaches\nimpose rigid temporal spacing: once a frame is chosen, an exclusion window\nsuppresses adjacent timestamps to reduce redundancy. While effective at\nlimiting overlap, this strategy frequently misses short, fine-grained cues near\nimportant events. Other methods instead emphasize visual diversity but neglect\nquery relevance. We propose AdaRD-Key, a training-free keyframe sampling module\nfor query-driven long-form video understanding. AdaRD-Key maximizes a unified\nRelevance--Diversity Max-Volume (RD-MV) objective, combining a\nquery-conditioned relevance score with a log-determinant diversity component to\nyield informative yet non-redundant frames. To handle broad queries with weak\nalignment to the video, AdaRD-Key employs a lightweight relevance-aware gating\nmechanism; when the relevance distribution indicates weak alignment, the method\nseamlessly shifts into a diversity-only mode, enhancing coverage without\nadditional supervision. Our pipeline is training-free, computationally\nefficient (running in real time on a single GPU), and compatible with existing\nVLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and\nVideo-MME demonstrate state-of-the-art performance, particularly on long-form\nvideos. Code available at https://github.com/Xian867/AdaRD-Key.",
        "url": "http://arxiv.org/abs/2510.02778v1",
        "published_date": "2025-10-03T07:19:34+00:00",
        "updated_date": "2025-10-03T07:19:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xian Zhang",
            "Zexi Wu",
            "Zinuo Li",
            "Hongming Xu",
            "Luqi Gong",
            "Farid Boussaid",
            "Naoufel Werghi",
            "Mohammed Bennamoun"
        ],
        "tldr": "The paper introduces AdaRD-Key, a training-free keyframe sampling module for long-form video understanding that adaptively balances query relevance and visual diversity, achieving state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "该论文介绍了一种名为AdaRD-Key的免训练关键帧采样模块，用于长视频理解，该模块自适应地平衡查询相关性和视觉多样性，并在基准数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization",
        "summary": "This paper introduces a novel approach to aesthetic quality improvement in\npre-trained text-to-image diffusion models when given a simple prompt. Our\nmethod, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained\ntext-to-image diffusion model as a backbone and optimizes the text embedding of\na given simple and uncurated prompt to enhance the visual quality of the\ngenerated image. We achieve this by a tripartite objective function that\nimproves the aesthetic fidelity of the generated image, ensures adherence to\nthe optimized text embedding, and minimal divergence from the initial prompt.\nThe latter is accomplished through a prompt preservation term. Additionally,\nPEO is training-free and backbone-independent. Quantitative and qualitative\nevaluations confirm the effectiveness of the proposed method, exceeding or\nequating the performance of state-of-the-art text-to-image and prompt\nadaptation methods.",
        "url": "http://arxiv.org/abs/2510.02599v1",
        "published_date": "2025-10-02T22:12:36+00:00",
        "updated_date": "2025-10-02T22:12:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hovhannes Margaryan",
            "Bo Wan",
            "Tinne Tuytelaars"
        ],
        "tldr": "The paper introduces PEO, a training-free method to optimize prompt embeddings for improved aesthetic quality in text-to-image diffusion models, outperforming or matching existing methods.",
        "tldr_zh": "该论文介绍了一种名为PEO的免训练方法，通过优化prompt嵌入来提高文本到图像扩散模型的美学质量，性能优于或与现有方法相当。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis",
        "summary": "Cross-modal medical image synthesis research focuses on reconstructing\nmissing imaging modalities from available ones to support clinical diagnosis.\nDriven by clinical necessities for flexible modality reconstruction, we explore\nK to N medical generation, where three critical challenges emerge: How can we\nmodel the heterogeneous contributions of different modalities to various target\ntasks? How can we ensure fusion quality control to prevent degradation from\nnoisy information? How can we maintain modality identity consistency in\nmulti-output generation? Driven by these clinical necessities, and drawing\ninspiration from SAM2's sequential frame paradigm and clinicians' progressive\nworkflow of incrementally adding and selectively integrating multi-modal\ninformation, we treat multi-modal medical data as sequential frames with\nquality-driven selection mechanisms. Our key idea is to \"learn\" adaptive\nweights for each modality-task pair and \"memorize\" beneficial fusion patterns\nthrough progressive enhancement. To achieve this, we design three collaborative\nmodules: PreWeightNet for global contribution assessment, ThresholdNet for\nadaptive filtering, and EffiWeightNet for effective weight computation.\nMeanwhile, to maintain modality identity consistency, we propose the Causal\nModality Identity Module (CMIM) that establishes causal constraints between\ngenerated images and target modality descriptions using vision-language\nmodeling. Extensive experimental results demonstrate that our proposed Med-K2N\noutperforms state-of-the-art methods by significant margins on multiple\nbenchmarks. Source code is available.",
        "url": "http://arxiv.org/abs/2510.02815v1",
        "published_date": "2025-10-03T08:47:17+00:00",
        "updated_date": "2025-10-03T08:47:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Yuan",
            "Yifan Gao",
            "Yuehua Ye",
            "Haoyue Li",
            "Xin Gao"
        ],
        "tldr": "The paper introduces Med-K2N, a novel framework for flexible K-to-N medical image synthesis that addresses challenges in multi-modal fusion and modality identity consistency using adaptive weighting and causal modeling, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了 Med-K2N，一种新颖的 K 到 N 灵活医学图像合成框架，通过自适应权重和因果建模解决多模态融合和模态一致性方面的挑战，并取得了最先进的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
        "summary": "Driving scene manipulation with sensor data is emerging as a promising\nalternative to traditional virtual driving simulators. However, existing\nframeworks struggle to generate realistic scenarios efficiently due to limited\nediting capabilities. To address these challenges, we present SIMSplat, a\npredictive driving scene editor with language-aligned Gaussian splatting. As a\nlanguage-controlled editor, SIMSplat enables intuitive manipulation using\nnatural language prompts. By aligning language with Gaussian-reconstructed\nscenes, it further supports direct querying of road objects, allowing precise\nand flexible editing. Our method provides detailed object-level editing,\nincluding adding new objects and modifying the trajectories of both vehicles\nand pedestrians, while also incorporating predictive path refinement through\nmulti-agent motion prediction to generate realistic interactions among all\nagents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's\nextensive editing capabilities and adaptability across a wide range of\nscenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
        "url": "http://arxiv.org/abs/2510.02469v1",
        "published_date": "2025-10-02T18:22:03+00:00",
        "updated_date": "2025-10-02T18:22:03+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Sung-Yeon Park",
            "Adam Lee",
            "Juanwu Lu",
            "Can Cui",
            "Luyang Jiang",
            "Rohit Gupta",
            "Kyungtae Han",
            "Ahmadreza Moradipari",
            "Ziran Wang"
        ],
        "tldr": "SIMSplat is a language-controlled driving scene editor using Gaussian splatting that allows for intuitive manipulation and realistic scenario generation via multi-agent motion prediction and object-level editing. It demonstrates adaptability across various scenarios using the Waymo dataset.",
        "tldr_zh": "SIMSplat是一个语言控制的驾驶场景编辑器，使用高斯溅射，通过多智能体运动预测和对象级别编辑，实现直观操作和逼真场景生成。它在Waymo数据集上展示了对各种场景的适应性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]