[
    {
        "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas",
        "summary": "Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.",
        "url": "http://arxiv.org/abs/2510.20820v1",
        "published_date": "2025-10-23T17:59:55+00:00",
        "updated_date": "2025-10-23T17:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guocheng Gordon Qian",
            "Ruihang Zhang",
            "Tsai-Shien Chen",
            "Yusuf Dalva",
            "Anujraaj Argo Goyal",
            "Willi Menapace",
            "Ivan Skorokhodov",
            "Meng Dong",
            "Arpit Sahni",
            "Daniil Ostashev",
            "Ju Hu",
            "Sergey Tulyakov",
            "Kuan-Chieh Jackson Wang"
        ],
        "tldr": "LayerComposer introduces a layered canvas and locking mechanism for interactive, personalized, multi-subject text-to-image generation, enabling better spatial control and identity preservation compared to existing methods.",
        "tldr_zh": "LayerComposer 提出了一个分层画布和锁定机制，用于交互式、个性化的多主体文本到图像生成，与现有方法相比，实现了更好的空间控制和身份保持。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation",
        "summary": "Large Vision-Language Models (VLMs) have achieved remarkable progress in\nmultimodal understanding, yet they struggle when reasoning over\ninformation-intensive images that densely interleave textual annotations with\nfine-grained graphical elements. The main challenges lie in precisely\nlocalizing critical cues in dense layouts and multi-hop reasoning to integrate\ndispersed evidence. We propose Speculative Verdict (SV), a training-free\nframework inspired by speculative decoding that combines multiple lightweight\ndraft experts with a large verdict model. In the draft stage, small VLMs act as\ndraft experts to generate reasoning paths that provide diverse localization\ncandidates; in the verdict stage, a strong VLM synthesizes these paths to\nproduce the final answer, minimizing computational cost while recovering\ncorrect answers. To further improve efficiency and accuracy, SV introduces a\nconsensus expert selection mechanism that forwards only high-agreement\nreasoning paths to the verdict. Empirically, SV achieves consistent gains on\nchallenging information-intensive and high-resolution visual question answering\nbenchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.\nBy synthesizing correct insights from multiple partially accurate reasoning\npaths, SV achieves both error correction and cost-efficiency compared to large\nproprietary models or training pipelines. Code is available at\nhttps://github.com/Tinaliu0123/speculative-verdict",
        "url": "http://arxiv.org/abs/2510.20812v1",
        "published_date": "2025-10-23T17:59:21+00:00",
        "updated_date": "2025-10-23T17:59:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yuhan Liu",
            "Lianhui Qin",
            "Shengjie Wang"
        ],
        "tldr": "The paper introduces Speculative Verdict (SV), a training-free framework that uses multiple lightweight VLMs as draft experts to generate reasoning paths, which are then synthesized by a large VLM for information-intensive visual reasoning, achieving improved accuracy and efficiency.",
        "tldr_zh": "该论文介绍了一种名为Speculative Verdict (SV) 的免训练框架，该框架使用多个轻量级VLM作为草稿专家来生成推理路径，然后由大型VLM综合这些路径以进行信息密集型视觉推理，从而提高准确性和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
        "summary": "We propose a novel AutoRegressive Generation-based paradigm for image\nSegmentation (ARGenSeg), achieving multimodal understanding and pixel-level\nperception within a unified framework. Prior works integrating image\nsegmentation into multimodal large language models (MLLMs) typically employ\neither boundary points representation or dedicated segmentation heads. These\nmethods rely on discrete representations or semantic prompts fed into\ntask-specific decoders, which limits the ability of the MLLM to capture\nfine-grained visual details. To address these challenges, we introduce a\nsegmentation framework for MLLM based on image generation, which naturally\nproduces dense masks for target objects. We leverage MLLM to output visual\ntokens and detokenize them into images using an universal VQ-VAE, making the\nsegmentation fully dependent on the pixel-level understanding of the MLLM. To\nreduce inference latency, we employ a next-scale-prediction strategy to\ngenerate required visual tokens in parallel. Extensive experiments demonstrate\nthat our method surpasses prior state-of-the-art approaches on multiple\nsegmentation datasets with a remarkable boost in inference speed, while\nmaintaining strong understanding capabilities.",
        "url": "http://arxiv.org/abs/2510.20803v1",
        "published_date": "2025-10-23T17:58:26+00:00",
        "updated_date": "2025-10-23T17:58:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaolong Wang",
            "Lixiang Ru",
            "Ziyuan Huang",
            "Kaixiang Ji",
            "Dandan Zheng",
            "Jingdong Chen",
            "Jun Zhou"
        ],
        "tldr": "The paper introduces ARGenSeg, a novel autoregressive image generation-based framework for image segmentation within MLLMs, achieving state-of-the-art performance and faster inference speeds by generating dense masks directly from visual tokens output by the MLLM.",
        "tldr_zh": "该论文介绍了ARGenSeg，一种新颖的基于自回归图像生成的图像分割框架，应用于多模态大型语言模型（MLLMs），通过直接从MLLM输出的视觉tokens生成密集mask，实现了最先进的性能和更快的推理速度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models",
        "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
        "url": "http://arxiv.org/abs/2510.20707v1",
        "published_date": "2025-10-23T16:17:47+00:00",
        "updated_date": "2025-10-23T16:17:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuyang Liu",
            "Xiyan Gui",
            "Yuchao Zhang",
            "Linfeng Zhang"
        ],
        "tldr": "This paper introduces MixKV, a novel KV cache compression method for LVLMs that balances importance and diversity of KV pairs, achieving significant performance improvements on multi-modal understanding and GUI grounding tasks.",
        "tldr_zh": "本文介绍了一种名为MixKV的新型KV缓存压缩方法，该方法平衡了LVLM中KV对的重要性和多样性，在多模态理解和GUI基础任务上取得了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward",
        "summary": "Multimodal large language models (MLLMs) that integrate visual and textual\nreasoning leverage chain-of-thought (CoT) prompting to tackle complex visual\ntasks, yet continue to exhibit visual hallucinations and an over-reliance on\ntextual priors. We present a systematic diagnosis of state-of-the-art\nvision-language models using a three-stage evaluation framework, uncovering key\nfailure modes. To address these, we propose an agent-based architecture that\ncombines LLM reasoning with lightweight visual modules, enabling fine-grained\nanalysis and iterative refinement of reasoning chains. Our results highlight\nfuture visual reasoning models should focus on integrating a broader set of\nspecialized tools for analyzing visual content. Our system achieves significant\ngains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or\nsurpassing much larger models. We will release our framework and evaluation\nsuite to facilitate future research.",
        "url": "http://arxiv.org/abs/2510.20696v1",
        "published_date": "2025-10-23T16:10:03+00:00",
        "updated_date": "2025-10-23T16:10:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jing Bi",
            "Guangyu Sun",
            "Ali Vosoughi",
            "Chen Chen",
            "Chenliang Xu"
        ],
        "tldr": "This paper diagnoses failure modes in visual reasoning of MLLMs, proposes an agent-based architecture with visual modules to address them, and achieves significant performance gains on MMMU and MathVista.",
        "tldr_zh": "该论文诊断了多模态大语言模型在视觉推理方面的失败模式，提出了一种基于代理的架构，该架构结合视觉模块来解决这些问题，并在MMMU和MathVista上取得了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset",
        "summary": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable\nprogress. However, two key challenges remain : 1) the absence of a large-scale\nhigh-quality UHR T2I dataset, and (2) the neglect of tailored training\nstrategies for fine-grained detail synthesis in UHR scenarios. To tackle the\nfirst challenge, we introduce \\textbf{UltraHR-100K}, a high-quality dataset of\n100K UHR images with rich captions, offering diverse content and strong visual\nfidelity. Each image exceeds 3K resolution and is rigorously curated based on\ndetail richness, content complexity, and aesthetic quality. To tackle the\nsecond challenge, we propose a frequency-aware post-training method that\nenhances fine-detail generation in T2I diffusion models. Specifically, we\ndesign (i) \\textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning\non detail-critical denoising steps, and (ii) \\textit{Soft-Weighting Frequency\nRegularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to\nsoftly constrain frequency components, encouraging high-frequency detail\npreservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks\ndemonstrate that our approach significantly improves the fine-grained detail\nquality and overall fidelity of UHR image generation. The code is available at\n\\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.",
        "url": "http://arxiv.org/abs/2510.20661v1",
        "published_date": "2025-10-23T15:34:53+00:00",
        "updated_date": "2025-10-23T15:34:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Zhao",
            "En Ci",
            "Yunzhe Xu",
            "Tiehan Fan",
            "Shanyan Guan",
            "Yanhao Ge",
            "Jian Yang",
            "Ying Tai"
        ],
        "tldr": "The paper introduces UltraHR-100K, a large-scale high-quality UHR image dataset, and a frequency-aware post-training method to enhance fine-grained detail synthesis in UHR text-to-image diffusion models.",
        "tldr_zh": "该论文介绍了一个大规模高质量的超高分辨率（UHR）图像数据集 UltraHR-100K，以及一种频率感知后训练方法，旨在增强超高分辨率文本到图像扩散模型中细粒度细节的合成。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging",
        "summary": "Recent progress in vision-language modeling for 3D medical imaging has been\nfueled by large-scale computed tomography (CT) corpora with paired free-text\nreports, stronger architectures, and powerful pretrained models. This has\nenabled applications such as automated report generation and text-conditioned\n3D image synthesis. Yet, current approaches struggle with high-resolution,\nlong-sequence volumes: contrastive pretraining often yields vision encoders\nthat are misaligned with clinical language, and slice-wise tokenization blurs\nfine anatomy, reducing diagnostic performance on downstream tasks. We introduce\nBTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder\nthat unifies 2D and 3D training and inference while producing compact,\nfrequency-aware volumetric tokens. A three-stage training curriculum enables\n(i) local reconstruction, (ii) overlapping-window tiling, and (iii)\nlong-context decoder refinement, during which the model learns from short slice\nexcerpts yet generalizes to scans exceeding 300 slices without additional\nmemory overhead. BTB3D sets a new state-of-the-art on two key tasks: it\nimproves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and\nMerlin for report generation; and it reduces FID by 75% and halves FVD compared\nto GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically\nconsistent 512*512*241 volumes. These results confirm that precise\nthree-dimensional tokenization, rather than larger language backbones alone, is\nessential for scalable vision-language modeling in 3D medical imaging. The\ncodebase is available at: https://github.com/ibrahimethemhamamci/BTB3D",
        "url": "http://arxiv.org/abs/2510.20639v1",
        "published_date": "2025-10-23T15:13:13+00:00",
        "updated_date": "2025-10-23T15:13:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ibrahim Ethem Hamamci",
            "Sezgin Er",
            "Suprosanna Shit",
            "Hadrien Reynaud",
            "Dong Yang",
            "Pengfei Guo",
            "Marc Edgar",
            "Daguang Xu",
            "Bernhard Kainz",
            "Bjoern Menze"
        ],
        "tldr": "The paper introduces BTB3D, a novel causal convolutional encoder-decoder architecture for 3D medical imaging that utilizes compact, frequency-aware volumetric tokens, achieving state-of-the-art results in report generation and text-to-CT synthesis.",
        "tldr_zh": "该论文介绍了BTB3D，一种用于3D医学影像的新型因果卷积编码器-解码器架构，它利用紧凑的、频率感知的体积令牌，在报告生成和文本到CT合成方面取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence",
        "summary": "Most video reasoning models only generate textual reasoning traces without\nindicating when and where key evidence appears. Recent models such as OpenAI-o3\nhave sparked wide interest in evidence-centered reasoning for images, yet\nextending this ability to videos is more challenging, as it requires joint\ntemporal tracking and spatial localization across dynamic scenes. We introduce\nOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporal\nevidence into video reasoning, and carefully collect training data and design\ntraining strategies to address the aforementioned challenges. The model\nhighlights key timestamps, objects, and bounding boxes alongside its answers,\nallowing reasoning to be grounded in concrete visual observations. To enable\nthis functionality, we first curate and build two high-quality datasets,\nSTGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed\ntemporal and spatial annotations, since most existing datasets offer either\ntemporal spans for videos or spatial boxes on images, lacking unified\nspatio-temporal supervision and reasoning traces. Then, we adopt a cold-start\nreinforcement learning strategy with multiple specially designed rewards that\njointly encourage answer accuracy, temporal alignment, and spatial precision.\nOn V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,\nraising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent\nimprovements are also observed on a broad range of video understanding\nbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond\naccuracy, the reasoning traces produced by Open-o3 Video also provide valuable\nsignals for test-time scaling, enabling confidence-aware verification and\nimproving answer reliability.",
        "url": "http://arxiv.org/abs/2510.20579v1",
        "published_date": "2025-10-23T14:05:56+00:00",
        "updated_date": "2025-10-23T14:05:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Jiahao Meng",
            "Xiangtai Li",
            "Haochen Wang",
            "Yue Tan",
            "Tao Zhang",
            "Lingdong Kong",
            "Yunhai Tong",
            "Anran Wang",
            "Zhiyang Teng",
            "Yujing Wang",
            "Zhuochen Wang"
        ],
        "tldr": "The paper introduces Open-o3 Video, a framework for grounded video reasoning that incorporates explicit spatio-temporal evidence, along with two new datasets (STGR-CoT-30k and STGR-RL-36k) and a reinforcement learning strategy to improve accuracy and reliability. It achieves SOTA results on V-STAR and demonstrates improvements on other video understanding benchmarks.",
        "tldr_zh": "该论文介绍了Open-o3 Video，一个基于显式时空证据的视频推理框架，以及两个新的数据集（STGR-CoT-30k和STGR-RL-36k）和一个强化学习策略，以提高准确性和可靠性。它在V-STAR上取得了SOTA结果，并在其他视频理解基准测试中展示了改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence",
        "summary": "The realization of Artificial General Intelligence (AGI) necessitates\nEmbodied AI agents capable of robust spatial perception, effective task\nplanning, and adaptive execution in physical environments. However, current\nlarge language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks\nsuffer from key limitations, including a significant gap between model design\nand agent requirements, an unavoidable trade-off between real-time latency and\nperformance, and the use of unauthentic, offline evaluation metrics. To address\nthese challenges, we propose EmbodiedBrain, a novel vision-language foundation\nmodel available in both 7B and 32B parameter sizes. Our framework features an\nagent-aligned data structure and employs a powerful training methodology that\nintegrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group\nRelative Policy Optimization (Step-GRPO), which boosts long-horizon task\nsuccess by integrating preceding steps as Guided Precursors. Furthermore, we\nincorporate a comprehensive reward system, including a Generative Reward Model\n(GRM) accelerated at the infrastructure level, to improve training efficiency.\nFor enable thorough validation, we establish a three-part evaluation system\nencompassing General, Planning, and End-to-End Simulation Benchmarks,\nhighlighted by the proposal and open-sourcing of a novel, challenging\nsimulation environment. Experimental results demonstrate that EmbodiedBrain\nachieves superior performance across all metrics, establishing a new\nstate-of-the-art for embodied foundation models. Towards paving the way for the\nnext generation of generalist embodied agents, we open-source all of our data,\nmodel weight, and evaluating methods, which are available at\nhttps://zterobot.github.io/EmbodiedBrain.github.io.",
        "url": "http://arxiv.org/abs/2510.20578v1",
        "published_date": "2025-10-23T14:05:55+00:00",
        "updated_date": "2025-10-23T14:05:55+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ding Zou",
            "Feifan Wang",
            "Mengyu Ge",
            "Siyuan Fan",
            "Zongbing Zhang",
            "Wei Chen",
            "Lingfeng Wang",
            "Zhongyou Hu",
            "Wenrui Yan",
            "Zhengwei Gao",
            "Hao Wang",
            "Weizhao Jin",
            "Yu Zhang",
            "Hainan Zhao",
            "Mingliang Zhang",
            "Xianxian Xi",
            "Yaru Zhang",
            "Wenyuan Li",
            "Zhengguang Gao",
            "Yurui Zhu"
        ],
        "tldr": "The paper introduces EmbodiedBrain, a new vision-language foundation model for embodied AI agents, claiming state-of-the-art performance through a novel training methodology and evaluation system, with open-sourced resources.",
        "tldr_zh": "该论文介绍了EmbodiedBrain，一种用于具身人工智能代理的新型视觉-语言基础模型，声称通过一种新颖的训练方法和评估系统实现了最先进的性能，并开源了相关资源。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning",
        "summary": "Inspired by recent advancements in LLM reasoning, the field of multimodal\nreasoning has seen remarkable progress, achieving significant performance gains\non intricate tasks such as mathematical problem-solving. Despite this progress,\ncurrent multimodal large reasoning models exhibit two key limitations. They\ntend to employ computationally expensive reasoning even for simple queries,\nleading to inefficiency. Furthermore, this focus on specialized reasoning often\nimpairs their broader, more general understanding capabilities. In this paper,\nwe propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed\nto address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by\nstructuring the original dense model into two distinct expert branches: a\nthinking branch tailored for complex, multi-step reasoning, and a non-thinking\nbranch optimized for rapid, direct inference on tasks like general VQA and OCR.\nA lightweight, trainable router dynamically allocates queries to the most\nsuitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into\nan MoE architecture. Comprehensive evaluations reveal that our approach not\nonly substantially enhances complex reasoning abilities but also improves the\nmodel's general capabilities, reversing the degradation trend observed in other\nreasoning-specialized models. Our work establishes a new paradigm for building\npowerful and versatile MLLMs, effectively resolving the prevalent\nreasoning-vs-generalization dilemma.",
        "url": "http://arxiv.org/abs/2510.20519v1",
        "published_date": "2025-10-23T13:02:49+00:00",
        "updated_date": "2025-10-23T13:02:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaohan Lan",
            "Fanfan Liu",
            "Haibo Qiu",
            "Siqi Yang",
            "Delian Ruan",
            "Peng Shi",
            "Lin Ma"
        ],
        "tldr": "The paper introduces Metis-HOME, a Hybrid Optimized Mixture-of-Experts framework for multimodal reasoning that balances complex reasoning and general understanding by using separate expert branches and a dynamic router, improving both capabilities.",
        "tldr_zh": "该论文介绍了Metis-HOME，一个混合优化的混合专家框架，用于多模态推理，通过使用独立的专家分支和一个动态路由器来平衡复杂推理和通用理解，从而提高两者的能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence",
        "summary": "Video reasoning, which requires multi-step deduction across frames, remains a\nmajor challenge for multimodal large language models (MLLMs). While\nreinforcement learning (RL)-based methods enhance reasoning capabilities, they\noften rely on text-only chains that yield ungrounded or hallucinated\nconclusions. Conversely, frame-retrieval approaches introduce visual grounding\nbut still struggle with inaccurate evidence localization. To address these\nchallenges, we present Conan, a framework for evidence-grounded multi-step\nvideo reasoning. Conan identifies contextual and evidence frames, reasons over\ncross-frame clues, and adaptively decides when to conclude or explore further.\nTo achieve this, we (1) construct Conan-91K, a large-scale dataset of\nautomatically generated reasoning traces that includes frame identification,\nevidence reasoning, and action decision, and (2) design a multi-stage\nprogressive cold-start strategy combined with an\nIdentification-Reasoning-Action (AIR) RLVR training framework to jointly\nenhance multi-step visual reasoning. Extensive experiments on six multi-step\nreasoning benchmarks demonstrate that Conan surpasses the baseline\nQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving\nstate-of-the-art performance. Furthermore, Conan generalizes effectively to\nlong-video understanding tasks, validating its strong scalability and\nrobustness.",
        "url": "http://arxiv.org/abs/2510.20470v1",
        "published_date": "2025-10-23T12:11:46+00:00",
        "updated_date": "2025-10-23T12:11:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kun Ouyang",
            "Yuanxin Liu",
            "Linli Yao",
            "Yishuo Cai",
            "Hao Zhou",
            "Jie Zhou",
            "Fandong Meng",
            "Xu Sun"
        ],
        "tldr": "The paper introduces Conan, a framework for evidence-grounded multi-step video reasoning using a new dataset (Conan-91K) and a multi-stage progressive RL training strategy (AIR) to improve accuracy and scalability compared to existing MLLMs. It outperforms Qwen2.5-VL-7B-Instruct by over 10% on several benchmarks.",
        "tldr_zh": "该论文介绍了 Conan，一个基于证据的多步骤视频推理框架，它使用一个新的数据集 (Conan-91K) 和一个多阶段渐进式 RL 训练策略 (AIR)，与现有的 MLLM 相比，提高了准确性和可扩展性。 在多个基准测试中，它的性能比 Qwen2.5-VL-7B-Instruct 高出 10% 以上。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
        "summary": "Multi-modal large language models (MLLMs) have emerged as a transformative\napproach for aligning visual and textual understanding. They typically require\nextremely high computational resources (e.g., thousands of GPUs) for training\nto achieve cross-modal alignment at multi-granularity levels. We argue that a\nkey source of this inefficiency lies in the vision encoders they widely equip\nwith, e.g., CLIP and SAM, which lack the alignment with language at\nmulti-granularity levels. To address this issue, in this paper, we leverage\nhyperbolic space, which inherently models hierarchical levels and thus provides\na principled framework for bridging the granularity gap between visual and\ntextual modalities at an arbitrary granularity level. Concretely, we propose an\nefficient training paradigm for MLLMs, dubbed as HyperET, which can optimize\nvisual representations to align with their textual counterparts at an arbitrary\ngranularity level through dynamic hyperbolic radius adjustment in hyperbolic\nspace. HyperET employs learnable matrices with M\\\"{o}bius multiplication\noperations, implemented via three effective configurations: diagonal scaling\nmatrices, block-diagonal matrices, and banded matrices, providing a flexible\nyet efficient parametrization strategy. Comprehensive experiments across\nmultiple MLLM benchmarks demonstrate that HyperET consistently improves both\nexisting pre-training and fine-tuning MLLMs clearly with less than 1\\%\nadditional parameters.",
        "url": "http://arxiv.org/abs/2510.20322v1",
        "published_date": "2025-10-23T08:16:44+00:00",
        "updated_date": "2025-10-23T08:16:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zelin Peng",
            "Zhengqin Xu",
            "Qingyang Liu",
            "Xiaokang Yang",
            "Wei Shen"
        ],
        "tldr": "The paper introduces HyperET, an efficient training paradigm for MLLMs that utilizes hyperbolic space to improve cross-modal alignment by aligning visual representations with textual counterparts through dynamic hyperbolic radius adjustment, achieving performance gains with minimal additional parameters.",
        "tldr_zh": "该论文介绍了HyperET，一种用于多模态大型语言模型的高效训练方法，它利用双曲空间通过动态双曲半径调整来对齐视觉表示和文本表示，从而改善跨模态对齐，并以极少的额外参数实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning",
        "summary": "GUI grounding, which maps natural-language instructions to actionable UI\nelements, is a core capability of GUI agents. Prior works largely treats\ninstructions as a static proxy for user intent, overlooking the impact of\ninstruction diversity and quality on grounding performance. Through a careful\ninvestigation of existing grounding datasets, we find a 23.3% flaw rate in\ntheir instructions and show that inference-time exploitation of instruction\ndiversity yields up to a substantial 76% relative performance improvement. In\nthis paper, we introduce the Instruction-as-Reasoning paradigm, treating\ninstructions as dynamic analytical pathways that offer distinct perspectives\nand enabling the model to select the most effective pathway during reasoning.\nTo achieve this, we propose a two-stage training framework: supervised\nfine-tuning (SFT) on synthesized, diverse instructions to instill\nmulti-perspective reasoning, followed by reinforcement learning (RL) to\noptimize pathway selection and composition. Our resulting models, UI-Ins-7B and\nUI-Ins-32B, achieve state-of-the-art results on five challenging grounding\nbenchmarks and exhibit emergent reasoning, selectively composing and\nsynthesizing novel instruction pathways at inference. In particular, UI-Ins-32B\nattains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on\nScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model\ndemonstrates strong agentic potential, achieving a 74.1% success rate on\nAndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals\nadditional insights such as how reasoning can be formulated to enhance rather\nthan hinder grounding performance, and how our method mitigates policy collapse\nin the SFT+RL framework. All code and model checkpoints will be publicly\nreleased in https://github.com/alibaba/UI-Ins.",
        "url": "http://arxiv.org/abs/2510.20286v1",
        "published_date": "2025-10-23T07:18:32+00:00",
        "updated_date": "2025-10-23T07:18:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Liangyu Chen",
            "Hanzhang Zhou",
            "Chenglin Cai",
            "Jianan Zhang",
            "Panrong Tong",
            "Quyu Kong",
            "Xu Zhang",
            "Chen Liu",
            "Yuqi Liu",
            "Wenxuan Wang",
            "Yue Wang",
            "Qin Jin",
            "Steven Hoi"
        ],
        "tldr": "The paper introduces UI-Ins, a framework that enhances GUI grounding by treating instructions as reasoning pathways and uses a two-stage SFT+RL training approach, achieving SOTA results on multiple benchmarks and showing emergent reasoning capabilities.",
        "tldr_zh": "该论文介绍了UI-Ins，一个通过将指令视为推理路径来增强GUI基础的框架，并使用两阶段SFT+RL训练方法，在多个基准测试中取得了SOTA结果，并展示了涌现推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context",
        "summary": "Large Vision-Language Models (LVLMs) have made significant progress in recent\nyears but are also prone to hallucination issues. They exhibit more\nhallucinations in longer, free-form responses, often attributed to accumulated\nuncertainties. In this paper, we ask: Does increased hallucination result\nsolely from length-induced errors, or is there a deeper underlying mechanism?\nAfter a series of preliminary experiments and findings, we suggest that the\nrisk of hallucinations is not caused by length itself but by the increased\nreliance on context for coherence and completeness in longer responses.\nBuilding on these insights, we propose a novel \"induce-detect-suppress\"\nframework that actively induces hallucinations through deliberately designed\ncontexts, leverages induced instances for early detection of high-risk cases,\nand ultimately suppresses potential object-level hallucinations during actual\ndecoding. Our approach achieves consistent, significant improvements across all\nbenchmarks, demonstrating its efficacy. The strong detection and improved\nhallucination mitigation not only validate our framework but, more importantly,\nre-validate our hypothesis on context. Rather than solely pursuing performance\ngains, this study aims to provide new insights and serves as a first step\ntoward a deeper exploration of hallucinations in LVLMs' longer responses.",
        "url": "http://arxiv.org/abs/2510.20229v1",
        "published_date": "2025-10-23T05:22:07+00:00",
        "updated_date": "2025-10-23T05:22:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Ge Zheng",
            "Jiaye Qian",
            "Jiajin Tang",
            "Sibei Yang"
        ],
        "tldr": "This paper investigates why LVLMs hallucinate more in longer responses, arguing it's due to increased reliance on context rather than length itself, and proposes an \"induce-detect-suppress\" framework to mitigate this.",
        "tldr_zh": "该论文研究了为什么大型视觉语言模型在较长回复中更容易产生幻觉，认为这是由于对上下文的更多依赖而非长度本身导致的，并提出了一个“诱导-检测-抑制”框架来缓解这个问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models",
        "summary": "This work investigates descriptive captions as an additional source of\nsupervision for biological multimodal foundation models. Images and captions\ncan be viewed as complementary samples from the latent morphospace of a\nspecies, each capturing certain biological traits. Incorporating captions\nduring training encourages alignment with this shared latent structure,\nemphasizing potentially diagnostic characters while suppressing spurious\ncorrelations. The main challenge, however, lies in obtaining faithful,\ninstance-specific captions at scale. This requirement has limited the\nutilization of natural language supervision in organismal biology compared with\nmany other scientific domains. We complement this gap by generating synthetic\ncaptions with multimodal large language models (MLLMs), guided by\nWikipedia-derived visual information and taxon-tailored format examples. These\ndomain-specific contexts help reduce hallucination and yield accurate,\ninstance-based descriptive captions. Using these captions, we train BIOCAP\n(i.e., BIOCLIP with Captions), a biological foundation model that captures rich\nsemantics and achieves strong performance in species classification and\ntext-image retrieval. These results demonstrate the value of descriptive\ncaptions beyond labels in bridging biological images with multimodal foundation\nmodels.",
        "url": "http://arxiv.org/abs/2510.20095v1",
        "published_date": "2025-10-23T00:34:21+00:00",
        "updated_date": "2025-10-23T00:34:21+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Ziheng Zhang",
            "Xinyue Ma",
            "Arpita Chowdhury",
            "Elizabeth G. Campolongo",
            "Matthew J. Thompson",
            "Net Zhang",
            "Samuel Stevens",
            "Hilmar Lapp",
            "Tanya Berger-Wolf",
            "Yu Su",
            "Wei-Lun Chao",
            "Jianyang Gu"
        ],
        "tldr": "The paper introduces BIOCAP, a biological foundation model trained with synthetically generated captions to improve species classification and text-image retrieval, addressing the lack of instance-specific captions in organismal biology.",
        "tldr_zh": "该论文介绍了BIOCAP，一个使用合成生成的标题训练的生物基础模型，旨在提高物种分类和文本-图像检索的性能，解决了生物学领域缺乏实例特定标题的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding",
        "summary": "Long video understanding remains challenging due to its complex, diverse, and\ntemporally scattered content. Although video large language models (Video-LLMs)\ncan process videos lasting tens of minutes, applying them to truly long\nsequences is computationally prohibitive and often leads to unfocused or\ninconsistent reasoning. A promising solution is to select only the most\ninformative frames, yet existing approaches typically ignore temporal\ndependencies or rely on unimodal evidence, limiting their ability to provide\ncomplete and query-relevant context. We propose a Semantic-Visual Consensus\nEvidence Selection (SeViCES) framework for effective and reliable long video\nunderstanding. SeViCES is training-free and model-agnostic, and introduces two\nkey components. The Semantic-Visual Consensus Frame Selection (SVCFS) module\nselects frames through (1) a temporal-aware semantic branch that leverages LLM\nreasoning over captions, and (2) a cluster-guided visual branch that aligns\nembeddings with semantic scores via mutual information. The Answer Consensus\nRefinement (ACR) module further resolves inconsistencies between semantic- and\nvisual-based predictions by fusing evidence and constraining the answer space.\nExtensive experiments on long video understanding benchmarks show that SeViCES\nconsistently outperforms state-of-the-art methods in both accuracy and\nrobustness, demonstrating the importance of consensus-driven evidence selection\nfor Video-LLMs.",
        "url": "http://arxiv.org/abs/2510.20622v1",
        "published_date": "2025-10-23T14:55:28+00:00",
        "updated_date": "2025-10-23T14:55:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Sheng",
            "Yanbin Hao",
            "Chenxu Li",
            "Shuo Wang",
            "Xiangnan He"
        ],
        "tldr": "The paper introduces SeViCES, a training-free, model-agnostic framework for long video understanding that selects informative frames using semantic-visual consensus to improve accuracy and robustness of Video-LLMs.",
        "tldr_zh": "该论文介绍了一种名为SeViCES的框架，它无需训练且与模型无关，通过语义-视觉共识选择信息量大的帧，从而提高Video-LLM的准确性和鲁棒性，用于长视频理解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization",
        "summary": "Recent advances in accelerating text-to-image (T2I) diffusion models have\nenabled the synthesis of high-fidelity images even in a single step. However,\npersonalizing these models to incorporate novel concepts remains a challenge\ndue to the limited capacity of one-step models to capture new concept\ndistributions effectively. We propose a bidirectional concept distillation\nframework, EchoDistill, to enable one-step diffusion personalization (1-SDP).\nOur approach involves an end-to-end training process where a multi-step\ndiffusion model (teacher) and a one-step diffusion model (student) are trained\nsimultaneously. The concept is first distilled from the teacher model to the\nstudent, and then echoed back from the student to the teacher. During the\nEchoDistill, we share the text encoder between the two models to ensure\nconsistent semantic understanding. Following this, the student model is\noptimized with adversarial losses to align with the real image distribution and\nwith alignment losses to maintain consistency with the teacher's output.\nFurthermore, we introduce the bidirectional echoing refinement strategy,\nwherein the student model leverages its faster generation capability to\nfeedback to the teacher model. This bidirectional concept distillation\nmechanism not only enhances the student ability to personalize novel concepts\nbut also improves the generative quality of the teacher model. Our experiments\ndemonstrate that this collaborative framework significantly outperforms\nexisting personalization methods over the 1-SDP setup, establishing a novel\nparadigm for rapid and effective personalization in T2I diffusion models.",
        "url": "http://arxiv.org/abs/2510.20512v1",
        "published_date": "2025-10-23T12:56:33+00:00",
        "updated_date": "2025-10-23T12:56:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixiong Yang",
            "Tao Wu",
            "Senmao Li",
            "Shiqi Yang",
            "Yaxing Wang",
            "Joost van de Weijer",
            "Kai Wang"
        ],
        "tldr": "The paper introduces EchoDistill, a bidirectional concept distillation framework for one-step diffusion model personalization, enhancing both student and teacher model performance through collaborative training.",
        "tldr_zh": "该论文介绍了EchoDistill，一种用于单步扩散模型个性化的双向概念蒸馏框架，通过协同训练提高学生和教师模型的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding",
        "summary": "Video Temporal Grounding (VTG) aims to localize temporal segments in long,\nuntrimmed videos that align with a given natural language query. This task\ntypically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection\n(HD). While recent advances have been progressed by powerful pretrained\nvision-language models such as CLIP and InternVideo2, existing approaches\ncommonly treat all text tokens uniformly during crossmodal attention,\ndisregarding their distinct semantic roles. To validate the limitations of this\napproach, we conduct controlled experiments demonstrating that VTG models\noverly rely on [EOS]-driven global semantics while failing to effectively\nutilize word-level signals, which limits their ability to achieve fine-grained\ntemporal alignment. Motivated by this limitation, we propose DualGround, a\ndual-branch architecture that explicitly separates global and local semantics\nby routing the [EOS] token through a sentence-level path and clustering word\ntokens into phrase-level units for localized grounding. Our method introduces\n(1) tokenrole- aware cross modal interaction strategies that align video\nfeatures with sentence-level and phrase-level semantics in a structurally\ndisentangled manner, and (2) a joint modeling framework that not only improves\nglobal sentence-level alignment but also enhances finegrained temporal\ngrounding by leveraging structured phrase-aware context. This design allows the\nmodel to capture both coarse and localized semantics, enabling more expressive\nand context-aware video grounding. DualGround achieves state-of-the-art\nperformance on both Moment Retrieval and Highlight Detection tasks across\nQVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of\ndisentangled semantic modeling in video-language alignment.",
        "url": "http://arxiv.org/abs/2510.20244v1",
        "published_date": "2025-10-23T05:53:01+00:00",
        "updated_date": "2025-10-23T05:53:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Minseok Kang",
            "Minhyeok Lee",
            "Minjung Kim",
            "Donghyeong Kim",
            "Sangyoun Lee"
        ],
        "tldr": "This paper introduces DualGround, a dual-branch architecture for Video Temporal Grounding (VTG) that explicitly separates and utilizes global sentence-level and local phrase-level semantics to improve fine-grained temporal alignment, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了 DualGround，一种用于视频时序定位 (VTG) 的双分支架构，它显式地分离和利用全局句子级和局部短语级语义，以提高细粒度时间对齐，并取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EditInfinity: Image Editing with Binary-Quantized Generative Models",
        "summary": "Adapting pretrained diffusion-based generative models for text-driven image\nediting with negligible tuning overhead has demonstrated remarkable potential.\nA classical adaptation paradigm, as followed by these methods, first infers the\ngenerative trajectory inversely for a given source image by image inversion,\nthen performs image editing along the inferred trajectory guided by the target\ntext prompts. However, the performance of image editing is heavily limited by\nthe approximation errors introduced during image inversion by diffusion models,\nwhich arise from the absence of exact supervision in the intermediate\ngenerative steps. To circumvent this issue, we investigate the\nparameter-efficient adaptation of VQ-based generative models for image editing,\nand leverage their inherent characteristic that the exact intermediate\nquantized representations of a source image are attainable, enabling more\neffective supervision for precise image inversion. Specifically, we propose\n\\emph{EditInfinity}, which adapts \\emph{Infinity}, a binary-quantized\ngenerative model, for image editing. We propose an efficient yet effective\nimage inversion mechanism that integrates text prompting rectification and\nimage style preservation, enabling precise image inversion. Furthermore, we\ndevise a holistic smoothing strategy which allows our \\emph{EditInfinity} to\nperform image editing with high fidelity to source images and precise semantic\nalignment to the text prompts. Extensive experiments on the PIE-Bench benchmark\nacross \"add\", \"change\", and \"delete\" editing operations, demonstrate the\nsuperior performance of our model compared to state-of-the-art diffusion-based\nbaselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.",
        "url": "http://arxiv.org/abs/2510.20217v1",
        "published_date": "2025-10-23T05:06:24+00:00",
        "updated_date": "2025-10-23T05:06:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahuan Wang",
            "Yuxin Chen",
            "Jun Yu",
            "Guangming Lu",
            "Wenjie Pei"
        ],
        "tldr": "The paper introduces EditInfinity, a method for text-driven image editing using binary-quantized generative models, addressing the limitations of diffusion-based models by leveraging exact intermediate quantized representations for precise image inversion. It demonstrates superior performance on the PIE-Bench benchmark.",
        "tldr_zh": "该论文介绍了EditInfinity，一种使用二元量化生成模型进行文本驱动图像编辑的方法。它通过利用精确的中间量化表示进行精确的图像反演，解决了基于扩散模型方法的局限性，并在PIE-Bench基准测试中表现出卓越的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval",
        "summary": "Existing approaches for image-to-recipe retrieval have the implicit\nassumption that a food image can fully capture the details textually documented\nin its recipe. However, a food image only reflects the visual outcome of a\ncooked dish and not the underlying cooking process. Consequently, learning\ncross-modal representations to bridge the modality gap between images and\nrecipes tends to ignore subtle, recipe-specific details that are not visually\napparent but are crucial for recipe retrieval. Specifically, the\nrepresentations are biased to capture the dominant visual elements, resulting\nin difficulty in ranking similar recipes with subtle differences in use of\ningredients and cooking methods. The bias in representation learning is\nexpected to be more severe when the training data is mixed of images and\nrecipes sourced from different cuisines. This paper proposes a novel causal\napproach that predicts the culinary elements potentially overlooked in images,\nwhile explicitly injecting these elements into cross-modal representation\nlearning to mitigate biases. Experiments are conducted on the standard\nmonolingual Recipe1M dataset and a newly curated multilingual multicultural\ncuisine dataset. The results indicate that the proposed causal representation\nlearning is capable of uncovering subtle ingredients and cooking actions and\nachieves impressive retrieval performance on both monolingual and multilingual\nmulticultural datasets.",
        "url": "http://arxiv.org/abs/2510.20393v1",
        "published_date": "2025-10-23T09:43:43+00:00",
        "updated_date": "2025-10-23T09:43:43+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Qing Wang",
            "Chong-Wah Ngo",
            "Yu Cao",
            "Ee-Peng Lim"
        ],
        "tldr": "This paper proposes a causal approach to mitigate representation bias in image-to-recipe retrieval, particularly in multicultural datasets, by predicting and injecting overlooked culinary elements into cross-modal representation learning.",
        "tldr_zh": "本文提出了一种因果方法，通过预测并将被忽略的烹饪元素注入跨模态表征学习中，来减轻图像到食谱检索中的表征偏差，尤其是在多元文化数据集中。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization",
        "summary": "We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone\nNavigation. The task retrieves the most relevant geo-referenced image from a\nlarge multi-platform corpus (satellite/drone/ground) given a natural-language\nquery. Two obstacles are severe inter-platform heterogeneity and a domain gap\nbetween generic training descriptions and platform-specific test queries. We\nmitigate these with a domain-aligned preprocessing pipeline and a\nMixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite\naugmentation, and removal of orientation words; (ii) an LLM-based caption\nrefinement pipeline to align textual semantics with the distinct visual\ncharacteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we\ntrain three platform experts using a progressive two-stage, hard-negative\nmining strategy to enhance discriminative power, and fuse their scores at\ninference. The system tops the official leaderboard, demonstrating robust\ncross-modal geo-localization under heterogeneous viewpoints.",
        "url": "http://arxiv.org/abs/2510.20291v1",
        "published_date": "2025-10-23T07:23:47+00:00",
        "updated_date": "2025-10-23T07:23:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "LinFeng Li",
            "Jian Zhao",
            "Zepeng Yang",
            "Yuhang Song",
            "Bojun Lin",
            "Tianle Zhang",
            "Yuchen Yuan",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "tldr": "The paper presents a mixture-of-experts framework for cross-modal geo-localization, specifically addressing the RoboSense 2025 competition. It uses domain alignment and LLM-based caption refinement to bridge the gap between text and heterogeneous visual platforms.",
        "tldr_zh": "该论文提出了一种用于跨模态地理定位的混合专家框架，专门针对RoboSense 2025竞赛。它使用领域对齐和基于LLM的标题优化来弥合文本和异构视觉平台之间的差距。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Breakdance Video classification in the age of Generative AI",
        "summary": "Large Vision Language models have seen huge application in several sports\nuse-cases recently. Most of these works have been targeted towards a limited\nsubset of popular sports like soccer, cricket, basketball etc; focusing on\ngenerative tasks like visual question answering, highlight generation. This\nwork analyzes the applicability of the modern video foundation models (both\nencoder and decoder) for a very niche but hugely popular dance sports -\nbreakdance. Our results show that Video Encoder models continue to outperform\nstate-of-the-art Video Language Models for prediction tasks. We provide\ninsights on how to choose the encoder model and provide a thorough analysis\ninto the workings of a finetuned decoder model for breakdance video\nclassification.",
        "url": "http://arxiv.org/abs/2510.20287v1",
        "published_date": "2025-10-23T07:18:54+00:00",
        "updated_date": "2025-10-23T07:18:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sauptik Dhar",
            "Naveen Ramakrishnan",
            "Michelle Munson"
        ],
        "tldr": "This paper explores the application of video foundation models for breakdance video classification, finding that video encoder models outperform video language models for prediction tasks in this niche sport.",
        "tldr_zh": "本文探讨了视频基础模型在霹雳舞视频分类中的应用，发现视频编码器模型在这项小众运动的预测任务中优于视频语言模型。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 6
    },
    {
        "title": "Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment",
        "summary": "This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for\nlung cancer (LC) classification, leveraging dynamic fuzzy logic-driven\nknowledge distillation (KD) to address uncertainty and complexity in disease\ndiagnosis. Unlike traditional models that rely on static KD with fixed weights,\nour method dynamically adjusts the distillation weight using fuzzy logic,\nenabling the student model to focus on high-confidence regions while reducing\nattention to ambiguous areas. This dynamic adjustment improves the model\nability to handle varying uncertainty levels across different regions of LC\nimages. We employ the Vision Transformer (ViT-B32) as the instructor model,\nwhich effectively transfers knowledge to the student model, MobileNet,\nenhancing the student generalization capabilities. The training process is\nfurther optimized using a dynamic wait adjustment mechanism that adapts the\ntraining procedure for improved convergence and performance. To enhance image\nquality, we introduce pixel-level image fusion improvement techniques such as\nGamma correction and Histogram Equalization. The processed images (Pix1 and\nPix2) are fused using a wavelet-based fusion method to improve image resolution\nand feature preservation. This fusion method uses the wavedec2 function to\nstandardize images to a 224x224 resolution, decompose them into multi-scale\nfrequency components, and recursively average coefficients at each level for\nbetter feature representation. To address computational efficiency, Genetic\nAlgorithm (GA) is used to select the most suitable pre-trained student model\nfrom a pool of 12 candidates, balancing model performance with computational\ncost. The model is evaluated on two datasets, including LC25000\nhistopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images\n(99.54% accuracy), demonstrating robustness across different imaging domains.",
        "url": "http://arxiv.org/abs/2510.20438v1",
        "published_date": "2025-10-23T11:19:52+00:00",
        "updated_date": "2025-10-23T11:19:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Saif Ur Rehman Khan",
            "Muhammad Nabeel Asim",
            "Sebastian Vollmer",
            "Andreas Dengel"
        ],
        "tldr": "The paper introduces FuzzyDistillViT-MobileNet, a novel lung cancer classification model using fuzzy logic-driven knowledge distillation and a genetic algorithm for student model selection, achieving high accuracy on histopathological and CT-scan images.",
        "tldr_zh": "该论文介绍了 FuzzyDistillViT-MobileNet，一种新型肺癌分类模型，采用模糊逻辑驱动的知识蒸馏和遗传算法进行学生模型选择，在组织病理学和 CT 扫描图像上实现了高精度。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]