[
    {
        "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
        "summary": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",
        "url": "http://arxiv.org/abs/2512.21337v1",
        "published_date": "2025-12-24T18:59:54+00:00",
        "updated_date": "2025-12-24T18:59:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Li-Zhong Szu-Tu",
            "Ting-Lin Wu",
            "Chia-Jui Chang",
            "He Syu",
            "Yu-Lun Liu"
        ],
        "tldr": "The paper introduces the YearGuessr dataset and benchmark to expose popularity bias in VLMs, showing they perform significantly better on famous buildings due to memorization, revealing a critical flaw in their reasoning.",
        "tldr_zh": "该论文介绍了YearGuessr数据集和基准，旨在揭示视觉语言模型（VLM）中的流行度偏差。研究表明，由于记忆效应，VLM在著名建筑物上的表现明显优于普通建筑物，揭示了其推理能力的关键缺陷。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Streaming Video Instruction Tuning",
        "summary": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",
        "url": "http://arxiv.org/abs/2512.21334v1",
        "published_date": "2025-12-24T18:59:36+00:00",
        "updated_date": "2025-12-24T18:59:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaer Xia",
            "Peixian Chen",
            "Mengdan Zhang",
            "Xing Sun",
            "Kaiyang Zhou"
        ],
        "tldr": "The paper introduces Streamo, a real-time streaming video LLM trained on a large-scale instruction-following dataset to perform diverse streaming video tasks such as narration, action understanding, and time-sensitive question answering.",
        "tldr_zh": "该论文介绍了Streamo，一个实时流视频LLM，通过大规模指令跟随数据集进行训练，以执行各种流视频任务，如叙述、动作理解和时间敏感的问答。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fast SAM2 with Text-Driven Token Pruning",
        "summary": "Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.",
        "url": "http://arxiv.org/abs/2512.21333v1",
        "published_date": "2025-12-24T18:59:05+00:00",
        "updated_date": "2025-12-24T18:59:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Avilasha Mandal",
            "Chaoning Zhang",
            "Fachrina Dewi Puspitasari",
            "Xudong Wang",
            "Jiaquan Zhang",
            "Caiyan Qin",
            "Guoqing Wang",
            "Yang Yang",
            "Heng Tao Shen"
        ],
        "tldr": "This paper introduces a text-guided token pruning method for SAM2 to reduce computational cost and memory usage in video object segmentation by selectively pruning irrelevant tokens before temporal propagation, resulting in faster inference and lower memory usage while maintaining segmentation accuracy.",
        "tldr_zh": "本文提出了一种针对SAM2的文本引导式token剪枝方法，通过在时序传播之前选择性地剪枝不相关的token，从而降低视频对象分割的计算成本和内存使用，从而在保持分割精度的同时实现更快的推理速度和更低的内存使用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]