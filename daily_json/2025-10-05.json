[
    {
        "title": "Exploring Instruction Data Quality for Explainable Image Quality Assessment",
        "summary": "In recent years, with the rapid development of powerful multimodal large\nlanguage models (MLLMs), explainable image quality assessment (IQA) has\ngradually become popular, aiming at providing quality-related descriptions and\nanswers of images. To achieve this goal, recent methods seek to construct a\nlarge-scale instruction tuning dataset to empower the MLLM with quality\nperception ability following the well-known scaling law. However, a large\namount of instruction tuning data may cause substantial computational costs and\nredundant data, which in turn will cause harm to the performance of the model.\nTo cope with this problem, in this paper, we challenge the scaling law and\nsystematically investigate the role of data quality of the instruction tuning\ndataset for explainable IQA. Using a powerful pre-trained MLLM, we first\ninvestigate the changes in model performance after fine-tuning with different\nsizes of instruction tuning data. We find that selecting a subset of the data\nset randomly using an appropriate ratio can even lead to better results than\ntraining with the entire instruction tuning dataset, demonstrating the\nredundancy of current explainable IQA instruction tuning data. Beyond randomly\nsampling a subset, we propose a clustering-based data selection framework with\nthree stages: clustering feature extraction, cluster quota allocation, and\ncluster sampling strategy. Then we systematically analyze the choices of each\nstage and propose a simple but efficient data selection method IQA-Select for\nexplainable IQA. The experimental results demonstrate that IQA-Select can\nachieve 102.1% and 103.7% performance of full fine-tuning using only 10%\nselected data in Q-Bench and AesBench respectively, significantly reducing\ncomputational costs while achieving better performance.",
        "url": "http://arxiv.org/abs/2510.03880v1",
        "published_date": "2025-10-04T17:12:54+00:00",
        "updated_date": "2025-10-04T17:12:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunhao Li",
            "Sijing Wu",
            "Huiyu Duan",
            "Yucheng Zhu",
            "Qi Jia",
            "Guangtao Zhai"
        ],
        "tldr": "This paper challenges the scaling law in explainable Image Quality Assessment (IQA) by demonstrating that high-quality, selected instruction tuning data outperforms full datasets, achieving better performance with significantly reduced computational costs.",
        "tldr_zh": "本文挑战了可解释图像质量评估 (IQA) 中的扩展定律，通过证明高质量、选定的指令调整数据优于完整数据集，从而以显着降低的计算成本实现更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-View Open-Vocabulary Object Detection in Aerial Imagery",
        "summary": "Traditional object detection models are typically trained on a fixed set of\nclasses, limiting their flexibility and making it costly to incorporate new\ncategories. Open-vocabulary object detection addresses this limitation by\nenabling models to identify unseen classes without explicit training.\nLeveraging pretrained models contrastively trained on abundantly available\nground-view image-text classification pairs provides a strong foundation for\nopen-vocabulary object detection in aerial imagery. Domain shifts, viewpoint\nvariations, and extreme scale differences make direct knowledge transfer across\ndomains ineffective, requiring specialized adaptation strategies. In this\npaper, we propose a novel framework for adapting open-vocabulary\nrepresentations from ground-view images to solve object detection in aerial\nimagery through structured domain alignment. The method introduces contrastive\nimage-to-image alignment to enhance the similarity between aerial and\nground-view embeddings and employs multi-instance vocabulary associations to\nalign aerial images with text embeddings. Extensive experiments on the xView,\nDOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.\nOur open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16\nmAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when\ncompared to finetuned closed-vocabulary dataset-specific model performance,\nthus paving the way for more flexible and scalable object detection systems in\naerial applications.",
        "url": "http://arxiv.org/abs/2510.03858v1",
        "published_date": "2025-10-04T16:12:03+00:00",
        "updated_date": "2025-10-04T16:12:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jyoti Kini",
            "Rohit Gupta",
            "Mubarak Shah"
        ],
        "tldr": "This paper introduces a novel framework for adapting open-vocabulary object detection from ground-view images to aerial imagery using contrastive image-to-image alignment and multi-instance vocabulary associations, achieving significant improvements over closed-vocabulary models in zero-shot settings.",
        "tldr_zh": "本文提出了一种新颖的框架，通过对比图像到图像的对齐和多实例词汇关联，将开放词汇目标检测从地面图像适配到航空图像，在零样本设置中相对于封闭词汇模型取得了显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UGround: Towards Unified Visual Grounding with Unrolled Transformers",
        "summary": "We present UGround, a \\textbf{U}nified visual \\textbf{Ground}ing paradigm\nthat dynamically selects intermediate layers across \\textbf{U}nrolled\ntransformers as ``mask as prompt'', diverging from the prevailing pipeline that\nleverages the fixed last hidden layer as ``\\texttt{<SEG>} as prompt''. UGround\naddresses two primary challenges posed by the prevailing paradigm: (1) its\nreliance on the fixed last hidden layer, which sequentially amplifies\ncumulative errors arising from layer-by-layer propagation without intermediate\ncorrection, and (2) its use of \\texttt{<SEG>} as a prompt, which implicitly\nprojects textual embeddings into visual space without explicit spatial cues\n(\\eg, coordinates). Central to UGround is Policy-Prompted Masking, which\ncomprises two key components: Stochastic Skip Connection (SSC) and Mask as\nPrompt (MasP). SSC is a reinforcement learning policy that, via stochastic\nsampling, allows each \\texttt{<SEG>} token to slide across unrolled transformer\nlayers, enabling dynamic layer selection at which it connects to the vision\nmodel (\\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,\nMasP uses the similarity map derived from the \\texttt{<SEG>} token and image\ntokens as a soft logit mask to prompt SAM for mask generation, offering\nexplicit spatial cues through its activation regions. To validate the\neffectiveness of UGround, we, for the first time, have unified visual grounding\nwithin a single framework from an attribute perspective, spanning from\ntraditional refer expression segmentation to newly proposed reasoning\nsegmentation, single-target to multi-target, positive query to false premise\n(empty target). All codes and models are publicly available at\n\\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.",
        "url": "http://arxiv.org/abs/2510.03853v1",
        "published_date": "2025-10-04T15:56:52+00:00",
        "updated_date": "2025-10-04T15:56:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Qian",
            "Xin Yin",
            "Chuanhang Deng",
            "Zhiyuan Peng",
            "Jian Xiong",
            "Wei Zhai",
            "Dejing Dou"
        ],
        "tldr": "UGround introduces a unified visual grounding paradigm using unrolled transformers and reinforcement learning to dynamically select intermediate layers for prompting, addressing limitations in fixed-layer approaches and improving spatial cue integration. It demonstrates unified visual grounding across various segmentation tasks.",
        "tldr_zh": "UGround 提出了一种统一的视觉定位范式，它使用展开的 Transformer 和强化学习来动态选择中间层进行提示，从而解决固定层方法的局限性并改善空间线索的集成。它展示了跨各种分割任务的统一视觉定位。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models",
        "summary": "Recent advances in image generation models have led to models that produce\nsynthetic images that are increasingly difficult for standard AI detectors to\nidentify, even though they often remain distinguishable by humans. To identify\nthis discrepancy, we introduce \\textbf{Mirage}, a curated dataset comprising a\ndiverse range of AI-generated images exhibiting visible artifacts, where\ncurrent state-of-the-art detection methods largely fail. Furthermore, we\ninvestigate whether Large Vision-Language Models (LVLMs), which are\nincreasingly employed as substitutes for human judgment in various tasks, can\nbe leveraged for explainable AI image detection. Our experiments on both Mirage\nand existing benchmark datasets demonstrate that while LVLMs are highly\neffective at detecting AI-generated images with visible artifacts, their\nperformance declines when confronted with images lacking such cues.",
        "url": "http://arxiv.org/abs/2510.03840v1",
        "published_date": "2025-10-04T15:38:39+00:00",
        "updated_date": "2025-10-04T15:38:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pranav Sharma",
            "Shivank Garg",
            "Durga Toshniwal"
        ],
        "tldr": "The paper introduces Mirage, a new dataset of AI-generated images with visible artifacts, and investigates using Large Vision-Language Models (LVLMs) for detecting these artifacts, showing LVLMs perform well with visible artifacts but struggle without them.",
        "tldr_zh": "该论文介绍了一个名为Mirage的新数据集，其中包含具有明显伪影的AI生成图像，并研究了使用大型视觉-语言模型（LVLM）检测这些伪影。结果表明，LVLM在检测具有明显伪影的图像方面表现良好，但在没有这些伪影的情况下表现不佳。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Bridging the Gap Between Multimodal Foundation Models and World Models",
        "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.",
        "url": "http://arxiv.org/abs/2510.03727v1",
        "published_date": "2025-10-04T08:14:20+00:00",
        "updated_date": "2025-10-04T08:14:20+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Xuehai He"
        ],
        "tldr": "This paper explores how to improve Multimodal Foundation Models (MFMs) to function as more effective world models by enhancing their reasoning and generative capabilities, including counterfactual reasoning, spatiotemporal understanding, and controllable generation.",
        "tldr_zh": "本文探讨了如何通过增强多模态基础模型（MFM）的推理和生成能力，使其更有效地作为世界模型发挥作用，包括反事实推理、时空理解和可控生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models",
        "summary": "Vision-language models trained on large-scale multimodal datasets show strong\ndemographic biases, but the role of training data in producing these biases\nremains unclear. A major barrier has been the lack of demographic annotations\nin web-scale datasets such as LAION-400M. We address this gap by creating\nperson-centric annotations for the full dataset, including over 276 million\nbounding boxes, perceived gender and race/ethnicity labels, and automatically\ngenerated captions. These annotations are produced through validated automatic\nlabeling pipelines combining object detection, multimodal captioning, and\nfinetuned classifiers. Using them, we uncover demographic imbalances and\nharmful associations, such as the disproportionate linking of men and\nindividuals perceived as Black or Middle Eastern with crime-related and\nnegative content. We also show that 60-70% of gender bias in CLIP and Stable\nDiffusion can be linearly explained by direct co-occurrences in the data. Our\nresources establish the first large-scale empirical link between dataset\ncomposition and downstream model bias.",
        "url": "http://arxiv.org/abs/2510.03721v1",
        "published_date": "2025-10-04T07:51:59+00:00",
        "updated_date": "2025-10-04T07:51:59+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.CY",
            "cs.LG"
        ],
        "authors": [
            "Leander Girrbach",
            "Stephan Alaniz",
            "Genevieve Smith",
            "Trevor Darrell",
            "Zeynep Akata"
        ],
        "tldr": "This paper introduces a large-scale, person-centric annotated dataset for LAION-400M, revealing demographic biases and their impact on vision-language models like CLIP and Stable Diffusion.",
        "tldr_zh": "该论文介绍了一个大规模、以人为中心的 LAION-400M 注释数据集，揭示了人口统计偏差及其对 CLIP 和 Stable Diffusion 等视觉语言模型的影响。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations",
        "summary": "Industrial accidents, particularly in high-risk domains such as surface and\nunderground mining, are frequently caused by unsafe worker behaviors.\nTraditional manual inspection remains labor-intensive, error-prone, and\ninsufficient for large-scale, dynamic environments, highlighting the urgent\nneed for intelligent and automated safety monitoring. In this paper, we present\nMonitorVLM, a novel vision--language framework designed to detect safety\nviolations directly from surveillance video streams. MonitorVLM introduces\nthree key innovations: (1) a domain-specific violation dataset comprising 9,000\nvision--question--answer (VQA) samples across 40 high-frequency mining\nregulations, enriched with augmentation and auxiliary detection cues; (2) a\nclause filter (CF) module that dynamically selects the Top-$K$ most relevant\nclauses, reducing inference latency by 13.56\\% while maintaining accuracy; and\n(3) a behavior magnifier (BM) module that enhances worker regions to improve\nfine-grained action recognition, yielding additional gains of 3.45% in\nprecision and 8.62% in recall. Experimental results demonstrate that MonitorVLM\nsignificantly outperforms baseline vision--language models, achieving\nimprovements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score\nover the 72B unfine-tuned baseline. A lightweight web-based interface further\nintegrates MonitorVLM into practical workflows, enabling automatic violation\nreporting with video timestamping. This study highlights the potential of\nmultimodal large models to enhance occupational safety monitoring in mining and\nbeyond.",
        "url": "http://arxiv.org/abs/2510.03666v1",
        "published_date": "2025-10-04T04:46:21+00:00",
        "updated_date": "2025-10-04T04:46:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiang Wu",
            "Sichao Wu",
            "Yinsong Ma",
            "Guangyuan Yu",
            "Haoyuan Xu",
            "Lifang Zheng",
            "Jingliang Duan"
        ],
        "tldr": "The paper introduces MonitorVLM, a vision-language framework for detecting safety violations in mining operations, featuring a domain-specific dataset, clause filter, and behavior magnifier, achieving significant performance improvements over baselines.",
        "tldr_zh": "该论文介绍了MonitorVLM，一个用于检测矿业安全违规行为的视觉-语言框架，具有特定领域的数数据集、子句过滤器和行为放大器，与基线相比实现了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FrameOracle: Learning What to See and How Much to See in Videos",
        "summary": "Vision-language models (VLMs) have advanced video understanding, but their\nperformance is limited by the number of input frames they can process. Existing\nframe sampling strategies, such as uniform or fixed-budget selection, often\nfail to adapt to variations in information density or task complexity,\nresulting in inefficiency and information loss. To address this, we present\nFrameOracle, a lightweight and plug-and-play module that predicts both (1)\nwhich frames are most relevant to a given query and (2) how many frames are\nneeded. FrameOracle is trained using a four-stage curriculum, with the first\nthree stages relying on weak proxy signals such as cross-modal similarity. In\nthe final stage, it leverages stronger supervision from a new dataset we\nintroduce, FrameOracle-41K, the first large-scale VideoQA collection to provide\nkeyframe annotations specifying the minimal set of frames required to answer\neach question. Extensive experiments across five VLMs and six benchmarks\ndemonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4\nframes without any loss in accuracy. When starting from 64-frame candidates, it\nreduces the input to an average of 13.9 frames while improving accuracy by\n1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable\nvideo understanding.",
        "url": "http://arxiv.org/abs/2510.03584v1",
        "published_date": "2025-10-04T00:24:44+00:00",
        "updated_date": "2025-10-04T00:24:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaoyu Li",
            "Tianzhi Li",
            "Fei Tao",
            "Zhenyu Zhao",
            "Ziqian Wu",
            "Maozheng Zhao",
            "Juntong Song",
            "Cheng Niu",
            "Pooyan Fazli"
        ],
        "tldr": "FrameOracle is a plug-and-play module that learns to select the most relevant frames and determine the optimal number of frames for VLMs, achieving better efficiency-accuracy trade-offs in video understanding. They introduce a new VideoQA dataset with keyframe annotations.",
        "tldr_zh": "FrameOracle是一个即插即用的模块，它学习选择最相关的帧，并确定VLM的最佳帧数，从而在视频理解中实现更好的效率-准确性权衡。他们引入了一个新的VideoQA数据集，带有关键帧注释。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Test-Time Scaling for Small Vision-Language Models",
        "summary": "Small Vision-Language Models (VLMs) provide a computationally efficient\nalternative to larger models, at the cost of weaker generalization abilities\nand downstream task performance. These shortcomings could be addressed by\ntest-time scaling techniques, but existing methods are typically\ncomputationally demanding, contradicting the resource-efficient design goals of\nsmall models. To address these limitations, we propose two novel and efficient\ntest-time scaling strategies that leverage the model-internal features rather\nthan external supervision: (i) Test-Time Augmentation (TTAug), which generates\nmultiple augmented inputs and aggregates outputs at the token level without\nparameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model\nparameters during inference using consensus-based pseudolabels from TTAug.\nThrough extensive experiments across nine benchmarks, we demonstrate consistent\nperformance improvements while maintaining computational efficiency suitable\nfor resource-constrained environments. The generality of our approach is\ndemonstrated both within models at different scales and across different VLMs\nwithout additional tuning.",
        "url": "http://arxiv.org/abs/2510.03574v1",
        "published_date": "2025-10-03T23:49:06+00:00",
        "updated_date": "2025-10-03T23:49:06+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Mehmet Onurcan Kaya",
            "Desmond Elliott",
            "Dim P. Papadopoulos"
        ],
        "tldr": "This paper introduces efficient test-time scaling techniques (TTAug and TTAdapt) for small Vision-Language Models, improving performance without sacrificing computational efficiency.",
        "tldr_zh": "本文介绍了针对小型视觉语言模型的高效测试时缩放技术（TTAug和TTAdapt），在不牺牲计算效率的前提下提高了性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis",
        "summary": "Deep learning for medical imaging is hampered by task-specific models that\nlack generalizability and prognostic capabilities, while existing 'universal'\napproaches suffer from simplistic conditioning and poor medical semantic\nunderstanding. To address these limitations, we introduce DuPLUS, a deep\nlearning framework for efficient multi-modal medical image analysis. DuPLUS\nintroduces a novel vision-language framework that leverages hierarchical\nsemantic prompts for fine-grained control over the analysis task, a capability\nabsent in prior universal models. To enable extensibility to other medical\ntasks, it includes a hierarchical, text-controlled architecture driven by a\nunique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize\nacross three imaging modalities, ten different anatomically various medical\ndatasets, encompassing more than 30 organs and tumor types. It outperforms the\nstate-of-the-art task specific and universal models on 8 out of 10 datasets. We\ndemonstrate extensibility of its text-controlled architecture by seamless\nintegration of electronic health record (EHR) data for prognosis prediction,\nand on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)\nof 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks\nand modalities from varying centers, establishing DuPLUS as a versatile and\nclinically relevant solution for medical image analysis. The code for this work\nis made available at: https://anonymous.4open.science/r/DuPLUS-6C52",
        "url": "http://arxiv.org/abs/2510.03483v1",
        "published_date": "2025-10-03T20:01:00+00:00",
        "updated_date": "2025-10-03T20:01:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Numan Saeed",
            "Tausifa Jan Saleem",
            "Fadillah Maani",
            "Muhammad Ridzuan",
            "Hu Wang",
            "Mohammad Yaqub"
        ],
        "tldr": "The paper introduces DuPLUS, a dual-prompt vision-language framework for universal medical image segmentation and prognosis, outperforming state-of-the-art methods and demonstrating extensibility to EHR data for prognosis prediction.",
        "tldr_zh": "该论文介绍了 DuPLUS，一个双提示视觉-语言框架，用于通用医学图像分割和预后，其性能优于现有技术，并展示了对 EHR 数据进行预后预测的扩展性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning",
        "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still\nface challenges in spatial reasoning for 3D scenes and complex object\nconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM that\nintegrates spatial features like depth maps, 3D coordinates, and edge maps\nthrough a multi-task learning framework. This approach enriches multimodal\nembeddings with spatial understanding. We propose two variants: SpatialViLT and\nMaskedSpatialViLT, focusing on full and masked object regions, respectively.\nAdditionally, SpatialEnsemble combines both approaches, achieving\nstate-of-the-art accuracy. Our models excel in spatial reasoning categories\nsuch as directional, topological, and proximity relations, as demonstrated on\nthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents a\nsignificant step in enhancing the spatial intelligence of AI systems, crucial\nfor advanced multimodal understanding and real-world applications.",
        "url": "http://arxiv.org/abs/2510.03441v1",
        "published_date": "2025-10-03T19:04:15+00:00",
        "updated_date": "2025-10-03T19:04:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "68T45, 68T10, 68T40"
        ],
        "authors": [
            "Chashi Mahiul Islam",
            "Oteo Mamo",
            "Samuel Jacob Chacko",
            "Xiuwen Liu",
            "Weikuan Yu"
        ],
        "tldr": "SpatialViLT enhances Vision-Language Models with spatial reasoning capabilities by integrating depth maps, 3D coordinates, and edge maps through multi-task learning, achieving state-of-the-art accuracy on the Visual Spatial Reasoning (VSR) dataset.",
        "tldr_zh": "SpatialViLT通过多任务学习整合深度图、3D坐标和边缘图等空间特征，增强了视觉语言模型的空间推理能力，并在视觉空间推理(VSR)数据集上实现了最先进的精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy",
        "summary": "Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and\ncolonoscopy play a critical role in diagnosing and managing gastrointestinal\n(GI) disorders. However, the documentation burden associated with these\nprocedures place significant strain on gastroenterologists, contributing to\ninefficiencies in clinical workflows and physician burnout. To address this\nchallenge, we propose a novel automated report generation model that leverages\na transformer-based vision encoder and text decoder within a two-stage training\nframework. In the first stage, both components are pre-trained on image/text\ncaption pairs to capture generalized vision-language features, followed by\nfine-tuning on images/report pairs to generate clinically meaningful findings.\nOur approach not only streamlines the documentation process but also holds\npromise for reducing physician workload and improving patient care.",
        "url": "http://arxiv.org/abs/2510.03543v1",
        "published_date": "2025-10-03T22:25:52+00:00",
        "updated_date": "2025-10-03T22:25:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Evandros Kaklamanos",
            "Kristjana Kristinsdottir",
            "Jonathan Huang",
            "Dustin Carlson",
            "Rajesh Keswani",
            "John Pandolfino",
            "Mozziyar Etemadi"
        ],
        "tldr": "The paper introduces a transformer-based model for automated report generation in gastrointestinal endoscopy, aiming to reduce the documentation burden on gastroenterologists and improve clinical workflows.",
        "tldr_zh": "该论文介绍了一种基于Transformer的模型，用于自动生成胃肠内窥镜检查报告，旨在减轻肠胃科医生的文档负担并改善临床工作流程。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis",
        "summary": "Foundation models (FMs) have transformed computational pathology by providing\npowerful, general-purpose feature extractors. However, adapting and\nbenchmarking individual FMs for specific diagnostic tasks is often\ntime-consuming and resource-intensive, especially given their scale and\ndiversity. To address this challenge, we introduce Group-Aggregative Selection\nMulti-Instance Learning (GAS-MIL), a flexible ensemble framework that\nseamlessly integrates features from multiple FMs, preserving their\ncomplementary strengths without requiring manual feature selection or extensive\ntask-specific fine-tuning. Across classification tasks in three cancer\ndatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL\nconsistently achieves superior or on-par performance relative to individual FMs\nand established MIL methods, demonstrating its robustness and generalizability.\nBy enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines\nmodel deployment for pathology and provides a scalable foundation for future\nmultimodal and precision oncology applications.",
        "url": "http://arxiv.org/abs/2510.03555v1",
        "published_date": "2025-10-03T22:59:40+00:00",
        "updated_date": "2025-10-03T22:59:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Peiran Quan",
            "Zifan Gu",
            "Zhuo Zhao",
            "Qin Zhou",
            "Donghan M. Yang",
            "Ruichen Rong",
            "Yang Xie",
            "Guanghua Xiao"
        ],
        "tldr": "The paper introduces GAS-MIL, a multi-instance learning framework that ensembles foundation models for digital pathology image analysis, achieving strong performance across multiple cancer datasets without extensive fine-tuning.",
        "tldr_zh": "该论文介绍了一种名为GAS-MIL的多示例学习框架，用于集成基础模型进行数字病理图像分析，在多个癌症数据集上实现了强大的性能，且无需大量微调。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]