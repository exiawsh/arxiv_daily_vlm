[
    {
        "title": "LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving",
        "summary": "Large vision-language models (VLMs) have shown promising capabilities in\nscene understanding, enhancing the explainability of driving behaviors and\ninteractivity with users. Existing methods primarily fine-tune VLMs on on-board\nmulti-view images and scene reasoning text, but this approach often lacks the\nholistic and nuanced scene recognition and powerful spatial awareness required\nfor autonomous driving, especially in complex situations. To address this gap,\nwe propose a novel vision-language framework tailored for autonomous driving,\ncalled LMAD. Our framework emulates modern end-to-end driving paradigms by\nincorporating comprehensive scene understanding and a task-specialized\nstructure with VLMs. In particular, we introduce preliminary scene interaction\nand specialized expert adapters within the same driving task structure, which\nbetter align VLMs with autonomous driving scenarios. Furthermore, our approach\nis designed to be fully compatible with existing VLMs while seamlessly\nintegrating with planning-oriented driving systems. Extensive experiments on\nthe DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts\nthe performance of existing VLMs on driving reasoning tasks,setting a new\nstandard in explainable autonomous driving.",
        "url": "http://arxiv.org/abs/2508.12404v1",
        "published_date": "2025-08-17T15:42:54+00:00",
        "updated_date": "2025-08-17T15:42:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nan Song",
            "Bozhou Zhang",
            "Xiatian Zhu",
            "Jiankang Deng",
            "Li Zhang"
        ],
        "tldr": "The paper introduces LMAD, a vision-language framework tailored for autonomous driving that enhances scene understanding and explainability by incorporating scene interaction and specialized expert adapters within a driving task structure, significantly improving performance on driving reasoning tasks.",
        "tldr_zh": "该论文介绍了一种为自动驾驶量身定制的视觉语言框架LMAD，通过在驾驶任务结构中结合场景交互和专门的专家适配器，增强了场景理解和可解释性，从而显著提高了驾驶推理任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models",
        "summary": "Despite significant advancements, Large Vision-Language Models (LVLMs)\ncontinue to face challenges in complex visual reasoning tasks that demand deep\ncontextual understanding, multi-angle analysis, or meticulous detail\nrecognition. Existing approaches often rely on single-shot image encoding and\nprompts, limiting their ability to fully capture nuanced visual information.\nInspired by the notion that strategically generated \"additional\" information\ncan serve as beneficial contextual augmentation, we propose Multi-Perspective\nContextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy\ndesigned to enhance LVLM performance. MPCAR operates in three stages: first, an\nLVLM generates N diverse and complementary descriptions or preliminary\nreasoning paths from various angles; second, these descriptions are\nintelligently integrated with the original question to construct a\ncomprehensive context-augmented prompt; and finally, this enriched prompt\nguides the ultimate LVLM for deep reasoning and final answer generation.\nCrucially, MPCAR achieves these enhancements without requiring any fine-tuning\nof the underlying LVLM's parameters. Extensive experiments on challenging\nVisual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and\nScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms\nestablished baseline methods. Our quantitative results show significant\naccuracy gains, particularly on tasks requiring robust contextual\nunderstanding, while human evaluations confirm improved coherence and\ncompleteness of the generated answers. Ablation studies further highlight the\nimportance of diverse prompt templates and the number of generated\nperspectives. This work underscores the efficacy of leveraging LVLMs' inherent\ngenerative capabilities to enrich input contexts, thereby unlocking their\nlatent reasoning potential for complex multimodal tasks.",
        "url": "http://arxiv.org/abs/2508.12400v1",
        "published_date": "2025-08-17T15:25:01+00:00",
        "updated_date": "2025-08-17T15:25:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Amirul Rahman",
            "Qiang Xu",
            "Xueying Huang"
        ],
        "tldr": "The paper introduces MPCAR, an inference-time strategy for improving LVLM performance on complex visual reasoning tasks by generating diverse contextual descriptions and integrating them into the prompt, achieving significant accuracy gains without fine-tuning.",
        "tldr_zh": "本文介绍了一种名为MPCAR的推理时策略，通过生成多样化的上下文描述并将其整合到提示中，从而提高LVLM在复杂视觉推理任务中的性能，无需微调即可显著提高准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Federated Cross-Modal Style-Aware Prompt Generation",
        "summary": "Prompt learning has propelled vision-language models like CLIP to excel in\ndiverse tasks, making them ideal for federated learning due to computational\nefficiency. However, conventional approaches that rely solely on final-layer\nfeatures miss out on rich multi-scale visual cues and domain-specific style\nvariations in decentralized client data. To bridge this gap, we introduce\nFedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework\nharnesses low, mid, and high-level features from CLIP's vision encoder\nalongside client-specific style indicators derived from batch-level statistics.\nBy merging intricate visual details with textual context, FedCSAP produces\nrobust, context-aware prompt tokens that are both distinct and non-redundant,\nthereby boosting generalization across seen and unseen classes. Operating\nwithin a federated learning paradigm, our approach ensures data privacy through\nlocal training and global aggregation, adeptly handling non-IID class\ndistributions and diverse domain-specific styles. Comprehensive experiments on\nmultiple image classification datasets confirm that FedCSAP outperforms\nexisting federated prompt learning methods in both accuracy and overall\ngeneralization.",
        "url": "http://arxiv.org/abs/2508.12399v1",
        "published_date": "2025-08-17T15:23:45+00:00",
        "updated_date": "2025-08-17T15:23:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Suraj Prasad",
            "Navyansh Mahla",
            "Sunny Gupta",
            "Amit Sethi"
        ],
        "tldr": "The paper introduces FedCSAP, a federated learning framework that generates style-aware prompts for vision-language models by leveraging multi-scale visual features and client-specific style indicators, demonstrating improved accuracy and generalization in image classification.",
        "tldr_zh": "该论文介绍了FedCSAP，一个联邦学习框架，它通过利用多尺度视觉特征和客户端特定的风格指标，为视觉-语言模型生成风格感知提示，从而在图像分类中提高了准确性和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Region-Level Context-Aware Multimodal Understanding",
        "summary": "Despite significant progress, existing research on Multimodal Large Language\nModels (MLLMs) mainly focuses on general visual understanding, overlooking the\nability to integrate textual context associated with objects for a more\ncontext-aware multimodal understanding -- an ability we refer to as\nRegion-level Context-aware Multimodal Understanding (RCMU). To address this\nlimitation, we first formulate the RCMU task, which requires models to respond\nto user instructions by integrating both image content and textual information\nof regions or objects. To equip MLLMs with RCMU capabilities, we propose\nRegion-level Context-aware Visual Instruction Tuning (RCVIT), which\nincorporates object information into the model input and enables the model to\nutilize bounding box coordinates to effectively associate objects' visual\ncontent with their textual information. To address the lack of datasets, we\nintroduce the RCMU dataset, a large-scale visual instruction tuning dataset\nthat covers multiple RCMU tasks. We also propose RC\\&P-Bench, a comprehensive\nbenchmark that can evaluate the performance of MLLMs in RCMU and multimodal\npersonalized understanding tasks. Additionally, we propose a reference-free\nevaluation metric to perform a comprehensive and fine-grained evaluation of the\nregion-level context-aware image descriptions. By performing RCVIT on Qwen2-VL\nmodels with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental\nresults indicate that RC-Qwen2-VL models not only achieve outstanding\nperformance on multiple RCMU tasks but also demonstrate successful applications\nin multimodal RAG and personalized conversation. Our data, model and benchmark\nare available at https://github.com/hongliang-wei/RC-MLLM",
        "url": "http://arxiv.org/abs/2508.12263v1",
        "published_date": "2025-08-17T07:18:43+00:00",
        "updated_date": "2025-08-17T07:18:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongliang Wei",
            "Xianqi Zhang",
            "Xingtao Wang",
            "Xiaopeng Fan",
            "Debin Zhao"
        ],
        "tldr": "This paper introduces Region-level Context-aware Multimodal Understanding (RCMU), a new task requiring MLLMs to integrate textual context of objects, and proposes RCVIT, RCMU dataset, RC&P-Bench, and RC-Qwen2-VL models to address it, showing improved performance in RCMU tasks and applications like multimodal RAG.",
        "tldr_zh": "本文介绍了区域级上下文感知多模态理解（RCMU）任务，该任务要求MLLM整合对象的文本上下文。为了解决该问题，本文提出了RCVIT、RCMU数据集、RC&P-Bench和RC-Qwen2-VL模型，并在RCMU任务和多模态RAG等应用中表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Infusing fine-grained visual knowledge to Vision-Language Models",
        "summary": "Large-scale contrastive pre-training produces powerful Vision-and-Language\nModels (VLMs) capable of generating representations (embeddings) effective for\na wide variety of visual and multimodal tasks. However, these pretrained\nembeddings remain suboptimal for fine-grained open-set visual retrieval, where\nstate-of-the-art results require fine-tuning the vision encoder using annotated\ndomain-specific samples. Naively performing such fine-tuning typically leads to\ncatastrophic forgetting, severely diminishing the model's general-purpose\nvisual and cross-modal capabilities.\n  In this work, we propose a fine-tuning method explicitly designed to achieve\noptimal balance between fine-grained domain adaptation and retention of the\npretrained VLM's broad multimodal knowledge. Drawing inspiration from continual\nlearning literature, we systematically analyze standard regularization\ntechniques aimed at knowledge retention and propose an efficient and effective\ncombination strategy. Additionally, we address the commonly overlooked yet\ncritical aspects of validation set design and hyperparameter tuning to ensure\nreproducibility and robust generalization across datasets and pretrained\nmodels. We extensively evaluate our method on both fine-grained and\ncoarse-grained image-image and image-text retrieval benchmarks. Our approach\nconsistently achieves strong results, notably retaining the visual-text\nalignment without utilizing any text data or the original text encoder during\nfine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .",
        "url": "http://arxiv.org/abs/2508.12137v1",
        "published_date": "2025-08-16T19:12:09+00:00",
        "updated_date": "2025-08-16T19:12:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikolaos-Antonios Ypsilantis",
            "Kaifeng Chen",
            "André Araujo",
            "Ondřej Chum"
        ],
        "tldr": "This paper introduces a fine-tuning method for VLMs that balances fine-grained domain adaptation with the retention of broad multimodal knowledge, using regularization techniques and careful validation set design. It achieves strong retrieval results while preserving visual-text alignment without text data during fine-tuning.",
        "tldr_zh": "本文提出了一种VLM的微调方法，该方法通过使用正则化技术和精心的验证集设计，在细粒度领域自适应和保留广泛的多模态知识之间取得平衡。该方法在微调过程中无需文本数据即可实现强大的检索结果，同时保留视觉-文本对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations",
        "summary": "Natural language explanations in visual question answering (VQA-NLE) aim to\nmake black-box models more transparent by elucidating their decision-making\nprocesses. However, we find that existing VQA-NLE systems can produce\ninconsistent explanations and reach conclusions without genuinely understanding\nthe underlying context, exposing weaknesses in either their inference pipeline\nor explanation-generation mechanism. To highlight these vulnerabilities, we not\nonly leverage an existing adversarial strategy to perturb questions but also\npropose a novel strategy that minimally alters images to induce contradictory\nor spurious outputs. We further introduce a mitigation method that leverages\nexternal knowledge to alleviate these inconsistencies, thereby bolstering model\nrobustness. Extensive evaluations on two standard benchmarks and two widely\nused VQA-NLE models underscore the effectiveness of our attacks and the\npotential of knowledge-based defenses, ultimately revealing pressing security\nand reliability concerns in current VQA-NLE systems.",
        "url": "http://arxiv.org/abs/2508.12430v1",
        "published_date": "2025-08-17T16:53:10+00:00",
        "updated_date": "2025-08-17T16:53:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yahsin Yeh",
            "Yilun Wu",
            "Bokai Ruan",
            "Honghan Shuai"
        ],
        "tldr": "This paper explores adversarial attacks on VQA-NLE systems, exposing inconsistencies in explanations and proposing a knowledge-based mitigation method to improve robustness.",
        "tldr_zh": "本文探讨了针对 VQA-NLE 系统的对抗性攻击，揭示了解释中的不一致性，并提出了一种基于知识的缓解方法来提高鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos",
        "summary": "Analyzing hand-object interaction in egocentric vision facilitates VR/AR\napplications and human-robot policy transfer. Existing research has mostly\nfocused on modeling the behavior paradigm of interactive actions (i.e., ``how\nto interact''). However, the more challenging and fine-grained problem of\ncapturing the critical moments of contact and separation between the hand and\nthe target object (i.e., ``when to interact'') is still underexplored, which is\ncrucial for immersive interactive experiences in mixed reality and robotic\nmotion planning. Therefore, we formulate this problem as temporal interaction\nlocalization (TIL). Some recent works extract semantic masks as TIL references,\nbut suffer from inaccurate object grounding and cluttered scenarios. Although\ncurrent temporal action localization (TAL) methods perform well in detecting\nverb-noun action segments, they rely on category annotations during training\nand exhibit limited precision in localizing hand-object contact/separation\nmoments. To address these issues, we propose a novel zero-shot approach dubbed\nEgoLoc to localize hand-object contact and separation timestamps in egocentric\nvideos. EgoLoc introduces hand-dynamics-guided sampling to generate\nhigh-quality visual prompts. It exploits the vision-language model to identify\ncontact/separation attributes, localize specific timestamps, and provide\nclosed-loop feedback for further refinement. EgoLoc eliminates the need for\nobject masks and verb-noun taxonomies, leading to generalizable zero-shot\nimplementation. Comprehensive experiments on the public dataset and our novel\nbenchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric\nvideos. It is also validated to effectively facilitate multiple downstream\napplications in egocentric vision and robotic manipulation tasks. Code and\nrelevant data will be released at https://github.com/IRMVLab/EgoLoc.",
        "url": "http://arxiv.org/abs/2508.12349v1",
        "published_date": "2025-08-17T12:38:56+00:00",
        "updated_date": "2025-08-17T12:38:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyi Ma",
            "Erhang Zhang",
            "Yin-Dong Zheng",
            "Yuchen Xie",
            "Yixuan Zhou",
            "Hesheng Wang"
        ],
        "tldr": "The paper introduces EgoLoc, a zero-shot approach for temporal interaction localization (TIL) in egocentric videos, focusing on detecting hand-object contact and separation moments without relying on object masks or verb-noun taxonomies. It uses hand-dynamics-guided visual prompts and vision-language models.",
        "tldr_zh": "本文介绍了一种名为EgoLoc的零样本方法，用于在以自我为中心的视频中进行时间交互定位（TIL），重点是在不依赖对象掩码或动词-名词分类的情况下检测手部-物体接触和分离的时刻。 它使用手部动态引导的视觉提示和视觉语言模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]