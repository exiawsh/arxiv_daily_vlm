[
    {
        "title": "CLIMP: Contrastive Language-Image Mamba Pretraining",
        "summary": "Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI's CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP's fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.",
        "url": "http://arxiv.org/abs/2601.06891v1",
        "published_date": "2026-01-11T12:31:55+00:00",
        "updated_date": "2026-01-11T12:31:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nimrod Shabtay",
            "Itamar Zimerman",
            "Eli Schwartz",
            "Raja Giryes"
        ],
        "tldr": "This paper introduces CLIMP, a Mamba-based vision-language model that outperforms CLIP-ViT-B in image retrieval and out-of-distribution robustness while being more efficient and supporting variable resolutions.",
        "tldr_zh": "该论文介绍了 CLIMP，一种基于 Mamba 的视觉语言模型，在图像检索和分布外鲁棒性方面优于 CLIP-ViT-B，同时更高效并支持可变分辨率。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
        "summary": "Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \\alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \\alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \\href{https://github.com/jiezhu23/ReFine-RFT}{Project Link}.",
        "url": "http://arxiv.org/abs/2601.06993v1",
        "published_date": "2026-01-11T17:07:47+00:00",
        "updated_date": "2026-01-11T17:07:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jie Zhu",
            "Yiyang Su",
            "Xiaoming Liu"
        ],
        "tldr": "This paper investigates the impact of Chain-of-Thought (CoT) reasoning on Fine-Grained Visual Classification (FGVC) with MLLMs, identifies a 'Cost of Thinking' phenomenon, and proposes ReFine-RFT, a novel framework achieving state-of-the-art results by controlling reasoning length and providing accuracy-oriented feedback.",
        "tldr_zh": "本文研究了思维链（CoT）推理对多模态大型语言模型（MLLMs）在细粒度视觉分类（FGVC）上的影响，发现了一种“思考的代价”现象，并提出了ReFine-RFT，通过控制推理长度和提供面向准确性的反馈，实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Personalized Understanding, Generating and Editing",
        "summary": "Unified large multimodal models (LMMs) have achieved remarkable progress in general-purpose multimodal understanding and generation. However, they still operate under a ``one-size-fits-all'' paradigm and struggle to model user-specific concepts (e.g., generate a photo of \\texttt{<maeve>}) in a consistent and controllable manner. Existing personalization methods typically rely on external retrieval, which is inefficient and poorly integrated into unified multimodal pipelines. Recent personalized unified models introduce learnable soft prompts to encode concept information, yet they either couple understanding and generation or depend on complex multi-stage training, leading to cross-task interference and ultimately to fuzzy or misaligned personalized knowledge. We present \\textbf{OmniPersona}, an end-to-end personalization framework for unified LMMs that, for the first time, integrates personalized understanding, generation, and image editing within a single architecture. OmniPersona introduces structurally decoupled concept tokens, allocating dedicated subspaces for different tasks to minimize interference, and incorporates an explicit knowledge replay mechanism that propagates personalized attribute knowledge across tasks, enabling consistent personalized behavior. To systematically evaluate unified personalization, we propose \\textbf{\\texttt{OmniPBench}}, extending the public UnifyBench concept set with personalized editing tasks and cross-task evaluation protocols integrating understanding, generation, and editing. Experimental results demonstrate that OmniPersona delivers competitive and robust performance across diverse personalization tasks. We hope OmniPersona will serve as a strong baseline and spur further research on controllable, unified personalization.",
        "url": "http://arxiv.org/abs/2601.06965v1",
        "published_date": "2026-01-11T15:46:34+00:00",
        "updated_date": "2026-01-11T15:46:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Zhong",
            "Tianwei Lin",
            "Ruike Zhu",
            "Yuqian Yuan",
            "Haoyu Zheng",
            "Liang Liang",
            "Wenqiao Zhang",
            "Feifei Shao",
            "Haoyuan Li",
            "Wanggui He",
            "Hao Jiang",
            "Yueting Zhuang"
        ],
        "tldr": "The paper introduces OmniPersona, a unified LMM personalization framework that integrates personalized understanding, generation, and editing with structurally decoupled concept tokens and knowledge replay, evaluated using the OmniPBench benchmark.",
        "tldr_zh": "本文介绍了OmniPersona，一个统一的LMM个性化框架，集成了个性化的理解、生成和编辑功能，采用结构解耦的概念令牌和知识重放机制，并使用OmniPBench基准进行评估。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Measuring Social Bias in Vision-Language Models with Face-Only Counterfactuals from Real Photos",
        "summary": "Vision-Language Models (VLMs) are increasingly deployed in socially consequential settings, raising concerns about social bias driven by demographic cues. A central challenge in measuring such social bias is attribution under visual confounding: real-world images entangle race and gender with correlated factors such as background and clothing, obscuring attribution. We propose a \\textbf{face-only counterfactual evaluation paradigm} that isolates demographic effects while preserving real-image realism. Starting from real photographs, we generate counterfactual variants by editing only facial attributes related to race and gender, keeping all other visual factors fixed. Based on this paradigm, we construct \\textbf{FOCUS}, a dataset of 480 scene-matched counterfactual images across six occupations and ten demographic groups, and propose \\textbf{REFLECT}, a benchmark comprising three decision-oriented tasks: two-alternative forced choice, multiple-choice socioeconomic inference, and numeric salary recommendation. Experiments on five state-of-the-art VLMs reveal that demographic disparities persist under strict visual control and vary substantially across task formulations. These findings underscore the necessity of controlled, counterfactual audits and highlight task design as a critical factor in evaluating social bias in multimodal models.",
        "url": "http://arxiv.org/abs/2601.06931v1",
        "published_date": "2026-01-11T14:35:06+00:00",
        "updated_date": "2026-01-11T14:35:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haodong Chen",
            "Qiang Huang",
            "Jiaqi Zhao",
            "Qiuping Jiang",
            "Xiaojun Chang",
            "Jun Yu"
        ],
        "tldr": "The paper introduces a face-only counterfactual evaluation paradigm (FOCUS dataset and REFLECT benchmark) to measure social bias in VLMs, demonstrating that demographic disparities persist even under strict visual control.",
        "tldr_zh": "该论文介绍了一种仅使用面部的反事实评估范式（FOCUS数据集和REFLECT基准）来衡量VLM中的社会偏见，表明即使在严格的视觉控制下，人口统计学差异仍然存在。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data",
        "summary": "Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.",
        "url": "http://arxiv.org/abs/2601.06847v1",
        "published_date": "2026-01-11T10:34:18+00:00",
        "updated_date": "2026-01-11T10:34:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mengmeng Zhang",
            "Xiaoping Wu",
            "Hao Luo",
            "Fan Wang",
            "Yisheng Lv"
        ],
        "tldr": "The paper introduces MedGround, an automated pipeline for generating high-quality, large-scale clinical referring grounding data and a resulting dataset, MedGround-35K, which improves the grounding performance of VLMs in the medical domain.",
        "tldr_zh": "该论文介绍了MedGround，一个用于生成高质量、大规模临床指代定位数据的自动化流程，以及由此产生的MedGround-35K数据集，该数据集提高了VLMs在医学领域的定位性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance across many tasks, yet most systems remain limited to offline inference, requiring complete inputs before generating outputs. Recent streaming methods reduce latency by interleaving perception and generation, but still enforce a sequential perception-generation cycle, limiting real-time interaction. In this work, we target a fundamental bottleneck that arises when extending MLLMs to real-time video understanding: the global positional continuity constraint imposed by standard positional encoding schemes. While natural in offline inference, this constraint tightly couples perception and generation, preventing effective input-output parallelism. To address this limitation, we propose a parallel streaming framework that relaxes positional continuity through three designs: Overlapped, Group-Decoupled, and Gap-Isolated. These designs enable simultaneous perception and generation, allowing the model to process incoming inputs while producing responses in real time. Extensive experiments reveal that Group-Decoupled achieves the best efficiency-performance balance, maintaining high fluency and accuracy while significantly reducing latency. We further show that the proposed framework yields up to 2x acceleration under balanced perception-generation workloads, establishing a principled pathway toward speak-while-watching real-time systems. We make all our code publicly available: https://github.com/EIT-NLP/Speak-While-Watching.",
        "url": "http://arxiv.org/abs/2601.06843v1",
        "published_date": "2026-01-11T10:12:11+00:00",
        "updated_date": "2026-01-11T10:12:11+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Junyan Lin",
            "Junlong Tong",
            "Hao Wu",
            "Jialiang Zhang",
            "Jinming Liu",
            "Xin Jin",
            "Xiaoyu Shen"
        ],
        "tldr": "This paper introduces a parallel streaming framework for multimodal large language models (MLLMs) that enables real-time video understanding by relaxing positional continuity constraints, achieving up to 2x acceleration in balanced workloads.",
        "tldr_zh": "该论文介绍了一个用于多模态大型语言模型（MLLM）的并行流式框架，通过放宽位置连续性约束来实现实时视频理解，在平衡工作负载下实现了高达2倍的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
        "summary": "While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.",
        "url": "http://arxiv.org/abs/2601.06803v1",
        "published_date": "2026-01-11T08:30:49+00:00",
        "updated_date": "2026-01-11T08:30:49+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yubo Wang",
            "Juntian Zhang",
            "Yichen Wu",
            "Yankai Lin",
            "Nils Lukas",
            "Yuhan Liu"
        ],
        "tldr": "The paper introduces Laser, a novel latent reasoning paradigm for VLMs that uses Dynamic Windowed Alignment Learning to improve performance and efficiency by maintaining a superposition of global features before focusing on local details, achieving SOTA results with significantly fewer inference tokens.",
        "tldr_zh": "该论文介绍了一种新的VLM潜在推理范式Laser，它使用动态窗口对齐学习来提高性能和效率，通过在关注局部细节之前保持全局特征的叠加，以显著减少的推理token实现了SOTA结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AutoTour: Automatic Photo Tour Guide with Smartphones and LLMs",
        "summary": "We present AutoTour, a system that enhances user exploration by automatically generating fine-grained landmark annotations and descriptive narratives for photos captured by users. The key idea of AutoTour is to fuse visual features extracted from photos with nearby geospatial features queried from open matching databases. Unlike existing tour applications that rely on pre-defined content or proprietary datasets, AutoTour leverages open and extensible data sources to provide scalable and context-aware photo-based guidance. To achieve this, we design a training-free pipeline that first extracts and filters relevant geospatial features around the user's GPS location. It then detects major landmarks in user photos through VLM-based feature detection and projects them into the horizontal spatial plane. A geometric matching algorithm aligns photo features with corresponding geospatial entities based on their estimated distance and direction. The matched features are subsequently grounded and annotated directly on the original photo, accompanied by large language model-generated textual and audio descriptions to provide an informative, tour-like experience. We demonstrate that AutoTour can deliver rich, interpretable annotations for both iconic and lesser-known landmarks, enabling a new form of interactive, context-aware exploration that bridges visual perception and geospatial understanding.",
        "url": "http://arxiv.org/abs/2601.06781v1",
        "published_date": "2026-01-11T05:13:39+00:00",
        "updated_date": "2026-01-11T05:13:39+00:00",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Huatao Xu",
            "Zihe Liu",
            "Zilin Zeng",
            "Baichuan Li",
            "Mo Li"
        ],
        "tldr": "AutoTour automatically generates landmark annotations and descriptive narratives for user photos by fusing visual features with geospatial data, offering a scalable and context-aware tour experience.",
        "tldr_zh": "AutoTour通过融合视觉特征和地理空间数据，自动为用户照片生成地标注释和描述性叙述，从而提供可扩展且上下文感知的旅行体验。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
        "summary": "While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.",
        "url": "http://arxiv.org/abs/2601.06944v1",
        "published_date": "2026-01-11T15:08:05+00:00",
        "updated_date": "2026-01-11T15:08:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuhang Su",
            "Mei Wang",
            "Yaoyao Zhong",
            "Guozhang Li",
            "Shixing Li",
            "Yihan Feng",
            "Hua Huang"
        ],
        "tldr": "The paper introduces SketchJudge, a new benchmark for evaluating MLLMs' ability to grade hand-drawn STEM diagrams, revealing limitations in their diagnostic and reasoning capabilities in noisy, symbolic visual contexts.",
        "tldr_zh": "该论文介绍了SketchJudge，一个新的基准测试，用于评估多模态大型语言模型(MLLM)在手绘STEM图表评分方面的能力，揭示了它们在嘈杂、符号视觉环境中诊断和推理能力的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Quantification and Classification of Carbon Nanotubes in Electron Micrographs using Vision Foundation Models",
        "summary": "Accurate characterization of carbon nanotube morphologies in electron microscopy images is vital for exposure assessment and toxicological studies, yet current workflows rely on slow, subjective manual segmentation. This work presents a unified framework leveraging vision foundation models to automate the quantification and classification of CNTs in electron microscopy images. First, we introduce an interactive quantification tool built on the Segment Anything Model (SAM) that segments particles with near-perfect accuracy using minimal user input. Second, we propose a novel classification pipeline that utilizes these segmentation masks to spatially constrain a DINOv2 vision transformer, extracting features exclusively from particle regions while suppressing background noise. Evaluated on a dataset of 1,800 TEM images, this architecture achieves 95.5% accuracy in distinguishing between four different CNT morphologies, significantly outperforming the current baseline despite using a fraction of the training data. Crucially, this instance-level processing allows the framework to resolve mixed samples, correctly classifying distinct particle types co-existing within a single field of view. These results demonstrate that integrating zero-shot segmentation with self-supervised feature learning enables high-throughput, reproducible nanomaterial analysis, transforming a labor-intensive bottleneck into a scalable, data-driven process.",
        "url": "http://arxiv.org/abs/2601.06673v1",
        "published_date": "2026-01-10T20:22:58+00:00",
        "updated_date": "2026-01-10T20:22:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sanjay Pradeep",
            "Chen Wang",
            "Matthew M. Dahm",
            "Jeff D. Eldredge",
            "Candace S. J. Tsai"
        ],
        "tldr": "This paper presents a framework using SAM and DINOv2 to automate the quantification and classification of carbon nanotube morphologies in electron microscopy images, achieving high accuracy with minimal user input and less training data than baseline methods.",
        "tldr_zh": "该论文提出了一个使用SAM和DINOv2的框架，用于自动量化和分类电子显微镜图像中的碳纳米管形态，以高精度、最少的用户输入和比基线方法更少的数据量实现了这一目标。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]