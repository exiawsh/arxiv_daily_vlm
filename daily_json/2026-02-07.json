[
    {
        "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
        "summary": "The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.",
        "url": "http://arxiv.org/abs/2602.06013v1",
        "published_date": "2026-02-05T18:52:48+00:00",
        "updated_date": "2026-02-05T18:52:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruihang Li",
            "Leigang Qu",
            "Jingxu Zhang",
            "Dongnan Gui",
            "Mengde Xu",
            "Xiaosong Zhang",
            "Han Hu",
            "Wenjie Wang",
            "Jiaqi Wang"
        ],
        "tldr": "This paper introduces GenArena, a pairwise comparison framework for evaluating visual generation models that addresses the limitations of pointwise scoring methods and achieves significantly better alignment with human perception, even enabling open-source models to outperform proprietary ones.",
        "tldr_zh": "该论文介绍了GenArena，一个用于评估视觉生成模型的成对比较框架，解决了点式评分方法的局限性，并实现了与人类感知的更好对齐，甚至使开源模型能够超越专有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.",
        "url": "http://arxiv.org/abs/2602.06040v1",
        "published_date": "2026-02-05T18:59:51+00:00",
        "updated_date": "2026-02-05T18:59:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jintao Tong",
            "Shilin Yan",
            "Hongwei Xue",
            "Xiaojun Tang",
            "Kunyu Shi",
            "Guannan Zhang",
            "Ruixuan Li",
            "Yixiong Zou"
        ],
        "tldr": "The paper introduces SwimBird, a multimodal large language model that adaptively switches between text-only, vision-only, and interleaved reasoning modes, achieving state-of-the-art results on diverse benchmarks.",
        "tldr_zh": "该论文介绍了SwimBird，一种多模态大型语言模型，它可以在纯文本、纯视觉和交错推理模式之间自适应切换，在各种基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Thinking with Geometry: Active Geometry Integration for Spatial Reasoning",
        "summary": "Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.",
        "url": "http://arxiv.org/abs/2602.06037v1",
        "published_date": "2026-02-05T18:59:32+00:00",
        "updated_date": "2026-02-05T18:59:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyuan Li",
            "Qihang Cao",
            "Tao Tang",
            "Kun Xiang",
            "Zihan Guo",
            "Jianhua Han",
            "Hang Xu",
            "Xiaodan Liang"
        ],
        "tldr": "The paper introduces GeoThinker, a framework for MLLMs that actively integrates geometric information for spatial reasoning by selectively retrieving geometric evidence based on internal reasoning demands, achieving state-of-the-art performance on VSI-Bench and demonstrating strong generalization capabilities.",
        "tldr_zh": "该论文介绍了一个名为GeoThinker的框架，用于多模态大型语言模型，通过根据内部推理需求选择性地检索几何证据，主动整合几何信息以进行空间推理，从而在VSI-Bench上实现了最先进的性能，并展示了强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
        "summary": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.",
        "url": "http://arxiv.org/abs/2602.06034v1",
        "published_date": "2026-02-05T18:59:21+00:00",
        "updated_date": "2026-02-05T18:59:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongyang Chen",
            "Chaoyang Wang",
            "Dezhao SU",
            "Xi Xiao",
            "Zeyu Zhang",
            "Jing Xiong",
            "Qing Li",
            "Yuzhang Shang",
            "Shichao Ka"
        ],
        "tldr": "V-Retrver introduces an agentic reasoning framework for multimodal retrieval, enabling MLLMs to actively verify visual evidence using external tools, leading to significant accuracy improvements and better generalization across multiple benchmarks.",
        "tldr_zh": "V-Retrver 引入了一种基于代理的推理框架，用于多模态检索，使 MLLM 能够使用外部工具主动验证视觉证据，从而显著提高准确性并在多个基准测试中实现更好的泛化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation",
        "summary": "Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.",
        "url": "http://arxiv.org/abs/2602.05998v1",
        "published_date": "2026-02-05T18:45:53+00:00",
        "updated_date": "2026-02-05T18:45:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jie Deng",
            "Kaichun Yao",
            "Libo Zhang"
        ],
        "tldr": "The paper introduces VisRefiner, a framework for screenshot-to-code generation that leverages visual differences between rendered predictions and target designs to improve model training and performance through difference-aligned supervision and reinforcement learning.",
        "tldr_zh": "该论文介绍了一种名为VisRefiner的框架，用于从屏幕截图生成代码，通过利用渲染预测和目标设计之间的视觉差异，并通过差异对齐的监督和强化学习来提高模型训练和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]