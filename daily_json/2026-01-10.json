[
    {
        "title": "Mechanisms of Prompt-Induced Hallucination in Vision-Language Models",
        "summary": "Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.",
        "url": "http://arxiv.org/abs/2601.05201v1",
        "published_date": "2026-01-08T18:23:03+00:00",
        "updated_date": "2026-01-08T18:23:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "William Rudman",
            "Michal Golovanevsky",
            "Dana Arad",
            "Yonatan Belinkov",
            "Ritambhara Singh",
            "Carsten Eickhoff",
            "Kyle Mahowald"
        ],
        "tldr": "This paper investigates the mechanisms behind prompt-induced hallucinations in VLMs using object counting tasks, identifying specific attention heads responsible for this behavior and showing that ablating these heads reduces hallucinations.",
        "tldr_zh": "本文通过物体计数任务研究了VLM中提示诱导幻觉的机制，识别出导致该行为的特定注意力头，并表明消融这些头可以减少幻觉。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
        "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
        "url": "http://arxiv.org/abs/2601.05175v1",
        "published_date": "2026-01-08T18:00:59+00:00",
        "updated_date": "2026-01-08T18:00:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuming Liu",
            "Mingchen Zhuge",
            "Changsheng Zhao",
            "Jun Chen",
            "Lemeng Wu",
            "Zechun Liu",
            "Chenchen Zhu",
            "Zhipeng Cai",
            "Chong Zhou",
            "Haozhe Liu",
            "Ernie Chang",
            "Saksham Suri",
            "Hongyu Xu",
            "Qi Qian",
            "Wei Wen",
            "Balakrishnan Varadarajan",
            "Zhuang Liu",
            "Hu Xu",
            "Florian Bordes",
            "Raghuraman Krishnamoorthi",
            "Bernard Ghanem",
            "Vikas Chandra",
            "Yunyang Xiong"
        ],
        "tldr": "The paper introduces VideoAuto-R1, a video understanding framework that uses a 'reason-when-necessary' strategy, achieving state-of-the-art accuracy with improved efficiency by selectively applying chain-of-thought reasoning based on initial answer confidence.",
        "tldr_zh": "该论文介绍了VideoAuto-R1，一种视频理解框架，采用“按需推理”策略，通过根据初始答案置信度选择性地应用思维链推理，实现了最先进的准确性并提高了效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]