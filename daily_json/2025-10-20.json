[
    {
        "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input",
        "summary": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement.",
        "url": "http://arxiv.org/abs/2510.16926v1",
        "published_date": "2025-10-19T16:53:01+00:00",
        "updated_date": "2025-10-19T16:53:01+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Chenxu Li",
            "Zhicai Wang",
            "Yuan Sheng",
            "Xingyu Zhu",
            "Yanbin Hao",
            "Xiang Wang"
        ],
        "tldr": "The paper introduces Res-Bench, a new benchmark for evaluating the robustness of MLLMs to varying input resolutions, along with novel metrics for assessing performance stability.",
        "tldr_zh": "该论文介绍了Res-Bench，一个新的基准测试，用于评估多模态大语言模型对不同输入分辨率的鲁棒性，并提出了评估性能稳定性的新指标。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback",
        "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\n\\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.",
        "url": "http://arxiv.org/abs/2510.16888v1",
        "published_date": "2025-10-19T15:38:06+00:00",
        "updated_date": "2025-10-19T15:38:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongjian Li",
            "Zheyuan Liu",
            "Qihui Zhang",
            "Bin Lin",
            "Shenghai Yuan",
            "Zhiyuan Yan",
            "Yang Ye",
            "Wangbo Yu",
            "Yuwei Niu",
            "Li Yuan"
        ],
        "tldr": "The paper introduces Edit-R1, a post-training framework for instruction-based image editing that uses Diffusion Negative-aware Finetuning (DiffusionNFT) and MLLM-based reward modeling to achieve state-of-the-art results on image editing benchmarks and is model-agnostic.",
        "tldr_zh": "本文介绍Edit-R1，一个用于指令图像编辑的后训练框架，它使用扩散负感知微调（DiffusionNFT）和基于MLLM的奖励建模，在图像编辑基准测试中取得了最先进的结果，并且具有模型无关性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding",
        "summary": "While brain-inspired artificial intelligence(AI) has demonstrated promising\nresults, current understanding of the parallels between artificial neural\nnetworks (ANNs) and human brain processing remains limited: (1) unimodal ANN\nstudies fail to capture the brain's inherent multimodal processing\ncapabilities, and (2) multimodal ANN research primarily focuses on high-level\nmodel outputs, neglecting the crucial role of individual neurons. To address\nthese limitations, we propose a novel neuron-level analysis framework that\ninvestigates the multimodal information processing mechanisms in\nvision-language models (VLMs) through the lens of human brain activity. Our\napproach uniquely combines fine-grained artificial neuron (AN) analysis with\nfMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP\nand METER. Our analysis reveals four key findings: (1) ANs successfully predict\nbiological neurons (BNs) activities across multiple functional networks\n(including language, vision, attention, and default mode), demonstrating shared\nrepresentational mechanisms; (2) Both ANs and BNs demonstrate functional\nredundancy through overlapping neural representations, mirroring the brain's\nfault-tolerant and collaborative information processing mechanisms; (3) ANs\nexhibit polarity patterns that parallel the BNs, with oppositely activated BNs\nshowing mirrored activation trends across VLM layers, reflecting the complexity\nand bidirectional nature of neural information processing; (4) The\narchitectures of CLIP and METER drive distinct BNs: CLIP's independent branches\nshow modality-specific specialization, whereas METER's cross-modal design\nyields unified cross-modal activation, highlighting the architecture's\ninfluence on ANN brain-like properties. These results provide compelling\nevidence for brain-like hierarchical processing in VLMs at the neuronal level.",
        "url": "http://arxiv.org/abs/2510.16870v1",
        "published_date": "2025-10-19T15:11:03+00:00",
        "updated_date": "2025-10-19T15:11:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yudan Ren",
            "Xinlong Wang",
            "Kexin Wang",
            "Tian Xia",
            "Zihan Ma",
            "Zhaowei Li",
            "Xiangrong Bi",
            "Xiao Li",
            "Xiaowei He"
        ],
        "tldr": "This paper explores neuron-level similarities between vision-language models (VLMs) and the human brain using fMRI data, revealing shared representational mechanisms and architectural influences on brain-like properties.",
        "tldr_zh": "该论文使用fMRI数据探索了视觉-语言模型（VLM）与人脑在神经元层面的相似性，揭示了共享的表征机制和架构对类脑属性的影响。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs",
        "summary": "Integrating diverse visual capabilities into a unified model is a significant\ntrend in Multimodal Large Language Models (MLLMs). Among these, the inclusion\nof segmentation poses a distinct set of challenges. To equip MLLMs with\npixel-level segmentation abilities, prevailing methods require finetuning the\nmodel to produce specific outputs compatible with a mask decoder. This process\ntypically alters the model's output space and compromises its intrinsic\ngeneralization, which undermines the goal of building a unified model. We\nintroduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel\nplug-and-play solution. LENS attaches a lightweight, trainable head to a\ncompletely frozen MLLM. By refining the spatial cues embedded in attention\nmaps, LENS extracts keypoints and describes them into point-wise features\ndirectly compatible with the mask decoder. Extensive experiments validate our\napproach: LENS achieves segmentation performance competitive with or superior\nto that of retraining-based methods. Crucially, it does so while fully\npreserving the MLLM's generalization capabilities, which are significantly\ndegraded by finetuning approaches. As such, the attachable design of LENS\nestablishes an efficient and powerful paradigm for extending MLLMs, paving the\nway for truly multi-talented, unified models.",
        "url": "http://arxiv.org/abs/2510.16785v1",
        "published_date": "2025-10-19T10:21:01+00:00",
        "updated_date": "2025-10-19T10:21:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiazhen Liu",
            "Long Chen"
        ],
        "tldr": "The paper introduces LENS, a plug-and-play module for frozen MLLMs that enables pixel-level segmentation by leveraging keypoints from attention maps, achieving competitive performance without compromising the MLLM's generalization capabilities.",
        "tldr_zh": "该论文介绍了LENS，一个用于冻结的多模态大型语言模型（MLLM）的即插即用模块，它通过利用注意力图中的关键点来实现像素级分割，在不损害MLLM泛化能力的前提下，实现了有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features",
        "summary": "The remarkable zero-shot reasoning capabilities of large-scale Visual\nLanguage Models (VLMs) on static images have yet to be fully translated to the\nvideo domain. Conventional video understanding models often rely on extensive,\ntask-specific training on annotated datasets, a process that is both costly and\nlimited in scalability. This paper introduces a novel, training-free framework\nfor video understanding that circumvents end-to-end training by synergistically\ncombining the rich semantic priors of pre-trained VLMs with classic machine\nlearning algorithms for pattern discovery. Our core idea is to reframe video\nunderstanding as a self-supervised spatio-temporal clustering problem within a\nhigh-dimensional semantic feature space. The proposed pipeline first transforms\na video stream into a semantic feature trajectory using the frozen visual\nencoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal\nSegmentation (KTS), a robust machine learning technique, to partition the\ncontinuous feature stream into discrete, semantically coherent event segments.\nThese segments are then subjected to unsupervised density-based clustering to\nidentify recurring macroscopic scenes and themes throughout the video. By\nselecting representative keyframes from each discovered cluster and leveraging\nthe VLM's generative capabilities for textual description, our framework\nautomatically produces a structured, multi-modal summary of the video content.\nThis approach provides an effective, interpretable, and model-agnostic pathway\nfor zero-shot, automated structural analysis of video content.",
        "url": "http://arxiv.org/abs/2510.16781v1",
        "published_date": "2025-10-19T10:13:34+00:00",
        "updated_date": "2025-10-19T10:13:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Shihao Ji",
            "Zihui Song"
        ],
        "tldr": "The paper introduces a training-free video understanding framework that combines pre-trained VLMs with spatio-temporal clustering for zero-shot video analysis and summarization.",
        "tldr_zh": "本文提出了一种无需训练的视频理解框架，该框架结合了预训练的视觉语言模型与时空聚类，用于零样本视频分析和摘要。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning",
        "summary": "Recent research has made significant progress in localizing and editing image\nregions based on text. However, most approaches treat these regions in\nisolation, relying solely on local cues without accounting for how each part\ncontributes to the overall visual and semantic composition. This often results\nin inconsistent edits, unnatural transitions, or loss of coherence across the\nimage. In this work, we propose Region in Context, a novel framework for\ntext-conditioned image editing that performs multilevel semantic alignment\nbetween vision and language, inspired by the human ability to reason about\nedits in relation to the whole scene. Our method encourages each region to\nunderstand its role within the global image context, enabling precise and\nharmonized changes. At its core, the framework introduces a dual-level guidance\nmechanism: regions are represented with full-image context and aligned with\ndetailed region-level descriptions, while the entire image is simultaneously\nmatched to a comprehensive scene-level description generated by a large\nvision-language model. These descriptions serve as explicit verbal references\nof the intended content, guiding both local modifications and global structure.\nExperiments show that it produces more coherent and instruction-aligned\nresults. Code is available at:\nhttps://github.com/thuyvuphuong/Region-in-Context.git",
        "url": "http://arxiv.org/abs/2510.16772v1",
        "published_date": "2025-10-19T09:36:02+00:00",
        "updated_date": "2025-10-19T09:36:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Thuy Phuong Vu",
            "Dinh-Cuong Hoang",
            "Minhhuy Le",
            "Phan Xuan Tan"
        ],
        "tldr": "This paper introduces \"Region in Context,\" a framework for text-conditioned image editing that uses multilevel semantic alignment to ensure coherent and instruction-aligned edits by considering the global image context during localized region modifications.",
        "tldr_zh": "本文介绍了一种名为“上下文区域（Region in Context）”的文本条件图像编辑框架，该框架使用多层次语义对齐来确保编辑的连贯性和指令一致性，通过在局部区域修改期间考虑全局图像上下文来实现。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes",
        "summary": "Existing research on 3D Large Language Models (LLMs) still struggles to\nachieve grounded question-answering, primarily due to the under-exploration of\nthe mech- anism of human-like scene-object grounded reasoning. This paper\nbridges the gap by presenting a novel framework. We first introduce a grounded\nChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a\ncomplex reasoning task into simpler and manageable problems, and building\ncorresponding visual clues based on multimodal expert modules. To enable such a\nmethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning\ndataset, consisting of 185K high-quality instances. Extensive experiments\nacross various complex 3D scene reasoning benchmarks demonstrate that our new\nframework achieves strong performance with high grounding-QA coherence. To the\nbest of our knowledge, this is the first successful application of CoT\nreasoning to 3D scene understanding, enabling step-by-step human-like reasoning\nand showing potential for extension to broader 3D scene understanding\nscenarios.",
        "url": "http://arxiv.org/abs/2510.16714v1",
        "published_date": "2025-10-19T04:57:49+00:00",
        "updated_date": "2025-10-19T04:57:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiongkun Linghu",
            "Jiangyong Huang",
            "Ziyu Zhu",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "tldr": "This paper introduces SCENECOT, a framework for grounded Chain-of-Thought reasoning in 3D scenes, along with a large-scale dataset, SCENECOT-185K, demonstrating improved performance on 3D scene understanding benchmarks.",
        "tldr_zh": "本文介绍了SCENECOT，一个用于3D场景中基于链式思维推理的框架，以及一个大型数据集SCENECOT-185K，展示了在3D场景理解基准测试中得到改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pursuing Minimal Sufficiency in Spatial Reasoning",
        "summary": "Spatial reasoning, the ability to ground language in 3D understanding,\nremains a persistent challenge for Vision-Language Models (VLMs). We identify\ntwo fundamental bottlenecks: inadequate 3D understanding capabilities stemming\nfrom 2D-centric pre-training, and reasoning failures induced by redundant 3D\ninformation. To address these, we first construct a Minimal Sufficient Set\n(MSS) of information before answering a given question: a compact selection of\n3D perception results from \\textit{expert models}. We introduce MSSR (Minimal\nSufficient Spatial Reasoner), a dual-agent framework that implements this\nprinciple. A Perception Agent programmatically queries 3D scenes using a\nversatile perception toolbox to extract sufficient information, including a\nnovel SOG (Situated Orientation Grounding) module that robustly extracts\nlanguage-grounded directions. A Reasoning Agent then iteratively refines this\ninformation to pursue minimality, pruning redundant details and requesting\nmissing ones in a closed loop until the MSS is curated. Extensive experiments\ndemonstrate that our method, by explicitly pursuing both sufficiency and\nminimality, significantly improves accuracy and achieves state-of-the-art\nperformance across two challenging benchmarks. Furthermore, our framework\nproduces interpretable reasoning paths, offering a promising source of\nhigh-quality training data for future models. Source code is available at\nhttps://github.com/gyj155/mssr.",
        "url": "http://arxiv.org/abs/2510.16688v1",
        "published_date": "2025-10-19T02:29:09+00:00",
        "updated_date": "2025-10-19T02:29:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yejie Guo",
            "Yunzhong Hou",
            "Wufei Ma",
            "Meng Tang",
            "Ming-Hsuan Yang"
        ],
        "tldr": "This paper introduces MSSR, a dual-agent framework that addresses limitations in Vision-Language Models' spatial reasoning by constructing a minimal sufficient set of 3D information, achieving state-of-the-art performance and interpretable reasoning paths.",
        "tldr_zh": "本文介绍了一种双智能体框架MSSR，该框架通过构建3D信息的最小充分集来解决视觉语言模型在空间推理方面的局限性，实现了最先进的性能和可解释的推理路径。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models",
        "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.",
        "url": "http://arxiv.org/abs/2510.16641v1",
        "published_date": "2025-10-18T21:00:12+00:00",
        "updated_date": "2025-10-18T21:00:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Young-Jun Lee",
            "Byung-Kwan Lee",
            "Jianshu Zhang",
            "Yechan Hwang",
            "Byungsoo Ko",
            "Han-Gyu Kim",
            "Dongyu Yao",
            "Xuankun Rong",
            "Eojin Joo",
            "Seung-Ho Han",
            "Bowon Ko",
            "Ho-Jin Choi"
        ],
        "tldr": "The paper introduces MultiVerse, a new challenging multi-turn VLM benchmark derived from existing datasets, and proposes a GPT-4o-based checklist evaluation method, revealing limitations of current VLMs in complex multi-turn conversations.",
        "tldr_zh": "该论文介绍了MultiVerse，一个新的、具有挑战性的多轮VLM基准，它源于现有数据集，并提出了一种基于GPT-4o的清单评估方法，揭示了当前VLM在复杂多轮对话中的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]