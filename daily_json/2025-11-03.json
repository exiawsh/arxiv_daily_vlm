[
    {
        "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation",
        "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve\nzero-shot navigation. However, existing methods often rely on panoramic\nobservations and two-stage pipelines involving waypoint predictors, which\nintroduce significant latency and limit real-world applicability. In this work,\nwe propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that\neliminates the need for panoramic views and waypoint predictors. Our approach\nuses only three frontal RGB-D images combined with natural language\ninstructions, enabling MLLMs to directly predict actions. To enhance decision\nrobustness, we introduce an Uncertainty-Aware Reasoning module that integrates\n(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past\nBidirectional Reasoning mechanism for globally coherent planning. Experiments\non both simulated and real-robot environments demonstrate that our method\nsignificantly reduces per-step latency while achieving competitive or superior\nperformance compared to panoramic-view baselines. These results demonstrate the\npracticality and effectiveness of Fast-SmartWay for real-world zero-shot\nembodied navigation.",
        "url": "http://arxiv.org/abs/2511.00933v1",
        "published_date": "2025-11-02T13:21:54+00:00",
        "updated_date": "2025-11-02T13:21:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Shi",
            "Zerui Li",
            "Yanyuan Qiao",
            "Qi Wu"
        ],
        "tldr": "The paper introduces Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that uses only frontal RGB-D images and natural language instructions to enable MLLMs to directly predict actions, achieving reduced latency and competitive performance.",
        "tldr_zh": "该论文介绍了Fast-SmartWay，一个端到端的零样本VLN-CE框架，仅使用正面RGB-D图像和自然语言指令，使MLLM能够直接预测动作，从而降低延迟并实现有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\neffectiveness in various general-domain scenarios, such as visual question\nanswering and image captioning. Recently, researchers have increasingly focused\non empowering MLLMs with medical conversational abilities, which hold\nsignificant promise for clinical applications. However, medical data presents\nunique challenges due to its heterogeneous nature -- encompassing diverse\nmodalities including 2D images, 3D volumetric scans, and temporal video\nsequences. The substantial domain gap and data format inconsistencies across\nthese modalities have hindered the development of unified medical MLLMs. To\naddress these challenges, we propose Fleming-VL, a unified end-to-end framework\nfor comprehensive medical visual understanding across heterogeneous modalities.\nFleming-VL tackles this problem from a data-centric perspective through three\nkey strategies: (1) scaling up pretraining by integrating long-context data\nfrom both natural and medical-specific domains; (2) complementing fine-tuning\nwith rare medical data, including holistic video analysis and underrepresented\n2D modalities such as ultrasound and dermoscopy images; (3) extending existing\nevaluation frameworks to incorporate 3D volumetric and video understanding\nbenchmarks. Through supervised fine-tuning (SFT) and group relative policy\noptimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive\nexperiments demonstrate that Fleming-VL achieves state-of-the-art performance\nacross multiple benchmarks, including medical VQA, video QA, and 3D medical\nimage understanding. We publicly release Fleming-VL to promote transparent,\nreproducible, and auditable progress in medical AI.",
        "url": "http://arxiv.org/abs/2511.00916v1",
        "published_date": "2025-11-02T12:30:22+00:00",
        "updated_date": "2025-11-02T12:30:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yan Shu",
            "Chi Liu",
            "Robin Chen",
            "Derek Li",
            "Bryan Dai"
        ],
        "tldr": "Fleming-VL is a unified MLLM framework for medical visual understanding across heterogeneous modalities (2D, 3D, video), achieved by scaling pretraining, complementing fine-tuning with rare data, and extending evaluation benchmarks, demonstrating SOTA performance.",
        "tldr_zh": "Fleming-VL 是一个统一的 MLLM 框架，用于跨异构模态（2D、3D、视频）的医学视觉理解。它通过扩大预训练规模、使用稀有数据补充微调并扩展评估基准来实现，并展示了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks",
        "summary": "Brain imaging analysis is vital for diagnosing and treating brain disorders,\nand multimodal large language models (MLLMs) are increasingly assisting in that\nanalysis. However, current brain-oriented visual question-answering (VQA)\nbenchmarks either cover a few imaging modalities or are limited to\ncoarse-grained pathological descriptions, hindering a comprehensive assessment\nof MLLMs throughout the full clinical continuum. To address these, we introduce\nOmniBrainBench, the first comprehensive multimodal VQA benchmark specifically\ndesigned to assess the multimodal comprehension capabilities of MLLMs in brain\nimaging analysis.OmniBrainBench consists of 15 distinct brain imaging\nmodalities collected from 30 verified medical sources, yielding 9,527 validated\nVQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15\nmulti-stage clinical tasks rigorously validated by a professional radiologist.\nEvaluation of 24 state-of-the-art models, including open-source, medical, and\nproprietary MLLMs, highlights the substantial challenges posed by\nOmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)\nbeat open-source and medical models but lag physicians; (2) medical MLLMs vary\nwidely in performance; (3) open-source MLLMs trail overall but excel in\nspecific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,\nrevealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new\nstandard for evaluating and advancing MLLMs in brain imaging analysis,\nhighlighting gaps compared to expert clinical reasoning. We release it at\nbenchmark \\& code.",
        "url": "http://arxiv.org/abs/2511.00846v1",
        "published_date": "2025-11-02T08:11:55+00:00",
        "updated_date": "2025-11-02T08:11:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhihao Peng",
            "Cheng Wang",
            "Shengyuan Liu",
            "Zhiying Liang",
            "Yixuan Yuan"
        ],
        "tldr": "The paper introduces OmniBrainBench, a new comprehensive multimodal VQA benchmark for evaluating MLLMs in brain imaging analysis across multi-stage clinical tasks, revealing performance gaps compared to expert clinical reasoning.",
        "tldr_zh": "该论文介绍了OmniBrainBench，这是一个新的综合多模态VQA基准，用于评估MLLM在跨多阶段临床任务的脑成像分析中的表现，揭示了与专家临床推理相比的性能差距。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding",
        "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA",
        "url": "http://arxiv.org/abs/2511.00810v1",
        "published_date": "2025-11-02T05:34:21+00:00",
        "updated_date": "2025-11-02T05:34:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Shijie Zhou",
            "Viet Dac Lai",
            "Hao Tan",
            "Jihyung Kil",
            "Wanrong Zhu",
            "Changyou Chen",
            "Ruiyi Zhang"
        ],
        "tldr": "The paper introduces GUI-AIMA, a novel attention-based framework for GUI grounding that leverages the intrinsic grounding capabilities of MLLMs and achieves state-of-the-art performance with high data efficiency.",
        "tldr_zh": "该论文介绍了GUI-AIMA，一种新颖的基于注意力的GUI grounding框架，利用MLLM的内在grounding能力，并以高数据效率实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack",
        "summary": "Visual-Language Pre-training (VLP) models have achieved significant\nperformance across various downstream tasks. However, they remain vulnerable to\nadversarial examples. While prior efforts focus on improving the adversarial\ntransferability of multimodal adversarial examples through cross-modal\ninteractions, these approaches suffer from overfitting issues, due to a lack of\ninput diversity by relying excessively on information from adversarial examples\nin one modality when crafting attacks in another. To address this issue, we\ndraw inspiration from strategies in some adversarial training methods and\npropose a novel attack called Local Shuffle and Sample-based Attack (LSSA).\nLSSA randomly shuffles one of the local image blocks, thus expanding the\noriginal image-text pairs, generating adversarial images, and sampling around\nthem. Then, it utilizes both the original and sampled images to generate the\nadversarial texts. Extensive experiments on multiple models and datasets\ndemonstrate that LSSA significantly enhances the transferability of multimodal\nadversarial examples across diverse VLP models and downstream tasks. Moreover,\nLSSA outperforms other advanced attacks on Large Vision-Language Models.",
        "url": "http://arxiv.org/abs/2511.00831v1",
        "published_date": "2025-11-02T06:55:49+00:00",
        "updated_date": "2025-11-02T06:55:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xin Liu",
            "Aoyang Zhou",
            "Aoyang Zhou"
        ],
        "tldr": "This paper introduces a novel attack, LSSA, to enhance the adversarial transferability of multimodal adversarial examples in VLP models by shuffling image blocks and sampling around the generated adversarial images, showing improved transferability and performance.",
        "tldr_zh": "本文提出了一种名为LSSA的新型攻击方法，通过混洗图像块并在生成的对抗图像周围进行采样，来增强VLP模型中多模态对抗样本的对抗可迁移性，实验表明该方法提高了可迁移性和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have demonstrated strong performance across\nvarious multimodal tasks, where position encoding plays a vital role in\nmodeling both the sequential structure of textual information and the spatial\nstructure of visual information. However, current VLMs commonly adopt\nmodality-unified 1D or 2D positional indexing strategies, which treat textual\nand visual tokens uniformly without accounting for their distinct structural\nproperties and sequential continuity for text and spatial coherence for vision.\nTo address this limitation, we propose OMEGA, a novel position encoding\nframework that employs Modality-Specific Position Encoding (MSPE) to assign\npositional indices while preserving the inherent structures of each modality\nacross separate coordinate dimensions. Additionally, to align the information\ndensity of multimodal data in the positional index space, OMEGA introduces\nGlobal Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the\nposition encoding step size of visual tokens based on the embedding entropy of\nboth modalities. Experimental results demonstrate that OMEGA consistently\nenhances VLM performance across diverse architectures and VQA benchmarks. On\nvisual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline\nposition encoding strategies on Qwen2.5-VL-3B, with consistent gains observed\nacross larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.",
        "url": "http://arxiv.org/abs/2511.00821v1",
        "published_date": "2025-11-02T06:19:44+00:00",
        "updated_date": "2025-11-02T06:19:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruoxiang Huang",
            "Xindian Ma",
            "Rundong Kong",
            "Zhen Yuan",
            "Peng Zhang"
        ],
        "tldr": "The paper introduces OMEGA, a novel position encoding framework for VLMs that uses modality-specific position encoding and global adaptive encoding step scaling to improve performance, showing gains on various VQA benchmarks and VLM architectures.",
        "tldr_zh": "该论文介绍了一种新的视觉语言模型位置编码框架OMEGA，它使用模态特定的位置编码和全局自适应编码步长缩放来提高性能，并在各种VQA基准测试和VLM架构上显示出收益。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models",
        "summary": "Social media has exacerbated the promotion of Western beauty norms, leading\nto negative self-image, particularly in women and girls, and causing harm such\nas body dysmorphia. Increasingly content on the internet has been artificially\ngenerated, leading to concerns that these norms are being exaggerated. The aim\nof this work is to study how generative AI models may encode 'beauty' and erase\n'ugliness', and discuss the implications of this for society. To investigate\nthese aims, we create two image generation pipelines: a text-to-image model and\na text-to-language model-to image model. We develop a structured beauty\ntaxonomy which we use to prompt three language models (LMs) and two\ntext-to-image models to cumulatively generate 5984 images using our two\npipelines. We then recruit women and non-binary social media users to evaluate\n1200 of the images through a Likert-scale within-subjects study. Participants\nshow high agreement in their ratings. Our results show that 86.5% of generated\nimages depicted people with lighter skin tones, 22% contained explicit content\ndespite Safe for Work (SFW) training, and 74% were rated as being in a younger\nage demographic. In particular, the images of non-binary individuals were rated\nas both younger and more hypersexualised, indicating troubling intersectional\neffects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such\nas \"a wide nose\") consistently produced higher Not SFW (NSFW) ratings\nregardless of gender. This work sheds light on the pervasive demographic biases\nrelated to beauty standards present in generative AI models -- biases that are\nactively perpetuated by model developers, such as via negative prompting. We\nconclude by discussing the implications of this on society, which include\npollution of the data streams and active erasure of features that do not fall\ninside the stereotype of what is considered beautiful by developers.",
        "url": "http://arxiv.org/abs/2511.00749v2",
        "published_date": "2025-11-02T00:31:13+00:00",
        "updated_date": "2025-11-04T22:07:28+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Tanvi Dinkar",
            "Aiqi Jiang",
            "Gavin Abercrombie",
            "Ioannis Konstas"
        ],
        "tldr": "This paper investigates how generative AI models encode beauty standards, finding biases in generated images related to skin tone, age, and hypersexualization, particularly affecting non-binary individuals and those with 'ugly' traits.",
        "tldr_zh": "该论文研究了生成式AI模型如何编码美学标准，发现生成图像中存在与肤色、年龄和过度性化相关的偏差，尤其影响非二元性别者和具有“丑陋”特征的人。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 6
    },
    {
        "title": "VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel",
        "summary": "Accurate vessel segmentation is critical for clinical applications such as\ndisease diagnosis and surgical planning, yet remains challenging due to thin,\nbranching structures and low texture contrast. While foundation models like the\nSegment Anything Model (SAM) have shown promise in generic segmentation, they\nperform sub-optimally on vascular structures. In this work, we present VesSAM,\na powerful and efficient framework tailored for 2D vessel segmentation. VesSAM\nintegrates (1) a convolutional adapter to enhance local texture features, (2) a\nmulti-prompt encoder that fuses anatomical prompts, including skeletons,\nbifurcation points, and segment midpoints, via hierarchical cross-attention,\nand (3) a lightweight mask decoder to reduce jagged artifacts. We also\nintroduce an automated pipeline to generate structured multi-prompt\nannotations, and curate a diverse benchmark dataset spanning 8 datasets across\n5 imaging modalities. Experimental results demonstrate that VesSAM consistently\noutperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%\nIoU, and achieves competitive performance compared to fully fine-tuned methods,\nwith significantly fewer parameters. VesSAM also generalizes well to\nout-of-distribution (OoD) settings, outperforming all baselines in average OoD\nDice and IoU.",
        "url": "http://arxiv.org/abs/2511.00981v1",
        "published_date": "2025-11-02T15:47:05+00:00",
        "updated_date": "2025-11-02T15:47:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Suzhong Fu",
            "Rui Sun",
            "Xuan Ding",
            "Jingqi Dong",
            "Yiming Yang",
            "Yao Zhu",
            "Min Chang Jordan Ren",
            "Delin Deng",
            "Angelica Aviles-Rivero",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "tldr": "VesSAM is a specialized framework for 2D vessel segmentation that leverages a convolutional adapter, multi-prompt encoder, and lightweight mask decoder, achieving state-of-the-art performance with fewer parameters than fine-tuned methods.",
        "tldr_zh": "VesSAM是一个专门用于二维血管分割的框架，它利用卷积适配器、多提示编码器和轻量级掩码解码器，以比微调方法更少的参数实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference",
        "summary": "We propose EVTAR, an End-to-End Virtual Try-on model with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages, human pose, densepose, or body keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts a\ntwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks, densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preserve\ngarment texture and fine-grained details better. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwith supplementary references and unpaired person images to support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2511.00956v1",
        "published_date": "2025-11-02T14:32:31+00:00",
        "updated_date": "2025-11-02T14:32:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liuzhuozheng Li",
            "Yue Gong",
            "Shanyuan Liu",
            "Bo Cheng",
            "Yuhang Ma",
            "Liebucha Wu",
            "Dengyang Jiang",
            "Zanyi Wang",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "tldr": "The paper introduces EVTAR, an end-to-end virtual try-on model that simplifies the input requirements and enhances accuracy by using additional unpaired visual references of the same garment, leading to more realistic results.",
        "tldr_zh": "该论文介绍了EVTAR，一个端到端的虚拟试穿模型，通过使用同一服装的额外无配对视觉参考，简化了输入要求并提高了准确性，从而产生更逼真的结果。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]