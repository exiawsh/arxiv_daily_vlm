[
    {
        "title": "Multimodal Iterative RAG for Knowledge Visual Question Answering",
        "summary": "While Multimodal Large Language Models (MLLMs) have significantly advanced\nmultimodal understanding, their performance remains limited on\nknowledge-intensive visual questions that require external knowledge beyond the\nimage. Retrieval-Augmented Generation (RAG) has become a promising solution for\nproviding models with external knowledge, its conventional single-pass\nframework often fails to gather sufficient knowledge. To overcome this\nlimitation, we propose MI-RAG, a Multimodal Iterative RAG framework that\nleverages reasoning to enhance retrieval and update reasoning over newly\nretrieved knowledge across modalities. At each iteration, MI-RAG leverages an\naccumulated reasoning record to dynamically formulate a multi-query. These\nqueries then drive a joint search across heterogeneous knowledge bases\ncontaining both visually-grounded and textual knowledge. The newly acquired\nknowledge is synthesized into the reasoning record, progressively refining\nunderstanding across iterations. Experiments on challenging benchmarks,\nincluding Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG\nsignificantly improves both retrieval recall and answer accuracy, establishing\na scalable approach for compositional reasoning in knowledge-intensive VQA.",
        "url": "http://arxiv.org/abs/2509.00798v2",
        "published_date": "2025-08-31T11:14:54+00:00",
        "updated_date": "2025-09-03T14:36:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Changin Choi",
            "Wonseok Lee",
            "Jungmin Ko",
            "Wonjong Rhee"
        ],
        "tldr": "The paper introduces MI-RAG, a Multimodal Iterative RAG framework that uses reasoning to improve knowledge retrieval for knowledge-intensive VQA tasks, achieving better recall and accuracy on benchmark datasets.",
        "tldr_zh": "该论文介绍了MI-RAG，一个多模态迭代RAG框架，它利用推理来改进知识密集型VQA任务的知识检索，并在基准数据集上实现了更好的召回率和准确率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving",
        "summary": "Recent advances in vision-language models (VLMs) have demonstrated impressive\nspatial reasoning capabilities for autonomous driving, yet existing methods\npredominantly focus on static scene understanding while neglecting the\nessential temporal dimension of real-world driving scenarios. To address this\ncritical limitation, we propose the OmniReason framework, which establishes\nrobust spatiotemporal reasoning by jointly modeling dynamic 3D environments and\ntheir underlying decision-making processes. Our work makes two fundamental\nadvances: (1) We introduce OmniReason-Data, two large-scale\nvision-language-action (VLA) datasets with dense spatiotemporal annotations and\nnatural language explanations, generated through a novel\nhallucination-mitigated auto-labeling pipeline that ensures both physical\nplausibility and temporal coherence; (2) We develop the OmniReason-Agent\narchitecture, which integrates a sparse temporal memory module for persistent\nscene context modeling and an explanation generator that produces\nhuman-interpretable decision rationales, facilitated by our spatiotemporal\nknowledge distillation approach that effectively captures spatiotemporal causal\nreasoning patterns. Comprehensive experiments demonstrate state-of-the-art\nperformance, where OmniReason-Agent achieves significant improvements in both\nopen-loop planning tasks and visual question answering (VQA) benchmarks, while\nestablishing new capabilities for interpretable, temporally-aware autonomous\nvehicles operating in complex, dynamic environments.",
        "url": "http://arxiv.org/abs/2509.00789v1",
        "published_date": "2025-08-31T10:34:44+00:00",
        "updated_date": "2025-08-31T10:34:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pei Liu",
            "Qingtian Ning",
            "Xinyan Lu",
            "Haipeng Liu",
            "Weiliang Ma",
            "Dangen She",
            "Peng Jia",
            "Xianpeng Lang",
            "Jun Ma"
        ],
        "tldr": "The paper introduces OmniReason, a framework with associated datasets and an agent architecture, for spatiotemporal reasoning in autonomous driving using VLMs, focusing on temporal coherence and interpretable decision-making.",
        "tldr_zh": "本文介绍了OmniReason框架，包含数据集和代理架构，用于自动驾驶中基于VLM的时空推理，重点关注时间连贯性和可解释的决策。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification",
        "summary": "We present a unified vision-language framework tailored for ENT endoscopy\nimage analysis that simultaneously tackles three clinically-relevant tasks:\nimage classification, image-to-image retrieval, and text-to-image retrieval.\nUnlike conventional CNN-based pipelines that struggle to capture cross-modal\nsemantics, our approach leverages the CLIP ViT-B/16 backbone and enhances it\nthrough Low-Rank Adaptation, multi-level CLS token aggregation, and spherical\nfeature interpolation. These components collectively enable efficient\nfine-tuning on limited medical data while improving representation diversity\nand semantic alignment across modalities. To bridge the gap between visual\ninputs and textual diagnostic context, we introduce class-specific natural\nlanguage prompts that guide the image encoder through a joint training\nobjective combining supervised classification with contrastive learning. We\nvalidated our framework through participation in the ACM MM'25 ENTRep Grand\nChallenge, achieving 95% accuracy and F1-score in classification, Recall@1 of\n0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, and\nMRR scores of 0.97 and 0.96. Ablation studies demonstrated the incremental\nbenefits of each architectural component, validating the effectiveness of our\ndesign for robust multimodal medical understanding in low-resource clinical\nsettings.",
        "url": "http://arxiv.org/abs/2509.00752v1",
        "published_date": "2025-08-31T09:03:39+00:00",
        "updated_date": "2025-08-31T09:03:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Y Hop Nguyen",
            "Doan Anh Phan Huu",
            "Trung Thai Tran",
            "Nhat Nam Mai",
            "Van Toi Giap",
            "Thao Thi Phuong Dao",
            "Trung-Nghia Le"
        ],
        "tldr": "This paper introduces a unified vision-language framework for ENT endoscopy image analysis, addressing image classification, image-to-image retrieval, and text-to-image retrieval. It achieves strong performance on the ACM MM'25 ENTRep Grand Challenge using a CLIP-based approach with several novel enhancements.",
        "tldr_zh": "本文提出了一个统一的视觉-语言框架，用于耳鼻喉内窥镜图像分析，解决了图像分类、图像到图像检索和文本到图像检索问题。该方法基于CLIP，并通过多项创新增强，在ACM MM'25 ENTRep Grand Challenge上取得了优异的成绩。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model",
        "summary": "Multimodal Large Language Models (MLLMs) have made significant progress in\nbridging visual perception with high-level textual reasoning. However, they\nface a fundamental contradiction: while excelling at complex semantic\nunderstanding, these models often fail at basic visual tasks that require\nprecise detail perception. This deficiency primarily stems from the prevalent\narchitectural reliance on a single vision encoder optimized for high-level\nsemantic alignment, which inherently sacrifices the ability to capture\nfine-grained visual information. To address this issue, we introduce Fusion to\nEnhance (FtZ), a novel vision tower framework. FtZ moves beyond the\nsingle-encoder design by innovatively composing a semantically powerful anchor\nencoder with a perception-rich augmenting encoder via a lightweight Multi-Head\nCross-Attention mechanism. Experimental results demonstrate that on several\nchallenging benchmarks demanding fine-grained visual understanding, such as\nTextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms\nbaselines that use only a single encoder or existing feature fusion methods.\nThis work proves that composing heterogeneous expert encoders is an efficient\nand effective path to overcoming the visual perception bottleneck in current\nMLLMs, offering a new design paradigm for building next-generation AI systems\nwith stronger perceptual capabilities.",
        "url": "http://arxiv.org/abs/2509.00664v1",
        "published_date": "2025-08-31T02:22:57+00:00",
        "updated_date": "2025-08-31T02:22:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yifei She",
            "Huangxuan Wu"
        ],
        "tldr": "The paper introduces Fusion to Enhance (FtZ), a novel vision tower framework for Multimodal Large Language Models (MLLMs) that combines a semantically powerful anchor encoder with a perception-rich augmenting encoder to improve fine-grained visual understanding, outperforming single-encoder baselines.",
        "tldr_zh": "该论文介绍了Fusion to Enhance (FtZ)，一种用于多模态大型语言模型(MLLMs)的新型视觉塔框架，它将语义强大的锚编码器与感知丰富的增强编码器相结合，以提高细粒度的视觉理解能力，优于单编码器基线。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model",
        "summary": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of\nrobot behaviors recorded in authentic human living and working environments.\nAll demonstrations are gathered using a consistent robotic embodiment, paired\nwith precise subtask-level language annotations to facilitate both training and\nevaluation. Building on this dataset, we introduce G0, a dual-system framework\nthat couples a Vision-Language Model (VLM) for multimodal planning with a\nVision-Language-Action (VLA) model for fine-grained execution. G0 is trained\nusing a three-stage curriculum: cross-embodiment pre-training,\nsingle-embodiment pre-training, and task-specific post-training. A\ncomprehensive benchmark spanning tabletop manipulation, few-shot learning, and\nlong-horizon mobile manipulation, demonstrates the effectiveness of our\napproach. In particular, we find that the single-embodiment pre-training stage,\ntogether with the Galaxea Open-World Dataset, plays a critical role in\nachieving strong performance.",
        "url": "http://arxiv.org/abs/2509.00576v1",
        "published_date": "2025-08-30T18:04:19+00:00",
        "updated_date": "2025-08-30T18:04:19+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tao Jiang",
            "Tianyuan Yuan",
            "Yicheng Liu",
            "Chenhao Lu",
            "Jianning Cui",
            "Xiao Liu",
            "Shuiqi Cheng",
            "Jiyang Gao",
            "Huazhe Xu",
            "Hang Zhao"
        ],
        "tldr": "The paper introduces Galaxea, a large-scale robot behavior dataset with language annotations, and G0, a dual-system VLM-VLA model trained on this dataset for robotic manipulation and planning.",
        "tldr_zh": "本文介绍了Galaxea，一个大规模的机器人行为数据集，带有语言注释；以及G0，一个基于该数据集训练的用于机器人操作和规划的双系统VLM-VLA模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation",
        "summary": "The emergence of vision language models (VLMs) has bridged vision and\nlanguage, enabling joint multimodal understanding beyond traditional\nvisual-only deep learning models. However, transferring VLMs from the natural\nimage domain to remote sensing (RS) segmentation remains challenging due to the\nlimited category diversity in RS datasets and the domain gap between natural\nand RS imagery. Here, we propose a training-free framework, DGL-RSIS, that\ndecouples visual and textual inputs, performing visual-language alignment at\nboth the local semantic and global contextual levels through tailored\nstrategies. Specifically, we first introduce a global-local decoupling (GLD)\nmodule, where text inputs are divided into local class nouns and global\nmodifiers using natural language processing (NLP) techniques; image inputs are\npartitioned into a set of class-agnostic mask proposals via unsupervised mask\nproposal networks. Second, visual and textual features are aligned at local\nscale, through a novel context-aware cropping strategy for extracting image\npatches with proper boundaries and introducing RS-specific knowledge to enrich\nthe text inputs. By matching the enhanced text features with mask-guided visual\nfeatures, we enable the mask classification, supporting open-vocabulary\nsemantic segmentation (OVSS). Third, at the global scale, we propose a\nCross-Scale Grad-CAM module to refine Grad-CAM maps using contextual\ninformation from global modifiers. A subsequent mask selection module\nintegrates pixel-level Grad-CAM activations into the mask-level segmentation\noutput, such that accurate and interpretable alignment can be realized across\nglobal and local dimensions for referring expression segmentation (RES).",
        "url": "http://arxiv.org/abs/2509.00598v1",
        "published_date": "2025-08-30T19:45:25+00:00",
        "updated_date": "2025-08-30T19:45:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boyi Li",
            "Ce Zhang",
            "Richard M. Timmerman",
            "Wenxuan Bao"
        ],
        "tldr": "The paper introduces DGL-RSIS, a training-free framework for remote sensing image segmentation that decouples visual and textual inputs, aligning them at local semantic and global contextual levels. It leverages VLMs for open-vocabulary semantic segmentation and referring expression segmentation in remote sensing imagery.",
        "tldr_zh": "该论文介绍了DGL-RSIS，一个无需训练的遥感图像分割框架，它将视觉和文本输入解耦，并在局部语义和全局上下文层面进行对齐。它利用VLM进行遥感图像中的开放词汇语义分割和指称表达式分割。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]