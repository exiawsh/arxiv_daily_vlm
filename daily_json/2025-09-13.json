[
    {
        "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography",
        "summary": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.",
        "url": "http://arxiv.org/abs/2509.10344v1",
        "published_date": "2025-09-12T15:33:18+00:00",
        "updated_date": "2025-09-12T15:33:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuexi Du",
            "Lihui Chen",
            "Nicha C. Dvornek"
        ],
        "tldr": "The paper introduces GLAM, a geometry-guided local alignment method for multi-view mammography VLM pretraining, leveraging multi-view relationships to improve performance on mammography datasets.",
        "tldr_zh": "该论文介绍了GLAM，一种几何引导的局部对齐方法，用于多视图乳腺X光照片VLM预训练，利用多视图关系来提高在乳腺X光照片数据集上的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Detecting Text Manipulation in Images using Vision Language Models",
        "summary": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.",
        "url": "http://arxiv.org/abs/2509.10278v1",
        "published_date": "2025-09-12T14:20:29+00:00",
        "updated_date": "2025-09-12T14:20:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vidit Vidit",
            "Pavel Korshunov",
            "Amir Mohammadi",
            "Christophe Ecabert",
            "Ketan Kotwal",
            "Sébastien Marcel"
        ],
        "tldr": "The paper investigates the performance of various VLMs, including open-source and closed-source models, on detecting text manipulation in images, highlighting the gap between them and the generalization issues of image manipulation-specific VLMs.",
        "tldr_zh": "本文研究了各种视觉语言模型（包括开源和闭源模型）在检测图像中文本篡改方面的性能，强调了它们之间的差距以及图像篡改专用视觉语言模型的泛化问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation",
        "summary": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.",
        "url": "http://arxiv.org/abs/2509.10260v1",
        "published_date": "2025-09-12T14:03:00+00:00",
        "updated_date": "2025-09-12T14:03:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jia Wang",
            "Jie Hu",
            "Xiaoqi Ma",
            "Hanghang Ma",
            "Yanbing Zeng",
            "Xiaoming Wei"
        ],
        "tldr": "This paper introduces MagicMirror, a comprehensive framework including a large-scale dataset, a VLM assessor, and an automated benchmark, for fine-grained evaluation of artifacts in text-to-image generation, revealing significant artifact issues even in state-of-the-art models.",
        "tldr_zh": "本文介绍了MagicMirror，一个综合框架，包括大规模数据集、VLM评估器和自动化基准，用于对文本到图像生成中的伪影进行细粒度评估，揭示了即使在最先进的模型中也存在严重的伪影问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VARCO-VISION-2.0 Technical Report",
        "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.",
        "url": "http://arxiv.org/abs/2509.10105v1",
        "published_date": "2025-09-12T09:55:56+00:00",
        "updated_date": "2025-09-12T09:55:56+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Young-rok Cha",
            "Jeongho Ju",
            "SunYoung Park",
            "Jong-Hyeon Lee",
            "Younghyun Yu",
            "Youngjune Kim"
        ],
        "tldr": "VARCO-VISION-2.0 is a new open-weight bilingual (Korean/English) Vision-Language Model with improved multi-image understanding, layout-aware OCR, and preference optimization, available in 14B and 1.7B versions.",
        "tldr_zh": "VARCO-VISION-2.0 是一种新的开源双语（韩语/英语）视觉语言模型，具有改进的多图像理解、布局感知 OCR 和偏好优化功能，提供 14B 和 1.7B 版本。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA",
        "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}",
        "url": "http://arxiv.org/abs/2509.10026v1",
        "published_date": "2025-09-12T07:45:44+00:00",
        "updated_date": "2025-09-12T07:45:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jing Huang",
            "Zhiya Tan",
            "Shutao Gong",
            "Fanwei Zeng",
            "Jianshu Li"
        ],
        "tldr": "The paper introduces LaV-CoT, a language-aware visual CoT framework for multilingual VQA, featuring a multi-stage reasoning pipeline and multi-aspect reward optimization. It achieves state-of-the-art results on multiple benchmarks and is validated through real-world A/B testing.",
        "tldr_zh": "该论文介绍了 LaV-CoT，一个用于多语言 VQA 的语言感知视觉 CoT 框架，具有多阶段推理流水线和多方面奖励优化。它在多个基准测试中取得了最先进的结果，并通过真实世界的 A/B 测试进行了验证。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention",
        "summary": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.",
        "url": "http://arxiv.org/abs/2509.10359v1",
        "published_date": "2025-09-12T15:47:50+00:00",
        "updated_date": "2025-09-12T15:47:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Matteo Trippodo",
            "Federico Becattini",
            "Lorenzo Seidenari"
        ],
        "tldr": "This paper introduces an adversarial attack, Attention Attack, targeting text-to-image editing methods by disrupting cross-attention using an automatically generated caption as a proxy for the edit prompt, and proposes new evaluation metrics for immunization success.",
        "tldr_zh": "该论文提出了一种针对文本到图像编辑方法的对抗攻击，即注意力攻击。该攻击通过使用自动生成的图像标题作为编辑提示的代理来扰乱交叉注意力，并且提出了用于免疫成功的新的评估指标。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Understanding Visual Grounding in Visual Language Models",
        "summary": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.",
        "url": "http://arxiv.org/abs/2509.10345v1",
        "published_date": "2025-09-12T15:33:49+00:00",
        "updated_date": "2025-09-12T15:33:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Georgios Pantazopoulos",
            "Eda B. Özyiğit"
        ],
        "tldr": "This survey paper reviews visual grounding techniques in Vision Language Models (VLMs), covering core components, applications, benchmarks, interrelations with reasoning, challenges, and future directions.",
        "tldr_zh": "这篇综述论文回顾了视觉语言模型（VLMs）中的视觉基础技术，涵盖了核心组件、应用、基准、与推理的相互关系、挑战和未来方向。",
        "relevance_score": 8,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration",
        "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math",
        "url": "http://arxiv.org/abs/2509.10059v1",
        "published_date": "2025-09-12T08:46:49+00:00",
        "updated_date": "2025-09-12T08:46:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yue Zhou",
            "Litong Feng",
            "Mengcheng Lan",
            "Xue Yang",
            "Qingyun Li",
            "Yiping Ke",
            "Xue Jiang",
            "Wayne Zhang"
        ],
        "tldr": "The paper introduces AVI-Math, a new benchmark for evaluating multimodal mathematical reasoning in aerial vehicle imagery, and finds that existing VLMs struggle with these tasks, highlighting limitations and suggesting future research directions.",
        "tldr_zh": "该论文介绍了一个新的基准测试 AVI-Math，用于评估航空图像中的多模态数学推理，并发现现有的 VLM 在这些任务中表现不佳，突出了局限性并提出了未来研究方向。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]