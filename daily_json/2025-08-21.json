[
    {
        "title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion",
        "summary": "Whole slide images (WSIs) in computational pathology (CPath) pose a major\ncomputational challenge due to their gigapixel scale, often requiring the\nprocessing of tens to hundreds of thousands of high-resolution patches per\nslide. This results in prohibitive encoding costs, with preprocessing and\ntraining times extending to days or even weeks-making WSI encoding the most\nsignificant bottleneck in real-world deployment. In this work, we propose\nWISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain\nvision-language models and large language models to address this challenge by\nselectively processing diagnostically relevant regions. WISE-FUSE first\ncomputes similarity scores between low-resolution patches and class-specific\ntextual descriptions using a knowledge distillation mechanism that preserves\nfine-grained diagnostic features. Based on these similarity scores, we select a\nsmall subset of informative regions for the target task, which quickly\neliminates irrelevant patches at the coarse level. The corresponding\nhigh-resolution patches are then selectively encoded and fused with textual\nembeddings to reinforce diagnostic context. Extensive experiments demonstrate\nthat WISE-FUSE reduces WSI encoding time by over threefold while achieving\ndiagnostic performance comparable to or surpassing that of exhaustive patch\nprocessing, offering a scalable and practical solution for CPath.",
        "url": "http://arxiv.org/abs/2508.14537v1",
        "published_date": "2025-08-20T08:41:19+00:00",
        "updated_date": "2025-08-20T08:41:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yonghan Shin",
            "SeungKyu Kim",
            "Won-Ki Jeong"
        ],
        "tldr": "WISE-FUSE accelerates whole slide image encoding by selectively processing diagnostically relevant regions using VLM and LLM knowledge fusion, achieving comparable or better performance than exhaustive methods with significantly reduced encoding time.",
        "tldr_zh": "WISE-FUSE通过VLM和LLM知识融合选择性地处理与诊断相关的区域，从而加速了全玻片图像编码，在显著减少编码时间的同时，实现了与穷举方法相当或更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference",
        "summary": "Explainable object recognition using vision-language models such as CLIP\ninvolves predicting accurate category labels supported by rationales that\njustify the decision-making process. Existing methods typically rely on\nprompt-based conditioning, which suffers from limitations in CLIP's text\nencoder and provides weak conditioning on explanatory structures. Additionally,\nprior datasets are often restricted to single, and frequently noisy, rationales\nthat fail to capture the full diversity of discriminative image features. In\nthis work, we introduce a multi-rationale explainable object recognition\nbenchmark comprising datasets in which each image is annotated with multiple\nground-truth rationales, along with evaluation metrics designed to offer a more\ncomprehensive representation of the task. To overcome the limitations of\nprevious approaches, we propose a contrastive conditional inference (CCI)\nframework that explicitly models the probabilistic relationships among image\nembeddings, category labels, and rationales. Without requiring any training,\nour framework enables more effective conditioning on rationales to predict\naccurate object categories. Our approach achieves state-of-the-art results on\nthe multi-rationale explainable object recognition benchmark, including strong\nzero-shot performance, and sets a new standard for both classification accuracy\nand rationale quality. Together with the benchmark, this work provides a more\ncomplete framework for evaluating future models in explainable object\nrecognition. The code will be made available online.",
        "url": "http://arxiv.org/abs/2508.14280v1",
        "published_date": "2025-08-19T21:28:12+00:00",
        "updated_date": "2025-08-19T21:28:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ali Rasekh",
            "Sepehr Kazemi Ranjbar",
            "Simon Gottschalk"
        ],
        "tldr": "The paper introduces a new multi-rationale explainable object recognition benchmark and a Contrastive Conditional Inference (CCI) framework that achieves state-of-the-art results without training, improving both classification accuracy and rationale quality in zero-shot settings.",
        "tldr_zh": "该论文介绍了一个新的多理由可解释对象识别基准，以及一个无需训练即可实现最先进结果的对比条件推理 (CCI) 框架，从而提高了零样本设置中的分类准确性和理由质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models",
        "summary": "Large multimodal models (LMMs) have gained impressive performance due to\ntheir outstanding capability in various understanding tasks. However, these\nmodels still suffer from some fundamental limitations related to robustness and\ngeneralization due to the alignment and correlation between visual and textual\nfeatures. In this paper, we introduce a simple but efficient learning mechanism\nfor improving the robust alignment between visual and textual modalities by\nsolving shuffling problems. In particular, the proposed approach can improve\nreasoning capability, visual understanding, and cross-modality alignment by\nintroducing two new tasks: reconstructing the image order and the text order\ninto the LMM's pre-training and fine-tuning phases. In addition, we propose a\nnew directed-token approach to capture visual and textual knowledge, enabling\nthe capability to reconstruct the correct order of visual inputs. Then, we\nintroduce a new Image-to-Response Guided loss to further improve the visual\nunderstanding of the LMM in its responses. The proposed approach consistently\nachieves state-of-the-art (SoTA) performance compared with prior LMMs on\nacademic task-oriented and instruction-following LMM benchmarks.",
        "url": "http://arxiv.org/abs/2508.14264v1",
        "published_date": "2025-08-19T20:53:24+00:00",
        "updated_date": "2025-08-19T20:53:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thanh-Dat Truong",
            "Huu-Thien Tran",
            "Tran Thai Son",
            "Bhiksha Raj",
            "Khoa Luu"
        ],
        "tldr": "The paper introduces a method called Directed-Tokens to improve the robustness and generalization of Large Multimodal Models (LMMs) by enhancing visual-textual alignment through reconstructing image and text order.",
        "tldr_zh": "该论文提出了一种名为 Directed-Tokens 的方法，通过重构图像和文本顺序来增强视觉-文本对齐，从而提高大型多模态模型 (LMM) 的鲁棒性和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
        "summary": "Symmetry is one of the most fundamental geometric cues in computer vision,\nand detecting it has been an ongoing challenge. With the recent advances in\nvision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP\nmodel can aid symmetry detection by leveraging the additional symmetry cues\nfound in the natural image descriptions. We propose CLIPSym, which leverages\nCLIP's image and language encoders and a rotation-equivariant decoder based on\na hybrid of Transformer and $G$-Convolution to detect rotation and reflection\nsymmetries. To fully utilize CLIP's language encoder, we have developed a novel\nprompting technique called Semantic-Aware Prompt Grouping (SAPG), which\naggregates a diverse set of frequent object-based prompts to better integrate\nthe semantic cues for symmetry detection. Empirically, we show that CLIPSym\noutperforms the current state-of-the-art on three standard symmetry detection\ndatasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations\nverifying the benefits of CLIP's pre-training, the proposed equivariant\ndecoder, and the SAPG technique. The code is available at\nhttps://github.com/timyoung2333/CLIPSym.",
        "url": "http://arxiv.org/abs/2508.14197v1",
        "published_date": "2025-08-19T18:43:14+00:00",
        "updated_date": "2025-08-19T18:43:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tinghan Yang",
            "Md Ashiqur Rahman",
            "Raymond A. Yeh"
        ],
        "tldr": "The paper introduces CLIPSym, a novel approach for symmetry detection that leverages CLIP's image and language encoders with a rotation-equivariant decoder and a semantic-aware prompting technique, achieving state-of-the-art results on standard datasets.",
        "tldr_zh": "该论文介绍了CLIPSym，一种新颖的对称性检测方法，它利用CLIP的图像和语言编码器，结合旋转等变解码器和语义感知提示技术，在标准数据集上实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RynnEC: Bringing MLLMs into Embodied World",
        "summary": "We introduce RynnEC, a video multimodal large language model designed for\nembodied cognition. Built upon a general-purpose vision-language foundation\nmodel, RynnEC incorporates a region encoder and a mask decoder, enabling\nflexible region-level video interaction. Despite its compact architecture,\nRynnEC achieves state-of-the-art performance in object property understanding,\nobject segmentation, and spatial reasoning. Conceptually, it offers a\nregion-centric video paradigm for the brain of embodied agents, providing\nfine-grained perception of the physical world and enabling more precise\ninteractions. To mitigate the scarcity of annotated 3D datasets, we propose an\negocentric video based pipeline for generating embodied cognition data.\nFurthermore, we introduce RynnEC-Bench, a region-centered benchmark for\nevaluating embodied cognitive capabilities. We anticipate that RynnEC will\nadvance the development of general-purpose cognitive cores for embodied agents\nand facilitate generalization across diverse embodied tasks. The code, model\ncheckpoints, and benchmark are available at:\nhttps://github.com/alibaba-damo-academy/RynnEC",
        "url": "http://arxiv.org/abs/2508.14160v1",
        "published_date": "2025-08-19T18:00:01+00:00",
        "updated_date": "2025-08-19T18:00:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Ronghao Dang",
            "Yuqian Yuan",
            "Yunxuan Mao",
            "Kehan Li",
            "Jiangpin Liu",
            "Zhikai Wang",
            "Xin Li",
            "Fan Wang",
            "Deli Zhao"
        ],
        "tldr": "RynnEC, a new video multimodal large language model with region-level interaction capabilities, achieves SOTA performance in embodied cognition tasks and introduces a new benchmark and data pipeline for the field.",
        "tldr_zh": "RynnEC是一个新型视频多模态大语言模型，具备区域级交互能力，在具身认知任务中实现了SOTA性能，并为该领域引入了新的基准测试和数据流水线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives",
        "summary": "The explosive growth of video streaming presents challenges in achieving high\naccuracy and low training costs for video-language retrieval. However, existing\nmethods rely on large-scale pre-training to improve video retrieval\nperformance, resulting in significant computational demands. Additionally, the\nfine-grained information in videos and texts remains underexplored. To\nalleviate these problems, we propose a novel framework to learn fine-grained\nfeatures for better alignment and introduce an inference pipeline to improve\nperformance without additional training. Specifically, we employ coarse-to-fine\nobjectives to understand the semantic information of video-text pairs,\nincluding contrastive and matching learning. The fine-grained data used for\ntraining is obtained through the Granularity-Aware Representation module, which\nis designed based on similarity analysis between video frames and words in\ncaptions. Furthermore, we observe that the repetition of keywords in the\noriginal captions, referred to as \"Repetition\", can enhance retrieval\nperformance and improve alignment between video and text. Based on this\ninsight, we propose a novel and effective inference pipeline that incorporates\na voting mechanism and a new Matching Entropy metric to achieve better\nretrieval performance without requiring additional pre-training. Experimental\nresults on four benchmarks demonstrate that the proposed method outperforms\nprevious approaches. Additionally, our inference pipeline achieves significant\nperformance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT\ndataset and a 1.6% increase on the DiDeMo dataset.",
        "url": "http://arxiv.org/abs/2508.14812v1",
        "published_date": "2025-08-20T16:03:56+00:00",
        "updated_date": "2025-08-20T16:03:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyu Zhao",
            "Jiaxi Gu",
            "Shicong Wang",
            "Xing Zhang",
            "Hang Xu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "tldr": "This paper proposes a novel video-language retrieval framework using coarse-to-fine objectives and a repetition-aware inference pipeline, achieving improved performance without large-scale pre-training.",
        "tldr_zh": "本文提出了一种新颖的视频语言检索框架，该框架使用粗到细的目标和一个重复感知的推理流水线，无需大规模预训练即可提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine",
        "summary": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field.",
        "url": "http://arxiv.org/abs/2508.14706v1",
        "published_date": "2025-08-20T13:30:20+00:00",
        "updated_date": "2025-08-20T13:30:20+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Junying Chen",
            "Zhenyang Cai",
            "Zhiheng Liu",
            "Yunjin Yang",
            "Rongsheng Wang",
            "Qingying Xiao",
            "Xiangyi Feng",
            "Zhan Su",
            "Jing Guo",
            "Xiang Wan",
            "Guangjun Yu",
            "Haizhou Li",
            "Benyou Wang"
        ],
        "tldr": "ShizhenGPT is a multimodal LLM for Traditional Chinese Medicine, trained on a large curated dataset, and demonstrates strong performance in TCM visual understanding and multimodal reasoning.",
        "tldr_zh": "ShizhenGPT是一个用于中医药的多模态LLM，它基于一个大型的精心策划的数据集进行训练，并在中医药视觉理解和多模态推理方面表现出强大的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments",
        "summary": "The detection of anomalies in manufacturing processes is crucial to ensure\nproduct quality and identify process deviations. Statistical and data-driven\napproaches remain the standard in industrial anomaly detection, yet their\nadaptability and usability are constrained by the dependence on extensive\nannotated datasets and limited flexibility under dynamic production conditions.\nRecent advances in the perception capabilities of foundation models provide\npromising opportunities for their adaptation to this downstream task. This\npaper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel\nframework that leverages the multimodal and reasoning capabilities of\nfoundation models for industrial anomaly detection. Specifically, PB-IAD\naddresses three key requirements of dynamic production environments: data\nsparsity, agile adaptability, and domain user centricity. In addition to the\nanomaly detection, the framework includes a prompt template that is\nspecifically designed for iteratively implementing domain-specific process\nknowledge, as well as a pre-processing module that translates domain user\ninputs into effective system prompts. This user-centric design allows domain\nexperts to customise the system flexibly without requiring data science\nexpertise. The proposed framework is evaluated by utilizing GPT-4.1 across\nthree distinct manufacturing scenarios, two data modalities, and an ablation\nstudy to systematically assess the contribution of semantic instructions.\nFurthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly\ndetection such as PatchCore. The results demonstrate superior performance,\nparticularly in data-sparse scenarios and low-shot settings, achieved solely\nthrough semantic instructions.",
        "url": "http://arxiv.org/abs/2508.14504v1",
        "published_date": "2025-08-20T07:53:13+00:00",
        "updated_date": "2025-08-20T07:53:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bernd Hofmann",
            "Albert Scheck",
            "Joerg Franke",
            "Patrick Bruendl"
        ],
        "tldr": "This paper introduces PB-IAD, a novel prompt-based industrial anomaly detection framework utilizing multimodal foundation models (specifically GPT-4.1) to address challenges in dynamic manufacturing environments with data sparsity, achieving superior performance over state-of-the-art methods.",
        "tldr_zh": "本文介绍了一种名为PB-IAD的新型基于prompt的工业异常检测框架，该框架利用多模态基础模型（特别是GPT-4.1）来解决动态制造环境中数据稀疏等挑战，并实现了优于现有技术的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities",
        "summary": "We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method\nfits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative\nmodel initially trained on the English corpus. Despite the notable image\ngeneration ability conditioned on English text inputs, Flux performs poorly\nwhen processing non-English prompts, particularly due to linguistic and\ncultural biases inherent in predominantly English-centric training datasets.\nExisting approaches, such as translating non-English prompts into English or\nfinetuning models for bilingual mappings, inadequately address culturally\nspecific semantics, compromising image authenticity and quality. To address\nthis issue, we introduce a novel method to bridge Chinese semantic\nunderstanding with compatibility in English-centric TTI model communities.\nExisting approaches relying on ControlNet-like architectures typically require\na massive parameter scale and lack direct control over Chinese semantics. In\ncomparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to\ncontrol the Flux backbone directly, significantly reducing the number of\nparameters while enhancing the model's understanding of Chinese semantics. This\nintegration significantly improves the generation quality and cultural\nauthenticity without extensive retraining of the entire model, thus maintaining\ncompatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and\nControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese\nand English prompts and achieves superior image generation quality, visual\nrealism, and faithful depiction of Chinese semantics.",
        "url": "http://arxiv.org/abs/2508.14405v1",
        "published_date": "2025-08-20T04:03:54+00:00",
        "updated_date": "2025-08-20T04:03:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Gong",
            "Shanyuan Liu",
            "Liuzhuozheng Li",
            "Jian Zhu",
            "Bo Cheng",
            "Liebucha Wu",
            "Xiaoyu Wu",
            "Yuhang Ma",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "tldr": "The paper introduces CTA-Flux, a method to adapt the English-centric Flux text-to-image model for Chinese prompts, leveraging MMDiT for improved semantic understanding and image quality while maintaining compatibility with existing plugins.",
        "tldr_zh": "该论文介绍了 CTA-Flux，一种用于将以英语为中心的 Flux 文本到图像模型适配于中文提示的方法，利用 MMDiT 提高了语义理解和图像质量，同时保持与现有插件的兼容性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting",
        "summary": "3D scene reconstruction and understanding have gained increasing popularity,\nyet existing methods still struggle to capture fine-grained, language-aware 3D\nrepresentations from 2D images. In this paper, we present GALA, a novel\nframework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting\n(3DGS). GALA distills a scene-specific 3D instance feature field via\nself-supervised contrastive learning. To extend to generalized language feature\nfields, we introduce the core contribution of GALA, a cross-attention module\nwith two learnable codebooks that encode view-independent semantic embeddings.\nThis design not only ensures intra-instance feature similarity but also\nsupports seamless 2D and 3D open-vocabulary queries. It reduces memory\nconsumption by avoiding per-Gaussian high-dimensional feature learning.\nExtensive experiments on real-world datasets demonstrate GALA's remarkable\nopen-vocabulary performance on both 2D and 3D.",
        "url": "http://arxiv.org/abs/2508.14278v1",
        "published_date": "2025-08-19T21:26:49+00:00",
        "updated_date": "2025-08-19T21:26:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Elena Alegret Regalado",
            "Kunyi Li",
            "Sen Wang",
            "Siyun Liang",
            "Michael Niemeyer",
            "Stefano Gasperini",
            "Nassir Navab",
            "Federico Tombari"
        ],
        "tldr": "The paper introduces GALA, a novel framework for open-vocabulary 3D scene understanding using 3D Gaussian Splatting, which distills a scene-specific 3D instance feature field and uses a cross-attention module for 2D/3D open-vocabulary queries.",
        "tldr_zh": "该论文介绍了GALA，一种使用3D高斯溅射进行开放词汇3D场景理解的新框架。它提炼了一个特定于场景的3D实例特征场，并使用一个交叉注意力模块进行2D/3D开放词汇查询。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration",
        "summary": "We present Vivid-VR, a DiT-based generative video restoration method built\nupon an advanced T2V foundation model, where ControlNet is leveraged to control\nthe generation process, ensuring content consistency. However, conventional\nfine-tuning of such controllable pipelines frequently suffers from distribution\ndrift due to limitations in imperfect multimodal alignment, resulting in\ncompromised texture realism and temporal coherence. To tackle this challenge,\nwe propose a concept distillation training strategy that utilizes the\npretrained T2V model to synthesize training samples with embedded textual\nconcepts, thereby distilling its conceptual understanding to preserve texture\nand temporal quality. To enhance generation controllability, we redesign the\ncontrol architecture with two key components: 1) a control feature projector\nthat filters degradation artifacts from input video latents to minimize their\npropagation through the generation pipeline, and 2) a new ControlNet connector\nemploying a dual-branch design. This connector synergistically combines\nMLP-based feature mapping with cross-attention mechanism for dynamic control\nfeature retrieval, enabling both content preservation and adaptive control\nsignal modulation. Extensive experiments show that Vivid-VR performs favorably\nagainst existing approaches on both synthetic and real-world benchmarks, as\nwell as AIGC videos, achieving impressive texture realism, visual vividness,\nand temporal consistency. The codes and checkpoints are publicly available at\nhttps://github.com/csbhr/Vivid-VR.",
        "url": "http://arxiv.org/abs/2508.14483v1",
        "published_date": "2025-08-20T07:14:01+00:00",
        "updated_date": "2025-08-20T07:14:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Bai",
            "Xiaoxu Chen",
            "Canqian Yang",
            "Zongyao He",
            "Sibin Deng",
            "Ying Chen"
        ],
        "tldr": "Vivid-VR improves video restoration using a DiT-based approach with concept distillation and a redesigned ControlNet architecture, achieving state-of-the-art results in texture realism and temporal consistency.",
        "tldr_zh": "Vivid-VR 提出了一种基于DiT的视频修复方法，该方法采用概念提炼和重新设计的ControlNet架构，在纹理真实度和时间一致性方面取得了最先进的成果。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]