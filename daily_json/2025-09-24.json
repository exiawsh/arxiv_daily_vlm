[
    {
        "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation",
        "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.",
        "url": "http://arxiv.org/abs/2509.19244v1",
        "published_date": "2025-09-23T17:05:46+00:00",
        "updated_date": "2025-09-23T17:05:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "tldr": "Lavida-O is a novel masked diffusion model (MDM) that unifies multimodal understanding and generation, achieving state-of-the-art results in object grounding, image generation, and image editing with improved speed and resolution compared to existing models.",
        "tldr_zh": "Lavida-O是一种新型的掩码扩散模型（MDM），它统一了多模态理解和生成，在物体定位、图像生成和图像编辑方面取得了最先进的成果，并且与现有模型相比，速度和分辨率都有所提高。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Learning neuroimaging models from health system-scale data",
        "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.",
        "url": "http://arxiv.org/abs/2509.18638v1",
        "published_date": "2025-09-23T04:49:59+00:00",
        "updated_date": "2025-09-23T04:49:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yiwei Lyu",
            "Samir Harake",
            "Asadur Chowdury",
            "Soumyanil Banerjee",
            "Rachel Gologorsky",
            "Shixuan Liu",
            "Anna-Katharina Meissner",
            "Akshay Rao",
            "Chenhui Zhao",
            "Akhil Kondepudi",
            "Cheng Jiang",
            "Xinhai Hou",
            "Rushikesh S. Joshi",
            "Volker Neuschmelting",
            "Ashok Srinivasan",
            "Dawn Kleindorfer",
            "Brian Athey",
            "Vikas Gulani",
            "Aditya Pandey",
            "Honglak Lee",
            "Todd Hollon"
        ],
        "tldr": "The paper introduces Prima, a vision language model for neuroimaging trained on a large health system dataset, achieving high diagnostic accuracy and fairness across diverse patient groups.",
        "tldr_zh": "该论文介绍了一种名为Prima的神经影像视觉语言模型，该模型基于大型健康系统数据集进行训练，并在不同的患者群体中实现了高诊断准确性和公平性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 10,
        "overall_priority_score": 9
    },
    {
        "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions",
        "summary": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have\nbecome the standard approach for learning discriminative vision-language\nrepresentations. However, these models often exhibit shallow language\nunderstanding, manifesting bag-of-words behaviour. These limitations are\nreinforced by their dual-encoder design, which induces a modality gap.\nAdditionally, the reliance on vast web-collected data corpora for training\nmakes the process computationally expensive and introduces significant privacy\nconcerns. To address these limitations, in this work, we challenge the\nnecessity of vision encoders for retrieval tasks by introducing a vision-free,\nsingle-encoder retrieval pipeline. Departing from the traditional text-to-image\nretrieval paradigm, we migrate to a text-to-text paradigm with the assistance\nof VLLM-generated structured image descriptions. We demonstrate that this\nparadigm shift has significant advantages, including a substantial reduction of\nthe modality gap, improved compositionality, and better performance on short\nand long caption queries, all attainable with only a few hours of calibration\non two GPUs. Additionally, substituting raw images with textual descriptions\nintroduces a more privacy-friendly alternative for retrieval. To further assess\ngeneralisation and address some of the shortcomings of prior compositionality\nbenchmarks, we release two benchmarks derived from Flickr30k and COCO,\ncontaining diverse compositional queries made of short captions, which we coin\nsubFlickr and subCOCO. Our vision-free retriever matches and often surpasses\ntraditional multimodal models. Importantly, our approach achieves\nstate-of-the-art zero-shot performance on multiple retrieval and\ncompositionality benchmarks, with models as small as 0.3B parameters. Code is\navailable at: https://github.com/IoannaNti/LexiCLIP",
        "url": "http://arxiv.org/abs/2509.19203v1",
        "published_date": "2025-09-23T16:22:27+00:00",
        "updated_date": "2025-09-23T16:22:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ioanna Ntinou",
            "Alexandros Xenos",
            "Yassine Ouali",
            "Adrian Bulat",
            "Georgios Tzimiropoulos"
        ],
        "tldr": "This paper introduces a vision-free retrieval pipeline that uses VLLM-generated text descriptions of images instead of the images themselves, achieving state-of-the-art zero-shot performance on retrieval and compositionality benchmarks with significantly reduced computational cost and improved privacy.",
        "tldr_zh": "本文介绍了一种无视觉的检索流程，该流程使用VLLM生成的图像文本描述代替图像本身，在检索和组合基准测试中实现了最先进的零样本性能，同时显著降低了计算成本并提高了隐私性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have demonstrated remarkable performance across\na variety of real-world tasks. However, existing VLMs typically process visual\ninformation by serializing images, a method that diverges significantly from\nthe parallel nature of human vision. Moreover, their opaque internal mechanisms\nhinder both deeper understanding and architectural innovation. Inspired by the\ndual-stream hypothesis of human vision, which distinguishes the \"what\" and\n\"where\" pathways, we deconstruct the visual processing in VLMs into object\nrecognition and spatial perception for separate study. For object recognition,\nwe convert images into text token maps and find that the model's perception of\nimage content unfolds as a two-stage process from shallow to deep layers,\nbeginning with attribute recognition and culminating in semantic\ndisambiguation. For spatial perception, we theoretically derive and empirically\nverify the geometric structure underlying the positional representation in\nVLMs. Based on these findings, we introduce an instruction-agnostic token\ncompression algorithm based on a plug-and-play visual decoder to improve\ndecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.\nThrough rigorous experiments, our work validates these analyses, offering a\ndeeper understanding of VLM internals and providing clear principles for\ndesigning more capable future architectures.",
        "url": "http://arxiv.org/abs/2509.19191v1",
        "published_date": "2025-09-23T16:07:18+00:00",
        "updated_date": "2025-09-23T16:07:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yueyan Li",
            "Chenggong Zhao",
            "Zeyuan Zang",
            "Caixia Yuan",
            "Xiaojie Wang"
        ],
        "tldr": "This paper analyzes visual processing in VLMs, breaking it down into object recognition and spatial perception. It then proposes improvements to decoding efficiency and spatial reasoning based on these findings.",
        "tldr_zh": "本文分析了视觉语言模型中的视觉处理，将其分解为对象识别和空间感知。然后，基于这些发现，提出了改进解码效率和空间推理的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference",
        "summary": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i",
        "url": "http://arxiv.org/abs/2509.19082v1",
        "published_date": "2025-09-23T14:38:25+00:00",
        "updated_date": "2025-09-23T14:38:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexey Nekrasov",
            "Ali Athar",
            "Daan de Geus",
            "Alexander Hermans",
            "Bastian Leibe"
        ],
        "tldr": "The paper presents Sa2VA-i, an improved version of the Sa2VA model for referring video object segmentation, addressing inconsistencies between training and inference. It achieves state-of-the-art results on multiple benchmarks with significant performance gains.",
        "tldr_zh": "本文介绍了Sa2VA-i，一个改进的Sa2VA模型，用于指代表达式引导的视频对象分割，解决了训练和推理过程中的不一致性。它在多个基准测试上取得了最先进的结果，并实现了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards",
        "summary": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.",
        "url": "http://arxiv.org/abs/2509.19003v1",
        "published_date": "2025-09-23T13:47:32+00:00",
        "updated_date": "2025-09-23T13:47:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Honghao Chen",
            "Xingzhou Lou",
            "Xiaokun Feng",
            "Kaiqi Huang",
            "Xinlong Wang"
        ],
        "tldr": "This paper introduces a framework for chain of step reasoning in vision-language models with fine-grained rewards, improving performance on challenging benchmarks and offering insights into multimodal reasoning.",
        "tldr_zh": "该论文提出了一个用于视觉-语言模型中带有细粒度奖励的步骤链推理框架，提高了在具有挑战性的基准测试中的性能，并为多模态推理提供了见解。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning",
        "summary": "While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.",
        "url": "http://arxiv.org/abs/2509.18938v1",
        "published_date": "2025-09-23T12:54:52+00:00",
        "updated_date": "2025-09-23T12:54:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Matheus Vinícius Todescato",
            "Joel Luís Carbonera"
        ],
        "tldr": "This paper introduces a zero-shot image classification framework leveraging a VLM and a pre-trained visual model in a self-learning cycle with pseudo-labeling, achieving superior performance without labeled data or VLM fine-tuning.",
        "tldr_zh": "本文提出了一种零样本图像分类框架，该框架利用VLM和预训练的视觉模型在一个自学习循环中使用伪标签，无需标注数据或微调VLM即可实现卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation",
        "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.",
        "url": "http://arxiv.org/abs/2509.18824v1",
        "published_date": "2025-09-23T09:12:46+00:00",
        "updated_date": "2025-09-23T09:12:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanzuo Lu",
            "Xin Xia",
            "Manlin Zhang",
            "Huafeng Kuang",
            "Jianbin Zheng",
            "Yuxi Ren",
            "Xuefeng Xiao"
        ],
        "tldr": "Hyper-Bagel is a unified acceleration framework for multimodal models, employing speculative decoding and multi-stage distillation to achieve significant speedups in both understanding and generation tasks, enabling near real-time interactive editing.",
        "tldr_zh": "Hyper-Bagel是一个统一的多模态模型加速框架，采用推测解码和多阶段蒸馏技术，显著加速理解和生成任务，实现近乎实时的交互式编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models",
        "summary": "We address the critical gap between the computational demands of\nvision-language models and the possible ultra-low-bit weight precision\n(bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated\nby the substantial computational cost and memory requirements of VLMs, which\nrestrict their applicability in hardware-constrained environments. We propose\nBi-VLM, which separates model weights non-uniformly based on the Gaussian\nquantiles. Our formulation groups the model weights into outlier (salient) and\nmultiple inlier (unsalient) subsets, ensuring that each subset contains a\nproportion of weights corresponding to its quantile in the distribution. We\npropose a saliency-aware hybrid quantization algorithm and use it to quantize\nweights by imposing different constraints on the scaler and binary matrices\nbased on the saliency metric and compression objective. We have evaluated our\napproach on different VLMs. For the language model part of the VLM, our Bi-VLM\noutperforms the SOTA by 3%-47% on the visual question answering task in terms\nof four different benchmarks and three different models. For the overall VLM,\nour Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the\nquantized models and observe that there is redundancy of image tokens 90% - 99%\nin the quantized models. This helps us to further prune the visual tokens to\nimprove efficiency.",
        "url": "http://arxiv.org/abs/2509.18763v1",
        "published_date": "2025-09-23T07:55:48+00:00",
        "updated_date": "2025-09-23T07:55:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xijun Wang",
            "Junyun Huang",
            "Rayyan Abdalla",
            "Chengyuan Zhang",
            "Ruiqi Xian",
            "Dinesh Manocha"
        ],
        "tldr": "The paper proposes Bi-VLM, a novel quantization method for Vision-Language Models that achieves significant performance improvements at ultra-low bit precisions, enabling deployment in resource-constrained environments, and identifies potential for token pruning to further improve efficiency.",
        "tldr_zh": "该论文提出了 Bi-VLM，一种新颖的视觉语言模型量化方法，可在超低位精度下实现显著的性能提升，从而能够在资源受限的环境中进行部署，并发现了通过token剪枝进一步提高效率的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage",
        "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.",
        "url": "http://arxiv.org/abs/2509.18754v1",
        "published_date": "2025-09-23T07:49:30+00:00",
        "updated_date": "2025-09-23T07:49:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuyang Liu",
            "Xinyuan Shi",
            "Bang Yang",
            "Peilin Zhou",
            "Jiahua Dong",
            "Long Chen",
            "Ian Reid",
            "Xiaondan Liang"
        ],
        "tldr": "The paper introduces COLT, a method for enhancing open-source video LLMs with continual tool usage by dynamically selecting relevant tools from a learnable tool codebook, addressing the challenge of evolving tool data streams. They also introduce VideoToolBench dataset for tool-use instruction tuning.",
        "tldr_zh": "该论文介绍了COLT，一种通过从可学习的工具代码簿中动态选择相关工具来增强开源视频LLM的持续工具使用的方法，解决了不断演变的工具数据流的挑战。 他们还介绍了用于工具使用指令调整的VideoToolBench数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Knowledge Transfer from Interaction Learning",
        "summary": "Current visual foundation models (VFMs) face a fundamental limitation in\ntransferring knowledge from vision language models (VLMs), while VLMs excel at\nmodeling cross-modal interactions through unified representation spaces,\nexisting VFMs predominantly adopt result-oriented paradigms that neglect the\nunderlying interaction processes. This representational discrepancy hinders\neffective knowledge transfer and limits generalization across diverse vision\ntasks. We propose Learning from Interactions (LFI), a cognitive-inspired\nframework that addresses this gap by explicitly modeling visual understanding\nas an interactive process. Our key insight is that capturing the dynamic\ninteraction patterns encoded in pre-trained VLMs enables more faithful and\nefficient knowledge transfer to VFMs. The approach centers on two technical\ninnovations, Interaction Queries, which maintain persistent relational\nstructures across network layers, and interaction-based supervision, derived\nfrom the cross-modal attention mechanisms of VLMs. Comprehensive experiments\ndemonstrate consistent improvements across multiple benchmarks, achieving 3.3\nand 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO\ndetection/segmentation respectively, with minimal parameter overhead and faster\nconvergence. The framework particularly excels in cross-domain settings,\ndelivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human\nevaluations further confirm its cognitive alignment, outperforming\nresult-oriented methods by 2.7 times in semantic consistency metrics.",
        "url": "http://arxiv.org/abs/2509.18733v1",
        "published_date": "2025-09-23T07:27:36+00:00",
        "updated_date": "2025-09-23T07:27:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yilin Gao",
            "Kangyi Chen",
            "Zhongxing Peng",
            "Hengjie Lu",
            "Shugong Xu"
        ],
        "tldr": "This paper introduces Learning from Interactions (LFI), a framework for transferring knowledge from VLMs to VFMs by explicitly modeling visual understanding as an interactive process, achieving performance gains across various benchmarks.",
        "tldr_zh": "本文介绍了从交互中学习（LFI），一个通过显式地将视觉理解建模为交互过程，从而将知识从 VLM 转移到 VFM 的框架，并在各种基准测试中实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation",
        "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG",
        "url": "http://arxiv.org/abs/2509.18639v1",
        "published_date": "2025-09-23T04:52:39+00:00",
        "updated_date": "2025-09-23T04:52:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhuiyi Lyu",
            "Chi Kit Wong",
            "Chenfei Liao",
            "Lutao Jiang",
            "Xu Zheng",
            "Zexin Lu",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "tldr": "The paper introduces Understanding-in-Generation (UiG), a novel framework that integrates the understanding capabilities of unified models into the image generation process via an 'Image Editing' bridge to improve text-to-image generation, showing a 3.92% gain on the TIIF benchmark.",
        "tldr_zh": "该论文介绍了一种名为 Understanding-in-Generation (UiG) 的新框架，该框架通过“图像编辑”桥梁将统一模型的理解能力集成到图像生成过程中，从而改进文本到图像的生成，在 TIIF 基准测试中获得了 3.92% 的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
        "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
        "url": "http://arxiv.org/abs/2509.18592v1",
        "published_date": "2025-09-23T03:23:03+00:00",
        "updated_date": "2025-09-23T03:23:03+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Neel P. Bhatt",
            "Yunhao Yang",
            "Rohan Siva",
            "Pranay Samineni",
            "Daniel Milan",
            "Zhangyang Wang",
            "Ufuk Topcu"
        ],
        "tldr": "The paper introduces VLN-Zero, a two-phase vision-language navigation framework that uses VLMs for scene graph construction and a neurosymbolic planner with caching for efficient zero-shot navigation in unseen environments, achieving significant performance gains.",
        "tldr_zh": "该论文介绍了VLN-Zero，一个两阶段的视觉语言导航框架，利用视觉语言模型构建场景图，并使用带有缓存的神经符号规划器，在未见环境中实现高效的零样本导航，并取得了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers",
        "summary": "While editing directly from life, photographers have found it too difficult\nto see simultaneously both the blue and the sky. Photographer and curator,\nSzarkowski insightfully revealed one of the notable gaps between general and\naesthetic visual understanding: while the former focuses on identifying the\nfactual element in an image (sky), the latter transcends such object\nidentification, viewing it instead as an aesthetic component--a pure color\nblock (blue). Such fundamental distinctions between general (detection,\nlocalization, etc.) and aesthetic (color, lighting, composition, etc.) visual\nunderstanding present a significant challenge for Multimodal Large Language\nModels (MLLMs). Although some recent works have made initial explorations, they\nare often limited to general and basic aesthetic commonsense. As a result, they\nfrequently fall short in real-world scenarios (Fig. 1), which require extensive\nexpertise--including photographic techniques, photo pre/post-processing\nknowledge, and more, to provide a detailed analysis and description. To\nfundamentally enhance the aesthetics understanding of MLLMs, we first introduce\na novel dataset, PhotoCritique, derived from extensive discussions among\nprofessional photographers and enthusiasts, and characterized by the large\nscale, expertise, and diversity. Then, to better learn visual aesthetics from\nPhotoCritique, we furthur propose a novel model, PhotoEye, featuring a\nlanguageguided multi-view vision fusion mechanism to understand image\naesthetics from multiple perspectives. Finally, we present a novel benchmark,\nPhotoBench, a comprehensive and professional benchmark for aesthetic visual\nunderstanding. On existing benchmarks and PhotoBench, our model demonstrates\nclear advantages over existing models.",
        "url": "http://arxiv.org/abs/2509.18582v1",
        "published_date": "2025-09-23T02:59:41+00:00",
        "updated_date": "2025-09-23T02:59:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daiqing Qi",
            "Handong Zhao",
            "Jing Shi",
            "Simon Jenni",
            "Yifei Fan",
            "Franck Dernoncourt",
            "Scott Cohen",
            "Sheng Li"
        ],
        "tldr": "The paper introduces a new dataset (PhotoCritique), model (PhotoEye), and benchmark (PhotoBench) to improve Multimodal Large Language Models' aesthetic visual understanding, enabling them to critique photos like professional photographers.",
        "tldr_zh": "该论文介绍了一个新的数据集(PhotoCritique)，模型(PhotoEye)和基准(PhotoBench)，旨在提高多模态大型语言模型的美学视觉理解能力，使其能够像专业摄影师一样评论照片。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Losing the Plot: How VLM responses degrade on imperfect charts",
        "summary": "Vision language models (VLMs) show strong results on chart understanding, yet\nexisting benchmarks assume clean figures and fact based queries. Real world\ncharts often contain distortions and demand reasoning beyond simple matching.\nWe evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp\nperformance drops under corruption or occlusion, with hallucinations such as\nvalue fabrication, trend misinterpretation, and entity confusion becoming more\nfrequent. Models remain overconfident in degraded settings, generating\nplausible but unsupported explanations.\n  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,\nand Reasoning Testing on Noisy and Occluded Input Selections), a dataset\ncombining chart corruptions, occlusions, and exam style multiple choice\nquestions inspired by Korea's CSAT English section. A key innovation is prompt\nreverse inconsistency, where models contradict themselves when asked to confirm\nversus deny the same statement. Our contributions are threefold: (1)\nbenchmarking state of the art VLMs, exposing systematic vulnerabilities in\nchart reasoning; (2) releasing CHART NOISe, the first dataset unifying\ncorruption, occlusion, and reverse inconsistency; and (3) proposing baseline\nmitigation strategies such as quality filtering and occlusion detection.\nTogether, these efforts establish a rigorous testbed for advancing robustness\nand reliability in chart understanding.",
        "url": "http://arxiv.org/abs/2509.18425v1",
        "published_date": "2025-09-22T21:12:20+00:00",
        "updated_date": "2025-09-22T21:12:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Philip Wootaek Shin",
            "Jack Sampson",
            "Vijaykrishnan Narayanan",
            "Andres Marquez",
            "Mahantesh Halappanavar"
        ],
        "tldr": "This paper identifies vulnerabilities in VLMs' chart understanding capabilities when faced with noisy or occluded charts, introduces a new benchmark dataset (CHART NOISe) to address these issues, and proposes initial mitigation strategies.",
        "tldr_zh": "该论文揭示了视觉语言模型在处理噪声或遮挡图表时，图表理解能力上的漏洞。它提出了一个新的基准数据集（CHART NOISe）以解决这些问题，并提出了初步的缓解策略。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models",
        "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.",
        "url": "http://arxiv.org/abs/2509.18405v1",
        "published_date": "2025-09-22T20:43:59+00:00",
        "updated_date": "2025-09-22T20:43:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sourav Halder",
            "Jinjun Tong",
            "Xinyu Wu"
        ],
        "tldr": "This paper introduces a training-free framework, CFD-Agent, that leverages VLM and MLLM for zero-shot check field detection, demonstrating strong performance on a curated dataset and potential for bootstrapping labeled datasets.",
        "tldr_zh": "本文介绍了一种无需训练的框架 CFD-Agent，它利用 VLM 和 MLLM 进行零样本支票字段检测，在精心策划的数据集上表现出强大的性能，并具有引导标记数据集的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs",
        "summary": "Contrastive vision-language models (VLMs) have made significant progress in\nbinding visual and textual information, but understanding long, dense captions\nremains an open challenge. We hypothesize that compositionality, the capacity\nto reason about object-attribute bindings and inter-object relationships, is\nkey to understanding longer captions. In this paper, we investigate the\ninteraction between compositionality and long-caption understanding, asking\nwhether training for one property enhances the other. We train and evaluate a\nrange of models that target each of these capabilities. Our results reveal a\nbidirectional relationship: compositional training improves performance on\nlong-caption retrieval, and training on long captions promotes\ncompositionality. However, these gains are sensitive to data quality and model\ndesign. We find that training on poorly structured captions, or with limited\nparameter updates, fails to support generalization. Likewise, strategies that\naim at retaining general alignment, such as freezing positional embeddings, do\nnot improve compositional understanding. Overall, we find that compositional\nunderstanding and long-caption understanding are intertwined capabilities that\ncan be jointly learned through training on dense, grounded descriptions.\nDespite these challenges, we show that models trained on high-quality,\nlong-caption data can achieve strong performance in both tasks, offering\npractical guidance for improving VLM generalization.",
        "url": "http://arxiv.org/abs/2509.19207v1",
        "published_date": "2025-09-23T16:28:51+00:00",
        "updated_date": "2025-09-23T16:28:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Israfel Salazar",
            "Desmond Elliott",
            "Yova Kementchedjhieva"
        ],
        "tldr": "This paper explores the bidirectional relationship between compositional understanding and long-caption understanding in VLMs, showing that training for one enhances the other, but is sensitive to data quality and model design.",
        "tldr_zh": "本文探讨了视觉语言模型中组合理解和长文本理解之间的双向关系，表明对其中一种能力的训练可以提升另一种能力，但对数据质量和模型设计很敏感。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models",
        "summary": "Traffic safety remains a critical global concern, with timely and accurate\naccident detection essential for hazard reduction and rapid emergency response.\nInfrastructure-based vision sensors offer scalable and efficient solutions for\ncontinuous real-time monitoring, facilitating automated detection of acci-\ndents directly from captured images. This research investigates the zero-shot\ncapabilities of multimodal large language models (MLLMs) for detecting and\ndescribing traffic accidents using images from infrastructure cameras, thus\nminimizing reliance on extensive labeled datasets. Main contributions include:\n(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,\nexplicitly addressing the scarcity of diverse, realistic, infrastructure-based\naccident data through controlled simulations; (2) Comparative performance\nanalysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent\nidentification and descriptive capabilities without prior fine-tuning; and (3)\nIntegration of advanced visual analytics, specifically YOLO for object\ndetection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for\ninstance segmentation, into enhanced prompts to improve model accuracy and\nexplainability. Key numerical results show Pixtral as the top performer with an\nF1-score of 0.71 and 83% recall, while Gemini models gained precision with\nenhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and\nrecall losses. Gemma 3 offered the most balanced performance with minimal\nmetric fluctuation. These findings demonstrate the substantial potential of\nintegrating MLLMs with advanced visual analytics techniques, enhancing their\napplicability in real-world automated traffic monitoring systems.",
        "url": "http://arxiv.org/abs/2509.19096v1",
        "published_date": "2025-09-23T14:47:33+00:00",
        "updated_date": "2025-09-23T14:47:33+00:00",
        "categories": [
            "cs.CV",
            "cs.SE"
        ],
        "authors": [
            "Ilhan Skender",
            "Kailin Tong",
            "Selim Solmaz",
            "Daniel Watzenig"
        ],
        "tldr": "This paper explores the zero-shot capabilities of multimodal large language models (MLLMs) for traffic accident detection using infrastructure camera images, comparing several models and integrating visual analytics techniques to improve performance.",
        "tldr_zh": "本文探讨了使用多模态大型语言模型 (MLLM) 通过基础设施摄像头图像进行交通事故检测的零样本能力，比较了几种模型，并集成了视觉分析技术以提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications",
        "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.",
        "url": "http://arxiv.org/abs/2509.19087v1",
        "published_date": "2025-09-23T14:40:52+00:00",
        "updated_date": "2025-09-23T14:40:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ganesh Mallya",
            "Yotam Gigi",
            "Dahun Kim",
            "Maxim Neumann",
            "Genady Beryozkin",
            "Tomer Shekel",
            "Anelia Angelova"
        ],
        "tldr": "This paper proposes a zero-shot approach to adapt generalist multimodal models like Gemini 2.5 to multi-spectral remote sensing imagery by injecting domain-specific information as instructions, demonstrating performance gains on land cover classification benchmarks.",
        "tldr_zh": "本文提出了一种零样本方法，通过将特定领域的知识作为指令注入，使像Gemini 2.5这样的通用多模态模型适应多光谱遥感图像，并在土地覆盖分类基准测试中展示了性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?",
        "summary": "This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.",
        "url": "http://arxiv.org/abs/2509.19070v1",
        "published_date": "2025-09-23T14:33:21+00:00",
        "updated_date": "2025-09-23T14:33:21+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zijian Ling",
            "Han Zhang",
            "Yazhuo Zhou",
            "Jiahao Cui"
        ],
        "tldr": "The paper introduces ColorBlindnessEval, a benchmark to test VLMs' robustness using Ishihara-like images, revealing their limitations in interpreting numbers in adversarial visual contexts and highlighting hallucination issues.",
        "tldr_zh": "该论文介绍了 ColorBlindnessEval，一个使用类似石原氏色盲测试的图像来测试 VLM 鲁棒性的基准，揭示了它们在对抗性视觉环境中解释数字的局限性，并强调了幻觉问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
        "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.",
        "url": "http://arxiv.org/abs/2509.19002v1",
        "published_date": "2025-09-23T13:46:31+00:00",
        "updated_date": "2025-09-23T13:46:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Hao Wang",
            "Eiki Murata",
            "Lingfang Zhang",
            "Ayako Sato",
            "So Fukuda",
            "Ziqi Yin",
            "Wentao Hu",
            "Keisuke Nakao",
            "Yusuke Nakamura",
            "Sebastian Zwirner",
            "Yi-Chia Chen",
            "Hiroyuki Otomo",
            "Hiroki Ouchi",
            "Daisuke Kawahara"
        ],
        "tldr": "The paper introduces VIR-Bench, a new benchmark for evaluating geospatial and temporal understanding in MLLMs using travel videos, revealing current models' limitations and demonstrating potential improvements in travel planning applications.",
        "tldr_zh": "该论文介绍了VIR-Bench，一个新的基准，用于评估MLLMs在旅行视频中的地理空间和时间理解能力，揭示了当前模型的局限性，并展示了旅行计划应用程序中潜在的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification",
        "summary": "Object Re-IDentification (ReID) aims to recognize individuals across\nnon-overlapping camera views. While recent advances have achieved remarkable\nprogress, most existing models are constrained to either single-domain or\ncross-domain scenarios, limiting their real-world applicability. Single-domain\nmodels tend to overfit to domain-specific features, whereas cross-domain models\noften rely on diverse normalization strategies that may inadvertently suppress\nidentity-specific discriminative cues. To address these limitations, we propose\nan Attribute Prompt Composition (APC) framework, which exploits textual\nsemantics to jointly enhance discrimination and generalization. Specifically,\nwe design an Attribute Prompt Generator (APG) consisting of a Semantic\nAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an\nover-complete attribute dictionary to provide rich semantic descriptions, while\nPCM adaptively composes relevant attributes from SAD to generate discriminative\nattribute-aware features. In addition, motivated by the strong generalization\nability of Vision-Language Models (VLM), we propose a Fast-Slow Training\nStrategy (FSTS) to balance ReID-specific discrimination and generalizable\nrepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)\nto rapidly acquire ReID-specific discriminative knowledge and a Slow Update\nStream (SUS) to retain the generalizable knowledge inherited from the\npre-trained VLM. Through a mutual interaction, the framework effectively\nfocuses on ReID-relevant features while mitigating overfitting. Extensive\nexperiments on both conventional and Domain Generalized (DG) ReID datasets\ndemonstrate that our framework surpasses state-of-the-art methods, exhibiting\nsuperior performances in terms of both discrimination and generalization. The\nsource code is available at https://github.com/AWangYQ/APC.",
        "url": "http://arxiv.org/abs/2509.18715v1",
        "published_date": "2025-09-23T07:03:08+00:00",
        "updated_date": "2025-09-23T07:03:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingquan Wang",
            "Pingping Zhang",
            "Chong Sun",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "tldr": "This paper introduces an Attribute Prompt Composition (APC) framework with a Fast-Slow Training Strategy (FSTS) to improve both discrimination and generalization in object Re-ID across different camera views by exploiting textual semantics and Vision-Language Models (VLMs). It outperforms state-of-the-art methods on both conventional and Domain Generalized ReID datasets.",
        "tldr_zh": "该论文提出了一种属性提示组合 (APC) 框架，结合快速-慢速训练策略 (FSTS)，通过利用文本语义和视觉语言模型 (VLM) 来提高对象 Re-ID 在不同相机视图中的判别能力和泛化能力。它在传统和领域泛化的 ReID 数据集上都优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
        "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.",
        "url": "http://arxiv.org/abs/2509.18711v1",
        "published_date": "2025-09-23T06:52:15+00:00",
        "updated_date": "2025-09-23T06:52:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ke Li",
            "Di Wang",
            "Ting Wang",
            "Fuyu Dong",
            "Yiming Zhang",
            "Luyao Zhang",
            "Xiangyu Wang",
            "Shaofeng Li",
            "Quan Wang"
        ],
        "tldr": "The paper introduces RSVG-ZeroOV, a training-free framework for zero-shot open-vocabulary visual grounding in remote sensing images, utilizing VLMs and diffusion models for improved object localization without task-specific training.",
        "tldr_zh": "该论文介绍了RSVG-ZeroOV，一个无需训练的框架，用于遥感图像中的零样本开放词汇视觉定位。该框架利用VLMs和扩散模型来改进物体定位，而无需特定任务的训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning",
        "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.",
        "url": "http://arxiv.org/abs/2509.18369v1",
        "published_date": "2025-09-22T19:49:35+00:00",
        "updated_date": "2025-09-22T19:49:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Riad Ahmed Anonto",
            "Sardar Md. Saffat Zabin",
            "M. Saifur Rahman"
        ],
        "tldr": "The paper introduces a novel tri-loss objective (PAL+InfoNCE+OT) to improve grounding in low-resource Bengali captioning by aligning real and synthetic image patches using cross-attention and optimal transport.",
        "tldr_zh": "本文提出了一种新颖的三重损失目标函数 (PAL+InfoNCE+OT)，通过使用交叉注意力和最优传输对齐真实和合成图像块，从而改进低资源孟加拉语图像描述的视觉定位。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography",
        "summary": "This study evaluates the capabilities of Multimodal Large Language Models\n(LLMs) and Vision Language Models (VLMs) in the task of single-label\nclassification of Christian Iconography. The goal was to assess whether\ngeneral-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,\ncan interpret the Iconography, typically addressed by supervised classifiers,\nand evaluate their performance. Two research questions guided the analysis:\n(RQ1) How do multimodal LLMs perform on image classification of Christian\nsaints? And (RQ2), how does performance vary when enriching input with\ncontextual information or few-shot exemplars? We conducted a benchmarking study\nusing three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and\nWikidata, filtered to include the top 10 most frequent classes. Models were\ntested under three conditions: (1) classification using class labels, (2)\nclassification with Iconclass descriptions, and (3) few-shot learning with five\nexemplars. Results were compared against ResNet50 baselines fine-tuned on the\nsame datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed\nthe ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,\nwhere Siglip reached the highest accuracy score, suggesting model sensitivity\nto image size and metadata alignment. Enriching prompts with class descriptions\ngenerally improved zero-shot performance, while few-shot learning produced\nlower results, with only occasional and minimal increments in accuracy. We\nconclude that general-purpose multimodal LLMs are capable of classification in\nvisually complex cultural heritage domains. These results support the\napplication of LLMs as metadata curation tools in digital humanities workflows,\nsuggesting future research on prompt optimization and the expansion of the\nstudy to other classification strategies and models.",
        "url": "http://arxiv.org/abs/2509.18839v1",
        "published_date": "2025-09-23T09:23:31+00:00",
        "updated_date": "2025-09-23T09:23:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gianmarco Spinaci",
            "Lukas Klic",
            "Giovanni Colavizza"
        ],
        "tldr": "The paper benchmarks the performance of VLMs and multimodal LLMs like GPT-4o and Gemini 2.5 on Christian Iconography classification, finding that they outperform ResNet50 baselines, suggesting their applicability in digital humanities metadata curation.",
        "tldr_zh": "该论文评估了视觉语言模型和多模态大型语言模型（如GPT-4o和Gemini 2.5）在基督教图像学分类上的性能，发现它们优于ResNet50基线，表明其在数字人文元数据管理中的应用潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning",
        "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.",
        "url": "http://arxiv.org/abs/2509.18372v1",
        "published_date": "2025-09-22T19:54:02+00:00",
        "updated_date": "2025-09-22T19:54:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Reeshad Khan",
            "John Gauch"
        ],
        "tldr": "TinyBEV presents a compact, camera-only Bird's Eye View framework that distills the full-stack capabilities of a large planning-oriented teacher model into a real-time student model, achieving significant parameter reduction and speed improvements while maintaining performance.",
        "tldr_zh": "TinyBEV 提出了一个紧凑的、仅使用摄像头的鸟瞰图框架，该框架将大型规划导向教师模型的全栈能力提炼成实时学生模型，在保持性能的同时，显著减少了参数和提高了速度。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]