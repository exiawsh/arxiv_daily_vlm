[
    {
        "title": "Learning to Think Fast and Slow for Visual Language Models",
        "summary": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.",
        "url": "http://arxiv.org/abs/2511.16670v1",
        "published_date": "2025-11-20T18:59:48+00:00",
        "updated_date": "2025-11-20T18:59:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenyu Lin",
            "Cheng Chi",
            "Jinlin Wu",
            "Sharon Li",
            "Kaiyang Zhou"
        ],
        "tldr": "This paper introduces DualMindVLM, a reinforcement learning approach that enables Visual Language Models (VLMs) to dynamically switch between fast and slow thinking modes based on task difficulty, achieving competitive performance with improved token efficiency.",
        "tldr_zh": "该论文介绍了 DualMindVLM，一种强化学习方法，使视觉语言模型 (VLM) 能够根据任务难度动态地在快速和慢速思考模式之间切换，从而在提高 token 效率的同时实现有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
        "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
        "url": "http://arxiv.org/abs/2511.16671v1",
        "published_date": "2025-11-20T18:59:52+00:00",
        "updated_date": "2025-11-20T18:59:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Ziyu Guo",
            "Renrui Zhang",
            "Hongyu Li",
            "Manyuan Zhang",
            "Xinyan Chen",
            "Sifan Wang",
            "Yan Feng",
            "Peng Pei",
            "Pheng-Ann Heng"
        ],
        "tldr": "The paper introduces Thinking-while-Generating (TwiG), a novel framework that interleaves textual reasoning with visual generation to produce more context-aware and semantically rich visual outputs. It explores zero-shot prompting, supervised fine-tuning, and reinforcement learning strategies within this framework.",
        "tldr_zh": "该论文介绍了Thinking-while-Generating (TwiG)，一种新颖的框架，它将文本推理与视觉生成交织在一起，以产生更具上下文感知和语义丰富的视觉输出。它探讨了零样本提示、监督微调和强化学习策略在该框架中的应用。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
        "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
        "url": "http://arxiv.org/abs/2511.16669v1",
        "published_date": "2025-11-20T18:59:44+00:00",
        "updated_date": "2025-11-20T18:59:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junhao Cheng",
            "Liang Hou",
            "Xin Tao",
            "Jing Liao"
        ],
        "tldr": "The paper introduces Video-Next-Event Prediction (VNEP), a novel task requiring video responses to procedural or predictive questions, and proposes VANS, a model leveraging reinforcement learning and a Joint-GRPO mechanism to align Vision-Language and Video Diffusion Models for state-of-the-art VNEP performance. They also release a dedicated dataset VANS-Data-100K.",
        "tldr_zh": "该论文介绍了视频下一步事件预测（VNEP）任务，该任务需要针对程序性或预测性问题提供视频回答。论文提出了VANS模型，该模型利用强化学习和Joint-GRPO机制对齐视觉语言模型和视频扩散模型，实现了VNEP任务的state-of-the-art性能。他们还发布了一个专门的数据集VANS-Data-100K。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards",
        "summary": "Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.",
        "url": "http://arxiv.org/abs/2511.16672v1",
        "published_date": "2025-11-20T18:59:54+00:00",
        "updated_date": "2025-11-20T18:59:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Omkat Thawakar",
            "Shravan Venkatraman",
            "Ritesh Thawkar",
            "Abdelrahman Shaker",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Salman Khan",
            "Fahad Khan"
        ],
        "tldr": "The paper introduces EvoLMM, a self-evolving framework for unsupervised improvement of Large Multimodal Models (LMMs) using a Proposer-Solver architecture and continuous self-rewarding. It shows improvements on multimodal reasoning benchmarks using Qwen2.5-VL.",
        "tldr_zh": "该论文提出了EvoLMM，一个自我演化的框架，通过Proposer-Solver架构和持续的自我奖励，以无监督的方式改进大型多模态模型（LMM）。该方法在使用Qwen2.5-VL模型时，在多模态推理基准测试上显示出改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]