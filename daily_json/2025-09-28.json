[
    {
        "title": "DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice",
        "summary": "Diagnosing and managing oral diseases necessitate advanced visual\ninterpretation across diverse imaging modalities and integrated information\nsynthesis. While current AI models excel at isolated tasks, they often fall\nshort in addressing the complex, multimodal requirements of comprehensive\nclinical dental practice. Here we introduce DentVLM, a multimodal\nvision-language model engineered for expert-level oral disease diagnosis.\nDentVLM was developed using a comprehensive, large-scale, bilingual dataset of\n110,447 images and 2.46 million visual question-answering (VQA) pairs. The\nmodel is capable of interpreting seven 2D oral imaging modalities across 36\ndiagnostic tasks, significantly outperforming leading proprietary and\nopen-source models by 19.6% higher accuracy for oral diseases and 27.9% for\nmalocclusions. In a clinical study involving 25 dentists, evaluating 1,946\npatients and encompassing 3,105 QA pairs, DentVLM surpassed the diagnostic\nperformance of 13 junior dentists on 21 of 36 tasks and exceeded that of 12\nsenior dentists on 12 of 36 tasks. When integrated into a collaborative\nworkflow, DentVLM elevated junior dentists' performance to senior levels and\nreduced diagnostic time for all practitioners by 15-22%. Furthermore, DentVLM\nexhibited promising performance across three practical utility scenarios,\nincluding home-based dental health management, hospital-based intelligent\ndiagnosis and multi-agent collaborative interaction. These findings establish\nDentVLM as a robust clinical decision support tool, poised to enhance primary\ndental care, mitigate provider-patient imbalances, and democratize access to\nspecialized medical expertise within the field of dentistry.",
        "url": "http://arxiv.org/abs/2509.23344v1",
        "published_date": "2025-09-27T14:47:37+00:00",
        "updated_date": "2025-09-27T14:47:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zijie Meng",
            "Jin Hao",
            "Xiwei Dai",
            "Yang Feng",
            "Jiaxiang Liu",
            "Bin Feng",
            "Huikai Wu",
            "Xiaotang Gai",
            "Hengchuan Zhu",
            "Tianxiang Hu",
            "Yangyang Wu",
            "Hongxia Xu",
            "Jin Li",
            "Jun Xiao",
            "Xiaoqiang Liu",
            "Joey Tianyi Zhou",
            "Fudong Zhu",
            "Zhihe Zhao",
            "Lunguo Xia",
            "Bing Fang",
            "Jimeng Sun",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "tldr": "DentVLM, a multimodal vision-language model for dental diagnosis, outperforms existing models and even improves dentists' performance in clinical settings, demonstrating its potential for democratizing access to specialized dental expertise.",
        "tldr_zh": "DentVLM是一种用于牙科诊断的多模态视觉语言模型，其性能优于现有模型，甚至可以提高牙医在临床环境中的表现，展示了其在普及专业牙科知识方面的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors",
        "summary": "A fundamental reason for the dominance of attention over RNNs and LSTMs in\nLLMs is its ability to capture long-range dependencies by modeling direct\ninteractions between all tokens, overcoming the sequential limitations of\nrecurrent architectures. Similarly, a key reason why today's vision language\nmodels (VLMs) hallucinate and underperform pure language models is that they\nrely on direct concatenation of image and text tokens with a modality-blinded\npositional encoding, which conveniently adopts the pretrained LLM backbone but\nforces unnecessary long-distance attention between semantically related tokens\nacross modalities. This underscores the urgent need for mechanisms that\nefficiently enhance token locality and cross-modal alignment. In response, we\npropose Attention Anchor, a parameter-free framework that efficiently groups\nsemantically similar tokens across modalities, improving cross-modal locality.\nBy inserting text tokens near relevant visual patches, we create semantic\nsignposts that reveal true content-based cross-modal attention scores, guiding\nthe model to focus on the correct image regions for tasks such as VQA, MMBench\nand POPE. This improves answer accuracy and reduces hallucinations without\ndisrupting the prompt's semantic flow. AttAnchor achieves improvements across\n13 out of 15 different metrics and benchmarks, including up to 32% gains on\nreasoning tasks and up to 15% improvements on hallucination benchmarks.\nAttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B\nand QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of\nour knowledge, this work is among the first to investigate mixed-modal token\ngrouping, where text and image tokens are clustered jointly into shared groups\nrather than being grouped within a single modality or merely aligned post-hoc\nwith additional alignment losses.",
        "url": "http://arxiv.org/abs/2509.23109v1",
        "published_date": "2025-09-27T04:37:26+00:00",
        "updated_date": "2025-09-27T04:37:26+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Junyang Zhang",
            "Tianyi Zhu",
            "Thierry Tambe"
        ],
        "tldr": "This paper introduces AttAnchor, a parameter-free framework that improves cross-modal token alignment in VLMs by grouping semantically similar tokens across modalities, leading to significant performance gains and reduced hallucinations.",
        "tldr_zh": "本文介绍了一种名为AttAnchor的无参数框架，通过跨模态分组语义相似的token，改善了VLM中的跨模态token对齐，从而显著提高性能并减少幻觉。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MMPB: It's Time for Multi-Modal Personalization",
        "summary": "Visual personalization is essential in user-facing AI systems such as smart\nhomes and healthcare, where aligning model behavior with user-centric concepts\nis critical. However, recent large Vision-Language Models (VLMs), despite their\nbroad applicability, remain underexplored in their ability to adapt to\nindividual users. In this paper, we introduce MMPB, the first extensive\nbenchmark for evaluating VLMs on personalization. MMPB comprises 10k\nimage-query pairs and includes 111 personalizable concepts across four\ncategories: humans, animals, objects, and characters, with the human category\nenriched with preference-grounded queries. We structure personalization into\nthree main task types, each highlighting a different key property of VLMs.\nUsing 23 widely used VLMs including both open- and closed-source models, we\nevaluate personalization performance via a three-stage protocol: concept\ninjection, multi-turn dialogue, and personalized querying. Our findings\nindicate that most VLMs (including some closed-source models) struggle with\npersonalization, particularly in maintaining consistency over dialogue,\nhandling user preferences, and adapting to visual cues. Our analysis reveals\nthat the challenges in VLM personalization (such as refusal behaviors and\nlong-context forgetting) highlight substantial room for improvement. By\nidentifying these limitations and offering a scalable benchmark, MMPB offers\nvaluable insights and a solid foundation for future research toward truly\npersonalized multi-modal AI. Project Page: aidaslab.github.io/MMPB",
        "url": "http://arxiv.org/abs/2509.22820v2",
        "published_date": "2025-09-26T18:24:48+00:00",
        "updated_date": "2025-09-30T03:41:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jaeik Kim",
            "Woojin Kim",
            "Woohyeon Park",
            "Jaeyoung Do"
        ],
        "tldr": "The paper introduces MMPB, a new benchmark dataset for evaluating the personalization capabilities of Vision-Language Models (VLMs), revealing that current VLMs struggle with personalization tasks like consistency in dialogue and adapting to user preferences.",
        "tldr_zh": "该论文介绍了MMPB，一个新的基准数据集，用于评估视觉-语言模型（VLM）的个性化能力，揭示了当前的VLM在诸如对话一致性和适应用户偏好等个性化任务方面存在困难。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding",
        "summary": "Multimodal large language models (MLLMs) have recently achieved remarkable\nprogress in radiology by integrating visual perception with natural language\nunderstanding. However, they often generate clinically unsupported\ndescriptions, known as medical hallucinations, which pose serious risks in\nmedical applications that demand accuracy and image-grounded outputs. Through\nempirical analysis, we find that prompt-induced hallucinations remain prevalent\nin radiology MLLMs, largely due to over-sensitivity to clinical sections. To\naddress this, we introduce Clinical Contrastive Cecoding (CCD), a training-free\nand retrieval-free inference framework that integrates structured clinical\nsignals from task-specific radiology expert models. CCD introduces a dual-stage\ncontrastive mechanism to refine token-level logits during generation, thereby\nenhancing clinical fidelity without modifying the base MLLM. Experiments on\nthree datasets and multiple models demonstrate that CCD consistently improves\noverall performance on radiology report generation (RRG). On the MIMIC-CXR\ndataset, it yields up to a 17% improvement in RadGraph-F1 when applied to\nstate-of-the-art RRG models. Our approach provides a lightweight and\ngeneralisable solution for mitigating medical hallucinations, effectively\nbridging expert models and MLLMs in radiology.",
        "url": "http://arxiv.org/abs/2509.23379v1",
        "published_date": "2025-09-27T16:01:09+00:00",
        "updated_date": "2025-09-27T16:01:09+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "I.2.10; J.3; I.5.4"
        ],
        "authors": [
            "Xi Zhang",
            "Zaiqiao Meng",
            "Jake Lever",
            "Edmond S. L. Ho"
        ],
        "tldr": "The paper introduces Clinical Contrastive Decoding (CCD), a training-free inference framework to mitigate hallucinations in radiology MLLMs by integrating structured clinical signals from expert models, showing significant improvements in radiology report generation.",
        "tldr_zh": "该论文介绍了一种名为临床对比解码（CCD）的无需训练的推理框架，通过整合专家模型的结构化临床信号来减少放射学多模态大语言模型（MLLM）中的幻觉，并在放射学报告生成方面显示出显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning",
        "summary": "Significant advancements in the reasoning capabilities of Large Language\nModels (LLMs) are now driven by test-time scaling laws, particularly those\nleveraging extended Chain-of-Thought (CoT) reasoning. Inspired by these\nbreakthroughs, researchers have extended these paradigms to Large Multimodal\nModels (LMMs). However, a critical limitation emerges: as their reasoning\nchains extend, LMMs increasingly rely on textual logic, progressively losing\ngrounding in the underlying visual information. This leads to reasoning paths\nthat diverge from the image content, culminating in erroneous conclusions. To\naddress this, we introduce a strikingly simple yet effective training-free\nvisual-reasoning pipeline. The core concept is to decouple the reasoning and\nperception processes. A powerful LLM orchestrates the high-level reasoning,\nstrategically interrogating a LMM to extract specific visual information\nrequired for its logical chain. The LMM, in turn, functions exclusively as a\nvisual question-answering engine, supplying the necessary perceptual details on\ndemand. This lightweight, plug-and-play approach requires no additional\ntraining or architectural changes. Comprehensive evaluations validate that our\nframework effectively governs the visual reasoning process, leading to a\nsignificant reduction in visually-unfounded reasoning steps and a substantial\nimprovement in reasoning fidelity.",
        "url": "http://arxiv.org/abs/2509.23322v1",
        "published_date": "2025-09-27T14:13:41+00:00",
        "updated_date": "2025-09-27T14:13:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongrui Jia",
            "Chaoya Jiang",
            "Shikun Zhang",
            "Wei Ye"
        ],
        "tldr": "This paper introduces a training-free framework that decouples reasoning and perception in LMMs by using an LLM to strategically query an LMM for visual information, improving reasoning fidelity.",
        "tldr_zh": "该论文介绍了一个无需训练的框架，通过使用大型语言模型有策略地查询大型多模态模型以获取视觉信息，从而解耦大型多模态模型中的推理和感知，提高推理的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning",
        "summary": "Vision-Language Models (VLMs) often appear culturally competent but rely on\nsuperficial pattern matching rather than genuine cultural understanding. We\nintroduce a diagnostic framework to probe VLM reasoning on fire-themed cultural\nimagery through both classification and explanation analysis. Testing multiple\nmodels on Western festivals, non-Western traditions, and emergency scenes\nreveals systematic biases: models correctly identify prominent Western\nfestivals but struggle with underrepresented cultural events, frequently\noffering vague labels or dangerously misclassifying emergencies as\ncelebrations. These failures expose the risks of symbolic shortcuts and\nhighlight the need for cultural evaluation beyond accuracy metrics to ensure\ninterpretable and fair multimodal systems.",
        "url": "http://arxiv.org/abs/2509.23311v1",
        "published_date": "2025-09-27T13:56:12+00:00",
        "updated_date": "2025-09-27T13:56:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haorui Yu",
            "Qiufeng Yi",
            "Yijia Chu",
            "Yang Zhao"
        ],
        "tldr": "This paper introduces a framework to evaluate Vision-Language Models (VLMs) on fire-themed cultural imagery, revealing biases and a lack of genuine cultural understanding in current VLMs, particularly for underrepresented cultures.",
        "tldr_zh": "该论文介绍了一个评估视觉语言模型（VLM）在以火为主题的文化图像上的表现的框架，揭示了当前VLM中的偏见以及缺乏对文化的真正理解，尤其是在代表性不足的文化中。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned",
        "summary": "Process Reward Models (PRMs) provide step-level supervision that improves the\nreliability of reasoning in large language models. While PRMs have been\nextensively studied in text-based domains, their extension to Vision Language\nModels (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on\nMonte Carlo Tree Search (MCTS) for data construction, which can often produce\nnoisy supervision signals and limit generalization across tasks. In this work,\nwe aim to elucidate the design space of VL-PRMs by exploring diverse strategies\nfor dataset construction, training, and test-time scaling. First, we introduce\na hybrid data synthesis framework that combines MCTS with judgments from a\nstrong VLM, producing more accurate step-level labels. Second, we propose\nperception-focused supervision, enabling our PRM to explicitly detect errors at\nthe visual grounding stage of reasoning. Third, we systematically evaluate\nmultiple test-time scaling strategies, showing that our PRMs can reliably guide\nVLMs toward more accurate solutions. Our experiments covering five diverse\nmultimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and\nMathVision) reveal several key insights: (i) VL-PRMs when used as Outcome\nReward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM\nguided process step selection, (ii) smaller VL-PRMs can match or even surpass\nlarger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning\nabilities in stronger VLM backbones, (iv) perception-level supervision leads to\nsignificant gains in test-time scaling, and (v) TTS performance of different\npolicies improve on advanced math reasoning datasets despite not training\nVL-PRMs on such datasets. We hope our work will motivate further research and\nsupport the advancement of VLMs.",
        "url": "http://arxiv.org/abs/2509.23250v1",
        "published_date": "2025-09-27T10:56:58+00:00",
        "updated_date": "2025-09-27T10:56:58+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Brandon Ong",
            "Tej Deep Pala",
            "Vernon Toh",
            "William Chandra Tjhi",
            "Soujanya Poria"
        ],
        "tldr": "This paper explores the design space of Vision-Language Process Reward Models (VL-PRMs) for improved reasoning in VLMs, introducing a hybrid data synthesis framework and perception-focused supervision, and evaluating test-time scaling strategies across diverse multimodal benchmarks.",
        "tldr_zh": "本文探索了视觉语言过程奖励模型（VL-PRM）的设计空间，以提高VLM中的推理能力。该研究引入了一种混合数据合成框架和以感知为中心的监督，并评估了在各种多模态基准测试中的测试时扩展策略。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection",
        "summary": "Vision-language models often hallucinate details, generating non-existent\nobjects or inaccurate attributes that compromise output reliability. Existing\nmethods typically address these issues via extensive human annotations or\nexternal supervision from more powerful models. In this work, we present a\nnovel framework that leverages the model's self-consistency between long\nresponses and short answers to generate preference pairs for training. We\nobserve that short binary questions tend to yield highly reliable responses,\nwhich can be used to query the target model to evaluate and rank its generated\nresponses. Specifically, we design a self-reflection pipeline where detailed\nmodel responses are compared against concise binary answers, and inconsistency\nsignals are utilized to automatically curate high-quality training data without\nhuman annotations or external model-based supervision. By relying solely on\nself-consistency rather than external supervision, our method offers a scalable\nand efficient solution that effectively reduces hallucinations using unlabeled\ndata. Extensive experiments on multiple benchmarks, i.e., AMBER,\nMultiObject-Hal (ROPE), Object HalBench, and MMHal-Bench, demonstrate\nsignificant improvements in factual grounding and reliability. Moreover, our\napproach maintains robust instruction-following ability, as evidenced by\nenhanced performance on LLaVA-Bench and MMBench.",
        "url": "http://arxiv.org/abs/2509.23236v1",
        "published_date": "2025-09-27T10:37:11+00:00",
        "updated_date": "2025-09-27T10:37:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingfei Han",
            "Haihong Hao",
            "Jinxing Zhou",
            "Zhihui Li",
            "Yuhui Zheng",
            "Xueqing Deng",
            "Linjie Yang",
            "Xiaojun Chang"
        ],
        "tldr": "This paper introduces a self-reflection framework for vision-language models that uses the model's self-consistency between long responses and short answers to reduce hallucinations without external supervision, showing strong improvements on multiple benchmarks.",
        "tldr_zh": "该论文介绍了一种视觉语言模型的自反思框架，利用模型在长回复和短答案之间的一致性来减少幻觉，无需外部监督，并在多个基准测试中表现出显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM",
        "summary": "Remote sensing change understanding (RSCU) is essential for analyzing remote\nsensing images and understanding how human activities affect the environment.\nHowever, existing datasets lack deep understanding and interactions in the\ndiverse change captioning, counting, and localization tasks. To tackle these\ngaps, we construct ChangeIMTI, a new large-scale interactive multi-task\ninstruction dataset that encompasses four complementary tasks including change\ncaptioning, binary change classification, change counting, and change\nlocalization. Building upon this new dataset, we further design a novel\nvision-guided vision-language model (ChangeVG) with dual-granularity awareness\nfor bi-temporal remote sensing images (i.e., two remote sensing images of the\nsame area at different times). The introduced vision-guided module is a\ndual-branch architecture that synergistically combines fine-grained spatial\nfeature extraction with high-level semantic summarization. These enriched\nrepresentations further serve as the auxiliary prompts to guide large\nvision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning,\nthereby facilitating the hierarchical cross-modal learning. We extensively\nconduct experiments across four tasks to demonstrate the superiority of our\napproach. Remarkably, on the change captioning task, our method outperforms the\nstrongest method Semantic-CC by 1.39 points on the comprehensive S*m metric,\nwhich integrates the semantic similarity and descriptive accuracy to provide an\noverall evaluation of change caption. Moreover, we also perform a series of\nablation studies to examine the critical components of our method.",
        "url": "http://arxiv.org/abs/2509.23105v1",
        "published_date": "2025-09-27T04:28:42+00:00",
        "updated_date": "2025-09-27T04:28:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junxiao Xue",
            "Quan Deng",
            "Xuecheng Wu",
            "Kelu Yao",
            "Xinyi Yin",
            "Fei Yu",
            "Wei Zhou",
            "Yanfei Zhong",
            "Yang Liu",
            "Dingkang Yang"
        ],
        "tldr": "The paper introduces ChangeIMTI, a large-scale interactive multi-task instruction dataset for remote sensing change understanding, along with a novel vision-guided vision-language model (ChangeVG) to enhance performance across change captioning, classification, counting, and localization tasks.",
        "tldr_zh": "该论文介绍了一个名为ChangeIMTI的大规模交互式多任务指令数据集，用于遥感变化理解，并提出了一个新的视觉引导视觉语言模型 (ChangeVG)，以提高变化描述、分类、计数和定位任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP",
        "summary": "Spatial grounding is crucial for referring image segmentation (RIS), where\nthe goal of the task is to localize an object described by language. Current\nfoundational vision-language models (VLMs), such as CLIP, excel at aligning\nimages and text but struggle with understanding spatial relationships. Within\nthe language stream, most existing methods often focus on the primary noun\nphrase when extracting local text features, undermining contextual tokens.\nWithin the vision stream, CLIP generates similar features for images with\ndifferent spatial layouts, resulting in limited sensitivity to spatial\nstructure. To address these limitations, we propose \\textsc{CoPatch}, a\nzero-shot RIS framework that leverages internal model components to enhance\nspatial representations in both text and image modalities. For language,\n\\textsc{CoPatch} constructs hybrid text features by incorporating context\ntokens carrying spatial cues. For vision, it extracts patch-level image\nfeatures using our novel path discovered from intermediate layers, where\nspatial structure is better preserved. These enhanced features are fused into a\nclustered image-text similarity map, \\texttt{CoMap}, enabling precise mask\nselection. As a result, \\textsc{CoPatch} significantly improves spatial\ngrounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+\n2--7 mIoU) without requiring any additional training. Our findings underscore\nthe importance of recovering and leveraging the untapped spatial knowledge\ninherently embedded in VLMs, thereby paving the way for opportunities in\nzero-shot RIS.",
        "url": "http://arxiv.org/abs/2509.23098v1",
        "published_date": "2025-09-27T04:12:10+00:00",
        "updated_date": "2025-09-27T04:12:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Na Min An",
            "Inha Kang",
            "Minhyun Lee",
            "Hyunjung Shim"
        ],
        "tldr": "The paper introduces CoPatch, a zero-shot Referring Image Segmentation framework that enhances spatial representation in VLMs by leveraging contextual tokens in language and patch-level image features from intermediate layers, improving performance on multiple datasets.",
        "tldr_zh": "本文介绍了一种名为 CoPatch 的零样本指代图像分割框架，该框架通过利用语言中的上下文标记和中间层的patch级图像特征来增强 VLM 中的空间表示，从而提高在多个数据集上的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Mask What Matters: Controllable Text-Guided Masking for Self-Supervised Medical Image Analysis",
        "summary": "The scarcity of annotated data in specialized domains such as medical imaging\npresents significant challenges to training robust vision models. While\nself-supervised masked image modeling (MIM) offers a promising solution,\nexisting approaches largely rely on random high-ratio masking, leading to\ninefficiency and poor semantic alignment. Moreover, region-aware variants\ntypically depend on reconstruction heuristics or supervised signals, limiting\ntheir adaptability across tasks and modalities. We propose Mask What Matters, a\ncontrollable text-guided masking framework for self-supervised medical image\nanalysis. By leveraging vision-language models for prompt-based region\nlocalization, our method flexibly applies differentiated masking to emphasize\ndiagnostically relevant regions while reducing redundancy in background areas.\nThis controllable design enables better semantic alignment, improved\nrepresentation learning, and stronger cross-task generalizability.\nComprehensive evaluation across multiple medical imaging modalities, including\nbrain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently\noutperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1\npercentage points in classification accuracy, +1.3 in box average precision\n(BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it\nachieves these improvements with substantially lower overall masking ratios\n(e.g., 40\\% vs. 70\\%). This work demonstrates that controllable, text-driven\nmasking can enable semantically aligned self-supervised learning, advancing the\ndevelopment of robust vision models for medical image analysis.",
        "url": "http://arxiv.org/abs/2509.23054v1",
        "published_date": "2025-09-27T02:26:56+00:00",
        "updated_date": "2025-09-27T02:26:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruilang Wang",
            "Shuotong Xu",
            "Bowen Liu",
            "Runlin Huang",
            "Donglong Chen",
            "Weifeng Su"
        ],
        "tldr": "This paper introduces a controllable text-guided masking framework called Mask What Matters for self-supervised medical image analysis, which leverages vision-language models for prompt-based region localization to improve representation learning and generalizability with lower masking ratios.",
        "tldr_zh": "本文介绍了一种可控的文本引导掩码框架 Mask What Matters，用于自监督医学图像分析，该框架利用视觉语言模型进行基于提示的区域定位，以更低的掩码比例提高表征学习和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning",
        "summary": "Traditional marine biological image recognition faces challenges of\nincomplete datasets and unsatisfactory model accuracy, particularly for\nfew-shot conditions of rare species where data scarcity significantly hampers\nthe performance. To address these issues, this study proposes an intelligent\nmarine fish recognition framework, FishAI 2.0, integrating multimodal few-shot\ndeep learning techniques with image generation for data augmentation. First, a\nhierarchical marine fish benchmark dataset, which provides a comprehensive data\nfoundation for subsequent model training, is utilized to train the FishAI 2.0\nmodel. To address the data scarcity of rare classes, the large language model\nDeepSeek was employed to generate high-quality textual descriptions, which are\ninput into Stable Diffusion 2 for image augmentation through a hierarchical\ndiffusion strategy that extracts latent encoding to construct a multimodal\nfeature space. The enhanced visual-textual datasets were then fed into a\nContrastive Language-Image Pre-Training (CLIP) based model, enabling robust\nfew-shot image recognition. Experimental results demonstrate that FishAI 2.0\nachieves a Top-1 accuracy of 91.67 percent and Top-5 accuracy of 97.97 percent\nat the family level, outperforming baseline CLIP and ViT models with a\nsubstantial margin for the minority classes with fewer than 10 training\nsamples. To better apply FishAI 2.0 to real-world scenarios, at the genus and\nspecies level, FishAI 2.0 respectively achieves a Top-1 accuracy of 87.58\npercent and 85.42 percent, demonstrating practical utility. In summary, FishAI\n2.0 improves the efficiency and accuracy of marine fish identification and\nprovides a scalable technical solution for marine ecological monitoring and\nconservation, highlighting its scientific value and practical applicability.",
        "url": "http://arxiv.org/abs/2509.22930v1",
        "published_date": "2025-09-26T20:54:35+00:00",
        "updated_date": "2025-09-26T20:54:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenghan Yang",
            "Peng Zhou",
            "Dong-Sheng Zhang",
            "Yueyun Wang",
            "Hong-Bin Shen",
            "Xiaoyong Pan"
        ],
        "tldr": "The paper introduces FishAI 2.0, a multimodal few-shot learning framework for marine fish image classification that leverages data augmentation with LLMs and diffusion models to improve accuracy, particularly for rare species.",
        "tldr_zh": "该论文介绍了FishAI 2.0，一个用于海洋鱼类图像分类的多模态少样本学习框架，它利用LLM和扩散模型进行数据增强，以提高准确性，尤其是在稀有物种方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models",
        "summary": "Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves\nadjusting the model to suit a particular task or dataset while minimizing\ncomputational resources and limiting the number of trainable parameters.\nHowever, it often faces challenges in striking a trade-off between aligning\nwith the target distribution: learning a novel concept from a limited image for\npersonalization and retaining the instruction ability needed for unifying\nmultiple tasks, all while maintaining editability (aligning with a variety of\nprompts or in-context generation). In this work, we introduce DEFT,\nDecompositional Efficient Fine-Tuning, an efficient fine-tuning framework that\nadapts a pre-trained weight matrix by decomposing its update into two\ncomponents with two trainable matrices: (1) a projection onto the complement of\na low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update.\nThe single trainable low-rank matrix defines the subspace, while the other\ntrainable low-rank matrix enables flexible parameter adaptation within that\nsubspace. We conducted extensive experiments on the Dreambooth and Dreambench\nPlus datasets for personalization, the InsDet dataset for object and scene\nadaptation, and the VisualCloze dataset for a universal image generation\nframework through visual in-context learning with both Stable Diffusion and a\nunified model. Our results demonstrated state-of-the-art performance,\nhighlighting the emergent properties of efficient fine-tuning. Our code is\navailable on \\href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.",
        "url": "http://arxiv.org/abs/2509.22793v1",
        "published_date": "2025-09-26T18:01:15+00:00",
        "updated_date": "2025-09-26T18:01:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Komal Kumar",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Salman Khan",
            "Ivan Laptev",
            "Hisham Cholakkal"
        ],
        "tldr": "The paper introduces DEFT, a decompositional efficient fine-tuning framework for Text-to-Image models that decomposes weight updates into a low-rank subspace projection and a low-rank update, achieving SOTA performance on personalization, adaptation, and visual in-context learning tasks.",
        "tldr_zh": "该论文介绍了DEFT，一种用于文本到图像模型的分解决构高效微调框架，它将权重更新分解为低秩子空间投影和低秩更新，在个性化、适应和视觉上下文学习任务上实现了SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "summary": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA)\nmodels often predict action chunks; however, this action chunking harms\nreactivity under inference delay and long horizons. We introduce Asynchronous\nAction Chunk Correction (A2C2), which is a lightweight real-time chunk\ncorrection head that runs every control step and adds a time-aware correction\nto any off-the-shelf VLA's action chunk. The module combines the latest\nobservation, the predicted action from VLA (base action), a positional feature\nthat encodes the index of the base action within the chunk, and some features\nfrom the base policy, then outputs a per-step correction. This preserves the\nbase model's competence while restoring closed-loop responsiveness. The\napproach requires no retraining of the base policy and is orthogonal to\nasynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic\nKinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent\nsuccess rate improvements across increasing delays and execution horizons (+23%\npoint and +7% point respectively, compared to RTC), and also improves\nrobustness for long horizons even with zero injected delay. Since the\ncorrection head is small and fast, there is minimal overhead compared to the\ninference of large VLA models. These results indicate that A2C2 is an\neffective, plug-in mechanism for deploying high-capacity chunking policies in\nreal-time control.",
        "url": "http://arxiv.org/abs/2509.23224v1",
        "published_date": "2025-09-27T10:07:49+00:00",
        "updated_date": "2025-09-27T10:07:49+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Kohei Sendai",
            "Maxime Alvarez",
            "Tatsuya Matsushima",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "tldr": "The paper introduces Asynchronous Action Chunk Correction (A2C2), a lightweight real-time module that corrects action chunks predicted by Vision-Language-Action (VLA) models to improve reactivity under inference delay and long horizons without retraining the base policy.",
        "tldr_zh": "本文介绍了一种异步动作块校正 (A2C2) 方法，它是一种轻量级的实时模块，用于校正视觉-语言-动作 (VLA) 模型预测的动作块，以提高推理延迟和长范围下的反应性，而无需重新训练基础策略。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LLMs Behind the Scenes: Enabling Narrative Scene Illustration",
        "summary": "Generative AI has established the opportunity to readily transform content\nfrom one medium to another. This capability is especially powerful for\nstorytelling, where visual illustrations can illuminate a story originally\nexpressed in text. In this paper, we focus on the task of narrative scene\nillustration, which involves automatically generating an image depicting a\nscene in a story. Motivated by recent progress on text-to-image models, we\nconsider a pipeline that uses LLMs as an interface for prompting text-to-image\nmodels to generate scene illustrations given raw story text. We apply\nvariations of this pipeline to a prominent story corpus in order to synthesize\nillustrations for scenes in these stories. We conduct a human annotation task\nto obtain pairwise quality judgments for these illustrations. The outcome of\nthis process is the SceneIllustrations dataset, which we release as a new\nresource for future work on cross-modal narrative transformation. Through our\nanalysis of this dataset and experiments modeling illustration quality, we\ndemonstrate that LLMs can effectively verbalize scene knowledge implicitly\nevoked by story text. Moreover, this capability is impactful for generating and\nevaluating illustrations.",
        "url": "http://arxiv.org/abs/2509.22940v1",
        "published_date": "2025-09-26T21:15:18+00:00",
        "updated_date": "2025-09-26T21:15:18+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Melissa Roemmele",
            "John Joon Young Chung",
            "Taewook Kim",
            "Yuqian Sun",
            "Alex Calderwood",
            "Max Kreminski"
        ],
        "tldr": "This paper introduces a pipeline leveraging LLMs to prompt text-to-image models for narrative scene illustration, resulting in the SceneIllustrations dataset, demonstrating the effectiveness of LLMs in verbalizing scene knowledge and improving illustration quality.",
        "tldr_zh": "本文介绍了一种利用LLM提示文本到图像模型进行叙事场景插图的流程，生成了SceneIllustrations数据集，证明了LLM在口头表达场景知识和提高插图质量方面的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]