[
    {
        "title": "2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model",
        "summary": "End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.",
        "url": "http://arxiv.org/abs/2509.02659v1",
        "published_date": "2025-09-02T17:52:29+00:00",
        "updated_date": "2025-09-02T17:52:29+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zilong Guo",
            "Yi Luo",
            "Long Sha",
            "Dongxu Wang",
            "Panqu Wang",
            "Chenyang Xu",
            "Yi Yang"
        ],
        "tldr": "This paper presents a vision-language model (VLM) approach to end-to-end autonomous driving using a single camera, achieving 2nd place in the CVPR2024 E2E Challenge and demonstrating the effectiveness of VLMs in vision-based driving.",
        "tldr_zh": "本文提出了一种基于视觉语言模型 (VLM) 的端到端自动驾驶方法，该方法仅使用单个摄像头，并在 CVPR2024 E2E 挑战赛中获得第二名，展示了 VLM 在视觉驾驶中的有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent",
        "summary": "With the raid evolution of large language models and multimodal foundation\nmodels, the mobile-agent landscape has proliferated without converging on the\nfundamental challenges. This paper identifies four core problems that must be\nsolved for mobile agents to deliver practical, scalable impact: (1)\ngeneralization across tasks, modalities, apps, and devices; (2) accuracy,\nspecifically precise on-screen interaction and click targeting; (3)\nlong-horizon capability for sustained, multi-step goals; and (4) efficiency,\nspecifically high-performance runtime on resource-constrained devices. We\npresent AppCopilot, a multimodal, multi-agent, general-purpose on-device\nassistant that operates across applications and constitutes a full-stack,\nclosed-loop system from data to deployment. AppCopilot operationalizes this\nposition through an end-to-end autonomous pipeline spanning data collection,\ntraining, deployment, high-quality and efficient inference, and mobile\napplication development. At the model layer, it integrates multimodal\nfoundation models with robust Chinese-English support. At the reasoning and\ncontrol layer, it combines chain-of-thought reasoning, hierarchical task\nplanning and decomposition, and multi-agent collaboration. At the execution\nlayer, it enables user personalization and experiential adaptation, voice\ninteraction, function calling, cross-app and cross-device orchestration, and\ncomprehensive mobile app support. The system design incorporates\nprofiling-driven optimization for latency, memory, and energy across\nheterogeneous hardware. Empirically, AppCopilot achieves significant\nimprovements along all four dimensions: stronger generalization,\nhigher-precision on-screen actions, more reliable long-horizon task completion,\nand faster, more resource-efficient runtime.",
        "url": "http://arxiv.org/abs/2509.02444v1",
        "published_date": "2025-09-02T15:48:21+00:00",
        "updated_date": "2025-09-02T15:48:21+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Jingru Fan",
            "Yufan Dang",
            "Jingyao Wu",
            "Huatao Li",
            "Runde Yang",
            "Xiyuan Yang",
            "Yuheng Wang",
            "Zhong Zhang",
            "Yaxi Lu",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Dahai Li",
            "Chen Qian"
        ],
        "tldr": "AppCopilot is a multimodal, multi-agent system designed for on-device mobile assistance, addressing challenges in generalization, accuracy, long-horizon tasks, and efficiency through a full-stack pipeline and optimized system design.",
        "tldr_zh": "AppCopilot是一个多模态、多代理系统，专为设备上的移动助手设计，通过全栈管道和优化的系统设计来解决泛化性、准确性、长时程任务和效率方面的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation",
        "summary": "Text-to-image diffusion models can generate stunning visuals, yet they often\nfail at tasks children find trivial--like placing a dog to the right of a teddy\nbear rather than to the left. When combinations get more unusual--a giraffe\nabove an airplane--these failures become even more pronounced. Existing methods\nattempt to fix these spatial reasoning failures through model fine-tuning or\ntest-time optimization with handcrafted losses that are suboptimal. Rather than\nimposing our assumptions about spatial encoding, we propose learning these\nobjectives directly from the model's internal representations. We introduce\nLearn-to-Steer, a novel framework that learns data-driven objectives for\ntest-time optimization rather than handcrafting them. Our key insight is to\ntrain a lightweight classifier that decodes spatial relationships from the\ndiffusion model's cross-attention maps, then deploy this classifier as a\nlearned loss function during inference. Training such classifiers poses a\nsurprising challenge: they can take shortcuts by detecting linguistic traces\nrather than learning true spatial patterns. We solve this with a dual-inversion\nstrategy that enforces geometric understanding. Our method dramatically\nimproves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to\n0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to\nmultiple relations and significantly improves accuracy.",
        "url": "http://arxiv.org/abs/2509.02295v1",
        "published_date": "2025-09-02T13:17:11+00:00",
        "updated_date": "2025-09-02T13:17:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sapir Esther Yiflach",
            "Yuval Atzmon",
            "Gal Chechik"
        ],
        "tldr": "This paper introduces Learn-to-Steer, a method for improving spatial reasoning in text-to-image diffusion models by learning data-driven loss functions from the model's internal representations, achieving significant accuracy improvements over existing approaches.",
        "tldr_zh": "本文介绍了一种名为 Learn-to-Steer 的方法，通过从模型的内部表示中学习数据驱动的损失函数，来提高文本到图像扩散模型中的空间推理能力，与现有方法相比，实现了显著的精度提升。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks",
        "summary": "We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed\n  to be very easy for humans and hard for the current generation of VLMs, and\nthis is empirically verified. Our results show a striking lack of spatial\nrelation understanding in open source and frontier commercial VLMs and a\nsurprisingly high performance of reasoning models. Additionally, we perform a\ndisentanglement analysis to separate the contributions of object localization\nand spatial reasoning in chain-of-thought-based models and find that the\nperformance on the benchmark is bottlenecked by spatial reasoning and not\nobject localization capabilities.\n  We release the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience",
        "url": "http://arxiv.org/abs/2509.02175v1",
        "published_date": "2025-09-02T10:32:58+00:00",
        "updated_date": "2025-09-02T10:32:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Nils Hoehing",
            "Mayug Maniparambil",
            "Ellen Rushe",
            "Noel E. O'Connor",
            "Anthony Ventresque"
        ],
        "tldr": "The paper introduces RocketScience, a new challenging VLM benchmark focused on spatial relation understanding, revealing limitations in current VLMs and highlighting the superior performance of reasoning models, particularly bottlenecked by spatial reasoning rather than object localization.",
        "tldr_zh": "该论文提出了RocketScience，一个新的具有挑战性的VLM基准，专注于空间关系理解。结果表明，当前VLM在这方面存在局限性，而推理模型表现更佳，瓶颈在于空间推理而非目标定位。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time",
        "summary": "Visual Place Recognition (VPR) has evolved from handcrafted descriptors to\ndeep learning approaches, yet significant challenges remain. Current\napproaches, including Vision Foundation Models (VFMs) and Multimodal Large\nLanguage Models (MLLMs), enhance semantic understanding but suffer from high\ncomputational overhead and limited cross-domain transferability when\nfine-tuned. To address these limitations, we propose a novel zero-shot\nframework employing Test-Time Scaling (TTS) that leverages MLLMs'\nvision-language alignment capabilities through Guidance-based methods for\ndirect similarity scoring. Our approach eliminates two-stage processing by\nemploying structured prompts that generate length-controllable JSON outputs.\nThe TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables\nreal-time adaptation without additional training costs, achieving superior\ngeneralization across diverse environments. Experimental results demonstrate\nsignificant improvements in cross-domain VPR performance with up to 210$\\times$\ncomputational efficiency gains.",
        "url": "http://arxiv.org/abs/2509.02129v1",
        "published_date": "2025-09-02T09:25:13+00:00",
        "updated_date": "2025-09-02T09:25:13+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jintao Cheng",
            "Weibin Li",
            "Jiehao Luo",
            "Xiaoyu Tang",
            "Zhijian He",
            "Jin Wu",
            "Yao Zou",
            "Wei Zhang"
        ],
        "tldr": "This paper introduces a Test-Time Scaling (TTS) framework for Visual Place Recognition (VPR) that leverages Multimodal Large Language Models (MLLMs) for efficient zero-shot similarity scoring, achieving significant computational efficiency and cross-domain generalization without fine-tuning.",
        "tldr_zh": "本文介绍了一种用于视觉位置识别（VPR）的测试时缩放（TTS）框架，该框架利用多模态大型语言模型（MLLM）进行高效的零样本相似性评分，在无需微调的情况下，实现了显著的计算效率和跨域泛化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination",
        "summary": "In recent years, integrating multimodal understanding and generation into a\nsingle unified model has emerged as a promising paradigm. While this approach\nachieves strong results in text-to-image (T2I) generation, it still struggles\nwith precise image editing. We attribute this limitation to an imbalanced\ndivision of responsibilities. The understanding module primarily functions as a\ntranslator that encodes user instructions into semantic conditions, while the\ngeneration module must simultaneously act as designer and painter, inferring\nthe original layout, identifying the target editing region, and rendering the\nnew content. This imbalance is counterintuitive because the understanding\nmodule is typically trained with several times more data on complex reasoning\ntasks than the generation module. To address this issue, we introduce\nDraw-In-Mind (DIM), a dataset comprising two complementary subsets: (i)\nDIM-T2I, containing 14M long-context image-text pairs to enhance complex\ninstruction comprehension; and (ii) DIM-Edit, consisting of 233K\nchain-of-thought imaginations generated by GPT-4o, serving as explicit design\nblueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable\nSANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM\ndataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale,\nDIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and\nGEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1\nand Step1X-Edit. These findings demonstrate that explicitly assigning the\ndesign responsibility to the understanding module provides significant benefits\nfor image editing. Our dataset and models will be available at\nhttps://github.com/showlab/DIM.",
        "url": "http://arxiv.org/abs/2509.01986v1",
        "published_date": "2025-09-02T06:06:52+00:00",
        "updated_date": "2025-09-02T06:06:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziyun Zeng",
            "Junhao Zhang",
            "Wei Li",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces Draw-In-Mind (DIM), a dataset and model that improves precise image editing by explicitly assigning the design responsibility to the understanding module. The model achieves state-of-the-art or competitive performance on image editing benchmarks despite its modest size.",
        "tldr_zh": "该论文介绍了 Draw-In-Mind (DIM)，一个数据集和模型，通过将设计责任明确分配给理解模块来提高精确图像编辑能力。该模型虽然规模不大，但在图像编辑基准测试中实现了最先进或具有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models",
        "summary": "Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)\nmodel, have demonstrated remarkable success in aligning visual and linguistic\nrepresentations. However, these models exhibit limitations when applied to\nspecialised visual domains, such as diagrams, which encode structured, symbolic\ninformation distinct from that of natural imagery.\n  In this paper, we introduce a novel training paradigm explicitly designed to\nenhance the comprehension of diagrammatic images within vision-language models.\nOur approach uses ``hard'' samples for our proposed contrastive learning that\nincorporates two specialised loss functions that leverage the inherent\nstructural properties of diagrams. By integrating these objectives into model\ntraining, our method enables models to develop a more structured and\nsemantically coherent understanding of diagrammatic content.\n  We empirically validate our approach on a benchmark dataset of flowcharts, as\na representative class of diagrammatic imagery, demonstrating substantial\nimprovements over standard CLIP and conventional hard negative CLIP learning\nparadigms for both image-text matching and visual question answering tasks. Our\nfindings underscore the significance of tailored training strategies for\nspecialised tasks and contribute to advancing diagrammatic understanding within\nthe broader landscape of vision-language integration.",
        "url": "http://arxiv.org/abs/2509.01959v1",
        "published_date": "2025-09-02T05:02:23+00:00",
        "updated_date": "2025-09-02T05:02:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hiroshi Sasaki"
        ],
        "tldr": "This paper introduces a structure-aware contrastive learning approach to improve vision-language models' understanding of diagrams, demonstrating improvements on flowchart understanding tasks compared to standard CLIP and hard negative CLIP.",
        "tldr_zh": "本文提出了一种结构感知的对比学习方法，旨在提升视觉语言模型对图表的理解能力。实验结果表明，与标准CLIP和硬负例CLIP相比，该方法在流程图理解任务上取得了显著提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving",
        "summary": "Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.",
        "url": "http://arxiv.org/abs/2509.01944v1",
        "published_date": "2025-09-02T04:32:24+00:00",
        "updated_date": "2025-09-02T04:32:24+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhenlong Yuan",
            "Jing Tang",
            "Jinguo Luo",
            "Rui Chen",
            "Chengxuan Qian",
            "Lei Sun",
            "Xiangxiang Chu",
            "Yujun Cai",
            "Dapeng Zhang",
            "Shuo Li"
        ],
        "tldr": "The paper introduces AutoDrive-R$^2$, a novel Vision-Language-Action framework for autonomous driving that enhances reasoning and self-reflection using chain-of-thought and reinforcement learning, achieving state-of-the-art performance on nuScenes and Waymo datasets.",
        "tldr_zh": "该论文介绍了AutoDrive-R$^2$，一种新颖的视觉-语言-动作自动驾驶框架，它通过思维链和强化学习来增强推理和自我反思能力，并在nuScenes和Waymo数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events",
        "summary": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC.",
        "url": "http://arxiv.org/abs/2509.01907v1",
        "published_date": "2025-09-02T03:01:23+00:00",
        "updated_date": "2025-09-02T03:01:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zhenyuan Chen",
            "Chenxi Wang",
            "Ningyu Zhang",
            "Feng Zhang"
        ],
        "tldr": "The paper introduces RSCC, a large-scale remote sensing change caption dataset with pre-/post-disaster image pairs and human-like captions, designed for training and evaluating vision-language models for disaster understanding.",
        "tldr_zh": "该论文介绍了RSCC，一个大规模遥感变化描述数据集，包含灾前/灾后图像对和人工生成的描述，旨在训练和评估用于灾害理解的视觉语言模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Wildfire Damage Assessment from Multi view Ground level Imagery Via Vision Language Models",
        "summary": "The escalating intensity and frequency of wildfires demand innovative\ncomputational methods for rapid and accurate property damage assessment.\nTraditional methods are often time consuming, while modern computer vision\napproaches typically require extensive labeled datasets, hindering immediate\npost-disaster deployment. This research introduces a novel, zero-shot framework\nleveraging pre-trained vision language models (VLMs) to classify damage from\nground-level imagery. We propose and evaluate two pipelines applied to the 2025\nEaton and Palisades fires in California, a VLM (Pipeline A) and a VLM + large\nlanguage model (LLM) approach (Pipeline B), that integrate structured prompts\nbased on specific wildfire damage indicators. A primary scientific contribution\nof this study is demonstrating the VLMs efficacy in synthesizing information\nfrom multiple perspectives to identify nuanced damage, a critical limitation in\nexisting literature. Our findings reveal that while single view assessments\nstruggled to classify affected structures (F1 scores ranging from 0.225 to\n0.511), the multi-view analysis yielded dramatic improvements (F1 scores\nranging from 0.857 to 0.947). Moreover, the McNemar test confirmed that\npipelines with a multi-view image assessment yields statistically significant\nclassification improvements; however, the improvements this research observed\nbetween Pipeline A and B were not statistically significant. Thus, future\nresearch can explore the potential of LLM prompting in damage assessment. The\npractical contribution is an immediately deployable, flexible, and\ninterpretable workflow that bypasses the need for supervised training,\nsignificantly accelerating triage and prioritization for disaster response\npractitioners.",
        "url": "http://arxiv.org/abs/2509.01895v1",
        "published_date": "2025-09-02T02:34:22+00:00",
        "updated_date": "2025-09-02T02:34:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Miguel Esparza",
            "Archit Gupta",
            "Ali Mostafavi",
            "Kai Yin",
            "Yiming Xiao"
        ],
        "tldr": "This paper introduces a zero-shot framework using vision language models for automated wildfire damage assessment from multi-view ground-level imagery, demonstrating improved accuracy compared to single-view assessments without requiring labeled data.",
        "tldr_zh": "本文介绍了一个使用视觉语言模型从多角度地面图像自动评估野火损害的零样本框架，与单角度评估相比，该框架无需标记数据即可提高准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedDINOv3: How to adapt vision foundation models for medical image segmentation?",
        "summary": "Accurate segmentation of organs and tumors in CT and MRI scans is essential\nfor diagnosis, treatment planning, and disease monitoring. While deep learning\nhas advanced automated segmentation, most models remain task-specific, lacking\ngeneralizability across modalities and institutions. Vision foundation models\n(FMs) pretrained on billion-scale natural images offer powerful and\ntransferable representations. However, adapting them to medical imaging faces\ntwo key challenges: (1) the ViT backbone of most foundation models still\nunderperform specialized CNNs on medical image segmentation, and (2) the large\ndomain gap between natural and medical images limits transferability. We\nintroduce MedDINOv3, a simple and effective framework for adapting DINOv3 to\nmedical segmentation. We first revisit plain ViTs and design a simple and\neffective architecture with multi-scale token aggregation. Then, we perform\ndomain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT\nslices, using a multi-stage DINOv3 recipe to learn robust dense features.\nMedDINOv3 matches or exceeds state-of-the-art performance across four\nsegmentation benchmarks, demonstrating the potential of vision foundation\nmodels as unified backbones for medical image segmentation. The code is\navailable at https://github.com/ricklisz/MedDINOv3.",
        "url": "http://arxiv.org/abs/2509.02379v2",
        "published_date": "2025-09-02T14:44:43+00:00",
        "updated_date": "2025-09-03T03:08:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuheng Li",
            "Yizhou Wu",
            "Yuxiang Lai",
            "Mingzhe Hu",
            "Xiaofeng Yang"
        ],
        "tldr": "MedDINOv3 adapts DINOv3, a vision foundation model, for medical image segmentation using a novel architecture and domain-adaptive pretraining, achieving state-of-the-art results on multiple benchmarks.",
        "tldr_zh": "MedDINOv3通过新颖的架构和领域自适应预训练，将视觉基础模型DINOv3应用于医学图像分割，并在多个基准测试中实现了最先进的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution Detection in Remote Sensing",
        "summary": "Out-of-distribution (OOD) detection represents a critical challenge in remote\nsensing applications, where reliable identification of novel or anomalous\npatterns is essential for autonomous monitoring, disaster response, and\nenvironmental assessment. Despite remarkable progress in OOD detection for\nnatural images, existing methods and benchmarks remain poorly suited to remote\nsensing imagery due to data scarcity, complex multi-scale scene structures, and\npronounced distribution shifts. To this end, we propose RS-OOD, a novel\nframework that leverages remote sensing-specific vision-language modeling to\nenable robust few-shot OOD detection. Our approach introduces three key\ninnovations: spatial feature enhancement that improved scene discrimination, a\ndual-prompt alignment mechanism that cross-verifies scene context against\nfine-grained semantics for spatial-semantic consistency, and a\nconfidence-guided self-training loop that dynamically mines pseudo-labels to\nexpand training data without manual annotation. RS-OOD consistently outperforms\nexisting methods across multiple remote sensing benchmarks and enables\nefficient adaptation with minimal labeled data, demonstrating the critical\nvalue of spatial-semantic integration.",
        "url": "http://arxiv.org/abs/2509.02273v1",
        "published_date": "2025-09-02T12:48:39+00:00",
        "updated_date": "2025-09-02T12:48:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingrui Ji",
            "Jiansheng Chen",
            "Jingbo Chen",
            "Anzhi Yue",
            "Chenhao Wang",
            "Kai Li",
            "Yao Zhu"
        ],
        "tldr": "The paper introduces RS-OOD, a vision-language framework tailored for out-of-distribution detection in remote sensing, utilizing spatial feature enhancement, dual-prompt alignment, and confidence-guided self-training to improve performance with limited labeled data.",
        "tldr_zh": "该论文介绍了RS-OOD，一个专门为遥感中的异常检测而设计的视觉语言框架，它利用空间特征增强、双提示对齐和置信度引导的自训练，在有限的标记数据下提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems",
        "summary": "Language-vision understanding has driven the development of advanced\nperception systems, most notably the emerging paradigm of Referring\nMulti-Object Tracking (RMOT). By leveraging natural-language queries, RMOT\nsystems can selectively track objects that satisfy a given semantic\ndescription, guided through Transformer-based spatial-temporal reasoning\nmodules. End-to-End (E2E) RMOT models further unify feature extraction,\ntemporal memory, and spatial reasoning within a Transformer backbone, enabling\nlong-range spatial-temporal modeling over fused textual-visual representations.\nDespite these advances, the reliability and robustness of RMOT remain\nunderexplored. In this paper, we examine the security implications of RMOT\nsystems from a design-logic perspective, identifying adversarial\nvulnerabilities that compromise both the linguistic-visual referring and\ntrack-object matching components. Additionally, we uncover a novel\nvulnerability in advanced RMOT models employing FIFO-based memory, whereby\ntargeted and consistent attacks on their spatial-temporal reasoning introduce\nerrors that persist within the history buffer over multiple subsequent frames.\nWe present VEIL, a novel adversarial framework designed to disrupt the unified\nreferring-matching mechanisms of RMOT models. We show that carefully crafted\ndigital and physical perturbations can corrupt the tracking logic reliability,\ninducing track ID switches and terminations. We conduct comprehensive\nevaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL\nand demonstrate the urgent need for security-aware RMOT designs for critical\nlarge-scale applications.",
        "url": "http://arxiv.org/abs/2509.02028v2",
        "published_date": "2025-09-02T07:17:32+00:00",
        "updated_date": "2025-09-03T02:28:19+00:00",
        "categories": [
            "cs.CV",
            "cs.CR"
        ],
        "authors": [
            "Halima Bouzidi",
            "Haoyu Liu",
            "Mohammad Abdullah Al Faruque"
        ],
        "tldr": "This paper investigates adversarial vulnerabilities in Referring Multi-Object Tracking (RMOT) systems, proposing a novel framework (VEIL) to disrupt the unified referring-matching mechanisms and demonstrating its effectiveness on the Refer-KITTI dataset.",
        "tldr_zh": "本文研究了Referring Multi-Object Tracking (RMOT) 系统中的对抗性漏洞，提出了一个名为VEIL的新框架来破坏统一的引用-匹配机制，并在Refer-KITTI数据集上验证了其有效性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]