[
    {
        "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
        "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
        "url": "http://arxiv.org/abs/2512.22120v1",
        "published_date": "2025-12-26T18:59:47+00:00",
        "updated_date": "2025-12-26T18:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuoshuo Zhang",
            "Yizhen Zhang",
            "Jingjing Fu",
            "Lei Song",
            "Jiang Bian",
            "Yujiu Yang",
            "Rui Wang"
        ],
        "tldr": "The paper introduces Bi-directional Perceptual Shaping (BiPS), a training method for VLMs that uses question-conditioned masked views to improve fine-grained visual reasoning and generalization, achieving significant performance gains across multiple benchmarks.",
        "tldr_zh": "本文介绍了一种名为双向感知塑造（BiPS）的VLM训练方法，该方法使用问题条件下的掩蔽视图来提高细粒度视觉推理和泛化能力，并在多个基准测试中取得了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]