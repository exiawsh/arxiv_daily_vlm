[
    {
        "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
        "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
        "url": "http://arxiv.org/abs/2511.21691v1",
        "published_date": "2025-11-26T18:59:56+00:00",
        "updated_date": "2025-11-26T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yusuf Dalva",
            "Guocheng Gordon Qian",
            "Maya Goldenberg",
            "Tsai-Shien Chen",
            "Kfir Aberman",
            "Sergey Tulyakov",
            "Pinar Yanardag",
            "Kuan-Chieh Jackson Wang"
        ],
        "tldr": "The paper introduces Canvas-to-Image, a unified diffusion model framework that uses a composite canvas image to integrate diverse control signals (text, references, layouts, poses) for improved compositional image generation, outperforming existing methods in control adherence.",
        "tldr_zh": "该论文介绍了 Canvas-to-Image，一个统一的扩散模型框架，它使用复合画布图像来集成各种控制信号（文本、参考、布局、姿势），以改进构图图像生成，并在控制一致性方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
        "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
        "url": "http://arxiv.org/abs/2511.21688v1",
        "published_date": "2025-11-26T18:59:39+00:00",
        "updated_date": "2025-11-26T18:59:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Wenbo Hu",
            "Jingli Lin",
            "Yilin Long",
            "Yunlong Ran",
            "Lihan Jiang",
            "Yifan Wang",
            "Chenming Zhu",
            "Runsen Xu",
            "Tai Wang",
            "Jiangmiao Pang"
        ],
        "tldr": "The paper introduces G$^2$VLM, a vision-language model that integrates 3D reconstruction with spatial understanding, demonstrating improved performance in spatial reasoning tasks through learned 3D visual geometry features.",
        "tldr_zh": "该论文介绍了G$^2$VLM，一个集成了3D重建和空间理解的视觉语言模型，通过学习到的3D视觉几何特征，在空间推理任务中表现出改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]