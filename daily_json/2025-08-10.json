[
    {
        "title": "MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering",
        "summary": "Complex Visual Question Answering (Complex VQA) tasks, which demand\nsophisticated multi-modal reasoning and external knowledge integration, present\nsignificant challenges for existing large vision-language models (LVLMs) often\nlimited by their reliance on high-level global features. To address this, we\npropose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model\ndesigned to enhance Complex VQA performance through the deep fusion of diverse\nvisual and linguistic information. MV-CoRe meticulously integrates global\nembeddings from pre-trained Vision Large Models (VLMs) and Language Large\nModels (LLMs) with fine-grained semantic-aware visual features, including\nobject detection characteristics and scene graph representations. An innovative\nMultimodal Fusion Transformer then processes and deeply integrates these\ndiverse feature sets, enabling rich cross-modal attention and facilitating\ncomplex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,\nincluding GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental\nresults demonstrate that MV-CoRe consistently outperforms established LVLM\nbaselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies\nconfirm the critical contribution of both object and scene graph features, and\nhuman evaluations further validate MV-CoRe's superior factual correctness and\nreasoning depth, underscoring its robust capabilities for deep visual and\nconceptual understanding.",
        "url": "http://arxiv.org/abs/2508.07023v1",
        "published_date": "2025-08-09T15:38:11+00:00",
        "updated_date": "2025-08-09T15:38:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingwei Peng",
            "Jiehao Chen",
            "Mateo Alejandro Rojas",
            "Meilin Zhang"
        ],
        "tldr": "The paper introduces MV-CoRe, a new VLM architecture that deeply fuses global embeddings with fine-grained visual features (object detection, scene graphs) using a Multimodal Fusion Transformer, achieving state-of-the-art results on Complex VQA benchmarks.",
        "tldr_zh": "该论文介绍了MV-CoRe，一种新的VLM架构，它使用多模态融合Transformer深度融合全局嵌入与细粒度视觉特征（物体检测，场景图），并在复杂VQA基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents",
        "summary": "The exponential growth of scientific literature in PDF format necessitates\nadvanced tools for efficient and accurate document understanding,\nsummarization, and content optimization. Traditional methods fall short in\nhandling complex layouts and multimodal content, while direct application of\nLarge Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks\nprecision and control for intricate editing tasks. This paper introduces\nDocRefine, an innovative framework designed for intelligent understanding,\ncontent refinement, and automated summarization of scientific PDF documents,\ndriven by natural language instructions. DocRefine leverages the power of\nadvanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent\nsystem comprising six specialized and collaborative agents: Layout & Structure\nAnalysis, Multimodal Content Understanding, Instruction Decomposition, Content\nRefinement, Summarization & Generation, and Fidelity & Consistency\nVerification. This closed-loop feedback architecture ensures high semantic\naccuracy and visual fidelity. Evaluated on the comprehensive DocEditBench\ndataset, DocRefine consistently outperforms state-of-the-art baselines across\nvarious tasks, achieving overall scores of 86.7% for Semantic Consistency Score\n(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction\nAdherence Rate (IAR). These results demonstrate DocRefine's superior capability\nin handling complex multimodal document editing, preserving semantic integrity,\nand maintaining visual consistency, marking a significant advancement in\nautomated scientific document processing.",
        "url": "http://arxiv.org/abs/2508.07021v1",
        "published_date": "2025-08-09T15:32:52+00:00",
        "updated_date": "2025-08-09T15:32:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kun Qian",
            "Wenjie Li",
            "Tianyu Sun",
            "Wenhong Wang",
            "Wenhan Luo"
        ],
        "tldr": "DocRefine is a multimodal LLM agent-based framework for intelligent scientific document understanding, content refinement, and summarization, achieving state-of-the-art performance on the DocEditBench dataset.",
        "tldr_zh": "DocRefine是一个基于多模态LLM代理的智能科学文档理解、内容优化和摘要框架，在DocEditBench数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing",
        "summary": "Text-to-image generation tasks have driven remarkable advances in diverse\nmedia applications, yet most focus on single-turn scenarios and struggle with\niterative, multi-turn creative tasks. Recent dialogue-based systems attempt to\nbridge this gap, but their single-agent, sequential paradigm often causes\nintention drift and incoherent edits. To address these limitations, we present\nTalk2Image, a novel multi-agent system for interactive image generation and\nediting in multi-turn dialogue scenarios. Our approach integrates three key\ncomponents: intention parsing from dialogue history, task decomposition and\ncollaborative execution across specialized agents, and feedback-driven\nrefinement based on a multi-view evaluation mechanism. Talk2Image enables\nstep-by-step alignment with user intention and consistent image editing.\nExperiments demonstrate that Talk2Image outperforms existing baselines in\ncontrollability, coherence, and user satisfaction across iterative image\ngeneration and editing tasks.",
        "url": "http://arxiv.org/abs/2508.06916v1",
        "published_date": "2025-08-09T10:00:20+00:00",
        "updated_date": "2025-08-09T10:00:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shichao Ma",
            "Yunhe Guo",
            "Jiahao Su",
            "Qihe Huang",
            "Zhengyang Zhou",
            "Yang Wang"
        ],
        "tldr": "Talk2Image introduces a multi-agent system for multi-turn image generation and editing, addressing limitations of single-agent systems by improving controllability, coherence, and user satisfaction.",
        "tldr_zh": "Talk2Image 提出了一个用于多轮图像生成和编辑的多智能体系统，通过提高可控性、连贯性和用户满意度，解决了单智能体系统的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification",
        "summary": "Person re-identification (ReID) aims to retrieve the images of an interested\nperson in the gallery images, with wide applications in medical rehabilitation,\nabnormal behavior detection, and public security. However, traditional person\nReID models suffer from uni-modal capability, leading to poor generalization\nability in multi-modal data, such as RGB, thermal, infrared, sketch images,\ntextual descriptions, etc. Recently, the emergence of multi-modal large\nlanguage models (MLLMs) shows a promising avenue for addressing this problem.\nDespite this potential, existing methods merely regard MLLMs as feature\nextractors or caption generators, which do not fully unleash their reasoning,\ninstruction-following, and cross-modal understanding capabilities. To bridge\nthis gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark\nspecifically designed for person ReID. The MMReID-Bench includes 20,710\nmulti-modal queries and gallery images covering 10 different person ReID tasks.\nComprehensive experiments demonstrate the remarkable capabilities of MLLMs in\ndelivering effective and versatile person ReID. Nevertheless, they also have\nlimitations in handling a few modalities, particularly thermal and infrared\ndata. We hope MMReID-Bench can facilitate the community to develop more robust\nand generalizable multimodal foundation models for person ReID.",
        "url": "http://arxiv.org/abs/2508.06908v1",
        "published_date": "2025-08-09T09:42:09+00:00",
        "updated_date": "2025-08-09T09:42:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinhao Li",
            "Zijian Chen",
            "Lirong Deng",
            "Changbo Wang",
            "Guangtao Zhai"
        ],
        "tldr": "The paper introduces MMReID-Bench, a new multi-modal multi-task benchmark for person re-identification, designed to leverage the capabilities of MLLMs beyond simple feature extraction, highlighting both their strengths and limitations across different modalities.",
        "tldr_zh": "该论文介绍了MMReID-Bench，一个新的多模态多任务行人重识别基准，旨在利用MLLM的能力，超越简单的特征提取，并突出了它们在不同模态下的优势和局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction",
        "summary": "Timely and accurate severe weather warnings are critical for disaster\nmitigation. However, current forecasting systems remain heavily reliant on\nmanual expert interpretation, introducing subjectivity and significant\noperational burdens. With the rapid development of AI technologies, the\nend-to-end \"AI weather station\" is gradually emerging as a new trend in\npredicting severe weather events. Three core challenges impede the development\nof end-to-end AI severe weather system: (1) scarcity of severe weather event\nsamples; (2) imperfect alignment between high-dimensional meteorological data\nand textual warnings; (3) existing multimodal language models are unable to\nhandle high-dimensional meteorological data and struggle to fully capture the\ncomplex dependencies across temporal sequences, vertical pressure levels, and\nspatial dimensions. To address these challenges, we introduce MP-Bench, the\nfirst large-scale temporal multimodal dataset for severe weather events\nprediction, comprising 421,363 pairs of raw multi-year meteorological data and\ncorresponding text caption, covering a wide range of severe weather scenarios\nacross China. On top of this dataset, we develop a meteorology multimodal large\nmodel (MMLM) that directly ingests 4D meteorological inputs. In addition, it is\ndesigned to accommodate the unique characteristics of 4D meteorological data\nflow, incorporating three plug-and-play adaptive fusion modules that enable\ndynamic feature extraction and integration across temporal sequences, vertical\npressure layers, and spatial dimensions. Extensive experiments on MP-Bench\ndemonstrate that MMLM performs exceptionally well across multiple tasks,\nhighlighting its effectiveness in severe weather understanding and marking a\nkey step toward realizing automated, AI-driven weather forecasting systems. Our\nsource code and dataset will be made publicly available.",
        "url": "http://arxiv.org/abs/2508.06859v1",
        "published_date": "2025-08-09T06:54:41+00:00",
        "updated_date": "2025-08-09T06:54:41+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shuo Tang",
            "Jian Xu",
            "Jiadong Zhang",
            "Yi Chen",
            "Qizhao Jin",
            "Lingdong Shen",
            "Chenglin Liu",
            "Shiming Xiang"
        ],
        "tldr": "The paper introduces MeteorPred, a multimodal large model and dataset (MP-Bench) for severe weather prediction, addressing challenges in handling high-dimensional meteorological data and capturing complex spatiotemporal dependencies. It claims superior performance on various tasks related to severe weather understanding.",
        "tldr_zh": "该论文介绍了MeteorPred，一个用于恶劣天气预测的多模态大型模型和数据集(MP-Bench)，旨在解决处理高维气象数据和捕捉复杂时空依赖关系方面的挑战。它声称在与恶劣天气理解相关的各种任务上表现优异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding",
        "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress\nacross a range of vision-language tasks and demonstrate strong potential for\ntraffic accident understanding. However, existing MLLMs in this domain\nprimarily focus on coarse-grained image-level or video-level comprehension and\noften struggle to handle fine-grained visual details or localized scene\ncomponents, limiting their applicability in complex accident scenarios. To\naddress these limitations, we propose SafePLUG, a novel framework that empowers\nMLLMs with both Pixel-Level Understanding and temporal Grounding for\ncomprehensive traffic accident analysis. SafePLUG supports both\narbitrary-shaped visual prompts for region-aware question answering and\npixel-level segmentation based on language instructions, while also enabling\nthe recognition of temporally anchored events in traffic accident scenarios. To\nadvance the development of MLLMs for traffic accident understanding, we curate\na new dataset containing multimodal question-answer pairs centered on diverse\naccident scenarios, with detailed pixel-level annotations and temporal event\nboundaries. Experimental results show that SafePLUG achieves strong performance\non multiple tasks, including region-based question answering, pixel-level\nsegmentation, temporal event localization, and accident event understanding.\nThese capabilities lay a foundation for fine-grained understanding of complex\ntraffic scenes, with the potential to improve driving safety and enhance\nsituational awareness in smart transportation systems. The code, dataset, and\nmodel checkpoints will be made publicly available at:\nhttps://zihaosheng.github.io/SafePLUG",
        "url": "http://arxiv.org/abs/2508.06763v1",
        "published_date": "2025-08-09T00:25:24+00:00",
        "updated_date": "2025-08-09T00:25:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zihao Sheng",
            "Zilin Huang",
            "Yen-Jung Chen",
            "Yansong Qu",
            "Yuhao Luo",
            "Yue Leng",
            "Sikai Chen"
        ],
        "tldr": "The paper introduces SafePLUG, a framework that enhances multimodal LLMs with pixel-level understanding and temporal grounding for improved traffic accident analysis, accompanied by a new dataset with fine-grained annotations.",
        "tldr_zh": "该论文介绍了SafePLUG，一个通过像素级理解和时间定位增强多模态LLM的框架，以改进交通事故分析，并提供了一个包含细粒度标注的新数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis",
        "summary": "Accurate diagnosis of skin diseases remains a significant challenge due to\nthe complex and diverse visual features present in dermatoscopic images, often\ncompounded by a lack of interpretability in existing purely visual diagnostic\nmodels. To address these limitations, this study introduces VL-MedGuide\n(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful\nmulti-modal understanding and reasoning capabilities of Visual-Language Large\nModels (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis\nof skin conditions. VL-MedGuide operates in two interconnected stages: a\nMulti-modal Concept Perception Module, which identifies and linguistically\ndescribes dermatologically relevant visual features through sophisticated\nprompt engineering, and an Explainable Disease Reasoning Module, which\nintegrates these concepts with raw visual information via Chain-of-Thought\nprompting to provide precise disease diagnoses alongside transparent\nrationales. Comprehensive experiments on the Derm7pt dataset demonstrate that\nVL-MedGuide achieves state-of-the-art performance in both disease diagnosis\n(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),\nsurpassing existing baselines. Furthermore, human evaluations confirm the high\nclarity, completeness, and trustworthiness of its generated explanations,\nbridging the gap between AI performance and clinical utility by offering\nactionable, explainable insights for dermatological practice.",
        "url": "http://arxiv.org/abs/2508.06624v1",
        "published_date": "2025-08-08T18:13:34+00:00",
        "updated_date": "2025-08-08T18:13:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kexin Yu",
            "Zihan Xu",
            "Jialei Xie",
            "Carter Adams"
        ],
        "tldr": "The paper introduces VL-MedGuide, a Visual-Language Large Model framework for skin disease diagnosis that achieves state-of-the-art performance with interpretable explanations, improving clinical utility.",
        "tldr_zh": "该论文介绍了VL-MedGuide，一个用于皮肤病诊断的视觉-语言大模型框架，该框架通过可解释的说明实现了最先进的性能，提高了临床实用性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification",
        "summary": "The proliferation of digital news media necessitates robust methods for\nverifying content veracity, particularly regarding the consistency between\nvisual and textual information. Traditional approaches often fall short in\naddressing the fine-grained cross-modal contextual consistency (FCCC) problem,\nwhich encompasses deeper alignment of visual narrative, emotional tone, and\nbackground information with text, beyond mere entity matching. To address this,\nwe propose ContextGuard-LVLM, a novel framework built upon advanced\nVision-Language Large Models (LVLMs) and integrating a multi-stage contextual\nreasoning mechanism. Our model is uniquely enhanced through reinforced or\nadversarial learning paradigms, enabling it to detect subtle contextual\nmisalignments that evade zero-shot baselines. We extend and augment three\nestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new\nfine-grained contextual annotations, including \"contextual sentiment,\" \"visual\nnarrative theme,\" and \"scene-event logical coherence,\" and introduce a\ncomprehensive CTXT (Contextual Coherence) entity type. Extensive experiments\ndemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art\nzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all\nfine-grained consistency tasks, showing significant improvements in complex\nlogical reasoning and nuanced contextual understanding. Furthermore, our model\nexhibits superior robustness to subtle perturbations and a higher agreement\nrate with human expert judgments on challenging samples, affirming its efficacy\nin discerning sophisticated forms of context detachment.",
        "url": "http://arxiv.org/abs/2508.06623v1",
        "published_date": "2025-08-08T18:10:24+00:00",
        "updated_date": "2025-08-08T18:10:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sihan Ma",
            "Qiming Wu",
            "Ruotong Jiang",
            "Frank Burns"
        ],
        "tldr": "The paper introduces ContextGuard-LVLM, a novel framework utilizing LVLMs and a multi-stage contextual reasoning mechanism, enhanced with reinforced/adversarial learning, to verify news veracity by detecting fine-grained cross-modal contextual inconsistencies.",
        "tldr_zh": "该论文介绍了ContextGuard-LVLM，一个新颖的框架，利用LVLMs和多阶段上下文推理机制，通过强化/对抗学习增强，通过检测细粒度的跨模态上下文不一致性来验证新闻的真实性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing",
        "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
        "url": "http://arxiv.org/abs/2508.06937v1",
        "published_date": "2025-08-09T11:06:58+00:00",
        "updated_date": "2025-08-09T11:06:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Weiyan Xie",
            "Han Gao",
            "Didan Deng",
            "Kaican Li",
            "April Hua Liu",
            "Yongxiang Huang",
            "Nevin L. Zhang"
        ],
        "tldr": "CannyEdit introduces a training-free image editing framework leveraging Selective Canny Control and Dual-Prompt Guidance to improve text adherence, context fidelity, and seamless integration in edited images, outperforming existing methods.",
        "tldr_zh": "CannyEdit 提出了一种免训练的图像编辑框架，该框架利用选择性 Canny 控制和双提示引导来提高编辑图像中的文本一致性、上下文保真度和无缝集成，优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
        "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
        "url": "http://arxiv.org/abs/2508.06905v1",
        "published_date": "2025-08-09T09:36:21+00:00",
        "updated_date": "2025-08-09T09:36:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruoxi Chen",
            "Dongping Chen",
            "Siyuan Wu",
            "Sinan Wang",
            "Shiyun Lang",
            "Petr Sushko",
            "Gaoyang Jiang",
            "Yao Wan",
            "Ranjay Krishna"
        ],
        "tldr": "The paper introduces MultiRef-bench and MultiRef dataset for controllable image generation using multiple visual references, highlighting the shortcomings of current state-of-the-art models in handling multi-reference conditioning and providing directions for future research.",
        "tldr_zh": "该论文介绍了MultiRef-bench和MultiRef数据集，用于使用多个视觉参考进行可控图像生成，强调了当前最先进的模型在处理多参考调节方面的不足，并为未来的研究提供了方向。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation",
        "summary": "Camouflaged Object Segmentation (COS) remains highly challenging due to the\nintrinsic visual similarity between target objects and their surroundings.\nWhile training-based COS methods achieve good performance, their performance\ndegrades rapidly with increased annotation sparsity. To circumvent this\nlimitation, recent studies have explored training-free COS methods, leveraging\nthe Segment Anything Model (SAM) by automatically generating visual prompts\nfrom a single task-generic prompt (\\textit{e.g.}, \"\\textit{camouflaged\nanimal}\") uniformly applied across all test images. However, these methods\ntypically produce only semantic-level visual prompts, causing SAM to output\ncoarse semantic masks and thus failing to handle scenarios with multiple\ndiscrete camouflaged instances effectively. To address this critical\nlimitation, we propose a simple yet powerful \\textbf{I}nstance-\\textbf{A}ware\n\\textbf{P}rompting \\textbf{F}ramework (IAPF), the first training-free COS\npipeline that explicitly converts a task-generic prompt into fine-grained\ninstance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt\nGenerator, utilizing task-generic queries to prompt a Multimodal Large Language\nModel (MLLM) for generating image-specific foreground and background tags; (2)\n\\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise\ninstance-level bounding box prompts, alongside the proposed Single-Foreground\nMulti-Background Prompting strategy to sample region-constrained point prompts\nwithin each box, enabling SAM to yield a candidate instance mask; (3)\nSelf-consistency Instance Mask Voting, which selects the final COS prediction\nby identifying the candidate mask most consistent across multiple candidate\ninstance masks. Extensive evaluations on standard COS benchmarks demonstrate\nthat the proposed IAPF significantly surpasses existing state-of-the-art\ntraining-free COS methods.",
        "url": "http://arxiv.org/abs/2508.06904v1",
        "published_date": "2025-08-09T09:35:32+00:00",
        "updated_date": "2025-08-09T09:35:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chao Yin",
            "Jide Li",
            "Xiaoqiang Li"
        ],
        "tldr": "This paper introduces IAPF, a training-free camouflaged object segmentation pipeline that uses instance-aware prompting with MLLMs and SAM to generate fine-grained instance masks, outperforming existing training-free methods.",
        "tldr_zh": "本文介绍了一种名为IAPF的免训练伪装对象分割流程，该流程使用实例感知的提示与MLLM和SAM相结合，生成精细的实例掩码，优于现有的免训练方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models",
        "summary": "Mainstream Multimodal Large Language Models (MLLMs) achieve visual\nunderstanding by using a vision projector to bridge well-pretrained vision\nencoders and large language models (LLMs). The inherent gap between visual and\ntextual modalities makes the embeddings from the vision projector critical for\nvisual comprehension. However, current alignment approaches treat visual\nembeddings as contextual cues and merely apply auto-regressive supervision to\ntextual outputs, neglecting the necessity of introducing equivalent direct\nvisual supervision, which hinders the potential finer alignment of visual\nembeddings. In this paper, based on our analysis of the refinement process of\nvisual embeddings in the LLM's shallow layers, we propose BASIC, a method that\nutilizes refined visual embeddings within the LLM as supervision to directly\nguide the projector in generating initial visual embeddings. Specifically, the\nguidance is conducted from two perspectives: (i) optimizing embedding\ndirections by reducing angles between initial and supervisory embeddings in\nsemantic space; (ii) improving semantic matching by minimizing disparities\nbetween the logit distributions of both visual embeddings. Without additional\nsupervisory models or artificial annotations, BASIC significantly improves the\nperformance of MLLMs across a wide range of benchmarks, demonstrating the\neffectiveness of our introduced direct visual supervision.",
        "url": "http://arxiv.org/abs/2508.06895v1",
        "published_date": "2025-08-09T09:00:45+00:00",
        "updated_date": "2025-08-09T09:00:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jianting Tang",
            "Yubo Wang",
            "Haoyu Cao",
            "Linli Xu"
        ],
        "tldr": "The paper introduces BASIC, a method to improve visual alignment in MLLMs by using refined visual embeddings within the LLM as direct supervision for the vision projector, leading to performance gains across benchmarks.",
        "tldr_zh": "该论文介绍了 BASIC，一种通过使用 MLLM 中精细化的视觉嵌入作为视觉投影器的直接监督来改善 MLLM 中视觉对齐的方法，从而在各项基准测试中实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding",
        "summary": "Long video understanding presents a significant challenge to multimodal large\nlanguage models (MLLMs) primarily due to the immense data scale. A critical and\nwidely adopted strategy for making this task computationally tractable is\nkeyframe retrieval, which seeks to identify a sparse set of video frames that\nare most salient to a given textual query. However, the efficacy of this\napproach is hindered by weak multimodal alignment between textual queries and\nvisual content and fails to capture the complex temporal semantic information\nrequired for precise reasoning. To address this, we propose Visual-Subtitle\nIntegeration(VSI), a multimodal keyframe search method that integrates\nsubtitles, timestamps, and scene boundaries into a unified multimodal search\nprocess. The proposed method captures the visual information of video frames as\nwell as the complementary textual information through a dual-stream search\nmechanism by Video Search Stream as well as Subtitle Match Stream,\nrespectively, and improves the keyframe search accuracy through the interaction\nof the two search streams. Experimental results show that VSI achieve 40.00%\nkey frame localization accuracy on the text-relevant subset of LongVideoBench\nand 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive\nbaselines by 20.35% and 15.79%, respectively. Furthermore, on the\nLongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA\ntasks, demonstrating the robustness and generalizability of the proposed\nmultimodal search strategy.",
        "url": "http://arxiv.org/abs/2508.06869v1",
        "published_date": "2025-08-09T07:38:48+00:00",
        "updated_date": "2025-08-09T07:38:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.2.10"
        ],
        "authors": [
            "Jianxiang He",
            "Shaoguang Wang",
            "Weiyu Guo",
            "Meisheng Hong",
            "Jungang Li",
            "Yijie Xu",
            "Ziyang Chen",
            "Hui Xiong"
        ],
        "tldr": "The paper introduces VSI, a multimodal keyframe selection method that integrates subtitles, timestamps, and scene boundaries to enhance long video understanding by improving keyframe search accuracy. It achieves SOTA results on LongVideoBench for medium-to-long video-QA tasks.",
        "tldr_zh": "该论文介绍了 VSI，一种多模态关键帧选择方法，它集成了字幕、时间戳和场景边界，通过提高关键帧搜索精度来增强长视频理解。该方法在 LongVideoBench 上针对中长视频问答任务取得了 SOTA 结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AGIC: Attention-Guided Image Captioning to Improve Caption Relevance",
        "summary": "Despite significant progress in image captioning, generating accurate and\ndescriptive captions remains a long-standing challenge. In this study, we\npropose Attention-Guided Image Captioning (AGIC), which amplifies salient\nvisual regions directly in the feature space to guide caption generation. We\nfurther introduce a hybrid decoding strategy that combines deterministic and\nprobabilistic sampling to balance fluency and diversity. To evaluate AGIC, we\nconduct extensive experiments on the Flickr8k and Flickr30k datasets. The\nresults show that AGIC matches or surpasses several state-of-the-art models\nwhile achieving faster inference. Moreover, AGIC demonstrates strong\nperformance across multiple evaluation metrics, offering a scalable and\ninterpretable solution for image captioning.",
        "url": "http://arxiv.org/abs/2508.06853v1",
        "published_date": "2025-08-09T06:42:25+00:00",
        "updated_date": "2025-08-09T06:42:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "L. D. M. S. Sai Teja",
            "Ashok Urlana",
            "Pruthwik Mishra"
        ],
        "tldr": "The paper introduces AGIC, an attention-guided image captioning model using feature space amplification and a hybrid decoding strategy. It claims to achieve state-of-the-art performance with faster inference on Flickr datasets.",
        "tldr_zh": "该论文介绍了AGIC，一种注意力引导的图像字幕模型，它使用特征空间放大和混合解码策略。它声称在Flickr数据集上实现了最先进的性能和更快的推理速度。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]