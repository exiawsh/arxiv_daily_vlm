[
    {
        "title": "Modest-Align: Data-Efficient Alignment for Vision-Language Models",
        "summary": "Cross-modal alignment aims to map heterogeneous modalities into a shared\nlatent space, as exemplified by models like CLIP, which benefit from\nlarge-scale image-text pretraining for strong recognition capabilities.\nHowever, when operating in resource-constrained settings with limited or\nlow-quality data, these models often suffer from overconfidence and degraded\nperformance due to the prevalence of ambiguous or weakly correlated image-text\npairs. Current contrastive learning approaches, which rely on single positive\npairs, further exacerbate this issue by reinforcing overconfidence on uncertain\nsamples. To address these challenges, we propose Modest-Align, a lightweight\nalignment framework designed for robustness and efficiency. Our approach\nleverages two complementary strategies -- Random Perturbation, which introduces\ncontrolled noise to simulate uncertainty, and Embedding Smoothing, which\ncalibrates similarity distributions in the embedding space. These mechanisms\ncollectively reduce overconfidence and improve performance on noisy or weakly\naligned samples. Extensive experiments across multiple benchmark datasets\ndemonstrate that Modest-Align outperforms state-of-the-art methods in retrieval\ntasks, achieving competitive results with over 100x less training data and 600x\nless GPU time than CLIP. Our method offers a practical and scalable solution\nfor cross-modal alignment in real-world, low-resource scenarios.",
        "url": "http://arxiv.org/abs/2510.21606v1",
        "published_date": "2025-10-24T16:11:10+00:00",
        "updated_date": "2025-10-24T16:11:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxiang Liu",
            "Yuan Wang",
            "Jiawei Du",
            "Joey Tianyi Zhou",
            "Mingkun Xu",
            "Zuozhu Liu"
        ],
        "tldr": "Modest-Align is a data-efficient vision-language alignment framework using random perturbation and embedding smoothing to improve performance in low-resource settings, achieving competitive results with significantly less data and compute compared to CLIP.",
        "tldr_zh": "Modest-Align是一个数据高效的视觉-语言对齐框架，它利用随机扰动和嵌入平滑来提高在低资源环境中的性能，与CLIP相比，它使用更少的数据和计算资源就获得了具有竞争力的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers",
        "summary": "Language and vision-language models have shown impressive performance across\na wide range of tasks, but their internal mechanisms remain only partly\nunderstood. In this work, we study how individual attention heads in\ntext-generative models specialize in specific semantic or visual attributes.\nBuilding on an established interpretability method, we reinterpret the practice\nof probing intermediate activations with the final decoding layer through the\nlens of signal processing. This lets us analyze multiple samples in a\nprincipled way and rank attention heads based on their relevance to target\nconcepts. Our results show consistent patterns of specialization at the head\nlevel across both unimodal and multimodal transformers. Remarkably, we find\nthat editing as few as 1% of the heads, selected using our method, can reliably\nsuppress or enhance targeted concepts in the model output. We validate our\napproach on language tasks such as question answering and toxicity mitigation,\nas well as vision-language tasks including image classification and captioning.\nOur findings highlight an interpretable and controllable structure within\nattention layers, offering simple tools for understanding and editing\nlarge-scale generative models.",
        "url": "http://arxiv.org/abs/2510.21518v1",
        "published_date": "2025-10-24T14:41:47+00:00",
        "updated_date": "2025-10-24T14:41:47+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Lorenzo Basile",
            "Valentino Maiorca",
            "Diego Doimo",
            "Francesco Locatello",
            "Alberto Cazzaniga"
        ],
        "tldr": "This paper introduces a signal processing-based method to probe and rank attention heads in multimodal transformers, revealing specialization patterns and enabling targeted editing of model outputs with minimal intervention.",
        "tldr_zh": "本文介绍了一种基于信号处理的方法来探测和排序多模态Transformer中的注意力头，揭示了其专业化模式，并能以最小的干预实现对模型输出的针对性编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection",
        "summary": "Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors\nwithin videos. Recently, offline VAD has garnered substantial research\nattention, which has been invigorated by the progress in large language models\n(LLMs) and vision-language models (VLMs), offering the potential for a more\nnuanced understanding of anomalies. However, online VAD has seldom received\nattention due to real-time constraints and computational intensity. In this\npaper, we introduce a novel Memory-based online scoring queue scheme for\nTraining-free VAD (MoniTor), to address the inherent complexities in online\nVAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the\ncapabilities of pre-trained large-scale models. To capture temporal\ndependencies more effectively, we incorporate a novel prediction mechanism\ninspired by Long Short-Term Memory (LSTM) networks. This ensures the model can\neffectively model past states and leverage previous predictions to identify\nanomalous behaviors. Thereby, it better understands the current frame.\nMoreover, we design a scoring queue and an anomaly prior to dynamically store\nrecent scores and cover all anomalies in the monitoring scenario, providing\nguidance for LLMs to distinguish between normal and abnormal behaviors over\ntime. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and\nXD-Violence) containing various surveillance and real-world scenarios. The\nresults demonstrate that MoniTor outperforms state-of-the-art methods and is\ncompetitive with weakly supervised methods without training. Code is available\nat https://github.com/YsTvT/MoniTor.",
        "url": "http://arxiv.org/abs/2510.21449v1",
        "published_date": "2025-10-24T13:28:29+00:00",
        "updated_date": "2025-10-24T13:28:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengtian Yang",
            "Yue Feng",
            "Yingshi Liu",
            "Jingrou Zhang",
            "Jie Qin"
        ],
        "tldr": "The paper introduces MoniTor, a training-free online video anomaly detection method leveraging VLMs and LLMs with a memory-based scoring queue and LSTM-inspired prediction mechanism, achieving state-of-the-art performance on UCF-Crime and XD-Violence datasets.",
        "tldr_zh": "该论文介绍了MoniTor，一种无需训练的在线视频异常检测方法，利用VLMs和LLMs，结合基于记忆的评分队列和LSTM启发的预测机制，在UCF-Crime和XD-Violence数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings",
        "summary": "As generative AI continues to evolve, Vision Language Models (VLMs) have\nemerged as promising tools in various healthcare applications. One area that\nremains relatively underexplored is their use in human activity recognition\n(HAR) for remote health monitoring. VLMs offer notable strengths, including\ngreater flexibility and the ability to overcome some of the constraints of\ntraditional deep learning models. However, a key challenge in applying VLMs to\nHAR lies in the difficulty of evaluating their dynamic and often\nnon-deterministic outputs. To address this gap, we introduce a descriptive\ncaption data set and propose comprehensive evaluation methods to evaluate VLMs\nin HAR. Through comparative experiments with state-of-the-art deep learning\nmodels, our findings demonstrate that VLMs achieve comparable performance and,\nin some cases, even surpass conventional approaches in terms of accuracy. This\nwork contributes a strong benchmark and opens new possibilities for the\nintegration of VLMs into intelligent healthcare systems.",
        "url": "http://arxiv.org/abs/2510.21424v1",
        "published_date": "2025-10-24T13:04:13+00:00",
        "updated_date": "2025-10-24T13:04:13+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Abderrazek Abid",
            "Thanh-Cong Ho",
            "Fakhri Karray"
        ],
        "tldr": "This paper introduces a descriptive caption dataset and evaluation methods for Vision Language Models (VLMs) in human activity recognition (HAR) for healthcare, showing VLMs achieve comparable or better performance than traditional methods.",
        "tldr_zh": "本文介绍了一个描述性字幕数据集以及用于评估视觉语言模型（VLM）在医疗保健领域中人体活动识别（HAR）的评估方法，表明VLM实现了与传统方法相当或更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bridging the gap to real-world language-grounded visual concept learning",
        "summary": "Human intelligence effortlessly interprets visual scenes along a rich\nspectrum of semantic dimensions. However, existing approaches to\nlanguage-grounded visual concept learning are limited to a few predefined\nprimitive axes, such as color and shape, and are typically explored in\nsynthetic datasets. In this work, we propose a scalable framework that\nadaptively identifies image-related concept axes and grounds visual concepts\nalong these axes in real-world scenes. Leveraging a pretrained vision-language\nmodel and our universal prompting strategy, our framework identifies a diverse\nimage-related axes without any prior knowledge. Our universal concept encoder\nadaptively binds visual features to the discovered axes without introducing\nadditional model parameters for each concept. To ground visual concepts along\nthe discovered axes, we optimize a compositional anchoring objective, which\nensures that each axis can be independently manipulated without affecting\nothers. We demonstrate the effectiveness of our framework on subsets of\nImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across\ndiverse real-world concepts that are too varied to be manually predefined. Our\nmethod also exhibits strong compositional generalization, outperforming\nexisting visual concept learning and text-based editing methods. The code is\navailable at https://github.com/whieya/Language-grounded-VCL.",
        "url": "http://arxiv.org/abs/2510.21412v1",
        "published_date": "2025-10-24T12:54:13+00:00",
        "updated_date": "2025-10-24T12:54:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Whie Jung",
            "Semin Kim",
            "Junee Kim",
            "Seunghoon Hong"
        ],
        "tldr": "The paper introduces a scalable framework for language-grounded visual concept learning in real-world scenes, adaptively identifying and grounding image-related concept axes using a pre-trained vision-language model and a compositional anchoring objective, achieving superior editing and generalization capabilities.",
        "tldr_zh": "该论文介绍了一个可扩展的框架，用于在真实世界场景中进行语言引导的视觉概念学习，利用预训练的视觉语言模型和一个组合锚定目标自适应地识别和定位图像相关的概念轴，实现了卓越的编辑和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence",
        "summary": "We propose the Multi-modal Untrimmed Video Retrieval task, along with a new\nbenchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims\nto retrieve untrimmed videos containing relevant segments using multi-modal\nqueries. It has the following features: 1) Practical retrieval paradigm: MUVR\nsupports video-centric multi-modal queries, expressing fine-grained retrieval\nneeds through long text descriptions, video tag prompts, and mask prompts. It\nadopts a one-to-many retrieval paradigm and focuses on untrimmed videos,\ntailored for long-video platform applications. 2) Multi-level visual\ncorrespondence: To cover common video categories (e.g., news, travel, dance)\nand precisely define retrieval matching criteria, we construct multi-level\nvisual correspondence based on core video content (e.g., news events, travel\nlocations, dance moves) which users are interested in and want to retrieve. It\ncovers six levels: copy, event, scene, instance, action, and others. 3)\nComprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,\nFilter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA\nassesses MLLMs in a question-answering format. We also propose a Reranking\nScore to evaluate the reranking ability of MLLMs. MUVR consists of 53K\nuntrimmed videos from the video platform Bilibili, with 1,050 multi-modal\nqueries and 84K matches. Extensive evaluations of 3 state-of-the-art video\nretrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals\nthe limitations of retrieval methods in processing untrimmed videos and\nmulti-modal queries, as well as MLLMs in multi-video understanding and\nreranking. Our code and benchmark is available at\nhttps://github.com/debby-0527/MUVR.",
        "url": "http://arxiv.org/abs/2510.21406v1",
        "published_date": "2025-10-24T12:50:02+00:00",
        "updated_date": "2025-10-24T12:50:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Feng",
            "Jinwei Hu",
            "Qijia Lu",
            "Jiawei Niu",
            "Li Tan",
            "Shuo Yuan",
            "Ziyi Yan",
            "Yizhen Jia",
            "Qingzhi He",
            "Shiping Ge",
            "Ethan Q. Chen",
            "Wentong Li",
            "Limin Wang",
            "Jie Qin"
        ],
        "tldr": "The paper introduces MUVR, a new benchmark for multi-modal untrimmed video retrieval on long-video platforms, featuring multi-level visual correspondence and comprehensive evaluation criteria, and reveals limitations of existing methods.",
        "tldr_zh": "该论文提出了MUVR，一个新的用于长视频平台上的多模态未裁剪视频检索的基准，具有多层次的视觉对应和全面的评估标准，并揭示了现有方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding",
        "summary": "Eye gaze offers valuable cues about attention, short-term intent, and future\nactions, making it a powerful signal for modeling egocentric behavior. In this\nwork, we propose a gaze-regularized framework that enhances VLMs for two key\negocentric understanding tasks: fine-grained future event prediction and\ncurrent activity understanding. Unlike prior approaches that rely solely on\nvisual inputs or use gaze as an auxiliary input signal , our method uses gaze\nonly during training. We introduce a gaze-regularized attention mechanism that\naligns model focus with human visual gaze. This design is flexible and modular,\nallowing it to generalize across multiple VLM architectures that utilize\nattention. Experimental results show that our approach improves semantic\nprediction scores by up to 11 for future event prediction and around 7 for\ncurrent activity understanding, compared to the corresponding baseline models\ntrained without gaze regularization. These results highlight the value of\ngaze-guided training in improving the accuracy and robustness of egocentric\nVLMs. Overall, this work establishes a foundation for using human gaze to\nenhance the predictive capabilities of VLMs in real-world scenarios like\nassistive robots and human-machine collaboration. Code and additional\ninformation is available at: https://github.com/anupampani/Gaze-VLM",
        "url": "http://arxiv.org/abs/2510.21356v1",
        "published_date": "2025-10-24T11:33:03+00:00",
        "updated_date": "2025-10-24T11:33:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Anupam Pani",
            "Yanchao Yang"
        ],
        "tldr": "This paper introduces a gaze-regularized attention mechanism to enhance VLMs for egocentric understanding tasks, using gaze data only during training to improve performance in future event prediction and current activity understanding.",
        "tldr_zh": "本文提出了一种基于注视正则化的注意力机制，用于增强 VLM 在以自我为中心的理解任务中的性能。该方法仅在训练期间使用注视数据，从而提高了未来事件预测和当前活动理解的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set",
        "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
        "url": "http://arxiv.org/abs/2510.21323v1",
        "published_date": "2025-10-24T10:29:31+00:00",
        "updated_date": "2025-10-24T10:29:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shufan Shen",
            "Junshu Sun",
            "Qingming Huang",
            "Shuhui Wang"
        ],
        "tldr": "This paper introduces VL-SAE, a sparse autoencoder, to interpret and enhance vision-language alignment by mapping multi-modal representations to a unified concept set, leading to improved performance in downstream tasks.",
        "tldr_zh": "本文介绍了一种稀疏自编码器VL-SAE，通过将多模态表示映射到一个统一的概念集合，来解释和增强视觉-语言对齐，从而提高下游任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning",
        "summary": "Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities\nacross a wide range of vision-language tasks. However, due to the restricted\ninput resolutions, MLLMs face significant challenges in precisely understanding\nand localizing visual details in high-resolution images -- particularly when\ndealing with extra-small objects embedded in cluttered contexts. To address\nthis issue, we propose \\textsc{FineRS}, a two-stage MLLM-based reinforcement\nlearning framework for jointly reasoning and segmenting extremely small objects\nwithin high-resolution scenes. \\textsc{FineRS} adopts a coarse-to-fine pipeline\ncomprising Global Semantic Exploration (GSE) and Localized Perceptual\nRefinement (LPR). Specifically, GSE performs instruction-guided reasoning to\ngenerate a textural response and a coarse target region, while LPR refines this\nregion to produce an accurate bounding box and segmentation mask. To couple the\ntwo stages, we introduce a locate-informed retrospective reward, where LPR's\noutputs are used to optimize GSE for more robust coarse region exploration. %\nAdditionally, we present \\textsc{FineRS}-4k, a new dataset for evaluating MLLMs\non attribute-level reasoning and pixel-level segmentation on subtle,\nsmall-scale targets in complex high-resolution scenes. Experimental results on\n\\textsc{FineRS}-4k and public datasets demonstrate that our method consistently\noutperforms state-of-the-art MLLM-based approaches on both instruction-guided\nsegmentation and visual reasoning tasks.",
        "url": "http://arxiv.org/abs/2510.21311v1",
        "published_date": "2025-10-24T10:14:17+00:00",
        "updated_date": "2025-10-24T10:14:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lu Zhang",
            "Jiazuo Yu",
            "Haomiao Xiong",
            "Ping Hu",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "You He"
        ],
        "tldr": "The paper introduces FineRS, a reinforcement learning framework leveraging MLLMs for fine-grained reasoning and segmentation of small objects in high-resolution images, and proposes a new dataset FineRS-4k for evaluation.",
        "tldr_zh": "该论文介绍了一种名为FineRS的强化学习框架，该框架利用多模态大语言模型对高分辨率图像中的小物体进行细粒度的推理和分割，并提出了一个新的数据集FineRS-4k用于评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution",
        "summary": "The rapid progress of multimodal large language models (MLLMs) calls for more\nreliable evaluation protocols. Existing static benchmarks suffer from the\npotential risk of data contamination and saturation, leading to inflated or\nmisleading performance evaluations. To address these issues, we first apply\nGraph formulation to represent a static or dynamic VQA sample. With the\nformulation, we propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic\nmultimodal evaluation framework. KBE first analyzes the original static\nbenchmark, then expands it by integrating multimodal knowledge, transforming\nthe static benchmark into a controllable, dynamic evolving version. Crucially,\nKBE can both reconstruct questions by Re-selecting visual information in the\noriginal image and expand existing questions with external textual knowledge.\nIt enables difficulty-controllable evaluation by adjusting the degree of\nquestion exploration. Extensive experiments demonstrate that KBE alleviates the\nrisk of data contamination, data saturation, and provides a more comprehensive\nassessment of MLLM capabilities.",
        "url": "http://arxiv.org/abs/2510.21182v1",
        "published_date": "2025-10-24T06:13:36+00:00",
        "updated_date": "2025-10-24T06:13:36+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Junzhe Zhang",
            "Huixuan Zhang",
            "Xiaojun Wan"
        ],
        "tldr": "This paper introduces KBE, a dynamic multimodal evaluation framework that combats data contamination and saturation in VQA benchmarks by evolving static datasets with multimodal knowledge, allowing for difficulty-controllable evaluation of MLLMs.",
        "tldr_zh": "该论文介绍了一种动态多模态评估框架KBE，通过使用多模态知识进化静态数据集来对抗VQA基准测试中的数据污染和饱和，从而实现对MLLM的难度可控评估。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments",
        "summary": "Visual reasoning in multimodal large language models (MLLMs) has primarily\nbeen studied in static, fully observable settings, limiting their effectiveness\nin real-world environments where information is often incomplete due to\nocclusion or limited field of view. Humans, in contrast, actively explore and\ninteract with their environment-moving, examining, and manipulating objects-to\ngather information through a closed-loop process integrating perception,\nreasoning, and action. Inspired by this human capability, we introduce the\nActive Visual Reasoning (AVR) task, extending visual reasoning to partially\nobservable, interactive environments. AVR necessitates agents to: (1) actively\nacquire information via sequential physical actions, (2) integrate observations\nacross multiple steps for coherent reasoning, and (3) dynamically adjust\ndecisions based on evolving visual feedback. To rigorously evaluate AVR, we\nintroduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive\nenvironments designed to assess both reasoning correctness and\ninformation-gathering efficiency. We present AVR-152k, a large-scale dataset\nthat offers rich Chain-of-Thought (CoT) annotations detailing iterative\nreasoning for uncertainty identification, action-conditioned information gain\nprediction, and information-maximizing action selection, crucial for training\nagents in a higher-order Markov Decision Process. Building on this, we develop\nPhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR,\nembodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath,\nGeometry30K). Our analysis also reveals that current embodied MLLMs, despite\ndetecting information incompleteness, struggle to actively acquire and\nintegrate new information through interaction, highlighting a fundamental gap\nin active reasoning capabilities.",
        "url": "http://arxiv.org/abs/2510.21111v1",
        "published_date": "2025-10-24T02:59:00+00:00",
        "updated_date": "2025-10-24T02:59:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijie Zhou",
            "Xuantang Xiong",
            "Yi Peng",
            "Manli Tao",
            "Chaoyang Zhao",
            "Honghui Dong",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "tldr": "The paper introduces Active Visual Reasoning (AVR) for MLLMs, a new task and benchmark (CLEVR-AVR) addressing the limitations of current MLLMs in interactive, partially observable environments, along with a novel model PhysVLM-AVR that achieves state-of-the-art performance. It also identifies a gap in current MLLMs' ability to actively acquire and integrate information through interaction.",
        "tldr_zh": "该论文介绍了多模态大语言模型（MLLM）的主动视觉推理（AVR）任务，一个新的基准测试（CLEVR-AVR），旨在解决现有MLLM在交互式、部分可观察环境中的局限性，并提出了一个名为PhysVLM-AVR的新模型，该模型实现了最先进的性能。该研究还指出当前MLLM在通过交互主动获取和整合信息方面的能力存在差距。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models",
        "summary": "Understanding and reasoning about complex 3D environments requires structured\nscene representations that capture not only objects but also their semantic and\nspatial relationships. While recent works on 3D scene graph generation have\nleveraged pretrained VLMs without task-specific fine-tuning, they are largely\nconfined to single-view settings, fail to support incremental updates as new\nobservations arrive and lack explicit geometric grounding in 3D space, all of\nwhich are essential for embodied scenarios. In this paper, we propose, ZING-3D,\na framework that leverages the vast knowledge of pretrained foundation models\nto enable open-vocabulary recognition and generate a rich semantic\nrepresentation of the scene in a zero-shot manner while also enabling\nincremental updates and geometric grounding in 3D space, making it suitable for\ndownstream robotics applications. Our approach leverages VLM reasoning to\ngenerate a rich 2D scene graph, which is grounded in 3D using depth\ninformation. Nodes represent open-vocabulary objects with features, 3D\nlocations, and semantic context, while edges capture spatial and semantic\nrelations with inter-object distances. Our experiments on scenes from the\nReplica and HM3D dataset show that ZING-3D is effective at capturing spatial\nand relational knowledge without the need of task-specific training.",
        "url": "http://arxiv.org/abs/2510.21069v1",
        "published_date": "2025-10-24T00:52:33+00:00",
        "updated_date": "2025-10-24T00:52:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Pranav Saxena",
            "Jimmy Chiun"
        ],
        "tldr": "ZING-3D is a framework that uses VLMs to generate incremental 3D scene graphs with open-vocabulary recognition and geometric grounding in 3D space, without task-specific training, for use in robotics applications.",
        "tldr_zh": "ZING-3D是一个框架，利用VLMs生成增量3D场景图，具有开放词汇识别和3D空间中的几何定位，无需特定任务训练，适用于机器人应用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models",
        "summary": "Current Vision-Language Models (VLMs) struggle to ground anatomical regions\nin 3D medical images and reason about them in a step-by-step manner, a key\nrequirement of real-world diagnostic assessment. This ability is essential for\naligning model outputs with the diagnostic workflows clinicians use in\npractice, enabling trustworthy clinician-AI collaboration. Existing 3D datasets\nprovide localization labels, but none support this \"grounded reasoning\"\nability. To address this gap, we introduce 3DReasonKnee, the first 3D grounded\nreasoning dataset for medical images, which provides 494k high-quality\nquintuples derived from 7,970 3D knee MRI volumes. Each quintuple includes: (1)\nthe 3D MRI volume, (2) a diagnostic question targeting a specific anatomical\nregion (3) a 3D bounding box localizing the relevant anatomical structures, (4)\nclinician-generated diagnostic reasoning steps that explicitly detail the 3D\nreasoning process, and (5) structured severity assessments for the relevant\nanatomical region. The creation and validation of 3DReasonKnee, involving over\n450 hours of expert clinician time for manually segmenting MRIs and generating\nreasoning chains, ensures its superior quality and clinical relevance. We\nestablish ReasonKnee-Bench to evaluate localization and diagnostic accuracy,\nproviding insight into VLM ability to perform grounding and severity assessment\nacross anatomical regions and diagnostic inquiries. We benchmark five\nstate-of-the-art VLMs, providing baseline performance for ReasonKnee-Bench. By\nproviding this unique resource of expert-annotated 3D reasoning pathways,\n3DReasonKnee serves as a repository of orthopedic surgeons' diagnostic\nexpertise and offers a vital testbed for advancing multimodal medical AI\nsystems towards 3D, clinically aligned, localized decision-making capabilities.\nThe dataset can be found in:\nhttps://huggingface.co/datasets/rajpurkarlab/3DReasonKnee",
        "url": "http://arxiv.org/abs/2510.20967v1",
        "published_date": "2025-10-23T19:54:49+00:00",
        "updated_date": "2025-10-23T19:54:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sraavya Sambara",
            "Sung Eun Kim",
            "Xiaoman Zhang",
            "Luyang Luo",
            "Shreya Johri",
            "Mohammed Baharoon",
            "Du Hyun Ro",
            "Pranav Rajpurkar"
        ],
        "tldr": "The paper introduces 3DReasonKnee, a new 3D grounded reasoning dataset for medical images with clinician-generated diagnostic reasoning steps, designed to improve VLMs' ability to perform localization and diagnostic accuracy in 3D medical imaging.",
        "tldr_zh": "该论文介绍了3DReasonKnee，一个新的3D医学图像数据集，它带有临床医生生成的诊断推理步骤，旨在提高VLM在3D医学影像中执行定位和诊断准确性的能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring",
        "summary": "With the widespread adoption of wearable devices in our daily lives, the\ndemand and appeal for remote patient monitoring have significantly increased.\nMost research in this field has concentrated on collecting sensor data,\nvisualizing it, and analyzing it to detect anomalies in specific diseases such\nas diabetes, heart disease and depression. However, this domain has a notable\ngap in the aspect of human-machine interaction. This paper proposes REMONI, an\nautonomous REmote health MONItoring system that integrates multimodal large\nlanguage models (MLLMs), the Internet of Things (IoT), and wearable devices.\nThe system automatically and continuously collects vital signs, accelerometer\ndata from a special wearable (such as a smartwatch), and visual data in patient\nvideo clips collected from cameras. This data is processed by an anomaly\ndetection module, which includes a fall detection model and algorithms to\nidentify and alert caregivers of the patient's emergency conditions. A\ndistinctive feature of our proposed system is the natural language processing\ncomponent, developed with MLLMs capable of detecting and recognizing a\npatient's activity and emotion while responding to healthcare worker's\ninquiries. Additionally, prompt engineering is employed to integrate all\npatient information seamlessly. As a result, doctors and nurses can access\nreal-time vital signs and the patient's current state and mood by interacting\nwith an intelligent agent through a user-friendly web application. Our\nexperiments demonstrate that our system is implementable and scalable for\nreal-life scenarios, potentially reducing the workload of medical professionals\nand healthcare costs. A full-fledged prototype illustrating the functionalities\nof the system has been developed and being tested to demonstrate the robustness\nof its various capabilities.",
        "url": "http://arxiv.org/abs/2510.21445v1",
        "published_date": "2025-10-24T13:23:38+00:00",
        "updated_date": "2025-10-24T13:23:38+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Thanh Cong Ho",
            "Farah Kharrat",
            "Abderrazek Abid",
            "Fakhri Karray"
        ],
        "tldr": "The paper introduces REMONI, a remote health monitoring system integrating wearables, IoT, and multimodal large language models (MLLMs) to provide real-time patient data and activity/emotion recognition for healthcare professionals. It aims to improve human-machine interaction in remote healthcare and reduce workload.",
        "tldr_zh": "该论文介绍了REMONI，一个集成了可穿戴设备、物联网和多模态大型语言模型（MLLM）的远程健康监测系统，旨在为医疗专业人员提供实时的患者数据和活动/情感识别，并改善远程医疗领域的人机交互，减少工作量。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models",
        "summary": "Text-to-image diffusion models, such as Stable Diffusion, have demonstrated\nremarkable capabilities in generating high-quality and diverse images from\nnatural language prompts. However, recent studies reveal that these models\noften replicate and amplify societal biases, particularly along demographic\nattributes like gender and race. In this paper, we introduce FairImagen\n(https://github.com/fuzihaofzh/FairImagen), a post-hoc debiasing framework that\noperates on prompt embeddings to mitigate such biases without retraining or\nmodifying the underlying diffusion model. Our method integrates Fair Principal\nComponent Analysis to project CLIP-based input embeddings into a subspace that\nminimizes group-specific information while preserving semantic content. We\nfurther enhance debiasing effectiveness through empirical noise injection and\npropose a unified cross-demographic projection method that enables simultaneous\ndebiasing across multiple demographic attributes. Extensive experiments across\ngender, race, and intersectional settings demonstrate that FairImagen\nsignificantly improves fairness with a moderate trade-off in image quality and\nprompt fidelity. Our framework outperforms existing post-hoc methods and offers\na simple, scalable, and model-agnostic solution for equitable text-to-image\ngeneration.",
        "url": "http://arxiv.org/abs/2510.21363v1",
        "published_date": "2025-10-24T11:47:15+00:00",
        "updated_date": "2025-10-24T11:47:15+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Zihao Fu",
            "Ryan Brown",
            "Shun Shao",
            "Kai Rawal",
            "Eoin Delaney",
            "Chris Russell"
        ],
        "tldr": "FairImagen is a post-hoc debiasing framework for text-to-image models that uses Fair PCA on CLIP embeddings to mitigate biases across demographic attributes without retraining the original model.",
        "tldr_zh": "FairImagen是一个用于文本到图像模型的后处理偏差缓解框架，它使用Fair PCA处理CLIP嵌入，以减轻跨人口属性的偏差，而无需重新训练原始模型。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation",
        "summary": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic\nreal-time rendering capabilities, is regarded as an effective tool for\nnarrowing the sim-to-real gap. However, it lacks fine-grained semantics and\nphysical executability for Visual-Language Navigation (VLN). To address this,\nwe propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments\nfor 3D Navigation), a new paradigm that upgrades 3DGS into an executable,\nsemantically and physically aligned environment. It comprises two components:\n(1) Object-Centric Semantic Grounding, which adds object-level fine-grained\nannotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds\ncollision objects into 3DGS and constructs rich physical interfaces. We release\nInteriorGS, containing 1K object-annotated 3DGS indoor scene data, and\nintroduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data.\nExperiments show that 3DGS scene data is more difficult to converge, while\nexhibiting strong generalizability, improving baseline performance by 31% on\nthe VLN-CE Unseen task. The data and code will be available soon.",
        "url": "http://arxiv.org/abs/2510.21307v1",
        "published_date": "2025-10-24T10:05:00+00:00",
        "updated_date": "2025-10-24T10:05:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingchen Miao",
            "Rong Wei",
            "Zhiqi Ge",
            "Xiaoquan sun",
            "Shiqi Gao",
            "Jingzhe Zhu",
            "Renhan Wang",
            "Siliang Tang",
            "Jun Xiao",
            "Rui Tang",
            "Juncheng Li"
        ],
        "tldr": "The paper introduces SAGE-3D, a new paradigm that upgrades 3D Gaussian Splatting with semantic and physical alignment for embodied navigation, including a new dataset and benchmark.",
        "tldr_zh": "该论文介绍了SAGE-3D，一种新的范式，通过语义和物理对齐来升级3D高斯溅射，用于具身导航，包括一个新的数据集和基准。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection",
        "summary": "Adapting CLIP for anomaly detection on unseen objects has shown strong\npotential in a zero-shot manner. However, existing methods typically rely on a\nsingle textual space to align with visual semantics across diverse objects and\ndomains. The indiscriminate alignment hinders the model from accurately\ncapturing varied anomaly semantics. We propose TokenCLIP, a token-wise\nadaptation framework that enables dynamic alignment between visual and\nlearnable textual spaces for fine-grained anomaly learning. Rather than mapping\nall visual tokens to a single, token-agnostic textual space, TokenCLIP aligns\neach token with a customized textual subspace that represents its visual\ncharacteristics. Explicitly assigning a unique learnable textual space to each\ntoken is computationally intractable and prone to insufficient optimization. We\ninstead expand the token-agnostic textual space into a set of orthogonal\nsubspaces, and then dynamically assign each token to a subspace combination\nguided by semantic affinity, which jointly supports customized and efficient\ntoken-wise adaptation. To this end, we formulate dynamic alignment as an\noptimal transport problem, where all visual tokens in an image are transported\nto textual subspaces based on semantic similarity. The transport constraints of\nOT ensure sufficient optimization across subspaces and encourage them to focus\non different semantics. Solving the problem yields a transport plan that\nadaptively assigns each token to semantically relevant subspaces. A top-k\nmasking is then applied to sparsify the plan and specialize subspaces for\ndistinct visual regions. Extensive experiments demonstrate the superiority of\nTokenCLIP.",
        "url": "http://arxiv.org/abs/2510.21171v2",
        "published_date": "2025-10-24T05:51:31+00:00",
        "updated_date": "2025-10-27T02:24:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qihang Zhou",
            "Binbin Gao",
            "Guansong Pang",
            "Xin Wang",
            "Jiming Chen",
            "Shibo He"
        ],
        "tldr": "TokenCLIP adapts CLIP for zero-shot anomaly detection by using a token-wise approach, dynamically aligning visual tokens with learnable textual subspaces through optimal transport for fine-grained anomaly learning, showing superior performance.",
        "tldr_zh": "TokenCLIP通过令牌级方法改进CLIP用于零样本异常检测，通过最优传输动态地将视觉令牌与可学习的文本子空间对齐，以进行细粒度的异常学习，并表现出卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation",
        "summary": "Reinforcement learning (RL) has shown promise in enhancing the general\nChain-of-Thought (CoT) reasoning capabilities of multimodal large language\nmodels (MLLMs). However, when applied to improve general CoT reasoning,\nexisting RL frameworks often struggle to generalize beyond the training\ndistribution. To address this, we propose NoisyGRPO, a systematic multimodal RL\nframework that introduces controllable noise into visual inputs for enhanced\nexploration and explicitly models the advantage estimation process via a\nBayesian framework. Specifically, NoisyGRPO improves RL training by: (1)\n\\textbf{Noise-Injected Exploration Policy}: Perturbing visual inputs with\nGaussian noise to encourage exploration across a wider range of visual\nscenarios; and (2) \\textbf{Bayesian Advantage Estimation}: Formulating\nadvantage estimation as a principled Bayesian inference problem, where the\ninjected noise level serves as a prior and the observed trajectory reward as\nthe likelihood. This Bayesian modeling fuses both sources of information to\ncompute a robust posterior estimate of trajectory advantage, effectively\nguiding MLLMs to prefer visually grounded trajectories over noisy ones.\nExperiments on standard CoT quality, general capability, and hallucination\nbenchmarks demonstrate that NoisyGRPO substantially improves generalization and\nrobustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL\n3B. The project page is available at\n\\href{https://artanic30.github.io/project_pages/NoisyGRPO/}{\\texttt{https://artanic30.github.io/project\\_pages/NoisyGRPO}}.",
        "url": "http://arxiv.org/abs/2510.21122v1",
        "published_date": "2025-10-24T03:23:34+00:00",
        "updated_date": "2025-10-24T03:23:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Longtian Qiu",
            "Shan Ning",
            "Jiaxuan Sun",
            "Xuming He"
        ],
        "tldr": "The paper introduces NoisyGRPO, a multimodal RL framework that uses noise injection and Bayesian estimation to improve the generalization and robustness of MLLMs in Chain-of-Thought reasoning, particularly for smaller models.",
        "tldr_zh": "该论文介绍了 NoisyGRPO，一个多模态强化学习框架，它使用噪声注入和贝叶斯估计来提高 MLLM 在思维链推理中的泛化能力和鲁棒性，尤其适用于较小的模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Knowledge-Driven Vision-Language Model for Plexus Detection in Hirschsprung's Disease",
        "summary": "Hirschsprung's disease is defined as the congenital absence of ganglion cells\nin some segment(s) of the colon. The muscle cannot make coordinated movements\nto propel stool in that section, most commonly leading to obstruction. The\ndiagnosis and treatment for this disease require a clear identification of\ndifferent region(s) of the myenteric plexus, where ganglion cells should be\npresent, on the microscopic view of the tissue slide. While deep learning\napproaches, such as Convolutional Neural Networks, have performed very well in\nthis task, they are often treated as black boxes, with minimal understanding\ngained from them, and may not conform to how a physician makes decisions. In\nthis study, we propose a novel framework that integrates expert-derived textual\nconcepts into a Contrastive Language-Image Pre-training-based vision-language\nmodel to guide plexus classification. Using prompts derived from expert sources\n(e.g., medical textbooks and papers) generated by large language models and\nreviewed by our team before being encoded with QuiltNet, our approach aligns\nclinically relevant semantic cues with visual features. Experimental results\nshow that the proposed model demonstrated superior discriminative capability\nacross different classification metrics as it outperformed CNN-based models,\nincluding VGG-19, ResNet-18, and ResNet-50; achieving an accuracy of 83.9%, a\nprecision of 86.6%, and a specificity of 87.6%. These findings highlight the\npotential of multi-modal learning in histopathology and underscore the value of\nincorporating expert knowledge for more clinically relevant model outputs.",
        "url": "http://arxiv.org/abs/2510.21083v1",
        "published_date": "2025-10-24T01:42:57+00:00",
        "updated_date": "2025-10-24T01:42:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youssef Megahed",
            "Atallah Madi",
            "Dina El Demellawy",
            "Adrian D. C. Chan"
        ],
        "tldr": "This paper introduces a knowledge-driven vision-language model (VLM) for improved plexus detection in Hirschsprung's disease, integrating expert-derived textual concepts to enhance classification performance beyond traditional CNNs.",
        "tldr_zh": "本文介绍了一种知识驱动的视觉语言模型（VLM），用于改善 Hirschsprung 病中的丛检测，整合了专家衍生的文本概念，从而提高了超越传统 CNN 的分类性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models",
        "summary": "Foundation models have advanced computer vision by enabling strong\nperformance across diverse tasks through large-scale pretraining and supervised\nfine-tuning. However, they may underperform in domains with distribution shifts\nand scarce labels, where supervised fine-tuning may be infeasible. While\ncontinued self-supervised learning for model adaptation is common for\ngenerative language models, this strategy has not proven effective for\nvision-centric encoder models. To address this challenge, we introduce a novel\nformulation of self-supervised fine-tuning for vision foundation models, where\nthe model is adapted to a new domain without requiring annotations, leveraging\nonly short multi-view object-centric videos. Our method is referred to as\nVESSA: Video-based objEct-centric Self-Supervised Adaptation for visual\nfoundation models. VESSA's training technique is based on a self-distillation\nparadigm, where it is critical to carefully tune prediction heads and deploy\nparameter-efficient adaptation techniques - otherwise, the model may quickly\nforget its pretrained knowledge and reach a degraded state. VESSA benefits\nsignificantly from multi-view object observations sourced from different frames\nin an object-centric video, efficiently learning robustness to varied capture\nconditions, without the need of annotations. Through comprehensive experiments\nwith 3 vision foundation models on 2 datasets, VESSA demonstrates consistent\nimprovements in downstream classification tasks, compared to the base models\nand previous adaptation methods. Code is publicly available at\nhttps://github.com/jesimonbarreto/VESSA.",
        "url": "http://arxiv.org/abs/2510.20994v1",
        "published_date": "2025-10-23T20:44:28+00:00",
        "updated_date": "2025-10-23T20:44:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jesimon Barreto",
            "Carlos Caetano",
            "André Araujo",
            "William Robson Schwartz"
        ],
        "tldr": "The paper introduces VESSA, a self-supervised fine-tuning method that adapts vision foundation models to new domains using multi-view object-centric videos, demonstrating improvements in downstream classification tasks without annotations.",
        "tldr_zh": "该论文介绍了VESSA，一种自监督微调方法，使用多视角以对象为中心的视频将视觉基础模型调整到新的领域，无需标注即可在下游分类任务中展示改进。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease Recognition in Complex Environments",
        "summary": "In complex orchard environments, the phenotypic heterogeneity of different\napple leaf diseases, characterized by significant variation among lesions,\nposes a challenge to traditional multi-scale feature fusion methods. These\nmethods only integrate multi-layer features extracted by convolutional neural\nnetworks (CNNs) and fail to adequately account for the relationships between\nlocal and global features. Therefore, this study proposes a multi-branch\nrecognition framework named CNN-Transformer-CLIP (CT-CLIP). The framework\nsynergistically employs a CNN to extract local lesion detail features and a\nVision Transformer to capture global structural relationships. An Adaptive\nFeature Fusion Module (AFFM) then dynamically fuses these features, achieving\noptimal coupling of local and global information and effectively addressing the\ndiversity in lesion morphology and distribution. Additionally, to mitigate\ninterference from complex backgrounds and significantly enhance recognition\naccuracy under few-shot conditions, this study proposes a multimodal image-text\nlearning approach. By leveraging pre-trained CLIP weights, it achieves deep\nalignment between visual features and disease semantic descriptions.\nExperimental results show that CT-CLIP achieves accuracies of 97.38% and 96.12%\non a publicly available apple disease and a self-built dataset, outperforming\nseveral baseline methods. The proposed CT-CLIP demonstrates strong capabilities\nin recognizing agricultural diseases, significantly enhances identification\naccuracy under complex environmental conditions, provides an innovative and\npractical solution for automated disease recognition in agricultural\napplications.",
        "url": "http://arxiv.org/abs/2510.21346v1",
        "published_date": "2025-10-24T11:23:47+00:00",
        "updated_date": "2025-10-24T11:23:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lemin Liu",
            "Fangchao Hu",
            "Honghua Jiang",
            "Yaru Chen",
            "Limin Liu",
            "Yongliang Qiao"
        ],
        "tldr": "The paper proposes CT-CLIP, a multi-modal (image and text) framework using CNN, Transformer, and CLIP for robust apple leaf disease recognition in complex environments, achieving high accuracy on public and self-built datasets.",
        "tldr_zh": "该论文提出了一种名为CT-CLIP的多模态（图像和文本）框架，该框架利用CNN、Transformer和CLIP，用于复杂环境下稳健的苹果叶片病害识别，并在公共和自建数据集上实现了高精度。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]