[
    {
        "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
        "summary": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
        "url": "http://arxiv.org/abs/2511.20648v1",
        "published_date": "2025-11-25T18:59:45+00:00",
        "updated_date": "2025-11-25T18:59:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunze Man",
            "Shihao Wang",
            "Guowen Zhang",
            "Johan Bjorck",
            "Zhiqi Li",
            "Liang-Yan Gui",
            "Jim Fan",
            "Jan Kautz",
            "Yu-Xiong Wang",
            "Zhiding Yu"
        ],
        "tldr": "The paper introduces LocateAnything3D, a VLM-native approach for 3D object detection that casts the problem as a next-token prediction task using a Chain-of-Sight method, achieving state-of-the-art results on the Omni3D benchmark.",
        "tldr_zh": "该论文介绍了 LocateAnything3D，一种 VLM 原生的 3D 对象检测方法，它将问题转化为使用 Chain-of-Sight 方法的下一个 token 预测任务，并在 Omni3D 基准测试中实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis",
        "summary": "How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.",
        "url": "http://arxiv.org/abs/2511.21397v1",
        "published_date": "2025-11-26T13:49:08+00:00",
        "updated_date": "2025-11-26T13:49:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Jiyun Bae",
            "Hyunjong Ok",
            "Sangwoo Mo",
            "Jaeho Lee"
        ],
        "tldr": "This paper investigates the effect of visual distractors on the performance of VLMs, finding that they decrease accuracy without increasing reasoning length, unlike textual distractors. They also propose a prompting strategy to mitigate bias.",
        "tldr_zh": "该论文研究了视觉干扰因素对视觉语言模型性能的影响，发现它们会降低准确率，而不会增加推理长度，这与文本干扰因素不同。他们还提出了一种提示策略来减轻偏差。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
        "summary": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.",
        "url": "http://arxiv.org/abs/2511.21395v1",
        "published_date": "2025-11-26T13:46:39+00:00",
        "updated_date": "2025-11-26T13:46:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qixun Wang",
            "Yang Shi",
            "Yifei Wang",
            "Yuanxing Zhang",
            "Pengfei Wan",
            "Kun Gai",
            "Xianghua Ying",
            "Yisen Wang"
        ],
        "tldr": "The paper introduces Monet, a framework for training MLLMs to reason directly within the latent visual space, addressing limitations of existing methods through a novel training pipeline and reinforcement learning approach. The model demonstrates improved performance on visual reasoning benchmarks, including out-of-distribution generalization.",
        "tldr_zh": "该论文介绍了Monet，一个训练MLLM在潜在视觉空间中直接推理的框架。该框架通过新颖的训练流程和强化学习方法解决了现有方法的局限性。该模型在视觉推理基准测试中表现出更高的性能，包括分布外泛化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning",
        "summary": "Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\\% m\\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.",
        "url": "http://arxiv.org/abs/2511.21375v1",
        "published_date": "2025-11-26T13:21:15+00:00",
        "updated_date": "2025-11-26T13:21:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Gu",
            "Haoji Zhang",
            "Qihang Fan",
            "Jingxuan Niu",
            "Zhipeng Zhang",
            "Libo Zhang",
            "Guang Chen",
            "Fan Chen",
            "Longyin Wen",
            "Sijie Zhu"
        ],
        "tldr": "The paper introduces STVG-o1, a framework that improves spatio-temporal video grounding performance of off-the-shelf MLLMs by using a bounding-box chain-of-thought mechanism and a reinforcement fine-tuning strategy with geometry-aware rewards, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了STVG-o1，一个通过使用边界框的链式思考机制和强化微调策略来提高现成的多模态大型语言模型在时空视频定位中的表现的框架，并达到了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding",
        "summary": "Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.",
        "url": "http://arxiv.org/abs/2511.21339v1",
        "published_date": "2025-11-26T12:44:51+00:00",
        "updated_date": "2025-11-26T12:44:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tae-Min Choi",
            "Tae Kyeong Jeong",
            "Garam Kim",
            "Jaemin Lee",
            "Yeongyoon Koh",
            "In Cheul Choi",
            "Jae-Ho Chung",
            "Jong Woong Park",
            "Juyoun Park"
        ],
        "tldr": "The paper introduces SurgMLLMBench, a unified multimodal benchmark dataset for surgical scene understanding, designed to evaluate interactive multimodal LLMs across various surgical domains with pixel-level segmentation and structured VQA annotations.",
        "tldr_zh": "该论文介绍了SurgMLLMBench，一个用于手术场景理解的统一多模态基准数据集，旨在评估交互式多模态LLM在各种手术领域中的表现，包含像素级分割和结构化VQA标注。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Co-Training Vision Language Models for Remote Sensing Multi-task Learning",
        "summary": "With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.",
        "url": "http://arxiv.org/abs/2511.21272v1",
        "published_date": "2025-11-26T10:55:07+00:00",
        "updated_date": "2025-11-26T10:55:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingyun Li",
            "Shuran Ma",
            "Junwei Luo",
            "Yi Yu",
            "Yue Zhou",
            "Fengxiang Wang",
            "Xudong Lu",
            "Xiaoxing Wang",
            "Xin He",
            "Yushi Chen",
            "Xue Yang",
            "Junchi Yan"
        ],
        "tldr": "The paper introduces RSCoVLM, a Vision Language Model baseline for Remote Sensing Multi-Task Learning, featuring a data curation engine, a dynamic-resolution strategy, and a zoom-in chain mechanism for UHR images, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了RSCoVLM，一个用于遥感多任务学习的视觉语言模型基线，它具有数据整理引擎、动态分辨率策略和用于超高分辨率图像的放大链机制，并取得了最先进的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scenes as Tokens: Multi-Scale Normal Distributions Transform Tokenizer for General 3D Vision-Language Understanding",
        "summary": "Recent advances in 3D vision-language models (VLMs) highlight a strong potential for 3D scene understanding and reasoning. However, effectively tokenizing 3D scenes into holistic scene tokens, and leveraging these tokens across diverse 3D understanding tasks, remain highly challenging. We present NDTokenizer3D, a generalist 3D VLM that performs a wide range of 3D scene understanding tasks while naturally supporting human interactions, thereby bridging language-level reasoning with 3D spatial understanding. The core of our approach is a novel three-stage scene tokenization pipeline built upon a Multi-Scale Normal Distributions Transform (NDT) representation, paired with a Multi-Scale NDT Decoder (MSDec). Specifically, NDTokenizer3D first constructs a multi-scale NDT representation from raw high-resolution point clouds, preserving both global context and fine-grained geometric details. Next, the MSDec progressively fuses cross-scale NDT features, producing holistic scene tokens consumable by LLM endpoints. Beyond tokenization, MSDec is repurposed as a general interface for human-interactive prompting (points, boxes, masks) and segmentation-mask decoding, unifying diverse 3D scene understanding tasks within a single architecture. With this compact and unified design, NDTokenizer3D offers a fine-grained, general-purpose 3D VLM, achieving remarkable improvements in 3D Referring Segmentation, 3D Visual Question Answering, and 3D Dense Captioning.",
        "url": "http://arxiv.org/abs/2511.21191v1",
        "published_date": "2025-11-26T09:12:17+00:00",
        "updated_date": "2025-11-26T09:12:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutao Tang",
            "Cheng Zhao",
            "Gaurav Mittal",
            "Rohith Kukkala",
            "Rama Chellappa",
            "Cheng Peng",
            "Mei Chen"
        ],
        "tldr": "The paper introduces NDTokenizer3D, a novel 3D Vision-Language Model (VLM) that tokenizes 3D scenes using a Multi-Scale Normal Distributions Transform (NDT) representation for enhanced 3D understanding and reasoning across diverse tasks, supporting human interaction.",
        "tldr_zh": "该论文介绍了NDTokenizer3D，一种新型的3D视觉语言模型（VLM），它使用多尺度正态分布变换（NDT）表示来标记3D场景，以增强3D理解和跨多种任务的推理，并支持人机交互。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs",
        "summary": "Visual encoding followed by token condensing has become the standard architectural paradigm in multi-modal large language models (MLLMs). Many recent MLLMs increasingly favor global native- resolution visual encoding over slice-based methods. To investigate this trend, we systematically compare their behavior on vision-language understanding and attention patterns, revealing that global encoding enhances overall capability but at the expense of greater computational overhead. To address this issue, we present LLaVA-UHD v3, an MLLM centered upon our proposed Progressive Visual Compression (PVC) method, which can be seamlessly integrated into standard Vision Transformer (ViT) to enable efficient native-resolution encoding. The PVC approach consists of two key modules: (i) refined patch embedding, which supports flexible patch-size scaling for fine-grained visual model- ing, (ii) windowed token compression, hierarchically deployed across ViT layers to progressively aggregate local token representations. Jointly modulated by these two modules, a widely pretrained ViT can be reconfigured into an efficient architecture while largely preserving generality. Evaluated across extensive benchmarks, the transformed ViT, termed ViT-UHD, demonstrates competitive performance with MoonViT while reducing TTFT (time-to-first-token) by 2.4x, when developed within an identical MLLM architecture. Building upon ViT-UHD, LLaVA-UHD v3 also achieves competitive performance to Qwen2-VL, while further reducing TTFT by 1.9x. We will release all code and checkpoints to support future research on efficient MLLMs.",
        "url": "http://arxiv.org/abs/2511.21150v1",
        "published_date": "2025-11-26T08:11:10+00:00",
        "updated_date": "2025-11-26T08:11:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shichu Sun",
            "Yichen Zhang",
            "Haolin Song",
            "Zonghao Guo",
            "Chi Chen",
            "Yidan Zhang",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "tldr": "The paper introduces LLaVA-UHD v3, an MLLM that uses Progressive Visual Compression (PVC) for efficient native-resolution visual encoding, achieving competitive performance with reduced time-to-first-token (TTFT).",
        "tldr_zh": "该论文介绍了LLaVA-UHD v3，一种使用渐进式视觉压缩（PVC）的多模态大型语言模型，用于高效的原生分辨率视觉编码，在实现具有竞争力的性能的同时，降低了首个令牌的时间（TTFT）。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens",
        "summary": "Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.",
        "url": "http://arxiv.org/abs/2511.21106v1",
        "published_date": "2025-11-26T06:45:59+00:00",
        "updated_date": "2025-11-26T06:45:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ze Feng",
            "Sen Yang",
            "Boqiang Duan",
            "Wankou Yang",
            "Jingdong Wang"
        ],
        "tldr": "This paper introduces EM-KD, a knowledge distillation approach for efficient MLLMs that aligns and distills knowledge from vision tokens, enhancing performance and efficiency. It uses Manhattan distance, Hungarian matching, Vision-Language Affinity Distillation (VLAD), and Vision Semantic Distillation (VSD) to achieve state-of-the-art results.",
        "tldr_zh": "本文介绍了一种名为EM-KD的知识蒸馏方法，用于高效的多模态大语言模型。该方法通过对齐和提炼视觉tokens的知识，从而提高模型的性能和效率。它使用曼哈顿距离、匈牙利匹配算法、视觉-语言亲和力蒸馏（VLAD）和视觉语义蒸馏（VSD）来实现最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing",
        "summary": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.",
        "url": "http://arxiv.org/abs/2511.21087v1",
        "published_date": "2025-11-26T06:13:32+00:00",
        "updated_date": "2025-11-26T06:13:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyun Zeng",
            "Hang Hua",
            "Jiebo Luo"
        ],
        "tldr": "The paper introduces MIRA, a multimodal iterative reasoning agent for instruction-guided image editing that addresses the difficulty diffusion models face with complex instructions, using a perception-reasoning-action loop and a novel dataset.",
        "tldr_zh": "该论文介绍了一种名为MIRA的多模态迭代推理代理，用于指令引导的图像编辑。它通过感知-推理-行动循环和新的数据集，解决了扩散模型在处理复杂指令时遇到的困难。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs",
        "summary": "Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\\times$ while maintaining information accuracy.",
        "url": "http://arxiv.org/abs/2511.20965v1",
        "published_date": "2025-11-26T01:34:08+00:00",
        "updated_date": "2025-11-26T01:34:08+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Md Adnan Arefeen",
            "Biplob Debnath",
            "Srimat Chakradhar"
        ],
        "tldr": "TrafficLens is introduced as a novel algorithm that improves the efficiency of multi-camera traffic video analysis using VLMs by employing a sequential processing approach and bypassing redundant computations, achieving a 4x speedup in video-to-text conversion.",
        "tldr_zh": "TrafficLens是一种新的算法，通过采用顺序处理方法和绕过冗余计算，提高了使用VLM进行多摄像头交通视频分析的效率，实现了视频到文本转换的4倍加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model",
        "summary": "Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR",
        "url": "http://arxiv.org/abs/2511.20956v1",
        "published_date": "2025-11-26T01:22:29+00:00",
        "updated_date": "2025-11-26T01:22:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Rawa Mohammed",
            "Mina Attin",
            "Bryar Shareef"
        ],
        "tldr": "BUSTR is a novel vision-language framework for generating breast ultrasound reports without paired image-report data, using descriptor-aware visual representations and a dual-level alignment objective, achieving improved performance on standard and clinical metrics.",
        "tldr_zh": "BUSTR 是一种新颖的视觉-语言框架，用于在没有配对图像-报告数据的情况下生成乳腺超声报告，它使用描述符感知视觉表示和双层对齐目标，在标准和临床指标上实现了更高的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction",
        "summary": "Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.",
        "url": "http://arxiv.org/abs/2511.20937v1",
        "published_date": "2025-11-26T00:06:02+00:00",
        "updated_date": "2025-11-26T00:06:02+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Qineng Wang",
            "Wenlong Huang",
            "Yu Zhou",
            "Hang Yin",
            "Tianwei Bao",
            "Jianwen Lyu",
            "Weiyu Liu",
            "Ruohan Zhang",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Manling Li"
        ],
        "tldr": "The paper introduces ENACT, a benchmark for evaluating embodied cognition in VLMs using egocentric interaction and world modeling via visual question answering, revealing a performance gap between current VLMs and humans, highlighting biases.",
        "tldr_zh": "该论文介绍了ENACT，一个通过视觉问答评估VLMs中具身认知能力的基准，利用以自我为中心的交互和世界建模，揭示了当前VLMs与人类之间的性能差距，并突出了其中的偏差。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries",
        "summary": "Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.",
        "url": "http://arxiv.org/abs/2511.20854v1",
        "published_date": "2025-11-25T21:02:26+00:00",
        "updated_date": "2025-11-25T21:02:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Sree Bhattacharyya",
            "Yaman Kumar Singla",
            "Sudhir Yarram",
            "Somesh Kumar Singh",
            "Harini S",
            "James Z. Wang"
        ],
        "tldr": "The paper introduces a large-scale unsupervised dataset for modeling visual memorability using tip-of-the-tongue retrieval queries from online platforms and demonstrates its effectiveness in recall generation and ToT retrieval tasks using fine-tuned vision-language models.",
        "tldr_zh": "该论文介绍了一个大型无监督数据集，用于利用在线平台的舌尖现象检索查询来建模视觉记忆性，并展示了其在使用微调的视觉语言模型进行回忆生成和舌尖现象检索任务中的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning",
        "summary": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.",
        "url": "http://arxiv.org/abs/2511.20814v1",
        "published_date": "2025-11-25T20:00:47+00:00",
        "updated_date": "2025-11-25T20:00:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Md Tanvirul Alam",
            "Saksham Aggarwal",
            "Justin Yang Chae",
            "Nidhi Rastogi"
        ],
        "tldr": "The paper introduces Sphinx, a synthetic environment for visual perception and reasoning tasks, and demonstrates its use in evaluating and improving large vision-language models through reinforcement learning with verifiable rewards.",
        "tldr_zh": "该论文介绍了一个名为Sphinx的合成环境，用于视觉感知和推理任务，并展示了其在通过可验证奖励的强化学习来评估和改进大型视觉语言模型方面的应用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
        "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .",
        "url": "http://arxiv.org/abs/2511.20785v1",
        "published_date": "2025-11-25T19:22:48+00:00",
        "updated_date": "2025-11-25T19:22:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zuhao Yang",
            "Sudong Wang",
            "Kaichen Zhang",
            "Keming Wu",
            "Sicong Leng",
            "Yifan Zhang",
            "Chengwei Qin",
            "Shijian Lu",
            "Xingxuan Li",
            "Lidong Bing"
        ],
        "tldr": "The paper introduces LongVT, an agentic framework for long video reasoning that leverages LMMs to zoom in on relevant clips, resample frames, and answer questions, along with a new dataset VideoSIAH for training and evaluation.",
        "tldr_zh": "该论文介绍了LongVT，一个用于长视频推理的agentic框架，它利用LMM放大相关片段、重新采样帧并回答问题，同时还提出了一个新的数据集VideoSIAH用于训练和评估。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text-Guided Semantic Image Encoder",
        "summary": "Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.",
        "url": "http://arxiv.org/abs/2511.20770v1",
        "published_date": "2025-11-25T19:04:04+00:00",
        "updated_date": "2025-11-25T19:04:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Raghuveer Thirukovalluru",
            "Xiaochuang Han",
            "Bhuwan Dhingra",
            "Emily Dinan",
            "Maha Elbayad"
        ],
        "tldr": "The paper introduces a Text-Guided Semantic Image Encoder (TIE) that conditions image representations on input text queries, leading to improved performance and efficiency in vision-language models.",
        "tldr_zh": "本文提出了一种文本引导的语义图像编码器 (TIE)，它可以根据输入的文本查询来调节图像表示，从而提高视觉语言模型的性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation",
        "summary": "Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.",
        "url": "http://arxiv.org/abs/2511.20651v1",
        "published_date": "2025-11-25T18:59:55+00:00",
        "updated_date": "2025-11-25T18:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuelu Feng",
            "Yunsheng Li",
            "Ziyu Wan",
            "Zixuan Gao",
            "Junsong Yuan",
            "Dongdong Chen",
            "Chunming Qiao"
        ],
        "tldr": "RubricRL introduces a rubric-based reward system for text-to-image generation using RL, providing interpretable and customizable rewards based on fine-grained visual criteria dynamically constructed for each prompt.",
        "tldr_zh": "RubricRL 引入了一种基于规则的奖励系统，用于文本到图像生成的强化学习，它提供基于细粒度视觉标准的，可解释和可定制的奖励，这些标准是为每个提示动态构建的。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
        "summary": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.",
        "url": "http://arxiv.org/abs/2511.20650v1",
        "published_date": "2025-11-25T18:59:53+00:00",
        "updated_date": "2025-11-25T18:59:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tooba Tehreem Sheikh",
            "Jean Lahoud",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Salman Khan",
            "Hisham Cholakkal"
        ],
        "tldr": "The paper introduces MedROV, a real-time open-vocabulary object detection model for medical imaging, along with a large-scale dataset (Omnis) and a pseudo-labeling strategy, achieving state-of-the-art performance and speed.",
        "tldr_zh": "该论文介绍了 MedROV，一个用于医学影像的实时开放词汇目标检测模型，以及一个大型数据集 (Omnis) 和伪标签策略，实现了最先进的性能和速度。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
        "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
        "url": "http://arxiv.org/abs/2511.20649v1",
        "published_date": "2025-11-25T18:59:46+00:00",
        "updated_date": "2025-11-25T18:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hidir Yesiltepe",
            "Tuna Han Salih Meral",
            "Adil Kaan Akan",
            "Kaan Oktay",
            "Pinar Yanardag"
        ],
        "tldr": "The paper introduces Infinity-RoPE, a training-free framework that enables autoregressive video diffusion models to generate infinite-horizon videos with fine-grained action control and cinematic transitions by addressing limitations in temporal horizon, prompt responsiveness, and scene transitions.",
        "tldr_zh": "该论文介绍了Infinity-RoPE，一个无需训练的框架，通过解决时间范围、提示响应和场景过渡方面的局限性，使自回归视频扩散模型能够生成具有细粒度动作控制和电影过渡的无限范围视频。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Language Memory for Spatial Reasoning",
        "summary": "Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.",
        "url": "http://arxiv.org/abs/2511.20644v1",
        "published_date": "2025-11-25T18:59:02+00:00",
        "updated_date": "2025-11-25T18:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zuntao Liu",
            "Yi Du",
            "Taimeng Fu",
            "Shaoshu Su",
            "Cherie Ho",
            "Chen Wang"
        ],
        "tldr": "The paper introduces VLM$^2$, a vision-language model with persistent memory for spatial reasoning in videos, addressing semantic-geometric misalignment and lack of long-term 3D representation.",
        "tldr_zh": "该论文介绍了VLM$^2$，一种具有持久记忆的视觉语言模型，用于视频中的空间推理，解决了语义-几何错位和缺乏长期3D表示的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining",
        "summary": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.",
        "url": "http://arxiv.org/abs/2511.20643v1",
        "published_date": "2025-11-25T18:58:07+00:00",
        "updated_date": "2025-11-25T18:58:07+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Adhiraj Ghosh",
            "Vishaal Udandarao",
            "Thao Nguyen",
            "Matteo Farina",
            "Mehdi Cherti",
            "Jenia Jitsev",
            "Sewoong Oh",
            "Elisa Ricci",
            "Ludwig Schmidt",
            "Matthias Bethge"
        ],
        "tldr": "The paper introduces DataConcept, a dataset with concept annotations for image-text pairs, and Concept-Aware Batch Sampling (CABS), a method for online, concept-based data curation to improve Vision-Language Model training by controlling concept distribution in batches.",
        "tldr_zh": "该论文介绍了DataConcept，一个带有图像-文本对的概念注释的数据集，以及概念感知批量采样（CABS），一种用于在线的、基于概念的数据管理方法，通过控制批量中的概念分布来改善视觉语言模型训练。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition",
        "summary": "Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.",
        "url": "http://arxiv.org/abs/2511.20641v1",
        "published_date": "2025-11-25T18:57:28+00:00",
        "updated_date": "2025-11-25T18:57:28+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Wei Tang",
            "Zuo-Zheng Wang",
            "Kun Zhang",
            "Tong Wei",
            "Min-Ling Zhang"
        ],
        "tldr": "This paper introduces CAPNET, a novel framework leveraging CLIP to address long-tailed multi-label visual recognition by explicitly modeling label correlations and using distribution-balanced loss, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "本文介绍了CAPNET，一种利用CLIP的新框架，通过显式建模标签相关性和使用分布平衡损失来解决长尾多标签视觉识别问题，并在标准基准测试上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
        "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
        "url": "http://arxiv.org/abs/2511.20573v1",
        "published_date": "2025-11-25T18:06:22+00:00",
        "updated_date": "2025-11-25T18:06:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenhui Gou",
            "Zilong Chen",
            "Zeyu Wang",
            "Feng Li",
            "Deyao Zhu",
            "Zicheng Duan",
            "Kunchang Li",
            "Chaorui Deng",
            "Hongyi Yuan",
            "Haoqi Fan",
            "Cihang Xie",
            "Jianfei Cai",
            "Hamid Rezatofighi"
        ],
        "tldr": "The paper introduces VQ-VA World, a framework and large-scale dataset for training open-source models to perform Visual Question-Visual Answering (VQ-VA), and demonstrates significant performance gains compared to existing open-source baselines.",
        "tldr_zh": "该论文介绍了VQ-VA World，一个用于训练开源模型以执行视觉问题-视觉回答(VQ-VA)的框架和大规模数据集，并展示了相比于现有开源基线模型的显著性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video Generation Models Are Good Latent Reward Models",
        "summary": "Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.",
        "url": "http://arxiv.org/abs/2511.21541v1",
        "published_date": "2025-11-26T16:14:18+00:00",
        "updated_date": "2025-11-26T16:14:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyue Mi",
            "Wenqing Yu",
            "Jiesong Lian",
            "Shibo Jie",
            "Ruizhe Zhong",
            "Zijun Liu",
            "Guozhen Zhang",
            "Zixiang Zhou",
            "Zhiyong Xu",
            "Yuan Zhou",
            "Qinglin Lu",
            "Fan Tang"
        ],
        "tldr": "The paper introduces Process Reward Feedback Learning (PRFL), a method that optimizes video generation models based on human preferences directly in the latent space, leading to improved alignment and efficiency compared to pixel-space methods.",
        "tldr_zh": "本文介绍了一种过程奖励反馈学习 (PRFL) 方法，该方法直接在潜在空间中基于人类偏好优化视频生成模型，与像素空间方法相比，提高了对齐度和效率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings",
        "summary": "We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel \"Latent Action Energy\" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.",
        "url": "http://arxiv.org/abs/2511.21428v1",
        "published_date": "2025-11-26T14:19:44+00:00",
        "updated_date": "2025-11-26T14:19:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiajie Zhang",
            "Sören Schwertfeger",
            "Alexander Kleiner"
        ],
        "tldr": "The paper presents an unsupervised framework for segmenting industrial videos into action primitives, creating structured Vision-Language-Action (VLA) pre-training data, and demonstrating its effectiveness on electric motor assembly tasks.",
        "tldr_zh": "该论文提出了一个无监督框架，用于将工业视频分割成动作原语，创建结构化的视觉-语言-动作(VLA)预训练数据，并在电机组装任务中验证了其有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DiverseVAR: Balancing Diversity and Quality of Next-Scale Visual Autoregressive Models",
        "summary": "We introduce DiverseVAR, a framework that enhances the diversity of text-conditioned visual autoregressive models (VAR) at test time without requiring retraining, fine-tuning, or substantial computational overhead. While VAR models have recently emerged as strong competitors to diffusion and flow models for image generation, they suffer from a critical limitation in diversity, often producing nearly identical images even for simple prompts. This issue has largely gone unnoticed amid the predominant focus on image quality. We address this limitation at test time in two stages. First, inspired by diversity enhancement techniques in diffusion models, we propose injecting noise into the text embedding. This introduces a trade-off between diversity and image quality: as diversity increases, the image quality sharply declines. To preserve quality, we propose scale-travel: a novel latent refinement technique inspired by time-travel strategies in diffusion models. Specifically, we use a multi-scale autoencoder to extract coarse-scale tokens that enable us to resume generation at intermediate stages. Extensive experiments show that combining text-embedding noise injection with our scale-travel refinement significantly enhances diversity while minimizing image-quality degradation, achieving a new Pareto frontier in the diversity-quality trade-off.",
        "url": "http://arxiv.org/abs/2511.21415v1",
        "published_date": "2025-11-26T14:06:52+00:00",
        "updated_date": "2025-11-26T14:06:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingue Park",
            "Prin Phunyaphibarn",
            "Phillip Y. Lee",
            "Minhyuk Sung"
        ],
        "tldr": "The paper introduces DiverseVAR, a method to improve the diversity of text-conditioned visual autoregressive models (VAR) at test time by injecting noise and using a scale-travel refinement technique to preserve image quality, achieving a better diversity-quality trade-off.",
        "tldr_zh": "该论文介绍了DiverseVAR，一种通过注入噪声和使用尺度旅行细化技术来提高文本条件视觉自回归模型（VAR）多样性的方法，旨在提高图像质量，从而在多样性和质量之间取得更好的平衡。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs",
        "summary": "The threat of Audio-Video (AV) forgery is rapidly evolving beyond human-centric deepfakes to include more diverse manipulations across complex natural scenes. However, existing benchmarks are still confined to DeepFake-based forgeries and single-granularity annotations, thus failing to capture the diversity and complexity of real-world forgery scenarios. To address this, we introduce AVFakeBench, the first comprehensive audio-video forgery detection benchmark that spans rich forgery semantics across both human subject and general subject. AVFakeBench comprises 12K carefully curated audio-video questions, covering seven forgery types and four levels of annotations. To ensure high-quality and diverse forgeries, we propose a multi-stage hybrid forgery framework that integrates proprietary models for task planning with expert generative models for precise manipulation. The benchmark establishes a multi-task evaluation framework covering binary judgment, forgery types classification, forgery detail selection, and explanatory reasoning. We evaluate 11 Audio-Video Large Language Models (AV-LMMs) and 2 prevalent detection methods on AVFakeBench, demonstrating the potential of AV-LMMs as emerging forgery detectors while revealing their notable weaknesses in fine-grained perception and reasoning.",
        "url": "http://arxiv.org/abs/2511.21251v1",
        "published_date": "2025-11-26T10:33:12+00:00",
        "updated_date": "2025-11-26T10:33:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuhan Xia",
            "Peipei Li",
            "Xuannan Liu",
            "Dongsen Zhang",
            "Xinyu Guo",
            "Zekun Li"
        ],
        "tldr": "The paper introduces AVFakeBench, a new audio-video forgery detection benchmark designed for AV-LMMs, covering diverse forgery types and multi-granularity annotations to address the limitations of existing benchmarks.",
        "tldr_zh": "该论文介绍了AVFakeBench，这是一个新的音频视频伪造检测基准，专为AV-LMM设计，涵盖了各种伪造类型和多粒度注释，旨在解决现有基准的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition",
        "summary": "Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories. However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time. In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions. Specifically, we propose a region-specific semantic activation module that employs discriminative and text-constrained semantics as queries to capture the most action-related region responses in each video frame, facilitating interaction among spatial and temporal dimensions with corresponding video features. The captured region responses are organized into action tracklets, which characterize region-based action dynamics by linking related responses across video frames in a coherent sequence. The text-constrained queries encode nuanced semantic representations derived from textual descriptions of action labels extracted by language branches within Visual Language Models (VLMs). To optimize the action tracklets, we design a multi-level tracklet contrastive constraint among region responses at spatial and temporal levels, enabling effective discrimination within each frame and correlation between adjacent frames. Additionally, a task-specific fine-tuning mechanism refines textual semantics such that semantic representations encoded by VLMs are preserved while optimized for task preferences. Comprehensive experiments on widely used action recognition benchmarks demonstrate the superiority to previous state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2511.21202v1",
        "published_date": "2025-11-26T09:32:06+00:00",
        "updated_date": "2025-11-26T09:32:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Baoli Sun",
            "Yihan Wang",
            "Xinzhu Ma",
            "Zhihui Wang",
            "Kun Lu",
            "Zhiyong Wang"
        ],
        "tldr": "This paper introduces the Action-Region Tracking (ART) framework for fine-grained video action recognition, using a query-response mechanism with text-constrained semantics derived from VLMs to track action dynamics and improve discrimination between similar actions. The framework demonstrates state-of-the-art performance on action recognition benchmarks.",
        "tldr_zh": "本文介绍了用于细粒度视频动作识别的动作区域跟踪 (ART) 框架，该框架使用查询-响应机制以及来自 VLM 的文本约束语义来跟踪动作动态，并提高相似动作之间的区分度。 该框架在动作识别基准测试中展示了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning",
        "summary": "Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., \"shape\", \"color\"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.",
        "url": "http://arxiv.org/abs/2511.21188v1",
        "published_date": "2025-11-26T09:11:22+00:00",
        "updated_date": "2025-11-26T09:11:22+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zheng Li",
            "Yibing Song",
            "Xin Zhang",
            "Lei Luo",
            "Xiang Li",
            "Jian Yang"
        ],
        "tldr": "AnchorOPT introduces dynamic, learnable anchors and a position matrix for adaptive prompt learning in CLIP models, achieving competitive performance with a simpler architecture.",
        "tldr_zh": "AnchorOPT 提出了一种基于动态、可学习锚点和位置矩阵的自适应提示学习方法，用于 CLIP 模型，它以更简单的架构实现了具有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Referring Video Object Segmentation with Cross-Modality Proxy Queries",
        "summary": "Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.21139v1",
        "published_date": "2025-11-26T07:45:41+00:00",
        "updated_date": "2025-11-26T07:45:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Baoli Sun",
            "Xinzhu Ma",
            "Ning Wang",
            "Zhihui Wang",
            "Zhiyong Wang"
        ],
        "tldr": "This paper introduces ProxyFormer, a novel architecture for Referring Video Object Segmentation (RVOS) that uses proxy queries to improve cross-modality alignment and inter-frame dependency modeling, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "本文介绍了一种名为ProxyFormer的新型架构，用于指代表视频对象分割（RVOS）。该架构利用代理查询来改进跨模态对齐和帧间依赖性建模，并在标准基准测试中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion",
        "summary": "We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.\n  However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.\n  We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.",
        "url": "http://arxiv.org/abs/2511.21129v1",
        "published_date": "2025-11-26T07:27:11+00:00",
        "updated_date": "2025-11-26T07:27:11+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Dianbing Xi",
            "Jiepeng Wang",
            "Yuanzhi Liang",
            "Xi Qiu",
            "Jialun Liu",
            "Hao Pan",
            "Yuchi Huo",
            "Rui Wang",
            "Haibin Huang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "tldr": "CtrlVDiff introduces a unified diffusion model for controllable video generation using multimodal inputs (depth, normals, segmentation, edges, intrinsics), trained on a new dataset MMVideo, enabling layer-wise video edits with improved controllability and fidelity.",
        "tldr_zh": "CtrlVDiff 提出了一个统一的扩散模型，通过使用多模态输入（深度、法线、分割、边缘、内参）进行可控视频生成，该模型在一个新的数据集 MMVideo 上进行训练，从而能够进行分层视频编辑，并提高了可控性和保真度。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning",
        "summary": "News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.",
        "url": "http://arxiv.org/abs/2511.21002v1",
        "published_date": "2025-11-26T03:03:52+00:00",
        "updated_date": "2025-11-26T03:03:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaoxing You",
            "Qiang Huang",
            "Lingyu Li",
            "Chi Zhang",
            "Xiaopeng Liu",
            "Min Zhang",
            "Jun Yu"
        ],
        "tldr": "The paper introduces MERGE, a multimodal entity-aware retrieval-augmented generation framework for news image captioning, which addresses incomplete information, weak cross-modal alignment, and suboptimal visual-entity grounding using an entity-centric multimodal knowledge base and achieves state-of-the-art results.",
        "tldr_zh": "本文介绍了一种用于新闻图像描述的名为MERGE的多模态实体感知检索增强生成框架。该框架通过构建实体中心的多模态知识库来解决信息不完整、跨模态对齐弱和视觉实体定位不佳等问题，并取得了当前最佳效果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation",
        "summary": "Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.",
        "url": "http://arxiv.org/abs/2511.20889v1",
        "published_date": "2025-11-25T22:11:51+00:00",
        "updated_date": "2025-11-25T22:11:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Taehoon Kim",
            "Henry Gouk",
            "Timothy Hospedales"
        ],
        "tldr": "This paper introduces Null-TTA, a novel test-time alignment method for diffusion models that optimizes the unconditional embedding in classifier-free guidance to align the model with specific rewards while preventing reward hacking and maintaining generalizability.",
        "tldr_zh": "本文介绍了一种新颖的扩散模型测试时对齐方法Null-TTA，该方法通过优化无分类器指导中的无条件嵌入，使模型与特定奖励对齐，同时防止奖励黑客攻击并保持泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence",
        "summary": "Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).",
        "url": "http://arxiv.org/abs/2511.20886v1",
        "published_date": "2025-11-25T22:06:30+00:00",
        "updated_date": "2025-11-25T22:06:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiancheng Pan",
            "Runze Wang",
            "Tianwen Qian",
            "Mohammad Mahdi",
            "Yanwei Fu",
            "Xiangyang Xue",
            "Xiaomeng Huang",
            "Luc Van Gool",
            "Danda Pani Paudel",
            "Yuqian Fu"
        ],
        "tldr": "The paper introduces V$^{2}$-SAM, a framework that adapts SAM2 for cross-view object correspondence using geometry and appearance-based prompts and a multi-expert design with cyclic consistency selection, achieving state-of-the-art results on several datasets.",
        "tldr_zh": "该论文介绍了V$^{2}$-SAM，一个将SAM2适配于跨视角物体对应关系的框架，它使用基于几何和外观的提示以及多专家设计与循环一致性选择，在多个数据集上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models",
        "summary": "Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.",
        "url": "http://arxiv.org/abs/2511.20795v1",
        "published_date": "2025-11-25T19:37:19+00:00",
        "updated_date": "2025-11-25T19:37:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Souradeep Dutta",
            "Keshav Bulia",
            "Neena S Nair"
        ],
        "tldr": "This paper presents a lightweight reproduction of the KRISP knowledge-enhanced VQA model, highlighting design flaws and scalability issues while demonstrating its applicability to resource-constrained environments and mitigating AI hallucinations within a defined knowledge domain.",
        "tldr_zh": "本文提出了一种轻量级的KRISP知识增强VQA模型复现，重点介绍了设计缺陷和可扩展性问题，同时展示了其在资源受限环境中的适用性，并在定义的知识领域内减轻了AI幻觉。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
        "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
        "url": "http://arxiv.org/abs/2511.20614v1",
        "published_date": "2025-11-25T18:40:25+00:00",
        "updated_date": "2025-11-25T18:40:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziheng Ouyang",
            "Yiren Song",
            "Yaoli Liu",
            "Shihao Zhu",
            "Qibin Hou",
            "Ming-Ming Cheng",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces ImageCritic, a reference-guided post-editing approach to correct inconsistencies in generated images, using a VLM-based dataset construction and attention alignment loss.",
        "tldr_zh": "该论文介绍了ImageCritic，一种参考图像引导的后编辑方法，通过基于VLM的数据集构建和注意力对齐损失来纠正生成图像中的不一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BanglaMM-Disaster: A Multimodal Transformer-Based Deep Learning Framework for Multiclass Disaster Classification in Bangla",
        "summary": "Natural disasters remain a major challenge for Bangladesh, so real-time monitoring and quick response systems are essential. In this study, we present BanglaMM-Disaster, an end-to-end deep learning-based multimodal framework for disaster classification in Bangla, using both textual and visual data from social media. We constructed a new dataset of 5,037 Bangla social media posts, each consisting of a caption and a corresponding image, annotated into one of nine disaster-related categories. The proposed model integrates transformer-based text encoders, including BanglaBERT, mBERT, and XLM-RoBERTa, with CNN backbones such as ResNet50, DenseNet169, and MobileNetV2, to process the two modalities. Using early fusion, the best model achieves 83.76% accuracy. This surpasses the best text-only baseline by 3.84% and the image-only baseline by 16.91%. Our analysis also shows reduced misclassification across all classes, with noticeable improvements for ambiguous examples. This work fills a key gap in Bangla multimodal disaster analysis and demonstrates the benefits of combining multiple data types for real-time disaster response in low-resource settings.",
        "url": "http://arxiv.org/abs/2511.21364v1",
        "published_date": "2025-11-26T13:11:46+00:00",
        "updated_date": "2025-11-26T13:11:46+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Ariful Islam",
            "Md Rifat Hossen",
            "Md. Mahmudul Arif",
            "Abdullah Al Noman",
            "Md Arifur Rahman"
        ],
        "tldr": "The paper introduces BanglaMM-Disaster, a multimodal deep learning framework that combines BanglaBERT and CNNs to classify disaster-related social media posts in Bangla, achieving improved accuracy compared to unimodal baselines.",
        "tldr_zh": "该论文介绍了BanglaMM-Disaster，一种多模态深度学习框架，结合了BanglaBERT和CNN，用于对孟加拉语的灾难相关社交媒体帖子进行分类，与单模态基线相比，实现了更高的准确率。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "HTTM: Head-wise Temporal Token Merging for Faster VGGT",
        "summary": "The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.",
        "url": "http://arxiv.org/abs/2511.21317v1",
        "published_date": "2025-11-26T12:04:03+00:00",
        "updated_date": "2025-11-26T12:04:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weitian Wang",
            "Lukas Meiner",
            "Rai Shubham",
            "Cecilia De La Parra",
            "Akash Kumar"
        ],
        "tldr": "The paper introduces Head-wise Temporal Token Merging (HTTM), a training-free method to accelerate VGGT for 3D scene reconstruction by merging tokens at the multi-head granularity, achieving up to 7x speedup with minimal performance drop.",
        "tldr_zh": "该论文介绍了头式时间令牌合并 (HTTM)，一种无需训练的方法，通过在多头粒度上合并令牌来加速用于 3D 场景重建的 VGGT，实现了高达 7 倍的加速，同时性能下降很小。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]