[
    {
        "title": "Multi-Level LVLM Guidance for Untrimmed Video Action Recognition",
        "summary": "Action recognition and localization in complex, untrimmed videos remain a\nformidable challenge in computer vision, largely due to the limitations of\nexisting methods in capturing fine-grained actions, long-term temporal\ndependencies, and high-level semantic information from low-level visual\nfeatures. This paper introduces the Event-Contextualized Video Transformer\n(ECVT), a novel architecture that leverages the advanced semantic understanding\ncapabilities of Large Vision-Language Models (LVLMs) to bridge this gap. ECVT\nemploys a dual-branch design, comprising a Video Encoding Branch for\nspatio-temporal feature extraction and a Cross-Modal Guidance Branch. The\nlatter utilizes an LVLM to generate multi-granularity semantic descriptions,\nincluding Global Event Prompting for macro-level narrative and Temporal\nSub-event Prompting for fine-grained action details. These multi-level textual\ncues are integrated into the video encoder's learning process through\nsophisticated mechanisms such as adaptive gating for high-level semantic\nfusion, cross-modal attention for fine-grained feature refinement, and an event\ngraph module for temporal context calibration. Trained end-to-end with a\ncomprehensive loss function incorporating semantic consistency and temporal\ncalibration terms, ECVT significantly enhances the model's ability to\nunderstand video temporal structures and event logic. Extensive experiments on\nActivityNet v1.3 and THUMOS14 datasets demonstrate that ECVT achieves\nstate-of-the-art performance, with an average mAP of 40.5% on ActivityNet v1.3\nand mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.",
        "url": "http://arxiv.org/abs/2508.17442v1",
        "published_date": "2025-08-24T16:45:21+00:00",
        "updated_date": "2025-08-24T16:45:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liyang Peng",
            "Sihan Zhu",
            "Yunjie Guo"
        ],
        "tldr": "The paper introduces a novel Event-Contextualized Video Transformer (ECVT) architecture that leverages Large Vision-Language Models (LVLMs) for improved untrimmed video action recognition, achieving state-of-the-art performance on ActivityNet v1.3 and THUMOS14 datasets.",
        "tldr_zh": "该论文介绍了一种新颖的事件上下文视频转换器（ECVT）架构，该架构利用大型视觉语言模型（LVLM）来改进未修剪视频中的动作识别，并在ActivityNet v1.3和THUMOS14数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing",
        "summary": "Despite the remarkable capabilities of text-to-image (T2I) generation models,\nreal-world applications often demand fine-grained, iterative image editing that\nexisting methods struggle to provide. Key challenges include granular\ninstruction understanding, robust context preservation during modifications,\nand the lack of intelligent feedback mechanisms for iterative refinement. This\npaper introduces RefineEdit-Agent, a novel, training-free intelligent agent\nframework designed to address these limitations by enabling complex, iterative,\nand context-aware image editing. RefineEdit-Agent leverages the powerful\nplanning capabilities of Large Language Models (LLMs) and the advanced visual\nunderstanding and evaluation prowess of Vision-Language Large Models (LVLMs)\nwithin a closed-loop system. Our framework comprises an LVLM-driven instruction\nparser and scene understanding module, a multi-level LLM-driven editing planner\nfor goal decomposition, tool selection, and sequence generation, an iterative\nimage editing module, and a crucial LVLM-driven feedback and evaluation loop.\nTo rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new\nbenchmark featuring 500 initial images with complex, multi-turn editing\ninstructions across nine visual dimensions. Extensive experiments demonstrate\nthat RefineEdit-Agent significantly outperforms state-of-the-art baselines,\nachieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for\nDirect Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and\n3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of\niterative refinement, backbone choices, tool usage, and robustness to\ninstruction complexity further validate the efficacy of our agentic design in\ndelivering superior edit fidelity and context preservation.",
        "url": "http://arxiv.org/abs/2508.17435v1",
        "published_date": "2025-08-24T16:28:18+00:00",
        "updated_date": "2025-08-24T16:28:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihan Liang",
            "Jiahao Sun",
            "Haoran Ma"
        ],
        "tldr": "The paper introduces RefineEdit-Agent, a novel LLM-LVLM driven framework for fine-grained, iterative image editing, and proposes a new benchmark LongBench-T2I-Edit, outperforming existing state-of-the-art methods.",
        "tldr_zh": "该论文介绍了RefineEdit-Agent，一种新颖的由LLM-LVLM驱动的框架，用于细粒度的迭代图像编辑，并提出了一个新的基准测试LongBench-T2I-Edit，性能优于现有的最先进方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models",
        "summary": "Vision-language models (VLMs) pre-trained on web-scale data exhibit promising\nzero-shot generalization but often suffer from semantic misalignment due to\ndomain gaps between pre-training and downstream tasks. Existing approaches\nprimarily focus on text prompting with class-specific descriptions and\nvisual-text adaptation via aligning cropped image regions with textual\ndescriptions. However, they still face the issues of incomplete textual prompts\nand noisy visual prompts. In this paper, we propose a novel constrained prompt\nenhancement (CPE) method to improve visual-textual alignment by constructing\ncomprehensive textual prompts and compact visual prompts from the semantic\nperspective. Specifically, our approach consists of two key components:\nTopology-Guided Synonymous Semantic Generation (TGSSG) and Category-Agnostic\nDiscriminative Region Selection (CADRS). Textually, to address the issue of\nincomplete semantic expression in textual prompts, our TGSSG first generates\nsynonymous semantic set for each category via large language models, and\nconstructs comprehensive textual prompts based on semantic ambiguity entropy\nand persistent homology analysis. Visually, to mitigate the irrelevant visual\nnoise introduced by random cropping, our CADRS identifies discriminative\nregions with activation maps outputted by a pre-trained vision model,\neffectively filtering out noisy regions and generating compact visual prompts.\nGiven the comprehensive set of textual prompts and compact set of visual\nprompts, we introduce two set-to-set matching strategies based on test-time\nadaptation (TTA) and optimal transport (OT) to achieve effective visual-textual\nalignment, and so improve zero-shot generalization of VLMs.",
        "url": "http://arxiv.org/abs/2508.17417v1",
        "published_date": "2025-08-24T15:45:22+00:00",
        "updated_date": "2025-08-24T15:45:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaojie Yin",
            "Qilong Wang",
            "Qinghua Hu"
        ],
        "tldr": "This paper introduces a constrained prompt enhancement (CPE) method to improve zero-shot generalization of VLMs by constructing comprehensive textual prompts and compact visual prompts, using topology-guided semantic generation and category-agnostic discriminative region selection.",
        "tldr_zh": "本文提出了一种约束提示增强（CPE）方法，通过构建全面的文本提示和紧凑的视觉提示来提高视觉语言模型的零样本泛化能力，该方法使用了拓扑引导的语义生成和类别无关的判别区域选择。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis",
        "summary": "Clinical decision-making often involves interpreting images (e.g., radiology)\nfor making diagnoses. Retrieving relevant visual information from medical\nliterature and hospital records could enhance diagnostic accuracy. In this\npaper, we develop a model in which a multimodal retriever is jointly optimized\nwith an LVLM for medical diagnosis, unlike standard RAG where LVLM error signal\nis not propagated down to the retriever. We show that using only\ngeneral-purpose backbones, with only lightweight fine-tuning, our model is able\nto achieve competitive results with medically-pretrained models across clinical\nmulti-label classification and visual question answering tasks. In a novel\nanalysis, we additionally find that in many cases different top retrieved\nimages each lead to different predictions for a given target, and that these\ncases are empirically challenging for all models, even for non-retrieval\nmodels. Our joint retrieval optimization significantly improves these\nchallenging cases over standard RAG. However, oracle analysis reveals that\nwhile the correct diagnosis is frequently achievable using one of the top\nretrieved images, in practice there is a large performance gap from the oracle,\nand rerankers using frontier LVLMs do not close this gap -- leaving ample room\nfor improvement by future methods. Code will be made publicly available.",
        "url": "http://arxiv.org/abs/2508.17394v1",
        "published_date": "2025-08-24T15:06:20+00:00",
        "updated_date": "2025-08-24T15:06:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nir Mazor",
            "Tom Hope"
        ],
        "tldr": "This paper introduces a jointly optimized vision-language model and retriever for medical diagnosis, achieving competitive results with lightweight fine-tuning and addressing challenging retrieval scenarios, while also highlighting limitations of current reranking methods.",
        "tldr_zh": "本文提出了一种联合优化的视觉语言模型和检索器，用于医疗诊断，通过轻量级微调实现了有竞争力的结果，并解决了具有挑战性的检索场景，同时也强调了当前重排序方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models",
        "summary": "Large Vision-Language Models (LVLMs) process multimodal inputs consisting of\ntext tokens and vision tokens extracted from images or videos. Due to the rich\nvisual information, a single image can generate thousands of vision tokens,\nleading to high computational costs during the prefilling stage and significant\nmemory overhead during decoding. Existing methods attempt to prune redundant\nvision tokens, revealing substantial redundancy in visual representations.\nHowever, these methods often struggle in shallow layers due to the lack of\nsufficient contextual information. We argue that many visual tokens are\ninherently redundant even in shallow layers and can be safely and effectively\npruned with appropriate contextual signals. In this work, we propose CoViPAL, a\nlayer-wise contextualized visual token pruning method that employs a\nPlug-and-Play Pruning Module (PPM) to predict and remove redundant vision\ntokens before they are processed by the LVLM. The PPM is lightweight,\nmodel-agnostic, and operates independently of the LVLM architecture, ensuring\nseamless integration with various models. Extensive experiments on multiple\nbenchmarks demonstrate that CoViPAL outperforms training-free pruning methods\nunder equal token budgets and surpasses training-based methods with comparable\nsupervision. CoViPAL offers a scalable and efficient solution to improve\ninference efficiency in LVLMs without compromising accuracy.",
        "url": "http://arxiv.org/abs/2508.17243v1",
        "published_date": "2025-08-24T07:47:00+00:00",
        "updated_date": "2025-08-24T07:47:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Zicong Tang",
            "Ziyang Ma",
            "Suqing Wang",
            "Zuchao Li",
            "Lefei Zhang",
            "Hai Zhao",
            "Yun Li",
            "Qianren Wang"
        ],
        "tldr": "The paper introduces CoViPAL, a layer-wise contextualized visual token pruning method for LVLMs, which uses a lightweight module to remove redundant vision tokens, improving inference efficiency without sacrificing accuracy.",
        "tldr_zh": "该论文介绍了一种名为CoViPAL的分层上下文视觉令牌剪枝方法，用于大型视觉语言模型。它使用一个轻量级模块来移除冗余的视觉令牌，从而提高推理效率且不牺牲准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding",
        "summary": "This paper introduces a multi-agent framework for comprehensive highway scene\nunderstanding, designed around a mixture-of-experts strategy. In this\nframework, a large generic vision-language model (VLM), such as GPT-4o, is\ncontextualized with domain knowledge to generates task-specific\nchain-of-thought (CoT) prompts. These fine-grained prompts are then used to\nguide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over short\nvideos, along with complementary modalities as applicable. The framework\nsimultaneously addresses multiple critical perception tasks, including weather\nclassification, pavement wetness assessment, and traffic congestion detection,\nachieving robust multi-task reasoning while balancing accuracy and\ncomputational efficiency. To support empirical validation, we curated three\nspecialized datasets aligned with these tasks. Notably, the pavement wetness\ndataset is multimodal, combining video streams with road weather sensor data,\nhighlighting the benefits of multimodal reasoning. Experimental results\ndemonstrate consistently strong performance across diverse traffic and\nenvironmental conditions. From a deployment perspective, the framework can be\nreadily integrated with existing traffic camera systems and strategically\napplied to high-risk rural locations, such as sharp curves, flood-prone\nlowlands, or icy bridges. By continuously monitoring the targeted sites, the\nsystem enhances situational awareness and delivers timely alerts, even in\nresource-constrained environments.",
        "url": "http://arxiv.org/abs/2508.17205v1",
        "published_date": "2025-08-24T03:55:24+00:00",
        "updated_date": "2025-08-24T03:55:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "eess.IV"
        ],
        "authors": [
            "Yunxiang Yang",
            "Ningning Xu",
            "Jidong J. Yang"
        ],
        "tldr": "The paper introduces a multi-agent, mixture-of-experts VLM framework for comprehensive highway scene understanding, using task-specific prompts to guide a smaller VLM, and curates datasets for weather, pavement wetness, and congestion assessment.",
        "tldr_zh": "该论文介绍了一种多智能体、混合专家VLM框架，用于全面理解高速公路场景。该框架使用特定于任务的提示来引导较小的VLM，并策划了用于天气、路面湿度和交通拥堵评估的数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes",
        "summary": "A key frontier for Multimodal Large Language Models (MLLMs) is the ability to\nperform deep mathematical and spatial reasoning directly from images, moving\nbeyond their established success in semantic description. Mathematical surface\nplots provide a rigorous testbed for this capability, as they isolate the task\nof reasoning from the semantic noise common in natural images. To measure\nprogress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over\nVisual Landscapes), a new benchmark designed to quantitatively evaluate these\ncore reasoning skills. The benchmark comprises two novel tasks: Topological\nCounting, identifying and enumerating features like local maxima; and\nTransformation Recognition, recognizing applied geometric transformations.\nGenerated from a curated library of functions with rigorous ambiguity\nfiltering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs\nstruggle significantly, often resorting to superficial heuristics instead of\nrobust spatial reasoning. MaRVL-QA provides a challenging new tool for the\nresearch community to measure progress, expose model limitations, and guide the\ndevelopment of MLLMs with more profound reasoning abilities.",
        "url": "http://arxiv.org/abs/2508.17180v1",
        "published_date": "2025-08-24T01:24:56+00:00",
        "updated_date": "2025-08-24T01:24:56+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nilay Pande",
            "Sahiti Yerramilli",
            "Jayant Sravan Tamarapalli",
            "Rynaa Grover"
        ],
        "tldr": "The paper introduces MaRVL-QA, a benchmark for evaluating mathematical and spatial reasoning capabilities of Multimodal Large Language Models (MLLMs) on mathematical surface plots, revealing limitations in current state-of-the-art models.",
        "tldr_zh": "该论文介绍了MaRVL-QA，一个用于评估多模态大型语言模型（MLLM）在数学表面图上进行数学和空间推理能力的基准，揭示了当前最先进模型的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Play and Pause: Turning GPT-4o Spatial Weakness into a Strength for In-Depth Interactive Video Learning",
        "summary": "Traditional video-based learning remains passive, offering limited\nopportunities for users to engage dynamically with content. While current\nAI-powered tools offer transcription and summarization, they lack real-time,\nregion-specific interaction capabilities. This paper introduces Untwist, an\nAI-driven system that enables interactive video learning by allowing users to\nask questions about the entire video or specific regions using a bounding box,\nreceiving context-aware, multimodal responses. By integrating GPT APIs with\nComputer Vision techniques, Untwist extracts, processes, and structures video\ncontent to enhance comprehension. Our approach addresses GPT-4o spatial\nweakness by leveraging annotated frames instead of raw coordinate data,\nsignificantly improving accuracy in localizing and interpreting video content.\nThis paper describes the system architecture, including video pre-processing\nand real-time interaction, and outlines how Untwist can transform passive video\nconsumption into an interactive, AI-driven learning experience with the\npotential to enhance engagement and comprehension.",
        "url": "http://arxiv.org/abs/2508.17160v1",
        "published_date": "2025-08-23T23:08:04+00:00",
        "updated_date": "2025-08-23T23:08:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sajad Goudarzi",
            "Samaneh Zamanifard"
        ],
        "tldr": "The paper introduces Untwist, an AI system leveraging GPT-4o and computer vision to enable interactive video learning through region-specific queries, addressing GPT-4o's spatial weakness with annotated frames.",
        "tldr_zh": "该论文介绍了一个名为Untwist的AI系统，该系统利用GPT-4o和计算机视觉，通过特定区域的查询实现交互式视频学习，并通过带注释的帧解决了GPT-4o的空间弱点。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structural Damage Detection Using AI Super Resolution and Visual Language Model",
        "summary": "Natural disasters pose significant challenges to timely and accurate damage\nassessment due to their sudden onset and the extensive areas they affect.\nTraditional assessment methods are often labor-intensive, costly, and hazardous\nto personnel, making them impractical for rapid response, especially in\nresource-limited settings. This study proposes a novel, cost-effective\nframework that leverages aerial drone footage, an advanced AI-based video\nsuper-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a\n27 billion parameter Visual Language Model (VLM). This integrated system is\ndesigned to improve low-resolution disaster footage, identify structural\ndamage, and classify buildings into four damage categories, ranging from\nno/slight damage to total destruction, along with associated risk levels. The\nmethodology was validated using pre- and post-event drone imagery from the 2023\nTurkey earthquakes (courtesy of The Guardian) and satellite data from the 2013\nMoore Tornado (xBD dataset). The framework achieved a classification accuracy\nof 84.5%, demonstrating its ability to provide highly accurate results.\nFurthermore, the system's accessibility allows non-technical users to perform\npreliminary analyses, thereby improving the responsiveness and efficiency of\ndisaster management efforts.",
        "url": "http://arxiv.org/abs/2508.17130v1",
        "published_date": "2025-08-23T20:12:06+00:00",
        "updated_date": "2025-08-23T20:12:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Catherine Hoier",
            "Khandaker Mamun Ahmed"
        ],
        "tldr": "This paper presents a framework using drone footage, AI super-resolution (VRT), and a Visual Language Model (Gemma3:27b) for automated structural damage detection and classification from natural disasters, achieving 84.5% accuracy.",
        "tldr_zh": "本文提出了一种框架，利用无人机拍摄的视频、AI超分辨率模型(VRT)和视觉语言模型(Gemma3:27b)，对自然灾害造成的结构性破坏进行自动检测和分类，准确率达到84.5%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "GRASP: Geospatial pixel Reasoning viA Structured Policy learning",
        "summary": "Geospatial pixel reasoning is a nascent remote-sensing task that aims to\ngenerate segmentation masks directly from natural-language instructions.\nPrevailing MLLM-based systems co-train a language model and a mask decoder with\ndense pixel supervision, which is expensive and often weak on out-of-domain\n(OOD) data. We introduce GRASP, a structured policy-learning framework. In our\ndesign, a multimodal large language model first emits task-relevant bounding\nboxes and positive points from a vision-language instruction. These outputs are\nthen passed to a pre-trained segmentation model, which consumes them as prompts\nto generate the final mask. Instead of supervised fine-tuning, we optimize the\nsystem purely with reinforcement learning: the model is trained solely with\nGRPO, guided by format rewards and accuracy rewards computed on boxes and\npoints (no mask supervision). This leverages strong priors in foundation\nmodels, minimizes trainable parameters, and enables learning from inexpensive\nannotations. We additionally curate GRASP-1k, which contains\nreasoning-intensive queries, detailed reasoning traces, and fine-grained\nsegmentation annotations. Evaluations on both in-domain and out-of-domain test\nsets show state-of-the-art results: about 4% improvement in-domain and up to\n54% on OOD benchmarks. The experiment results evidence our model's robust\ngeneralization and demonstrate that complex geospatial segmentation behaviors\ncan be learned via RL from weak spatial cues. Code and the dataset will be\nreleased open-source.",
        "url": "http://arxiv.org/abs/2508.17102v1",
        "published_date": "2025-08-23T18:05:06+00:00",
        "updated_date": "2025-08-23T18:05:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengjie Jiang",
            "Yunqi Zhou",
            "Jiafeng Yan",
            "Jing Li"
        ],
        "tldr": "The paper introduces GRASP, a reinforcement learning framework for geospatial pixel reasoning from language instructions, using bounding boxes and points as prompts to a pre-trained segmentation model, achieving state-of-the-art results, especially on out-of-domain data.",
        "tldr_zh": "该论文介绍了GRASP，一个用于从语言指令进行地理空间像素推理的强化学习框架，使用边界框和点作为预训练分割模型的提示，实现了最先进的结果，尤其是在领域外数据上。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation",
        "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\nInterpretation, Textual Image Design, Entity-Reasoning and\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\nreasoning accuracy and image quality. We benchmark various T2I generation\nmodels, and provide comprehensive analysis on their performances.",
        "url": "http://arxiv.org/abs/2508.17472v1",
        "published_date": "2025-08-24T17:59:38+00:00",
        "updated_date": "2025-08-24T17:59:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyue Sun",
            "Rongyao Fang",
            "Chengqi Duan",
            "Xian Liu",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces T2I-ReasonBench, a benchmark for evaluating the reasoning capabilities of text-to-image models across four dimensions, and benchmarks existing models.",
        "tldr_zh": "该论文介绍了T2I-ReasonBench，一个用于评估文本到图像模型在四个维度上的推理能力的基准，并对现有模型进行了基准测试。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs",
        "summary": "We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)\non cricket scorecards, designed to evaluate large vision-language models\n(LVLMs) on complex numerical and cross-lingual reasoning over semi-structured\ntabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated\nscorecard images from ODI, T20, and Test formats, accompanied by 1,500 English\nQA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English\nscorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi\nscorecards, with all questions and answers kept in English to enable controlled\ncross-script evaluation. The task demands reasoning over structured numerical\ndata, multi-image context, and implicit domain knowledge. Empirical results\nshow that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle\non the English subset despite it being their primary training language and\nexhibit a further drop in performance on the Hindi subset. This reveals key\nlimitations in structure-aware visual text understanding, numerical reasoning,\nand cross-lingual generalization. The dataset is publicly available via Hugging\nFace at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM\nresearch in this direction.",
        "url": "http://arxiv.org/abs/2508.17334v1",
        "published_date": "2025-08-24T12:43:27+00:00",
        "updated_date": "2025-08-24T12:43:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Somraj Gautam",
            "Abhirama Subramanyam Penamakuri",
            "Abhishek Bhandari",
            "Gaurav Harit"
        ],
        "tldr": "The paper introduces MMCRICBENCH-3K, a new VQA benchmark for evaluating LVLMs on numerical and cross-lingual reasoning using cricket scorecards, revealing limitations in current state-of-the-art models.",
        "tldr_zh": "该论文介绍了MMCRICBENCH-3K，一个新的VQA基准，用于评估LVLMs在板球记分卡上的数值和跨语言推理能力，揭示了当前最先进模型的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling",
        "summary": "In this study, we introduce a novel cover image generation task that produces\nboth a concise summary and a visually corresponding image from a given\ntext-only document. Because no existing datasets are available for this task,\nwe propose a multimodal pseudo-labeling method to construct high-quality\ndatasets at low cost. We first collect documents that contain multiple images\nwith their captions, and their summaries by excluding factually inconsistent\ninstances. Our approach selects one image from the multiple images accompanying\nthe documents. Using the gold summary, we independently rank both the images\nand their captions. Then, we annotate a pseudo-label for an image when both the\nimage and its corresponding caption are ranked first in their respective\nrankings. Finally, we remove documents that contain direct image references\nwithin texts. Experimental results demonstrate that the proposed multimodal\npseudo-labeling method constructs more precise datasets and generates higher\nquality images than text- and image-only pseudo-labeling methods, which\nconsider captions and images separately. We release our code at:\nhttps://github.com/HyeyeeonKim/MMCIG",
        "url": "http://arxiv.org/abs/2508.17199v1",
        "published_date": "2025-08-24T03:24:35+00:00",
        "updated_date": "2025-08-24T03:24:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyeyeon Kim",
            "Sungwoo Han",
            "Jingun Kwon",
            "Hidetaka Kamigaito",
            "Manabu Okumura"
        ],
        "tldr": "The paper introduces a method for generating cover images for text-only documents using multimodal pseudo-labeling to create a new dataset. The approach ranks images and captions to select the best image for a given document summary.",
        "tldr_zh": "该论文介绍了一种使用多模态伪标签生成文本文档封面图像的方法，用于创建新的数据集。该方法通过对图像和标题进行排序，为给定的文档摘要选择最佳图像。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science",
        "summary": "PlantVillageVQA is a large-scale visual question answering (VQA) dataset\nderived from the widely used PlantVillage image corpus. It was designed to\nadvance the development and evaluation of vision-language models for\nagricultural decision-making and analysis. The PlantVillageVQA dataset\ncomprises 193,609 high-quality question-answer (QA) pairs grounded over 55,448\nimages spanning 14 crop species and 38 disease conditions. Questions are\norganised into 3 levels of cognitive complexity and 9 distinct categories. Each\nquestion category was phrased manually following expert guidance and generated\nvia an automated two-stage pipeline: (1) template-based QA synthesis from image\nmetadata and (2) multi-stage linguistic re-engineering. The dataset was\niteratively reviewed by domain experts for scientific accuracy and relevancy.\nThe final dataset was evaluated using three state-of-the-art models for quality\nassessment. Our objective remains to provide a publicly available, standardised\nand expert-verified database to enhance diagnostic accuracy for plant disease\nidentifications and advance scientific research in the agricultural domain. Our\ndataset will be open-sourced at\nhttps://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.",
        "url": "http://arxiv.org/abs/2508.17117v1",
        "published_date": "2025-08-23T19:04:57+00:00",
        "updated_date": "2025-08-23T19:04:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Syed Nazmus Sakib",
            "Nafiul Haque",
            "Mohammad Zabed Hossain",
            "Shifat E. Arman"
        ],
        "tldr": "The paper introduces PlantVillageVQA, a large-scale, expert-verified VQA dataset based on the PlantVillage image corpus, designed for advancing vision-language models in agricultural applications, particularly plant disease diagnosis.",
        "tldr_zh": "该论文介绍了 PlantVillageVQA，一个基于 PlantVillage 图像语料库的大规模、专家验证的 VQA 数据集，旨在推进视觉语言模型在农业应用，特别是植物病害诊断中的应用。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "E-BayesSAM: Efficient Bayesian Adaptation of SAM with Self-Optimizing KAN-Based Interpretation for Uncertainty-Aware Ultrasonic Segmentation",
        "summary": "Although the Segment Anything Model (SAM) has advanced medical image\nsegmentation, its Bayesian adaptation for uncertainty-aware segmentation\nremains hindered by three key issues: (1) instability in Bayesian fine-tuning\nof large pre-trained SAMs; (2) high computation cost due to SAM's massive\nparameters; (3) SAM's black-box design limits interpretability. To overcome\nthese, we propose E-BayesSAM, an efficient framework combining Token-wise\nVariational Bayesian Inference (T-VBI) for efficienty Bayesian adaptation and\nSelf-Optimizing Kolmogorov-Arnold Network (SO-KAN) for improving\ninterpretability. T-VBI innovatively reinterprets SAM's output tokens as\ndynamic probabilistic weights and reparameterizes them as latent variables\nwithout auxiliary training, enabling training-free VBI for uncertainty\nestimation. SO-KAN improves token prediction with learnable spline activations\nvia self-supervised learning, providing insight to prune redundant tokens to\nboost efficiency and accuracy. Experiments on five ultrasound datasets\ndemonstrated that E-BayesSAM achieves: (i) real-time inference (0.03s/image),\n(ii) superior segmentation accuracy (average DSC: Pruned E-BayesSAM's 89.0\\%\nvs. E-BayesSAM's 88.0% vs. MedSAM's 88.3%), and (iii) identification of four\ncritical tokens governing SAM's decisions. By unifying efficiency, reliability,\nand interpretability, E-BayesSAM bridges SAM's versatility with clinical needs,\nadvancing deployment in safety-critical medical applications. The source code\nis available at https://github.com/mp31192/E-BayesSAM.",
        "url": "http://arxiv.org/abs/2508.17408v1",
        "published_date": "2025-08-24T15:29:21+00:00",
        "updated_date": "2025-08-24T15:29:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Huang",
            "Zhong Liu",
            "Huiying Wen",
            "Bingsheng Huang",
            "Xin Chen",
            "Shuo Li"
        ],
        "tldr": "The paper introduces E-BayesSAM, an efficient and interpretable Bayesian adaptation of SAM for uncertainty-aware ultrasonic segmentation, achieving real-time inference and superior accuracy with a self-optimizing KAN-based interpretation.",
        "tldr_zh": "该论文介绍了E-BayesSAM，一种高效且可解释的SAM贝叶斯自适应方法，用于不确定性感知的超声分割，通过自优化KAN解释实现了实时推理和卓越的准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]