[
    {
        "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
        "url": "http://arxiv.org/abs/2510.24514v1",
        "published_date": "2025-10-28T15:26:20+00:00",
        "updated_date": "2025-10-28T15:26:20+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Huanyu Zhang",
            "Wenshan Wu",
            "Chengzu Li",
            "Ning Shang",
            "Yan Xia",
            "Yangyu Huang",
            "Yifan Zhang",
            "Li Dong",
            "Zhang Zhang",
            "Liang Wang",
            "Tieniu Tan",
            "Furu Wei"
        ],
        "tldr": "The paper introduces Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad to improve reasoning capabilities by interleaving textual reasoning with the generation of visual latents, demonstrating comparable or superior performance on MazePlanning.",
        "tldr_zh": "该论文介绍了Latent Sketchpad，一个为MLLM配备内部视觉草稿板的框架，通过将文本推理与视觉潜在变量的生成交织，以提高推理能力，并在MazePlanning上展示了可比较甚至更优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows",
        "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.",
        "url": "http://arxiv.org/abs/2510.24411v1",
        "published_date": "2025-10-28T13:22:39+00:00",
        "updated_date": "2025-10-28T13:22:39+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Qiushi Sun",
            "Mukai Li",
            "Zhoumianze Liu",
            "Zhihui Xie",
            "Fangzhi Xu",
            "Zhangyue Yin",
            "Kanzhi Cheng",
            "Zehao Li",
            "Zichen Ding",
            "Qi Liu",
            "Zhiyong Wu",
            "Zhuosheng Zhang",
            "Ben Kao",
            "Lingpeng Kong"
        ],
        "tldr": "The paper introduces OS-Sentinel, a hybrid framework for detecting unsafe operations of mobile GUI agents powered by VLMs, using a new sandbox environment and benchmark. It combines formal verification and a VLM-based contextual judge for improved safety detection.",
        "tldr_zh": "本文介绍OS-Sentinel，一个混合框架，用于检测由VLM驱动的移动GUI代理的不安全操作，它使用一个新的沙箱环境和基准。它结合了形式验证和基于VLM的上下文判断器，以提高安全性检测。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What do vision-language models see in the context? Investigating multimodal in-context learning",
        "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.",
        "url": "http://arxiv.org/abs/2510.24331v1",
        "published_date": "2025-10-28T11:55:24+00:00",
        "updated_date": "2025-10-28T11:55:24+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Gabriel O. dos Santos",
            "Esther Colombini",
            "Sandra Avila"
        ],
        "tldr": "This paper investigates in-context learning (ICL) in Vision-Language Models (VLMs), revealing limitations in their ability to effectively integrate visual and textual information from demonstrations, particularly regarding attention mechanisms.",
        "tldr_zh": "本文研究了视觉语言模型 (VLM) 中的上下文学习 (ICL)，揭示了它们在有效整合来自演示的视觉和文本信息方面的局限性，尤其是在注意力机制方面。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model",
        "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.",
        "url": "http://arxiv.org/abs/2510.24285v1",
        "published_date": "2025-10-28T10:42:57+00:00",
        "updated_date": "2025-10-28T10:42:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Juntian Zhang",
            "Song Jin",
            "Chuanqi Cheng",
            "Yuhan Liu",
            "Yankai Lin",
            "Xun Zhang",
            "Yufei Zhang",
            "Fei Jiang",
            "Guojun Yin",
            "Wei Lin",
            "Rui Yan"
        ],
        "tldr": "The paper introduces ViPER, a self-bootstrapping framework for improving the visual perception abilities of VLMs through a two-stage reinforcement learning approach using self-critiquing and self-prediction, leading to performance gains on fine-grained perception tasks.",
        "tldr_zh": "该论文介绍了一种名为ViPER的自举框架，通过使用自批判和自预测的两阶段强化学习方法，提高视觉语言模型的视觉感知能力，从而在细粒度感知任务上取得性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs",
        "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.",
        "url": "http://arxiv.org/abs/2510.24214v1",
        "published_date": "2025-10-28T09:29:37+00:00",
        "updated_date": "2025-10-28T09:29:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinhong Deng",
            "Wen Li",
            "Joey Tianyi Zhou",
            "Yang He"
        ],
        "tldr": "This paper introduces SCOPE, a novel visual token pruning strategy for Multimodal Large Language Models (MLLMs) that jointly models token saliency and coverage to improve efficiency while preserving semantic completeness, outperforming existing methods on vision-language understanding benchmarks.",
        "tldr_zh": "本文介绍了一种名为SCOPE的新型视觉token剪枝策略，用于多模态大型语言模型（MLLM）。该策略联合建模token的显著性和覆盖率，以提高效率并同时保留语义完整性。在视觉-语言理解基准测试中，SCOPE的表现优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning",
        "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.",
        "url": "http://arxiv.org/abs/2510.24152v1",
        "published_date": "2025-10-28T07:43:30+00:00",
        "updated_date": "2025-10-28T07:43:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aodi Wu",
            "Xubo Luo"
        ],
        "tldr": "The paper presents a framework to enhance VLMs for autonomous driving using task-specific prompting, spatial reasoning, and visual assembly, achieving high accuracy on a driving scene understanding challenge. The code and prompts are available.",
        "tldr_zh": "该论文提出了一个框架，通过任务特定的提示、空间推理和视觉组装来增强用于自动驾驶的视觉语言模型，在驾驶场景理解挑战中实现了高精度。代码和提示已开源。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Compositional Image Synthesis with Inference-Time Scaling",
        "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.",
        "url": "http://arxiv.org/abs/2510.24133v1",
        "published_date": "2025-10-28T07:16:21+00:00",
        "updated_date": "2025-10-28T07:16:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Minsuk Ji",
            "Sanghyeok Lee",
            "Namhyuk Ahn"
        ],
        "tldr": "This paper introduces a training-free framework that combines LLMs and object-centric VLMs with self-refinement to improve the compositional accuracy of text-to-image models, particularly addressing issues with object counts, attributes, and spatial relations.",
        "tldr_zh": "本文介绍了一个无需训练的框架，该框架结合了大型语言模型（LLM）和以对象为中心的视觉语言模型（VLM），并通过自我完善来提高文本到图像模型的组合准确性，特别解决了对象计数、属性和空间关系方面的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reasoning Visual Language Model for Chest X-Ray Analysis",
        "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.",
        "url": "http://arxiv.org/abs/2510.23968v1",
        "published_date": "2025-10-28T00:48:00+00:00",
        "updated_date": "2025-10-28T00:48:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andriy Myronenko",
            "Dong Yang",
            "Baris Turkbey",
            "Mariam Aboian",
            "Sena Azamat",
            "Esra Akcicek",
            "Hongxu Yin",
            "Pavlo Molchanov",
            "Marc Edgar",
            "Yufan He",
            "Pengfei Guo",
            "Yucheng Tang",
            "Daguang Xu"
        ],
        "tldr": "The paper introduces a novel vision-language model (VLM) framework for chest X-ray analysis that incorporates chain-of-thought reasoning, mirroring expert radiologists' thought processes and improving interpretability and auditability, validated through a reader study.",
        "tldr_zh": "该论文介绍了一种用于胸部X光分析的新型视觉语言模型（VLM）框架，该框架结合了链式思考推理，模仿了放射科专家的思维过程，提高了可解释性和可审计性，并通过读者研究进行了验证。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation",
        "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.",
        "url": "http://arxiv.org/abs/2510.23894v1",
        "published_date": "2025-10-27T22:05:08+00:00",
        "updated_date": "2025-10-27T22:05:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinxin Zhou",
            "Jiachen Jiang",
            "Zhihui Zhu"
        ],
        "tldr": "The paper introduces LHT-CLIP, a training-free framework that improves CLIP's visual discriminability for semantic segmentation by analyzing and enhancing its performance at layer, head, and token levels, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了LHT-CLIP，一个无需训练的框架，通过分析和增强CLIP在层、头和token级别的性能，来提高其视觉区分能力，用于语义分割，并实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space",
        "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.",
        "url": "http://arxiv.org/abs/2510.24446v1",
        "published_date": "2025-10-28T14:09:05+00:00",
        "updated_date": "2025-10-28T14:09:05+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Viktoriia Zinkovich",
            "Anton Antonov",
            "Andrei Spiridonov",
            "Denis Shepelev",
            "Andrey Moskalenko",
            "Daria Pugacheva",
            "Elena Tutubalina",
            "Andrey Kuznetsov",
            "Vlad Shakhuro"
        ],
        "tldr": "The paper introduces SPARTA, a black-box adversarial paraphrasing method using reinforcement learning in a text autoencoder latent space, to evaluate the robustness of reasoning segmentation models in Vision-Language Models (VLMs). It demonstrates vulnerabilities in current models to semantically equivalent paraphrases.",
        "tldr_zh": "该论文介绍了SPARTA，一种黑盒对抗性释义方法，它利用文本自编码器的潜在空间中的强化学习来评估视觉-语言模型（VLM）中推理分割模型的鲁棒性。 它展示了当前模型在语义等价释义方面的漏洞。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning",
        "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.",
        "url": "http://arxiv.org/abs/2510.24321v1",
        "published_date": "2025-10-28T11:39:22+00:00",
        "updated_date": "2025-10-28T11:39:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ivica Dimitrovski",
            "Vlatko Spasev",
            "Ivan Kitanovski"
        ],
        "tldr": "This paper explores prompt learning techniques to adapt CLIP for few-shot remote sensing image scene classification, demonstrating improved performance over baseline methods, particularly in cross-domain scenarios.",
        "tldr_zh": "本文探索了prompt learning技术，以将CLIP应用于小样本遥感图像场景分类，证明了其优于基线方法的性能，尤其是在跨域场景中。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification",
        "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.",
        "url": "http://arxiv.org/abs/2510.24078v1",
        "published_date": "2025-10-28T05:40:14+00:00",
        "updated_date": "2025-10-28T05:40:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "William Yang",
            "Xindi Wu",
            "Zhiwei Deng",
            "Esin Tureci",
            "Olga Russakovsky"
        ],
        "tldr": "The paper introduces BOB, a fine-tuning strategy for text-to-image models that improves synthetic data generation for fine-grained classification by conditioning on class-agnostic attributes and marginalizing them during generation, achieving state-of-the-art performance in low-shot scenarios.",
        "tldr_zh": "该论文介绍了一种名为BOB的微调策略，用于文本到图像模型，通过在生成过程中调节类别无关属性并将其边缘化，从而改进用于细粒度分类的合成数据生成，并在少样本场景中实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Enhancing CLIP Robustness via Cross-Modality Alignment",
        "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.",
        "url": "http://arxiv.org/abs/2510.24038v1",
        "published_date": "2025-10-28T03:47:44+00:00",
        "updated_date": "2025-10-28T03:47:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingyu Zhu",
            "Beier Zhu",
            "Shuo Wang",
            "Kesen Zhao",
            "Hanwang Zhang"
        ],
        "tldr": "The paper introduces COLA, a training-free, optimal transport-based framework for enhancing CLIP's robustness against adversarial attacks by explicitly addressing cross-modal misalignment in feature space.",
        "tldr_zh": "该论文介绍了COLA，一个免训练的、基于最优传输的框架，通过显式地解决特征空间中的跨模态不对齐问题，来增强CLIP对抗攻击的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning",
        "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.",
        "url": "http://arxiv.org/abs/2510.23907v1",
        "published_date": "2025-10-27T22:29:08+00:00",
        "updated_date": "2025-10-27T22:29:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Eddison Pham",
            "Prisha Priyadarshini",
            "Adrian Maliackel",
            "Kanishk Bandi",
            "Cristian Meo",
            "Kevin Zhu"
        ],
        "tldr": "The paper introduces DynaStride, a pipeline for generating coherent, scene-level captions for instructional videos by adaptively sampling frames and employing a multimodal chain-of-thought process. It demonstrates improved performance over strong baselines like VLLaMA3 and GPT-4o.",
        "tldr_zh": "该论文介绍了DynaStride，一种为教学视频生成连贯的场景级标题的流程，通过自适应地采样帧并采用多模态思维链过程。它展示了相对于VLLaMA3和GPT-4o等强大基线的改进性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices",
        "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.",
        "url": "http://arxiv.org/abs/2510.23775v1",
        "published_date": "2025-10-27T19:01:24+00:00",
        "updated_date": "2025-10-27T19:01:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "eess.IV"
        ],
        "authors": [
            "Aryan Mathur",
            "Asaduddin Ahmed",
            "Pushti Amit Vasoya",
            "Simeon Kandan Sonar",
            "Yasir Z",
            "Madesh Kuppusamy"
        ],
        "tldr": "This paper presents an explainable AI-generated image detection system using a lightweight CNN and a Vision-Language Model to classify, localize, and explain artifacts in low-resolution images, suitable for edge devices.",
        "tldr_zh": "该论文提出了一个可解释的AI生成图像检测系统，使用轻量级CNN和视觉语言模型来分类、定位和解释低分辨率图像中的伪影，适用于边缘设备。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors",
        "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io",
        "url": "http://arxiv.org/abs/2510.23930v1",
        "published_date": "2025-10-27T23:32:19+00:00",
        "updated_date": "2025-10-27T23:32:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xirui Jin",
            "Renbiao Jin",
            "Boying Li",
            "Danping Zou",
            "Wenxian Yu"
        ],
        "tldr": "PlanarGS improves 3D Gaussian Splatting for indoor scene reconstruction by incorporating vision-language planar priors, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "PlanarGS通过结合视觉-语言平面先验，改进了用于室内场景重建的3D高斯溅射方法，并在标准基准测试中取得了领先成果。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
        "url": "http://arxiv.org/abs/2510.23763v1",
        "published_date": "2025-10-27T18:49:03+00:00",
        "updated_date": "2025-10-27T18:49:03+00:00",
        "categories": [
            "cs.RO",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Siyin Wang",
            "Jinlan Fu",
            "Feihong Liu",
            "Xinzhe He",
            "Huangxuan Wu",
            "Junhao Shi",
            "Kexin Huang",
            "Zhaoye Fei",
            "Jingjing Gong",
            "Zuxuan Wu",
            "Yugang Jiang",
            "See-Kiong Ng",
            "Tat-Seng Chua",
            "Xipeng Qiu"
        ]
    }
]