[
    {
        "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
        "summary": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models.To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies.To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment\n\\& Sparse Action Enhancement that refine historical reasoning traces and\nbalances the distribution of sparse but critical actions, leading to more\ncoherent planning and better generalization in complex UI tasks. Our\ncontributions include the publish of SOTA open-source UI agents, comprehensive\ndata cleaning protocols and a novel self-evolving framework for improving\nnavigation performance, which encourage further research and development in the\ncommunity. Code is available at https://github.com/antgroup/UI-Venus.",
        "url": "http://arxiv.org/abs/2508.10833v1",
        "published_date": "2025-08-14T16:58:07+00:00",
        "updated_date": "2025-08-14T16:58:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhangxuan Gu",
            "Zhengwen Zeng",
            "Zhenyu Xu",
            "Xingran Zhou",
            "Shuheng Shen",
            "Yunfei Liu",
            "Beitong Zhou",
            "Changhua Meng",
            "Tianyu Xia",
            "Weizhi Chen",
            "Yue Wen",
            "Jingya Dou",
            "Fei Tang",
            "Jinzhen Lin",
            "Yulin Liu",
            "Zhenlin Guo",
            "Yichen Gong",
            "Heng Jia",
            "Changlong Gao",
            "Yuan Guo",
            "Yong Deng",
            "Zhenyu Guo",
            "Liang Chen",
            "Weiqiang Wang"
        ],
        "tldr": "UI-Venus, a multimodal large language model based UI agent, achieves state-of-the-art performance on UI grounding and navigation using reinforcement finetuning with a novel self-evolving framework, surpassing existing baselines.",
        "tldr_zh": "UI-Venus，一个基于多模态大型语言模型的UI代理，通过强化微调和一种新颖的自演化框架，在UI基础和导航方面实现了最先进的性能，超越了现有的基线。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences",
        "summary": "Recent advances in AI-generated content have fueled the rise of highly\nrealistic synthetic videos, posing severe risks to societal trust and digital\nintegrity. Existing benchmarks for video authenticity detection typically\nsuffer from limited realism, insufficient scale, and inadequate complexity,\nfailing to effectively evaluate modern vision-language models against\nsophisticated forgeries. To address this critical gap, we introduce AEGIS, a\nnovel large-scale benchmark explicitly targeting the detection of\nhyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises\nover 10,000 rigorously curated real and synthetic videos generated by diverse,\nstate-of-the-art generative models, including Stable Video Diffusion,\nCogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary\narchitectures. In particular, AEGIS features specially constructed challenging\nsubsets enhanced with robustness evaluation. Furthermore, we provide multimodal\nannotations spanning Semantic-Authenticity Descriptions, Motion Features, and\nLow-level Visual Features, facilitating authenticity detection and supporting\ndownstream tasks such as multimodal fusion and forgery localization. Extensive\nexperiments using advanced vision-language models demonstrate limited detection\ncapabilities on the most challenging subsets of AEGIS, highlighting the\ndataset's unique complexity and realism beyond the current generalization\ncapabilities of existing models. In essence, AEGIS establishes an indispensable\nevaluation benchmark, fundamentally advancing research toward developing\ngenuinely robust, reliable, broadly generalizable video authenticity detection\nmethodologies capable of addressing real-world forgery threats. Our dataset is\navailable on https://huggingface.co/datasets/Clarifiedfish/AEGIS.",
        "url": "http://arxiv.org/abs/2508.10771v1",
        "published_date": "2025-08-14T15:55:49+00:00",
        "updated_date": "2025-08-14T15:55:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jieyu Li",
            "Xin Zhang",
            "Joey Tianyi Zhou"
        ],
        "tldr": "The paper introduces AEGIS, a large-scale, challenging benchmark for evaluating the authenticity of AI-generated videos, designed to address the limitations of existing datasets and push the boundaries of current vision-language models.",
        "tldr_zh": "该论文介绍了AEGIS，一个大规模且具有挑战性的AI生成视频真伪性评估基准，旨在解决现有数据集的局限性，并推动当前视觉-语言模型的发展。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models",
        "summary": "Spatio-physical reasoning, a foundation capability for understanding the real\nphysics world, is a critical step towards building robust world models. While\nrecent vision language models (VLMs) have shown remarkable progress in\nspecialized domains like multimodal mathematics and pure spatial understanding,\ntheir capability for spatio-physical reasoning remains largely unexplored. This\npaper provides a comprehensive diagnostic analysis of mainstream VLMs,\nrevealing that current models perform inadequately on this crucial task.\nFurther detailed analysis shows that this underperformance is largely\nattributable to biases caused by human-like prior and a lack of deep reasoning.\nTo address these challenges, we apply supervised fine-tuning followed by\nrule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant\nimprovements in spatio-physical reasoning capabilities and surpassing leading\nproprietary models. Nevertheless, despite this success, the model's\ngeneralization to new physics scenarios remains limited -- underscoring the\npressing need for new approaches in spatio-physical reasoning.",
        "url": "http://arxiv.org/abs/2508.10770v1",
        "published_date": "2025-08-14T15:55:48+00:00",
        "updated_date": "2025-08-14T15:55:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tiancheng Han",
            "Yunfei Gao",
            "Yong Li",
            "Wuzhou Yu",
            "Qiaosheng Zhang",
            "Wenqi Shao"
        ],
        "tldr": "This paper diagnoses the limitations of existing VLMs in spatio-physical reasoning, identifies biases and lack of deep reasoning as key issues, and presents fine-tuning and reinforcement learning based improvements on Qwen2.5-VL-7B.",
        "tldr_zh": "本文诊断了现有视觉语言模型在时空物理推理方面的局限性，指出偏差和缺乏深度推理是关键问题，并提出了基于微调和强化学习的Qwen2.5-VL-7B改进方案。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly pushed the frontier of egocentric video question answering\n(EgocentricQA). However, existing benchmarks and studies are mainly limited to\ncommon daily activities such as cooking and cleaning. In contrast, real-world\ndeployment inevitably encounters domain shifts, where target domains differ\nsubstantially in both visual style and semantic content. To bridge this gap, we\nintroduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the\ncross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four\ndiverse and challenging domains, including surgery, industry, extreme sports,\nand animal perspective, representing realistic and high-impact application\nscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,\nspanning four key QA tasks: prediction, recognition, localization, and\ncounting. Each QA pair provides both OpenQA and CloseQA formats to support\nfine-grained evaluation. Extensive experiments show that most existing MLLMs,\nwhether general-purpose or egocentric-specialized, struggle to generalize to\ndomains beyond daily life, highlighting the limitations of current models.\nFurthermore, we conduct several pilot studies, \\eg, fine-tuning and\nreinforcement learning, to explore potential improvements. We hope EgoCross and\nour accompanying analysis will serve as a foundation for advancing\ndomain-adaptive, robust egocentric video understanding. Data and codes will be\nreleased at:\n\\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}",
        "url": "http://arxiv.org/abs/2508.10729v1",
        "published_date": "2025-08-14T15:11:20+00:00",
        "updated_date": "2025-08-14T15:11:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yanjun Li",
            "Yuqian Fu",
            "Tianwen Qian",
            "Qi'ao Xu",
            "Silong Dai",
            "Danda Pani Paudel",
            "Luc Van Gool",
            "Xiaoling Wang"
        ],
        "tldr": "The paper introduces EgoCross, a new benchmark for evaluating the cross-domain generalization capabilities of MLLMs in egocentric video question answering across diverse domains like surgery and sports, revealing the limitations of current models.",
        "tldr_zh": "该论文介绍了EgoCross，一个新的基准，用于评估多模态大型语言模型在以自我为中心的视频问答中跨领域泛化的能力，涵盖手术和运动等不同领域，揭示了当前模型的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning",
        "summary": "Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in\nautomating industrial quality inspection. Recently, some FS-IAD methods based\non Large Vision-Language Models (LVLMs) have been proposed with some\nachievements through prompt learning or fine-tuning. However, existing LVLMs\nfocus on general tasks but lack basic industrial knowledge and reasoning\ncapabilities related to FS-IAD, making these methods far from specialized human\nquality inspectors. To address these challenges, we propose a unified\nframework, IADGPT, designed to perform FS-IAD in a human-like manner, while\nalso handling associated localization and reasoning tasks, even for diverse and\nnovel industrial products. To this end, we introduce a three-stage progressive\ntraining strategy inspired by humans. Specifically, the first two stages\ngradually guide IADGPT in acquiring fundamental industrial knowledge and\ndiscrepancy awareness. In the third stage, we design an in-context\nlearning-based training paradigm, enabling IADGPT to leverage a few-shot image\nas the exemplars for improved generalization to novel products. In addition, we\ndesign a strategy that enables IADGPT to output image-level and pixel-level\nanomaly scores using the logits output and the attention map, respectively, in\nconjunction with the language output to accomplish anomaly reasoning. To\nsupport our training, we present a new dataset comprising 100K images across\n400 diverse industrial product categories with extensive attribute-level\ntextual annotations. Experiments indicate IADGPT achieves considerable\nperformance gains in anomaly detection and demonstrates competitiveness in\nanomaly localization and reasoning. We will release our dataset in\ncamera-ready.",
        "url": "http://arxiv.org/abs/2508.10681v1",
        "published_date": "2025-08-14T14:24:47+00:00",
        "updated_date": "2025-08-14T14:24:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengyang Zhao",
            "Teng Fu",
            "Haiyang Yu",
            "Ke Niu",
            "Bin Li"
        ],
        "tldr": "The paper introduces IADGPT, a unified LVLM framework for few-shot industrial anomaly detection, localization, and reasoning, trained progressively and using in-context learning, along with a new dataset for this purpose.",
        "tldr_zh": "该论文介绍了IADGPT，一个统一的LVLM框架，用于少样本工业异常检测、定位和推理，通过渐进式训练和上下文学习进行训练，并为此目的提供了一个新的数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models",
        "summary": "Large visual language models (LVLMs) have demonstrated impressive performance\nin coarse-grained geo-localization at the country or city level, but they\nstruggle with fine-grained street-level localization within urban areas. In\nthis paper, we explore integrating city-wide address localization capabilities\ninto LVLMs, facilitating flexible address-related question answering using\nstreet-view images. A key challenge is that the street-view visual\nquestion-and-answer (VQA) data provides only microscopic visual cues, leading\nto subpar performance in fine-tuned models. To tackle this issue, we\nincorporate perspective-invariant satellite images as macro cues and propose\ncross-view alignment tuning including a satellite-view and street-view image\ngrafting mechanism, along with an automatic label generation mechanism. Then\nLVLM's global understanding of street distribution is enhanced through\ncross-view matching. Our proposed model, named AddressVLM, consists of\ntwo-stage training protocols: cross-view alignment tuning and address\nlocalization tuning. Furthermore, we have constructed two street-view VQA\ndatasets based on image address localization datasets from Pittsburgh and San\nFrancisco. Qualitative and quantitative evaluations demonstrate that AddressVLM\noutperforms counterpart LVLMs by over 9% and 12% in average address\nlocalization accuracy on these two datasets, respectively.",
        "url": "http://arxiv.org/abs/2508.10667v1",
        "published_date": "2025-08-14T14:06:28+00:00",
        "updated_date": "2025-08-14T14:06:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shixiong Xu",
            "Chenghao Zhang",
            "Lubin Fan",
            "Yuan Zhou",
            "Bin Fan",
            "Shiming Xiang",
            "Gaofeng Meng",
            "Jieping Ye"
        ],
        "tldr": "The paper introduces AddressVLM, a novel LVLM approach enhancing street-level address localization by using cross-view alignment (satellite and street-view images) and a two-stage training protocol. It demonstrates improved accuracy on newly constructed Pittsburgh and San Francisco datasets.",
        "tldr_zh": "该论文介绍了 AddressVLM，一种新颖的 LVLM 方法，通过使用跨视图对齐（卫星和街景图像）和两阶段训练协议来增强街道级地址定位。 它展示了在新建的匹兹堡和旧金山数据集上提高了准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SemPT: Semantic Prompt Tuning for Vision-Language Models",
        "summary": "Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning.",
        "url": "http://arxiv.org/abs/2508.10645v1",
        "published_date": "2025-08-14T13:41:59+00:00",
        "updated_date": "2025-08-14T13:41:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiao Shi",
            "Yangjun Ou",
            "Zhenzhong Chen"
        ],
        "tldr": "SemPT introduces a novel prompt tuning framework for VLMs that leverages shared attribute-level knowledge for improved generalization in visual transfer learning, achieving state-of-the-art performance across various benchmarks.",
        "tldr_zh": "SemPT 提出了一种新的 VLM 提示调整框架，该框架利用共享的属性级别知识来改进视觉迁移学习中的泛化能力，并在各种基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation",
        "summary": "Understanding environmental changes from aerial imagery is vital for climate\nresilience, urban planning, and ecosystem monitoring. Yet, current vision\nlanguage models (VLMs) overlook causal signals from environmental sensors, rely\non single-source captions prone to stylistic bias, and lack interactive\nscenario-based reasoning. We present ChatENV, the first interactive VLM that\njointly reasons over satellite image pairs and real-world sensor data. Our\nframework: (i) creates a 177k-image dataset forming 152k temporal pairs across\n62 land-use classes in 197 countries with rich sensor metadata (e.g.,\ntemperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for\nstylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using\nefficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV\nachieves strong performance in temporal and \"what-if\" reasoning (e.g., BERT-F1\n0.903) and rivals or outperforms state-of-the-art temporal models, while\nsupporting interactive scenario-based analysis. This positions ChatENV as a\npowerful tool for grounded, sensor-aware environmental monitoring.",
        "url": "http://arxiv.org/abs/2508.10635v1",
        "published_date": "2025-08-14T13:33:44+00:00",
        "updated_date": "2025-08-14T13:33:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hosam Elgendy",
            "Ahmed Sharshar",
            "Ahmed Aboeitta",
            "Mohsen Guizani"
        ],
        "tldr": "ChatENV is a novel interactive VLM that combines satellite imagery and sensor data for environmental monitoring, demonstrating strong performance in temporal and scenario-based reasoning.",
        "tldr_zh": "ChatENV 是一种新型交互式视觉语言模型，它结合了卫星图像和传感器数据进行环境监测，并在时间和基于场景的推理方面表现出强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs",
        "summary": "While Multimodal Large Language Models (MLLMs) show immense promise for\nachieving truly human-like interactions, progress is hindered by the lack of\nfine-grained evaluation frameworks for human-centered scenarios, encompassing\nboth the understanding of complex human intentions and the provision of\nempathetic, context-aware responses. Here we introduce HumanSense, a\ncomprehensive benchmark designed to evaluate the human-centered perception and\ninteraction capabilities of MLLMs, with a particular focus on deep\nunderstanding of extended multimodal contexts and the formulation of rational\nfeedback. Our evaluation reveals that leading MLLMs still have considerable\nroom for improvement, particularly for advanced interaction-oriented tasks.\nSupplementing visual input with audio and text information yields substantial\nimprovements, and Omni-modal models show advantages on these tasks.\nFurthermore, we argue that appropriate feedback stems from a contextual\nanalysis of the interlocutor's needs and emotions, with reasoning ability\nserving as the key to unlocking it. Accordingly, we employ a multi-stage,\nmodality-progressive reinforcement learning to enhance the reasoning abilities\nof an Omni model, achieving substantial gains on evaluation results.\nAdditionally, we observe that successful reasoning processes exhibit highly\nconsistent thought patterns. By designing corresponding prompts, we also\nenhance the performance of non-reasoning models in a training-free manner.\nProject page:\n\\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/",
        "url": "http://arxiv.org/abs/2508.10576v1",
        "published_date": "2025-08-14T12:14:15+00:00",
        "updated_date": "2025-08-14T12:14:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zheng Qin",
            "Ruobing Zheng",
            "Yabing Wang",
            "Tianqi Li",
            "Yi Yuan",
            "Jingdong Chen",
            "Le Wang"
        ],
        "tldr": "The paper introduces HumanSense, a new benchmark for evaluating the human-centered perception and interaction capabilities of MLLMs, and explores methods to improve empathetic and context-aware responses through reasoning and multi-modal input.",
        "tldr_zh": "该论文介绍了HumanSense，一个新的基准，用于评估多模态大型语言模型的人本感知和交互能力。该论文还探讨了通过推理和多模态输入来提高共情和上下文感知响应的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation",
        "summary": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.",
        "url": "http://arxiv.org/abs/2508.10572v1",
        "published_date": "2025-08-14T12:11:15+00:00",
        "updated_date": "2025-08-14T12:11:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tuyen Tran",
            "Thao Minh Le",
            "Truyen Tran"
        ],
        "tldr": "The paper introduces a novel agentic system, Multi-Modal Agent, that leverages LLMs to dynamically generate workflows for multimodal-guided video object segmentation, improving upon existing fixed pipeline methods.",
        "tldr_zh": "该论文介绍了一种新颖的代理系统，即多模态代理，它利用大型语言模型动态生成用于多模态引导的视频对象分割的工作流程，从而改进了现有的固定管道方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset",
        "summary": "Medical image grounding aims to align natural language phrases with specific\nregions in medical images, serving as a foundational task for intelligent\ndiagnosis, visual question answering (VQA), and automated report generation\n(MRG). However, existing research is constrained by limited modality coverage,\ncoarse-grained annotations, and the absence of a unified, generalizable\ngrounding framework. To address these challenges, we construct a large-scale\nmedical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level\nannotations across seven imaging modalities, covering diverse anatomical\nstructures and pathological findings. The dataset supports both segmentation\nand grounding tasks with hierarchical region labels, ranging from organ-level\nboundaries to fine-grained lesions. Based on this foundation, we propose\nMed-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather\nthan relying on explicitly designed expert modules, Med-GLIP implicitly\nacquires hierarchical semantic understanding from diverse training data --\nenabling it to recognize multi-granularity structures, such as distinguishing\nlungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP\nconsistently outperforms state-of-the-art baselines across multiple grounding\nbenchmarks. Furthermore, integrating its spatial outputs into downstream tasks,\nincluding medical VQA and report generation, leads to substantial performance\ngains. Our dataset will be released soon.",
        "url": "http://arxiv.org/abs/2508.10528v1",
        "published_date": "2025-08-14T11:02:38+00:00",
        "updated_date": "2025-08-14T11:02:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziye Deng",
            "Ruihan He",
            "Jiaxiang Liu",
            "Yuan Wang",
            "Zijie Meng",
            "Songtao Jiang",
            "Yong Xie",
            "Zuozhu Liu"
        ],
        "tldr": "The paper introduces Med-GLIP, a large-scale medical grounding dataset (5.3M annotations) and a modality-aware grounding framework, demonstrating improved performance on grounding, medical VQA, and report generation tasks.",
        "tldr_zh": "该论文介绍了Med-GLIP，一个大规模的医学图像grounding数据集（530万个标注），以及一个模态感知的grounding框架，在grounding、医学VQA和报告生成任务上表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.",
        "url": "http://arxiv.org/abs/2508.10433v1",
        "published_date": "2025-08-14T08:15:41+00:00",
        "updated_date": "2025-08-14T08:15:41+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Runqi Qiao",
            "Qiuna Tan",
            "Peiqing Yang",
            "Yanzi Wang",
            "Xiaowan Wang",
            "Enhui Wan",
            "Sitong Zhou",
            "Guanting Dong",
            "Yuchen Zeng",
            "Yida Xu",
            "Jie Wang",
            "Chong Sun",
            "Chen Li",
            "Honggang Zhang"
        ],
        "tldr": "We-Math 2.0 is a system designed to improve mathematical reasoning in MLLMs, featuring a structured knowledge system, model-centric data, and reinforcement learning, showing promising generalization in experiments.",
        "tldr_zh": "We-Math 2.0是一个旨在提升MLLM数学推理能力的系统，包含结构化知识体系、模型中心的数据以及强化学习，实验表明其具有良好的泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance",
        "summary": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence\ndataset with verifiable provenance. It is a curated approximately 10% open\nsubset of an original 1.2 million, quality-accepted corpus of food images\nannotated for a wide range of information (such as dish name, region of\ncreation). The corpus was collected over six weeks from over 87,000\ncontributors using the Codatta contribution model, which combines community\nsourcing with configurable AI-assisted quality checks; each submission is\nlinked to a wallet address in a secure off-chain ledger for traceability, with\na full on-chain protocol on the roadmap. We describe the schema, pipeline, and\nQA, and validate utility by fine-tuning large vision-language models (ChatGPT\n5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning\nyields consistent gains over out-of-box baselines across standard metrics; we\nreport results primarily on the MM-Food-100K subset. We release MM-Food-100K\nfor publicly free access and retain approximately 90% for potential commercial\naccess with revenue sharing to contributors.",
        "url": "http://arxiv.org/abs/2508.10429v1",
        "published_date": "2025-08-14T07:59:31+00:00",
        "updated_date": "2025-08-14T07:59:31+00:00",
        "categories": [
            "cs.AI",
            "cs.CR",
            "cs.CV",
            "I.2.10; I.2.6"
        ],
        "authors": [
            "Yi Dong",
            "Yusuke Muraoka",
            "Scott Shi",
            "Yi Zhang"
        ],
        "tldr": "The paper introduces MM-Food-100K, a 100,000-sample multimodal food dataset with verifiable provenance, and demonstrates its utility by fine-tuning vision-language models for nutrition prediction.",
        "tldr_zh": "该论文介绍了 MM-Food-100K，一个包含10万样本的多模态食品数据集，具有可验证的来源，并通过微调视觉-语言模型进行营养预测来展示其效用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes",
        "summary": "Vision-Language Models (VLMs) have been applied to autonomous driving to\nsupport decision-making in complex real-world scenarios. However, their\ntraining on static, web-sourced image-text pairs fundamentally limits the\nprecise spatiotemporal reasoning required to understand and predict dynamic\ntraffic scenes. We address this critical gap with STRIDE-QA, a large-scale\nvisual question answering (VQA) dataset for physically grounded reasoning from\nan ego-centric perspective. Constructed from 100 hours of multi-sensor driving\ndata in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the\nlargest VQA dataset for spatiotemporal reasoning in urban driving, offering 16\nmillion QA pairs over 285K frames. Grounded by dense, automatically generated\nannotations including 3D bounding boxes, segmentation masks, and multi-object\ntracks, the dataset uniquely supports both object-centric and ego-centric\nreasoning through three novel QA tasks that require spatial localization and\ntemporal prediction. Our benchmarks demonstrate that existing VLMs struggle\nsignificantly, achieving near-zero scores on prediction consistency. In\ncontrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,\nachieving 55% success in spatial localization and 28% consistency in future\nmotion prediction, compared to near-zero scores from general-purpose VLMs.\nTherefore, STRIDE-QA establishes a comprehensive foundation for developing more\nreliable VLMs for safety-critical autonomous systems.",
        "url": "http://arxiv.org/abs/2508.10427v1",
        "published_date": "2025-08-14T07:57:06+00:00",
        "updated_date": "2025-08-14T07:57:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keishi Ishihara",
            "Kento Sasaki",
            "Tsubasa Takahashi",
            "Daiki Shiono",
            "Yu Yamaguchi"
        ],
        "tldr": "The paper introduces STRIDE-QA, a large-scale VQA dataset for spatiotemporal reasoning in autonomous driving, highlighting the limitations of current VLMs and demonstrating significant performance gains through fine-tuning on their dataset.",
        "tldr_zh": "该论文介绍了STRIDE-QA，一个用于自动驾驶中时空推理的大规模VQA数据集。它强调了当前VLM的局限性，并展示了通过在STRIDE-QA数据集上进行微调所带来的显著性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
        "summary": "Existing vision-and-language navigation models often deviate from the correct\ntrajectory when executing instructions. However, these models lack effective\nerror correction capability, hindering their recovery from errors. To address\nthis challenge, we propose Self-correction Flywheel, a novel post-training\nparadigm. Instead of considering the model's error trajectories on the training\nset as a drawback, our paradigm emphasizes their significance as a valuable\ndata source. We have developed a method to identify deviations in these error\ntrajectories and devised innovative techniques to automatically generate\nself-correction data for perception and action. These self-correction data\nserve as fuel to power the model's continued training. The brilliance of our\nparadigm is revealed when we re-evaluate the model on the training set,\nuncovering new error trajectories. At this time, the self-correction flywheel\nbegins to spin. Through multiple flywheel iterations, we progressively enhance\nour monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE\nand RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success\nrates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%\nand 16.4%. Real robot tests in various indoor and outdoor environments\ndemonstrate \\method's superior capability of error correction, dynamic obstacle\navoidance, and long instruction following.",
        "url": "http://arxiv.org/abs/2508.10416v1",
        "published_date": "2025-08-14T07:39:26+00:00",
        "updated_date": "2025-08-14T07:39:26+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Zhuoyuan Yu",
            "Yuxing Long",
            "Zihan Yang",
            "Chengyan Zeng",
            "Hongwei Fan",
            "Jiyao Zhang",
            "Hao Dong"
        ],
        "tldr": "The paper introduces CorrectNav, a self-correction flywheel paradigm for vision-language-action navigation models, achieving state-of-the-art results on R2R-CE and RxR-CE benchmarks and demonstrating improved error correction and obstacle avoidance in real-world environments.",
        "tldr_zh": "该论文介绍了CorrectNav，一种用于视觉-语言-动作导航模型的自纠正飞轮范式，在R2R-CE和RxR-CE基准测试中取得了最先进的结果，并在真实环境中展示了改进的错误纠正和避障能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models",
        "summary": "Vision-language instruction tuning achieves two main purposes: learning\nvisual concepts and learning visual skills. In this paper, we found that\nvision-language benchmarks fall into the dichotomy of mainly benefiting from\ntraining on instructions with similar skills or visual concepts. Inspired by\nthe discovery, we designed a simple targeted training data selection method to\noptimize the performance of a given benchmark. We first extract the\nconcepts/skills from the benchmark, determine whether the benchmark\npredominantly benefits from similar concepts or skills, and finally select\ninstructions with the most matching concepts/skills. Experiments on 10+\nbenchmarks validate the effectiveness of our targeted data selection method,\nshowing +0.9\\% over the best existing baseline averaged over all benchmarks and\n+1.5\\% on the skill-focused subset. Our findings underscore the importance of\nrecognizing the inherent trade-off within instruction selection, which requires\nbalancing the acquisition of conceptual knowledge against visual skill.",
        "url": "http://arxiv.org/abs/2508.10339v1",
        "published_date": "2025-08-14T04:48:38+00:00",
        "updated_date": "2025-08-14T04:48:38+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Andrew Bai",
            "Justin Cui",
            "Ruochen Wang",
            "Cho-Jui Hsieh"
        ],
        "tldr": "The paper identifies a dichotomy in vision-language benchmarks (concept vs. skill focused) and proposes a targeted data selection method to optimize performance on these benchmarks, achieving improved results.",
        "tldr_zh": "该论文指出视觉-语言基准测试存在概念与技能的二分法，并提出了一种有针对性的数据选择方法来优化这些基准测试的性能，取得了改进的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver",
        "summary": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic\nagents to integrate multimodal understanding with action execution. However,\nour empirical analysis reveals that current VLAs struggle to allocate visual\nattention to target regions. Instead, visual attention is always dispersed. To\nguide the visual attention grounding on the correct target, we propose\nReconVLA, a reconstructive VLA model with an implicit grounding paradigm.\nConditioned on the model's visual outputs, a diffusion transformer aims to\nreconstruct the gaze region of the image, which corresponds to the target\nmanipulated objects. This process prompts the VLA model to learn fine-grained\nrepresentations and accurately allocate visual attention, thus effectively\nleveraging task-specific visual information and conducting precise\nmanipulation. Moreover, we curate a large-scale pretraining dataset comprising\nover 100k trajectories and 2 million data samples from open-source robotic\ndatasets, further boosting the model's generalization in visual reconstruction.\nExtensive experiments in simulation and the real world demonstrate the\nsuperiority of our implicit grounding method, showcasing its capabilities of\nprecise manipulation and generalization. Our project page is\nhttps://zionchow.github.io/ReconVLA/.",
        "url": "http://arxiv.org/abs/2508.10333v1",
        "published_date": "2025-08-14T04:20:19+00:00",
        "updated_date": "2025-08-14T04:20:19+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Wenxuan Song",
            "Ziyang Zhou",
            "Han Zhao",
            "Jiayi Chen",
            "Pengxiang Ding",
            "Haodong Yan",
            "Yuxin Huang",
            "Feilong Tang",
            "Donglin Wang",
            "Haoang Li"
        ],
        "tldr": "The paper introduces ReconVLA, a reconstructive Vision-Language-Action model that improves visual attention in robotic manipulation by using a diffusion transformer to reconstruct gaze regions, leading to more precise actions.",
        "tldr_zh": "该论文介绍了ReconVLA，一种重建式视觉-语言-动作模型，通过使用扩散Transformer重建注视区域来改善机器人操作中的视觉注意力，从而实现更精确的动作。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics",
        "summary": "Recent advances in Vision-Language Models (VLMs) and large language models\n(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI\nagents like robots. However, existing visual reasoning benchmarks often suffer\nfrom several limitations: they lack a clear definition of reasoning complexity,\noffer have no control to generate questions over varying difficulty and task\ncustomization, and fail to provide structured, step-by-step reasoning\nannotations (workflows). To bridge these gaps, we formalize reasoning\ncomplexity, introduce an adaptive query engine that generates customizable\nquestions of varying complexity with detailed intermediate annotations, and\nextend the JRDB dataset with human-object interaction and geometric\nrelationship annotations to create JRDB-Reasoning, a benchmark tailored for\nvisual reasoning in human-crowded environments. Our engine and benchmark enable\nfine-grained evaluation of visual reasoning frameworks and dynamic assessment\nof visual-language models across reasoning levels.",
        "url": "http://arxiv.org/abs/2508.10287v1",
        "published_date": "2025-08-14T02:31:22+00:00",
        "updated_date": "2025-08-14T02:31:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Simindokht Jahangard",
            "Mehrzad Mohammadi",
            "Yi Shen",
            "Zhixi Cai",
            "Hamid Rezatofighi"
        ],
        "tldr": "The paper introduces JRDB-Reasoning, a new visual reasoning benchmark built on the JRDB dataset, designed to overcome limitations of existing benchmarks by offering difficulty-graded questions, customizable task settings, and structured reasoning annotations for embodied AI in human-crowded environments.",
        "tldr_zh": "该论文介绍了 JRDB-Reasoning，一个新的视觉推理基准，建立在 JRDB 数据集之上，旨在通过提供难度分级的问答、可定制的任务设置和结构化的推理注释来克服现有基准的局限性，适用于在拥挤人群环境中部署的具身智能体。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance",
        "summary": "This paper addresses the performance bottlenecks of existing text-driven\nimage generation methods in terms of semantic alignment accuracy and structural\nconsistency. A high-fidelity image generation method is proposed by integrating\ntext-image contrastive constraints with structural guidance mechanisms. The\napproach introduces a contrastive learning module that builds strong\ncross-modal alignment constraints to improve semantic matching between text and\nimage. At the same time, structural priors such as semantic layout maps or edge\nsketches are used to guide the generator in spatial-level structural modeling.\nThis enhances the layout completeness and detail fidelity of the generated\nimages. Within the overall framework, the model jointly optimizes contrastive\nloss, structural consistency loss, and semantic preservation loss. A\nmulti-objective supervision mechanism is adopted to improve the semantic\nconsistency and controllability of the generated content. Systematic\nexperiments are conducted on the COCO-2014 dataset. Sensitivity analyses are\nperformed on embedding dimensions, text length, and structural guidance\nstrength. Quantitative metrics confirm the superior performance of the proposed\nmethod in terms of CLIP Score, FID, and SSIM. The results show that the method\neffectively bridges the gap between semantic alignment and structural fidelity\nwithout increasing computational complexity. It demonstrates a strong ability\nto generate semantically clear and structurally complete images, offering a\nviable technical path for joint text-image modeling and image generation.",
        "url": "http://arxiv.org/abs/2508.10280v1",
        "published_date": "2025-08-14T02:15:11+00:00",
        "updated_date": "2025-08-14T02:15:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Danyi Gao"
        ],
        "tldr": "This paper proposes a high-fidelity text-to-image generation method that improves semantic alignment and structural consistency by integrating contrastive learning and structural guidance, achieving superior performance on COCO-2014 without increasing computational complexity.",
        "tldr_zh": "本文提出了一种高保真文本到图像生成方法，通过整合对比学习和结构指导来提高语义对齐和结构一致性，在COCO-2014上实现了卓越的性能，而没有增加计算复杂性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs",
        "summary": "Large Vision-Language Models (LVLMs) have shown strong performance across\nmultimodal tasks. However, they often produce hallucinations -- text that is\ninconsistent with visual input, due to the limited ability to verify\ninformation in different regions of the image. To address this, we propose\nMulti-Region Fusion Decoding (MRFD), a training-free decoding method that\nimproves factual grounding by modeling inter-region consistency. MRFD\nidentifies salient regions using cross-attention, generates initial responses\nfor each, and computes reliability weights based on Jensen-Shannon Divergence\n(JSD) among the responses. These weights guide a consistency-aware fusion of\nper-region predictions, using region-aware prompts inspired by Chain-of-Thought\nreasoning. Experiments across multiple LVLMs and benchmarks show that MRFD\nsignificantly reduces hallucinations and improves response factuality without\nrequiring model updates.",
        "url": "http://arxiv.org/abs/2508.10264v1",
        "published_date": "2025-08-14T01:17:39+00:00",
        "updated_date": "2025-08-14T01:17:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haonan Ge",
            "Yiwei Wang",
            "Ming-Hsuan Yang",
            "Yujun Cai"
        ],
        "tldr": "This paper introduces Multi-Region Fusion Decoding (MRFD), a training-free method to mitigate hallucinations in LVLMs by modeling inter-region consistency and improving factual grounding, showing significant improvements in experiments.",
        "tldr_zh": "本文介绍了一种名为多区域融合解码（MRFD）的免训练方法，通过对区域间一致性进行建模并提高事实基础，来减轻大型视觉语言模型（LVLM）中的幻觉问题，实验表明该方法有显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SynSpill: Improved Industrial Spill Detection With Synthetic Data",
        "summary": "Large-scale Vision-Language Models (VLMs) have transformed general-purpose\nvisual recognition through strong zero-shot capabilities. However, their\nperformance degrades significantly in niche, safety-critical domains such as\nindustrial spill detection, where hazardous events are rare, sensitive, and\ndifficult to annotate. This scarcity -- driven by privacy concerns, data\nsensitivity, and the infrequency of real incidents -- renders conventional\nfine-tuning of detectors infeasible for most industrial settings.\n  We address this challenge by introducing a scalable framework centered on a\nhigh-quality synthetic data generation pipeline. We demonstrate that this\nsynthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of\nVLMs and substantially boosts the performance of state-of-the-art object\ndetectors such as YOLO and DETR. Notably, in the absence of synthetic data\n(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than\nthese detectors. When SynSpill is used, both VLMs and detectors achieve marked\nimprovements, with their performance becoming comparable.\n  Our results underscore that high-fidelity synthetic data is a powerful means\nto bridge the domain gap in safety-critical applications. The combination of\nsynthetic generation and lightweight adaptation offers a cost-effective,\nscalable pathway for deploying vision systems in industrial environments where\nreal data is scarce/impractical to obtain.\n  Project Page: https://synspill.vercel.app",
        "url": "http://arxiv.org/abs/2508.10171v1",
        "published_date": "2025-08-13T20:09:58+00:00",
        "updated_date": "2025-08-13T20:09:58+00:00",
        "categories": [
            "cs.CV",
            "cs.ET"
        ],
        "authors": [
            "Aaditya Baranwal",
            "Abdul Mueez",
            "Jason Voelker",
            "Guneet Bhatia",
            "Shruti Vyas"
        ],
        "tldr": "The paper introduces SynSpill, a framework leveraging synthetic data for parameter-efficient fine-tuning of VLMs to improve spill detection in industrial settings where real data is scarce, achieving comparable performance to fine-tuned object detectors.",
        "tldr_zh": "该论文介绍了 SynSpill，一个利用合成数据对 VLM 进行参数高效微调的框架，以提高工业环境中溢出检测的性能，在真实数据稀缺的情况下，实现了与微调后的目标检测器相当的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Performance of GPT-5 in Brain Tumor MRI Reasoning",
        "summary": "Accurate differentiation of brain tumor types on magnetic resonance imaging\n(MRI) is critical for guiding treatment planning in neuro-oncology. Recent\nadvances in large language models (LLMs) have enabled visual question answering\n(VQA) approaches that integrate image interpretation with natural language\nreasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and\nGPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor\nSegmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain\nmetastases (MET). Each case included multi-sequence MRI triplanar mosaics and\nstructured clinical features transformed into standardized VQA items. Models\nwere assessed in a zero-shot chain-of-thought setting for accuracy on both\nvisual and reasoning tasks. Results showed that GPT-5-mini achieved the highest\nmacro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),\nand GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single\nmodel dominating across all cohorts. These findings suggest that GPT-5 family\nmodels can achieve moderate accuracy in structured neuro-oncological VQA tasks,\nbut not at a level acceptable for clinical use.",
        "url": "http://arxiv.org/abs/2508.10865v1",
        "published_date": "2025-08-14T17:35:31+00:00",
        "updated_date": "2025-08-14T17:35:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mojtaba Safari",
            "Shansong Wang",
            "Mingzhe Hu",
            "Zach Eidex",
            "Qiang Li",
            "Xiaofeng Yang"
        ],
        "tldr": "The paper evaluates the performance of GPT-4o and GPT-5 family models on a brain tumor visual question answering (VQA) benchmark derived from BraTS datasets, finding moderate accuracy but not at clinically acceptable levels.",
        "tldr_zh": "该论文评估了GPT-4o和GPT-5系列模型在源自BraTS数据集的脑肿瘤视觉问答(VQA)基准测试上的性能，发现准确率适中，但未达到临床可接受的水平。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Retrieval-Augmented Prompt for OOD Detection",
        "summary": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in-the-wild, enabling accurate identification of test\nsamples that differ from the training data distribution. Existing methods rely\non auxiliary outlier samples or in-distribution (ID) data to generate outlier\ninformation for training, but due to limited outliers and their mismatch with\nreal test OOD samples, they often fail to provide sufficient semantic\nsupervision, leading to suboptimal performance. To address this, we propose a\nnovel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP\naugments a pre-trained vision-language model's prompts by retrieving external\nknowledge, offering enhanced semantic supervision for OOD detection. During\ntraining, RAP retrieves descriptive words for outliers based on joint\nsimilarity with external textual knowledge and uses them to augment the model's\nOOD prompts. During testing, RAP dynamically updates OOD prompts in real-time\nbased on the encountered OOD samples, enabling the model to rapidly adapt to\nthe test environment. Our extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance on large-scale OOD detection benchmarks. For\nexample, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the\naverage FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous\nmethods. Additionally, comprehensive ablation studies validate the\neffectiveness of each module and the underlying motivations of our approach.",
        "url": "http://arxiv.org/abs/2508.10556v1",
        "published_date": "2025-08-14T11:52:43+00:00",
        "updated_date": "2025-08-14T11:52:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruisong Han",
            "Zongbo Han",
            "Jiahao Zhang",
            "Mingyue Cheng",
            "Changqing Zhang"
        ],
        "tldr": "The paper introduces Retrieval-Augmented Prompt (RAP), a novel method for Out-of-Distribution (OOD) detection that uses external knowledge retrieval to enhance semantic supervision and improve OOD detection performance in vision-language models.",
        "tldr_zh": "该论文介绍了一种名为检索增强提示（RAP）的新型分布外（OOD）检测方法，该方法利用外部知识检索来增强语义监督，并提高视觉语言模型中的OOD检测性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models",
        "summary": "Text-to-Image (T2I) diffusion models have made significant progress in\ngenerating diverse high-quality images from textual prompts. However, these\nmodels still face challenges in suppressing content that is strongly entangled\nwith specific words. For example, when generating an image of ``Charlie\nChaplin\", a ``mustache\" consistently appears even if explicitly instructed not\nto include it, as the concept of ``mustache\" is strongly entangled with\n``Charlie Chaplin\". To address this issue, we propose a novel approach to\ndirectly suppress such entangled content within the text embedding space of\ndiffusion models. Our method introduces a delta vector that modifies the text\nembedding to weaken the influence of undesired content in the generated image,\nand we further demonstrate that this delta vector can be easily obtained\nthrough a zero-shot approach. Furthermore, we propose a Selective Suppression\nwith Delta Vector (SSDV) method to adapt delta vector into the cross-attention\nmechanism, enabling more effective suppression of unwanted content in regions\nwhere it would otherwise be generated. Additionally, we enabled more precise\nsuppression in personalized T2I models by optimizing delta vector, which\nprevious baselines were unable to achieve. Extensive experimental results\ndemonstrate that our approach significantly outperforms existing methods, both\nin terms of quantitative and qualitative metrics.",
        "url": "http://arxiv.org/abs/2508.10407v1",
        "published_date": "2025-08-14T07:25:48+00:00",
        "updated_date": "2025-08-14T07:25:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Eunseo Koh",
            "Seunghoo Hong",
            "Tae-Young Kim",
            "Simon S. Woo",
            "Jae-Pil Heo"
        ],
        "tldr": "This paper introduces a delta vector approach to modify text embeddings in T2I diffusion models, effectively suppressing unwanted content entangled with specific words, even in personalized models, outperforming existing methods.",
        "tldr_zh": "该论文提出了一种delta向量方法来修改T2I扩散模型中的文本嵌入，有效地抑制了与特定词语纠缠在一起的不需要的内容，即使在个性化模型中也表现出色，优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection",
        "summary": "Driver distraction detection is essential for improving traffic safety and\nreducing road accidents. However, existing models often suffer from degraded\ngeneralization when deployed in real-world scenarios. This limitation primarily\narises from the few-shot learning challenge caused by the high cost of data\nannotation in practical environments, as well as the substantial domain shift\nbetween training datasets and target deployment conditions. To address these\nissues, we propose a Pose-driven Quality-controlled Data Augmentation Framework\n(PQ-DAF) that leverages a vision-language model for sample filtering to\ncost-effectively expand training data and enhance cross-domain robustness.\nSpecifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to\naccurately capture key driver pose features and synthesize diverse training\nexamples. A sample quality assessment module, built upon the CogVLM\nvision-language model, is then introduced to filter out low-quality synthetic\nsamples based on a confidence threshold, ensuring the reliability of the\naugmented dataset. Extensive experiments demonstrate that PQ-DAF substantially\nimproves performance in few-shot driver distraction detection, achieving\nsignificant gains in model generalization under data-scarce conditions.",
        "url": "http://arxiv.org/abs/2508.10397v1",
        "published_date": "2025-08-14T06:54:28+00:00",
        "updated_date": "2025-08-14T06:54:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haibin Sun",
            "Xinghui Song"
        ],
        "tldr": "The paper introduces PQ-DAF, a pose-driven, quality-controlled data augmentation framework using a vision-language model for driver distraction detection in data-scarce scenarios, demonstrating improved generalization performance.",
        "tldr_zh": "该论文提出了 PQ-DAF，一种姿态驱动、质量控制的数据增强框架，使用视觉语言模型来检测数据稀缺场景中的驾驶员分心，并展示了改进的泛化性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs",
        "summary": "As the oldest mature writing system, Oracle Bone Script (OBS) has long posed\nsignificant challenges for archaeological decipherment due to its rarity,\nabstractness, and pictographic diversity. Current deep learning-based methods\nhave made exciting progress on the OBS decipherment task, but existing\napproaches often ignore the intricate connections between glyphs and the\nsemantics of OBS. This results in limited generalization and interpretability,\nespecially when addressing zero-shot settings and undeciphered OBS. To this\nend, we propose an interpretable OBS decipherment method based on Large\nVision-Language Models, which synergistically combines radical analysis and\npictograph-semantic understanding to bridge the gap between glyphs and meanings\nof OBS. Specifically, we propose a progressive training strategy that guides\nthe model from radical recognition and analysis to pictographic analysis and\nmutual analysis, thus enabling reasoning from glyph to meaning. We also design\na Radical-Pictographic Dual Matching mechanism informed by the analysis\nresults, significantly enhancing the model's zero-shot decipherment\nperformance. To facilitate model training, we propose the Pictographic\nDecipherment OBS Dataset, which comprises 47,157 Chinese characters annotated\nwith OBS images and pictographic analysis texts. Experimental results on public\nbenchmarks demonstrate that our approach achieves state-of-the-art Top-10\naccuracy and superior zero-shot decipherment capabilities. More importantly,\nour model delivers logical analysis processes, possibly providing\narchaeologically valuable reference results for undeciphered OBS, and thus has\npotential applications in digital humanities and historical research. The\ndataset and code will be released in https://github.com/PKXX1943/PD-OBS.",
        "url": "http://arxiv.org/abs/2508.10113v1",
        "published_date": "2025-08-13T18:13:32+00:00",
        "updated_date": "2025-08-13T18:13:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaixin Peng",
            "Mengyang Zhao",
            "Haiyang Yu",
            "Teng Fu",
            "Bin Li"
        ],
        "tldr": "This paper presents an interpretable Oracle Bone Script decipherment method using Large Vision-Language Models, incorporating radical and pictographic analysis, achieving SOTA accuracy and zero-shot performance with a newly created dataset.",
        "tldr_zh": "本文提出了一种基于大型视觉语言模型的可解释甲骨文识别方法，该方法结合了部首和象形文字分析，并在新创建的数据集上实现了最先进的准确率和零样本性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model",
        "summary": "Morphing attack detection has become an essential component of face\nrecognition systems for ensuring a reliable verification scenario. In this\npaper, we present a multimodal learning approach that can provide a textual\ndescription of morphing attack detection. We first show that zero-shot\nevaluation of the proposed framework using Contrastive Language-Image\nPretraining (CLIP) can yield not only generalizable morphing attack detection,\nbut also predict the most relevant text snippet. We present an extensive\nanalysis of ten different textual prompts that include both short and long\ntextual prompts. These prompts are engineered by considering the human\nunderstandable textual snippet. Extensive experiments were performed on a face\nmorphing dataset that was developed using a publicly available face biometric\ndataset. We present an evaluation of SOTA pre-trained neural networks together\nwith the proposed framework in the zero-shot evaluation of five different\nmorphing generation techniques that are captured in three different mediums.",
        "url": "http://arxiv.org/abs/2508.10110v1",
        "published_date": "2025-08-13T18:06:29+00:00",
        "updated_date": "2025-08-13T18:06:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sushrut Patwardhan",
            "Raghavendra Ramachandra",
            "Sushma Venkatesh"
        ],
        "tldr": "This paper explores using CLIP for zero-shot morphing attack detection, showing its ability to generalize and predict relevant textual descriptions of attacks.",
        "tldr_zh": "本文探索了使用CLIP进行零样本变形攻击检测，展示了其泛化能力并能预测攻击相关的文本描述。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?",
        "summary": "Prior work has analyzed the robustness of visual encoders to image\ntransformations and corruptions, particularly in cases where such alterations\nare not seen during training. When this occurs, they introduce a form of\ndistribution shift at test time, often leading to performance degradation. The\nprimary focus has been on severe corruptions that, when applied aggressively,\ndistort useful signals necessary for accurate semantic predictions.\n  We take a different perspective by analyzing parameters of the image\nacquisition process and transformations that may be subtle or even\nimperceptible to the human eye. We find that such parameters are systematically\nencoded in the learned visual representations and can be easily recovered. More\nstrikingly, their presence can have a profound impact, either positively or\nnegatively, on semantic predictions. This effect depends on whether there is a\nstrong correlation or anti-correlation between semantic labels and these\nacquisition-based or processing-based labels. Our code and data are available\nat: https://github.com/ryan-caesar-ramos/visual-encoder-traces",
        "url": "http://arxiv.org/abs/2508.10637v1",
        "published_date": "2025-08-14T13:34:13+00:00",
        "updated_date": "2025-08-14T13:34:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ryan Ramos",
            "Vladan Stojnić",
            "Giorgos Kordopatis-Zilos",
            "Yuta Nakashima",
            "Giorgos Tolias",
            "Noa Garcia"
        ],
        "tldr": "This paper investigates how subtle image acquisition and processing parameters are encoded in visual representations learned by models like CLIP, and how these encoded parameters can impact semantic predictions, both positively and negatively.",
        "tldr_zh": "本文研究了诸如CLIP等模型学习的视觉表示中如何编码图像采集和处理的细微参数，以及这些编码参数如何对语义预测产生影响，包括积极和消极的影响。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]