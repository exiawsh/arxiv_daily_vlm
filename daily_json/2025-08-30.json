[
    {
        "title": "DriveQA: Passing the Driving Knowledge Test",
        "summary": "If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks.",
        "url": "http://arxiv.org/abs/2508.21824v1",
        "published_date": "2025-08-29T17:59:53+00:00",
        "updated_date": "2025-08-29T17:59:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maolin Wei",
            "Wanzhou Liu",
            "Eshed Ohn-Bar"
        ],
        "tldr": "The paper introduces DriveQA, a new benchmark for evaluating LLMs and MLLMs on driving knowledge, showing their weaknesses and the benefits of fine-tuning and pretraining for improved performance on driving-related tasks.",
        "tldr_zh": "该论文介绍了DriveQA，一个新的基准测试，用于评估LLMs和MLLMs在驾驶知识方面的能力，揭示了它们的弱点，并展示了微调和预训练对提高驾驶相关任务性能的益处。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VoCap: Video Object Captioning and Segmentation from Any Prompt",
        "summary": "Understanding objects in videos in terms of fine-grained localization masks\nand detailed semantic properties is a fundamental task in video understanding.\nIn this paper, we propose VoCap, a flexible video model that consumes a video\nand a prompt of various modalities (text, box or mask), and produces a\nspatio-temporal masklet with a corresponding object-centric caption. As such\nour model addresses simultaneously the tasks of promptable video object\nsegmentation, referring expression segmentation, and object captioning. Since\nobtaining data for this task is tedious and expensive, we propose to annotate\nan existing large-scale segmentation dataset (SAV) with pseudo object captions.\nWe do so by preprocessing videos with their ground-truth masks to highlight the\nobject of interest and feed this to a large Vision Language Model (VLM). For an\nunbiased evaluation, we collect manual annotations on the validation set. We\ncall the resulting dataset SAV-Caption. We train our VoCap model at scale on a\nSAV-Caption together with a mix of other image and video datasets. Our model\nyields state-of-the-art results on referring expression video object\nsegmentation, is competitive on semi-supervised video object segmentation, and\nestablishes a benchmark for video object captioning. Our dataset will be made\navailable at https://github.com/google-deepmind/vocap.",
        "url": "http://arxiv.org/abs/2508.21809v1",
        "published_date": "2025-08-29T17:43:58+00:00",
        "updated_date": "2025-08-29T17:43:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jasper Uijlings",
            "Xingyi Zhou",
            "Xiuye Gu",
            "Arsha Nagrani",
            "Anurag Arnab",
            "Alireza Fathi",
            "David Ross",
            "Cordelia Schmid"
        ],
        "tldr": "The paper introduces VoCap, a video model that simultaneously performs promptable video object segmentation, referring expression segmentation, and object captioning, trained on a newly created SAV-Caption dataset using pseudo-labels generated by a VLM.",
        "tldr_zh": "该论文介绍了VoCap，一个视频模型，可以同时执行可提示的视频对象分割、指代表达式分割和对象字幕生成，该模型在一个新创建的SAV-Caption数据集上进行训练，该数据集使用VLM生成的伪标签。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
        "summary": "GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.",
        "url": "http://arxiv.org/abs/2508.21767v1",
        "published_date": "2025-08-29T16:40:57+00:00",
        "updated_date": "2025-08-29T16:40:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixiong Zeng",
            "Jing Huang",
            "Liming Zheng",
            "Wenkang Han",
            "Yufeng Zhong",
            "Lei Chen",
            "Longrong Yang",
            "Yingjie Chu",
            "Yuzhi He",
            "Lin Ma"
        ],
        "tldr": "The paper introduces UItron, an open-source foundational model for GUI agents, featuring advanced perception, grounding, and planning capabilities, with a focus on Chinese mobile apps.",
        "tldr_zh": "该论文介绍了UItron，一个开源的GUI代理基础模型，具有先进的感知、理解和规划能力，重点关注中国移动应用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models",
        "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nimpressive capabilities across various multimodal tasks. They continue,\nhowever, to struggle with trivial scenarios such as reading values from Digital\nMeasurement Devices (DMDs), particularly in real-world conditions involving\nclutter, occlusions, extreme viewpoints, and motion blur; common in\nhead-mounted cameras and Augmented Reality (AR) applications. Motivated by\nthese limitations, this work introduces CAD2DMD-SET, a synthetic data\ngeneration tool designed to support visual question answering (VQA) tasks\ninvolving DMDs. By leveraging 3D CAD models, advanced rendering, and\nhigh-fidelity image composition, our tool produces diverse, VQA-labelled\nsynthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present\nDMDBench, a curated validation set of 1,000 annotated real-world images\ndesigned to evaluate model performance under practical constraints.\nBenchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein\nSimilarity (ANLS) and further fine-tuning LoRA's of these models with\nCAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL\nshowcasing a score increase of 200% without degrading on other tasks. This\ndemonstrates that the CAD2DMD-SET training dataset substantially improves the\nrobustness and performance of LVLMs when operating under the previously stated\nchallenging conditions. The CAD2DMD-SET tool is expected to be released as\nopen-source once the final version of this manuscript is prepared, allowing the\ncommunity to add different measurement devices and generate their own datasets.",
        "url": "http://arxiv.org/abs/2508.21732v1",
        "published_date": "2025-08-29T15:57:43+00:00",
        "updated_date": "2025-08-29T15:57:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "João Valente",
            "Atabak Dehban",
            "Rodrigo Ventura"
        ],
        "tldr": "The paper introduces CAD2DMD-SET, a synthetic data generation tool for fine-tuning LVLMs to read digital measurement devices, along with DMDBench, a real-world validation dataset. Fine-tuning with CAD2DMD-SET significantly improves LVLM performance on this task.",
        "tldr_zh": "该论文介绍了CAD2DMD-SET，一个用于微调LVLM以读取数字测量设备的合成数据生成工具，以及DMDBench，一个真实世界的验证数据集。使用CAD2DMD-SET进行微调显著提高了LVLM在该任务上的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images",
        "summary": "Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains.",
        "url": "http://arxiv.org/abs/2508.21565v1",
        "published_date": "2025-08-29T12:21:57+00:00",
        "updated_date": "2025-08-29T12:21:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Juneyoung Ro",
            "Namwoo Kim",
            "Yoonjin Yoon"
        ],
        "tldr": "The paper comparatively studies the performance of VLMs on urban scene understanding, showing that fine-tuning with a synthetic CoT dataset can significantly improve their spatial reasoning abilities, especially for complex queries.",
        "tldr_zh": "该论文对比研究了视觉-语言模型在城市场景理解方面的表现，表明使用合成的CoT数据集进行微调可以显著提高其空间推理能力，尤其是在处理复杂查询时。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification",
        "summary": "While foundation models (FMs) offer strong potential for AI-based dementia\ndiagnosis, their integration into federated learning (FL) systems remains\nunderexplored. In this benchmarking study, we systematically evaluate the\nimpact of key design choices: classification head architecture, fine-tuning\nstrategy, and aggregation method, on the performance and efficiency of\nfederated FM tuning using brain MRI data. Using a large multi-cohort dataset,\nwe find that the architecture of the classification head substantially\ninfluences performance, freezing the FM encoder achieves comparable results to\nfull fine-tuning, and advanced aggregation methods outperform standard\nfederated averaging. Our results offer practical insights for deploying FMs in\ndecentralized clinical settings and highlight trade-offs that should guide\nfuture method development.",
        "url": "http://arxiv.org/abs/2508.21458v1",
        "published_date": "2025-08-29T09:43:02+00:00",
        "updated_date": "2025-08-29T09:43:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaouther Mouheb",
            "Marawan Elbatel",
            "Janne Papma",
            "Geert Jan Biessels",
            "Jurgen Claassen",
            "Huub Middelkoop",
            "Barbara van Munster",
            "Wiesje van der Flier",
            "Inez Ramakers",
            "Stefan Klein",
            "Esther E. Bron"
        ],
        "tldr": "This paper benchmarks federated fine-tuning of SAM-Med3D for MRI-based dementia classification, evaluating different classification head architectures, fine-tuning strategies, and aggregation methods. They find that classification head architecture is important, freezing the encoder works well, and advanced aggregation is beneficial.",
        "tldr_zh": "本文对基于MRI的老年痴呆症分类进行了SAM-Med3D的联邦微调基准测试，评估了不同的分类头架构、微调策略和聚合方法。研究发现分类头架构很重要，冻结编码器效果良好，高级聚合方法有益。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist",
        "summary": "Image captioning is fundamental for applications like video instruction\nsystems and exploration robots, yet deploying such models on local devices is\nchallenging due to the high computational demands of multimodal large language\nmodels (MLLMs). To address this, we first explore lightweight captioning by\nimplementing a specialist based on a 125M-parameter language model, 56 times\nsmaller than LLaMA-7B, and evaluating its performance on both single-sentence\nand detailed captioning tasks. Surprisingly, we find that our model can achieve\nperformance comparable to large multimodal generalists, suggesting its\npotential to serve as a strong visual specialist for on-device applications.\nWhile promising, our model also exhibits a limitation: like other MLLMs, it\nsuffers from visual blindness, occasionally resulting in semantic captioning\nerrors. We carry out toy experiments and investigate the underlying causes,\nwhere we observe that the problems arise from ineffective attention mechanisms\nand limited visual representations. To alleviate them, we develop a novel\ncaptioning framework, Sharp-Eyed Refinement, which enhances caption quality\nthrough improved visual grounding. At its core, our DeepLens extracts detailed\nvisual representations by concentrating on informative regions identified\nduring the initial glance. Our experiments confirm both the advantages of our\nspecialist over prior small captioning models and large generalists and the\neffectiveness of our framework.",
        "url": "http://arxiv.org/abs/2508.21451v1",
        "published_date": "2025-08-29T09:29:27+00:00",
        "updated_date": "2025-08-29T09:29:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junha Song",
            "Yongsik Jo",
            "So Yeon Min",
            "Quanting Xie",
            "Taehwan Kim",
            "Yonatan Bisk",
            "Jaegul Choo"
        ],
        "tldr": "This paper explores lightweight image captioning using a small language model specialist and introduces a Sharp-Eyed Refinement framework to address visual blindness issues, achieving performance comparable to larger generalist models.",
        "tldr_zh": "本文探索了使用小型语言模型专家进行轻量级图像描述，并引入了一个Sharp-Eyed Refinement框架来解决视觉盲点问题，实现了与大型通用模型相媲美的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations",
        "summary": "Evaluating domain generalization (DG) for foundational models like CLIP is\nchallenging, as web-scale pretraining data potentially covers many existing\nbenchmarks. Consequently, current DG evaluation may neither be sufficiently\nchallenging nor adequately test genuinely unseen data scenarios. To better\nassess the performance of CLIP on DG in-the-wild, a scenario where CLIP\nencounters challenging unseen data, we consider two approaches: (1) evaluating\non 33 diverse datasets with quantified out-of-distribution (OOD) scores after\nfine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'\nsome domains as an approximation. We observe that CLIP's performance\ndeteriorates significantly on more OOD datasets. To address this, we present\nCLIP-DCA (Disentangling Classification from enhanced domain Aware\nrepresentations). Our approach is motivated by the observation that while\nstandard domain invariance losses aim to make representations domain-invariant,\nthis can be harmful to foundation models by forcing the discarding of\ndomain-aware representations beneficial for generalization. We instead\nhypothesize that enhancing domain awareness is a prerequisite for effective\ndomain-invariant classification in foundation models. CLIP-DCA identifies and\nenhances domain awareness within CLIP's encoders using a separate domain head\nand synthetically generated diverse domain data. Simultaneously, it encourages\ndomain-invariant classification through disentanglement from the domain\nfeatures. CLIP-DCA shows significant improvements within this challenging\nevaluation compared to existing methods, particularly on datasets that are more\nOOD.",
        "url": "http://arxiv.org/abs/2508.21769v1",
        "published_date": "2025-08-29T16:43:08+00:00",
        "updated_date": "2025-08-29T16:43:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ha Min Son",
            "Zhe Zhao",
            "Shahbaz Rezaei",
            "Xin Liu"
        ],
        "tldr": "The paper introduces CLIP-DCA, a domain generalization method for CLIP that enhances domain awareness and disentangles it from classification, achieving improved performance on challenging out-of-distribution datasets.",
        "tldr_zh": "该论文介绍了CLIP-DCA，一种针对CLIP的领域泛化方法，通过增强领域感知并将领域特征与分类解耦，从而在具有挑战性的分布外数据集中实现了性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting",
        "summary": "Pairwise comparison is often favored over absolute rating or ordinal\nclassification in subjective or difficult annotation tasks due to its improved\nreliability. However, exhaustive comparisons require a massive number of\nannotations (O(n^2)). Recent work has greatly reduced the annotation burden\n(O(n log n)) by actively sampling pairwise comparisons using a sorting\nalgorithm. We further improve annotation efficiency by (1) roughly pre-ordering\nitems using the Contrastive Language-Image Pre-training (CLIP) model\nhierarchically without training, and (2) replacing easy, obvious human\ncomparisons with automated comparisons. The proposed EZ-Sort first produces a\nCLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,\nand finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation\nwas conducted using various datasets: face-age estimation (FGNET), historical\nimage chronology (DHCI), and retinal image quality assessment (EyePACS). It\nshowed that EZ-Sort reduced human annotation cost by 90.5% compared to\nexhaustive pairwise comparisons and by 19.8% compared to prior work (when n =\n100), while improving or maintaining inter-rater reliability. These results\ndemonstrate that combining CLIP-based priors with uncertainty-aware sampling\nyields an efficient and scalable solution for pairwise ranking.",
        "url": "http://arxiv.org/abs/2508.21550v1",
        "published_date": "2025-08-29T12:06:49+00:00",
        "updated_date": "2025-08-29T12:06:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T05, 68T09",
            "I.5.4"
        ],
        "authors": [
            "Yujin Park",
            "Haejun Chung",
            "Ikbeom Jang"
        ],
        "tldr": "The paper introduces EZ-Sort, a method for efficient pairwise comparison using CLIP-based pre-ordering and human-in-the-loop sorting, achieving significant annotation cost reduction while maintaining reliability.",
        "tldr_zh": "该论文介绍了EZ-Sort，一种利用CLIP预排序和人机协同排序的高效配对比较方法，在保持可靠性的同时，显著降低了标注成本。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones",
        "summary": "Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such\nas target matching and navigation. However, the wide field of view and complex\ncompositional semantics in drone scenarios pose challenges for vision-language\nunderstanding. Mainstream Vision-Language Models (VLMs) emphasize global\nalignment while lacking fine-grained semantics, and existing hierarchical\nmethods depend on precise entity partitioning and strict containment, limiting\neffectiveness in dynamic environments. To address this, we propose the\nHierarchical Cross-Granularity Contrastive and Matching learning (HCCM)\nframework with two components: (1) Region-Global Image-Text Contrastive\nLearning (RG-ITC), which avoids precise scene partitioning and captures\nhierarchical local-to-global semantics by contrasting local visual regions with\nglobal text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),\nwhich dispenses with rigid constraints and instead evaluates local semantic\nconsistency within global cross-modal representations, enhancing compositional\nreasoning. Moreover, drone text descriptions are often incomplete or ambiguous,\ndestabilizing alignment. HCCM introduces a Momentum Contrast and Distillation\n(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM\nachieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text\nretrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot\ngeneralization with 39.93% mean recall (mR), outperforming fine-tuned\nbaselines.",
        "url": "http://arxiv.org/abs/2508.21539v1",
        "published_date": "2025-08-29T11:50:24+00:00",
        "updated_date": "2025-08-29T11:50:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Ruan",
            "Jinliang Lin",
            "Yingxin Lai",
            "Zhiming Luo",
            "Shaozi Li"
        ],
        "tldr": "The paper introduces HCCM, a hierarchical cross-granularity contrastive and matching learning framework for natural language-guided drones that improves vision-language understanding by using region-global contrastive learning and matching while addressing the issue of incomplete drone text descriptions with a momentum contrast and distillation mechanism.",
        "tldr_zh": "该论文介绍了HCCM，一个用于自然语言引导无人机的分层跨粒度对比和匹配学习框架。它通过使用区域-全局对比学习和匹配来提高视觉-语言理解能力，同时通过动量对比和蒸馏机制解决无人机文本描述不完整的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding",
        "summary": "Video multimodal large language models (Video-MLLMs) have achieved remarkable\nprogress in video understanding. However, they remain vulnerable to\nhallucination-producing content inconsistent with or unrelated to video inputs.\nPrevious video hallucination benchmarks primarily focus on short-videos. They\nattribute hallucinations to factors such as strong language priors, missing\nframes, or vision-language biases introduced by the visual encoder. While these\ncauses indeed account for most hallucinations in short videos, they still\noversimplify the cause of hallucinations. Sometimes, models generate incorrect\noutputs but with correct frame-level semantics. We refer to this type of\nhallucination as Semantic Aggregation Hallucination (SAH), which arises during\nthe process of aggregating frame-level semantics into event-level semantic\ngroups. Given that SAH becomes particularly critical in long videos due to\nincreased semantic complexity across multiple events, it is essential to\nseparate and thoroughly investigate the causes of this type of hallucination.\nTo address the above issues, we introduce ELV-Halluc, the first benchmark\ndedicated to long-video hallucination, enabling a systematic investigation of\nSAH. Our experiments confirm the existence of SAH and show that it increases\nwith semantic complexity. Additionally, we find that models are more prone to\nSAH on rapidly changing semantics. Moreover, we discuss potential approaches to\nmitigate SAH. We demonstrate that positional encoding strategy contributes to\nalleviating SAH, and further adopt DPO strategy to enhance the model's ability\nto distinguish semantics within and across events. To support this, we curate a\ndataset of 8K adversarial data pairs and achieve improvements on both\nELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.",
        "url": "http://arxiv.org/abs/2508.21496v1",
        "published_date": "2025-08-29T10:25:03+00:00",
        "updated_date": "2025-08-29T10:25:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hao Lu",
            "Jiahao Wang",
            "Yaolun Zhang",
            "Ruohui Wang",
            "Xuanyu Zheng",
            "Yepeng Tang",
            "Dahua Lin",
            "Lewei Lu"
        ],
        "tldr": "The paper introduces ELV-Halluc, a new benchmark for evaluating Semantic Aggregation Hallucinations (SAH) in long video understanding with Video-MLLMs, and proposes methods to mitigate SAH using positional encoding and DPO.",
        "tldr_zh": "该论文介绍了ELV-Halluc，一个新的基准，用于评估视频多模态大语言模型（Video-MLLMs）在长视频理解中出现的语义聚合幻觉（SAH），并提出了利用位置编码和DPO来缓解SAH的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China",
        "summary": "With the deepening of poverty alleviation and rural revitalization\nstrategies, improving the rural living environment and enhancing the quality of\nlife have become key priorities. Rural livability is a key indicator for\nmeasuring the effectiveness of these efforts. Current measurement approaches\nface significant limitations, as questionnaire-based methods are difficult to\nscale, while urban-oriented visual perception methods are poorly suited for\nrural contexts. In this paper, a rural-specific livability assessment framework\nwas proposed based on drone imagery and multimodal large language models\n(MLLMs). To comprehensively assess village livability, this study first used a\ntop-down approach to collect large-scale drone imagery of 1,766 villages in 146\ncounties across China. In terms of the model framework, an efficient image\ncomparison mechanism was developed, incorporating binary search interpolation\nto determine effective image pairs while reducing comparison iterations.\nBuilding on expert knowledge, a chain-of-thought prompting suitable for\nnationwide rural livability measurement was constructed, considering both\nliving quality and ecological habitability dimensions. This approach enhanced\nthe rationality and reliability of the livability assessment. Finally, this\nstudy characterized the spatial heterogeneity of rural livability across China\nand thoroughly analyzed its influential factors. The results show that: (1) The\nrural livability in China demonstrates a dual-core-periphery spatial pattern,\nradiating outward from Sichuan and Zhejiang provinces with declining gradients;\n(2) Among various influential factors, government fiscal expenditure emerged as\nthe core determinant, with each unit increase corresponding to a 3.9 - 4.9 unit\nenhancement in livability. The findings provide valuable insights for rural\nconstruction policy-making.",
        "url": "http://arxiv.org/abs/2508.21738v1",
        "published_date": "2025-08-29T16:04:06+00:00",
        "updated_date": "2025-08-29T16:04:06+00:00",
        "categories": [
            "cs.CY",
            "cs.CV"
        ],
        "authors": [
            "Weihuan Deng",
            "Yaofu Huang",
            "Luan Chen",
            "Xun Li",
            "Yao Yao"
        ],
        "tldr": "This paper proposes an AI-powered framework using drone imagery and multimodal large language models to assess rural livability in China, revealing spatial patterns and key influential factors.",
        "tldr_zh": "本文提出了一种基于无人机图像和多模态大型语言模型的人工智能驱动框架，用于评估中国农村的宜居性，揭示了空间格局和关键影响因素。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Morae: Proactively Pausing UI Agents for User Choices",
        "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences.",
        "url": "http://arxiv.org/abs/2508.21456v1",
        "published_date": "2025-08-29T09:39:00+00:00",
        "updated_date": "2025-08-29T09:39:00+00:00",
        "categories": [
            "cs.HC",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yi-Hao Peng",
            "Dingzeyu Li",
            "Jeffrey P. Bigham",
            "Amy Pavel"
        ],
        "tldr": "The paper introduces Morae, a UI agent designed to proactively pause during task execution to allow blind and low-vision users to make informed choices, improving task completion and user satisfaction compared to existing agents.",
        "tldr_zh": "该论文介绍了Morae，一个UI代理，旨在在任务执行过程中主动暂停，让视障用户做出明智的选择，从而与现有代理相比，提高任务完成度和用户满意度。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 6
    },
    {
        "title": "Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?",
        "summary": "The reasoning abilities of Multimodal Large Language Models (MLLMs) have\ngarnered a lot of attention in recent times, with advances made in frontiers\nlike coding, mathematics, and science. However, very limited experiments have\nbeen done to assess their performance in simple perception tasks performed over\nuncontaminated, generated images containing basic shapes and structures. To\naddress this issue, the paper introduces a dataset, Percept-V, containing a\ntotal of 7200 program-generated images equally divided into 30 categories, each\ntesting a combination of visual perception skills. Unlike previously proposed\ndatasets, Percept-V comprises very basic tasks of varying complexity that test\nthe perception abilities of MLLMs. This dataset is then tested on\nstate-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large\nReasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their\nperformance. Contrary to the evidence that MLLMs excel in many complex tasks,\nour experiments show a significant drop in the models' performance with\nincreasing problem complexity across all categories. An analysis of the\nperformances also reveals that the tested MLLMs exhibit a similar trend in\naccuracy across categories, testing a particular cognitive skill and find some\nskills to be more difficult than others.",
        "url": "http://arxiv.org/abs/2508.21143v1",
        "published_date": "2025-08-28T18:22:38+00:00",
        "updated_date": "2025-08-28T18:22:38+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Samrajnee Ghosh",
            "Naman Agarwal",
            "Hemanshu Garg",
            "Chinmay Mittal",
            "Mausam",
            "Parag Singla"
        ],
        "tldr": "This paper introduces Percept-V, a dataset of basic visual perception tasks, and evaluates the performance of state-of-the-art MLLMs and LRMs on it, revealing a performance drop with increasing complexity.",
        "tldr_zh": "本文介绍了一个名为 Percept-V 的基础视觉感知任务数据集，并评估了最先进的多模态大语言模型（MLLM）和大型推理模型（LRM）在该数据集上的性能，结果表明随着复杂性的增加，性能会下降。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]