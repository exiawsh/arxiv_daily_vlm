[
    {
        "title": "SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation",
        "summary": "State-of-the-art text-to-image models produce visually impressive results but\noften struggle with precise alignment to text prompts, leading to missing\ncritical elements or unintended blending of distinct concepts. We propose a\nnovel approach that learns a high-success-rate distribution conditioned on a\ntarget prompt, ensuring that generated images faithfully reflect the\ncorresponding prompts. Our method explicitly models the signal component during\nthe denoising process, offering fine-grained control that mitigates\nover-optimization and out-of-distribution artifacts. Moreover, our framework is\ntraining-free and seamlessly integrates with both existing diffusion and flow\nmatching architectures. It also supports additional conditioning modalities --\nsuch as bounding boxes -- for enhanced spatial alignment. Extensive experiments\ndemonstrate that our approach outperforms current state-of-the-art methods. The\ncode is available at https://github.com/grimalPaul/gsn-factory.",
        "url": "http://arxiv.org/abs/2508.13866v1",
        "published_date": "2025-08-19T14:31:15+00:00",
        "updated_date": "2025-08-19T14:31:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Paul Grimal",
            "Michaël Soumm",
            "Hervé Le Borgne",
            "Olivier Ferret",
            "Akihiro Sugimoto"
        ],
        "tldr": "This paper introduces SAGA, a training-free method to improve text-to-image generation by modeling the signal component during denoising, leading to better text alignment and fewer artifacts.",
        "tldr_zh": "本文介绍了一种名为SAGA的免训练方法，通过在去噪过程中对信号分量进行建模来改善文本到图像的生成，从而实现更好的文本对齐和更少的伪影。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports",
        "summary": "We introduce Med-CTX, a fully transformer based multimodal framework for\nexplainable breast cancer ultrasound segmentation. We integrate clinical\nradiology reports to boost both performance and interpretability. Med-CTX\nachieves exact lesion delineation by using a dual-branch visual encoder that\ncombines ViT and Swin transformers, as well as uncertainty aware fusion.\nClinical language structured with BI-RADS semantics is encoded by\nBioClinicalBERT and combined with visual features utilising cross-modal\nattention, allowing the model to provide clinically grounded, model generated\nexplanations. Our methodology generates segmentation masks, uncertainty maps,\nand diagnostic rationales all at once, increasing confidence and transparency\nin computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice\nscore of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and\nSwin. Clinical text plays a key role in segmentation accuracy and explanation\nquality, as evidenced by ablation studies that show a -5.4% decline in Dice\nscore and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP\nscore: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new\nbar for trustworthy, multimodal medical architecture.",
        "url": "http://arxiv.org/abs/2508.13796v1",
        "published_date": "2025-08-19T12:55:10+00:00",
        "updated_date": "2025-08-19T12:55:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Enobong Adahada",
            "Isabel Sassoon",
            "Kate Hone",
            "Yongmin Li"
        ],
        "tldr": "Med-CTX, a fully transformer-based multimodal framework, integrates radiology reports for explainable breast cancer ultrasound segmentation, achieving state-of-the-art performance and interpretability through cross-modal attention and uncertainty-aware fusion.",
        "tldr_zh": "Med-CTX是一个完全基于Transformer的多模态框架，它集成了放射学报告，用于可解释的乳腺癌超声分割。通过跨模态注意力和不确定性感知融合，实现了最先进的性能和可解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks",
        "summary": "Large Vision-Language Models (LVLMs) demonstrate strong performance on\nsingle-image tasks. However, we observe that their performance degrades\nsignificantly when handling multi-image inputs. This occurs because visual cues\nfrom different images become entangled in the model's output. We refer to this\nphenomenon as cross-image information leakage. To address this issue, we\npropose FOCUS, a training-free and architecture-agnostic decoding strategy that\nmitigates cross-image information leakage during inference. FOCUS sequentially\nmasks all but one image with random noise, guiding the model to focus on the\nsingle clean image. We repeat this process across all target images to obtain\nlogits under partially masked contexts. These logits are aggregated and then\ncontrastively refined using a noise-only reference input, which suppresses the\nleakage and yields more accurate outputs. FOCUS consistently improves\nperformance across four multi-image benchmarks and diverse LVLM families. This\ndemonstrates that FOCUS offers a general and practical solution for enhancing\nmulti-image reasoning without additional training or architectural\nmodifications.",
        "url": "http://arxiv.org/abs/2508.13744v1",
        "published_date": "2025-08-19T11:31:39+00:00",
        "updated_date": "2025-08-19T11:31:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yeji Park",
            "Minyoung Lee",
            "Sanghyuk Chun",
            "Junsuk Choe"
        ],
        "tldr": "The paper introduces FOCUS, a training-free inference strategy to mitigate cross-image information leakage in LVLMs for multi-image tasks, improving performance without additional training.",
        "tldr_zh": "该论文介绍了一种名为FOCUS的免训练推理策略，用于缓解LVLMs在多图像任务中的跨图像信息泄漏问题，无需额外训练即可提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance",
        "summary": "Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT.",
        "url": "http://arxiv.org/abs/2508.13739v1",
        "published_date": "2025-08-19T11:23:09+00:00",
        "updated_date": "2025-08-19T11:23:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Cao",
            "Yanjie Li",
            "Kaisheng Liang",
            "Yuni Lai",
            "Bin Xiao"
        ],
        "tldr": "The paper introduces IPGA, a novel targeted adversarial attack method for VLMs that leverages the intermediate projector module (Q-Former) to achieve more fine-grained control over adversarial perturbations, demonstrating improved attack effectiveness and transferability across different VLMs.",
        "tldr_zh": "该论文介绍了一种名为IPGA的新型VLM目标对抗攻击方法，该方法利用中间投影模块（Q-Former）对对抗扰动实现更精细的控制，从而提高了攻击效果和在不同VLM上的可迁移性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical Vision-Language Retrieval of Educational Metaverse Content in Agriculture",
        "summary": "Every day, a large amount of educational content is uploaded online across\ndifferent areas, including agriculture and gardening. When these videos or\nmaterials are grouped meaningfully, they can make learning easier and more\neffective. One promising way to organize and enrich such content is through the\nMetaverse, which allows users to explore educational experiences in an\ninteractive and immersive environment. However, searching for relevant\nMetaverse scenarios and finding those matching users' interests remains a\nchallenging task. A first step in this direction has been done recently, but\nexisting datasets are small and not sufficient for training advanced models. In\nthis work, we make two main contributions: first, we introduce a new dataset\ncontaining 457 agricultural-themed virtual museums (AgriMuseums), each enriched\nwith textual descriptions; and second, we propose a hierarchical\nvision-language model to represent and retrieve relevant AgriMuseums using\nnatural language queries. In our experimental setting, the proposed method\nachieves up to about 62\\% R@1 and 78\\% MRR, confirming its effectiveness, and\nit also leads to improvements on existing benchmarks by up to 6\\% R@1 and 11\\%\nMRR. Moreover, an extensive evaluation validates our design choices. Code and\ndataset are available at\nhttps://github.com/aliabdari/Agricultural_Metaverse_Retrieval .",
        "url": "http://arxiv.org/abs/2508.13713v1",
        "published_date": "2025-08-19T10:19:31+00:00",
        "updated_date": "2025-08-19T10:19:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ali Abdari",
            "Alex Falcon",
            "Giuseppe Serra"
        ],
        "tldr": "This paper introduces a new agricultural-themed virtual museum dataset (AgriMuseums) and proposes a hierarchical vision-language model for retrieving relevant AgriMuseums based on natural language queries, demonstrating improved retrieval performance.",
        "tldr_zh": "本文介绍了一个新的农业主题虚拟博物馆数据集 (AgriMuseums)，并提出了一种分层视觉-语言模型，用于根据自然语言查询检索相关的 AgriMuseums，实验结果表明检索性能得到了提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes",
        "summary": "The aspiration for artificial general intelligence, fueled by the rapid\nprogress of multimodal models, demands human-comparable performance across\ndiverse environments. We propose HumanPCR, an evaluation suite for probing\nMLLMs' capacity about human-related visual contexts across three hierarchical\nlevels: Perception, Comprehension, and Reasoning (denoted by Human-P, Human-C,\nand Human-R, respectively). Human-P and Human-C feature over 6,000\nhuman-verified multiple choice questions, assessing massive tasks of 9\ndimensions, including but not limited to essential skills frequently overlooked\nby existing benchmarks. Human-R offers a challenging manually curated video\nreasoning test that requires integrating multiple visual evidences, proactively\nextracting context beyond question cues, and applying human-like expertise.\nEach question includes human-annotated Chain-of-Thought (CoT) rationales with\nkey visual evidence to support further research. Extensive evaluations on over\n30 state-of-the-art models exhibit significant challenges in human-centric\nvisual understanding, particularly in tasks involving detailed space\nperception, temporal understanding, and mind modeling. Moreover, analysis of\nHuman-R reveals the struggle of models in extracting essential proactive visual\nevidence from diverse human scenes and their faulty reliance on query-guided\nretrieval. Even with advanced techniques like scaling visual contexts and\ntest-time thinking yield only limited benefits. We hope HumanPCR and our\nfindings will advance the development, evaluation, and human-centric\napplication of multimodal models.",
        "url": "http://arxiv.org/abs/2508.13692v1",
        "published_date": "2025-08-19T09:52:04+00:00",
        "updated_date": "2025-08-19T09:52:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keliang Li",
            "Hongze Shen",
            "Hao Shi",
            "Ruibing Hou",
            "Hong Chang",
            "Jie Huang",
            "Chenghao Jia",
            "Wen Wang",
            "Yiling Wu",
            "Dongmei Jiang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "tldr": "The paper introduces HumanPCR, a new benchmark for evaluating MLLMs' ability to understand human-centric visual contexts across perception, comprehension, and reasoning, revealing limitations in current models.",
        "tldr_zh": "本文介绍了一个新的基准测试 HumanPCR，用于评估 MLLM 在理解以人为中心的视觉环境方面的能力，涵盖感知、理解和推理三个方面，揭示了当前模型的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models",
        "summary": "Vision-language models (VLMs) have emerged as powerful tools for enabling\nautomated traffic analysis; however, current approaches often demand\nsubstantial computational resources and struggle with fine-grained\nspatio-temporal understanding. This paper introduces STER-VLM, a\ncomputationally efficient framework that enhances VLM performance through (1)\ncaption decomposition to tackle spatial and temporal information separately,\n(2) temporal frame selection with best-view filtering for sufficient temporal\ninformation, and (3) reference-driven understanding for capturing fine-grained\nmotion and dynamic context and (4) curated visual/textual prompt techniques.\nExperimental results on the WTS \\cite{kong2024wts} and BDD \\cite{BDD} datasets\ndemonstrate substantial gains in semantic richness and traffic scene\ninterpretation. Our framework is validated through a decent test score of\n55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in\nadvancing resource-efficient and accurate traffic analysis for real-world\napplications.",
        "url": "http://arxiv.org/abs/2508.13470v1",
        "published_date": "2025-08-19T03:03:29+00:00",
        "updated_date": "2025-08-19T03:03:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tinh-Anh Nguyen-Nhu",
            "Triet Dao Hoang Minh",
            "Dat To-Thanh",
            "Phuc Le-Gia",
            "Tuan Vo-Lan",
            "Tien-Huy Nguyen"
        ],
        "tldr": "STER-VLM is a computationally efficient VLM framework for traffic analysis that enhances performance through caption decomposition, temporal frame selection, reference-driven understanding, and curated prompts, achieving gains in semantic richness and traffic scene interpretation.",
        "tldr_zh": "STER-VLM是一个计算高效的VLM框架，用于交通分析。它通过分解字幕，选择时间帧，参考驱动理解以及精心策划的提示来提高性能，从而在语义丰富性和交通场景理解方面取得进展。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference",
        "summary": "Comprehensive highway scene understanding and robust traffic risk inference\nare vital for advancing Intelligent Transportation Systems (ITS) and autonomous\ndriving. Traditional approaches often struggle with scalability and\ngeneralization, particularly under the complex and dynamic conditions of\nreal-world environments. To address these challenges, we introduce a novel\nstructured prompting and knowledge distillation framework that enables\nautomatic generation of high-quality traffic scene annotations and contextual\nrisk assessments. Our framework orchestrates two large Vision-Language Models\n(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy\nto produce rich, multi-perspective outputs. These outputs serve as\nknowledge-enriched pseudo-annotations for supervised fine-tuning of a much\nsmaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision\nfor Intelligent Scene and Traffic Analysis), is capable of understanding\nlow-resolution traffic videos and generating semantically faithful, risk-aware\ncaptions. Despite its significantly reduced parameter count, VISTA achieves\nstrong performance across established captioning metrics (BLEU-4, METEOR,\nROUGE-L, and CIDEr) when benchmarked against its teacher models. This\ndemonstrates that effective knowledge distillation and structured multi-agent\nsupervision can empower lightweight VLMs to capture complex reasoning\ncapabilities. The compact architecture of VISTA facilitates efficient\ndeployment on edge devices, enabling real-time risk monitoring without\nrequiring extensive infrastructure upgrades.",
        "url": "http://arxiv.org/abs/2508.13439v1",
        "published_date": "2025-08-19T01:44:02+00:00",
        "updated_date": "2025-08-19T01:44:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "eess.IV"
        ],
        "authors": [
            "Yunxiang Yang",
            "Ningning Xu",
            "Jidong J. Yang"
        ],
        "tldr": "The paper introduces VISTA, a compact 3B-scale VLM for traffic video understanding and risk assessment, achieved through structured prompting and multi-agent knowledge distillation from larger VLMs (GPT-4o and o3-mini). It demonstrates strong performance with efficient edge deployment.",
        "tldr_zh": "该论文介绍了一种紧凑的30亿参数规模的VLM——VISTA，用于交通视频理解和风险评估。该模型通过结构化提示和多智能体知识蒸馏从更大的VLM（GPT-4o 和 o3-mini）中获得。VISTA性能优越，且能够在边缘设备上高效部署。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Easy Option Bias in Multiple-Choice Question Answering",
        "summary": "In this early study, we observe an Easy-Options Bias (EOB) issue in some\nmultiple-choice Visual Question Answering (VQA) benchmarks such as MMStar,\nRealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias\nallows vision-language models (VLMs) to select the correct answer using only\nthe vision (V) and options (O) as inputs, without the need for the question\n(Q). Through grounding experiments, we attribute the bias to an imbalance in\nvisual relevance: the correct answer typically aligns more closely with the\nvisual contents than the negative options in feature space, creating a shortcut\nfor VLMs to infer the answer via simply vision-option similarity matching. To\nfix this, we introduce GroundAttack, a toolkit that automatically generates\nhard negative options as visually plausible as the correct answer. We apply it\nto the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these\nEOB-free annotations, current VLMs approach to random accuracies under (V+O)\nsettings, and drop to non-saturated accuracies under (V+Q+O) settings,\nproviding a more realistic evaluation of VLMs' QA ability. Codes and new\nannotations will be released soon.",
        "url": "http://arxiv.org/abs/2508.13428v1",
        "published_date": "2025-08-19T01:03:45+00:00",
        "updated_date": "2025-08-19T01:03:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Hao Zhang",
            "Chen Li",
            "Basura Fernando"
        ],
        "tldr": "This paper identifies and addresses an 'Easy-Options Bias' (EOB) in VQA benchmarks where models can answer correctly using only visual and option inputs. They propose a toolkit, GroundAttack, to generate hard negative options, creating more robust VQA datasets.",
        "tldr_zh": "该论文发现并解决了VQA基准测试中的“简单选项偏差”（EOB）问题，即模型仅使用视觉和选项输入即可正确回答。他们提出了一个名为GroundAttack的工具包，用于生成更难的负面选项，从而创建更强大的VQA数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving",
        "summary": "Vision-Language Models (VLMs) have emerged as a promising paradigm in\nautonomous driving (AD), offering a unified framework for perception,\nreasoning, and decision-making by jointly modeling visual inputs and natural\nlanguage instructions. However, their deployment is hindered by the significant\ncomputational overhead incurred when processing high-resolution, multi-view\nimages, a standard setup in AD systems with six or more synchronized cameras.\nThis overhead stems from the large number of visual tokens generated during\nencoding, increasing inference latency and memory consumption due to the\nquadratic complexity of self-attention. To address these challenges, we propose\nPrune2Drive, a plug-and-play visual token pruning framework for multi-view VLMs\nin autonomous driving. Prune2Drive introduces two core innovations: (i) a\ndiversity-aware token selection mechanism inspired by farthest point sampling,\nwhich prioritizes semantic and spatial coverage across views rather than\nrelying solely on attention scores, and (ii) a view-adaptive pruning controller\nthat learns optimal pruning ratios for each camera view based on their\nimportance to downstream driving tasks. Unlike prior methods, Prune2Drive does\nnot require model retraining or access to attention maps, making it compatible\nwith modern efficient attention implementations. Extensive experiments on two\nlarge-scale multi-view driving benchmarks, DriveLM and DriveLMM-o1, show that\nPrune2Drive achieves significant speedups and memory savings while maintaining\nor improving task performance. When retaining only 10% of the visual tokens,\nour method achieves a 6.40$\\times$ speedup in the prefilling phase and consumes\n13.4% of the original FLOPs, with only a 3% performance drop on the DriveLM\nbenchmark.",
        "url": "http://arxiv.org/abs/2508.13305v1",
        "published_date": "2025-08-18T18:47:26+00:00",
        "updated_date": "2025-08-18T18:47:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minhao Xiong",
            "Zichen Wen",
            "Zhuangcheng Gu",
            "Xuyang Liu",
            "Rui Zhang",
            "Hengrui Kang",
            "Jiabing Yang",
            "Junyuan Zhang",
            "Weijia Li",
            "Conghui He",
            "Yafei Wang",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces Prune2Drive, a plug-and-play framework for accelerating multi-view vision-language models in autonomous driving by pruning visual tokens without retraining, achieving significant speedups and memory savings.",
        "tldr_zh": "该论文介绍了 Prune2Drive，一个即插即用的框架，通过修剪视觉标记来加速自动驾驶中的多视角视觉语言模型，无需重新训练，从而显著提高速度和节省内存。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models",
        "summary": "Recently, multimodal large language models (MLLMs) have achieved significant\nadvancements across various domains, and corresponding evaluation benchmarks\nhave been continuously refined and improved. In this process, benchmarks in the\nscientific domain have played an important role in assessing the reasoning\ncapabilities of MLLMs. However, existing benchmarks still face three key\nchallenges: 1) Insufficient evaluation of models' reasoning abilities in\nmultilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive\nmodality coverage; 3) Lack of fine-grained annotation of scientific knowledge\npoints. To address these gaps, we propose MME-SCI, a comprehensive and\nchallenging benchmark. We carefully collected 1,019 high-quality\nquestion-answer pairs, which involve 3 distinct evaluation modes. These pairs\ncover four subjects, namely mathematics, physics, chemistry, and biology, and\nsupport five languages: Chinese, English, French, Spanish, and Japanese. We\nconducted extensive experiments on 16 open-source models and 4 closed-source\nmodels, and the results demonstrate that MME-SCI is widely challenging for\nexisting MLLMs. For instance, under the Image-only evaluation mode, o4-mini\nachieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,\nphysics, chemistry, and biology, respectively, indicating a significantly\nhigher difficulty level compared to existing benchmarks. More importantly,\nusing MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed\nexisting models' performance in depth and identified their weaknesses in\nspecific domains. The Data and Evaluation Code are available at\nhttps://github.com/JCruan519/MME-SCI.",
        "url": "http://arxiv.org/abs/2508.13938v1",
        "published_date": "2025-08-19T15:27:55+00:00",
        "updated_date": "2025-08-19T15:27:55+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Jiacheng Ruan",
            "Dan Jiang",
            "Xian Gao",
            "Ting Liu",
            "Yuzhuo Fu",
            "Yangyang Kang"
        ],
        "tldr": "The paper introduces MME-SCI, a new challenging multimodal science benchmark designed to evaluate MLLMs' reasoning abilities across multiple languages and scientific domains with fine-grained knowledge annotation.",
        "tldr_zh": "该论文介绍了MME-SCI，一个新的具有挑战性的多模态科学基准，旨在评估MLLM在多语言和科学领域中的推理能力，并具有细粒度的知识注释。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction",
        "summary": "With the growing demand for short videos and personalized content, automated\nVideo Log (Vlog) generation has become a key direction in multimodal content\ncreation. Existing methods mostly rely on predefined scripts, lacking dynamism\nand personal expression. Therefore, there is an urgent need for an automated\nVlog generation approach that enables effective multimodal collaboration and\nhigh personalization. To this end, we propose PersonaVlog, an automated\nmultimodal stylized Vlog generation framework that can produce personalized\nVlogs featuring videos, background music, and inner monologue speech based on a\ngiven theme and reference image. Specifically, we propose a multi-agent\ncollaboration framework based on Multimodal Large Language Models (MLLMs). This\nframework efficiently generates high-quality prompts for multimodal content\ncreation based on user input, thereby improving the efficiency and creativity\nof the process. In addition, we incorporate a feedback and rollback mechanism\nthat leverages MLLMs to evaluate and provide feedback on generated results,\nthereby enabling iterative self-correction of multimodal content. We also\npropose ThemeVlogEval, a theme-based automated benchmarking framework that\nprovides standardized metrics and datasets for fair evaluation. Comprehensive\nexperiments demonstrate the significant advantages and potential of our\nframework over several baselines, highlighting its effectiveness and great\npotential for generating automated Vlogs.",
        "url": "http://arxiv.org/abs/2508.13602v1",
        "published_date": "2025-08-19T08:10:43+00:00",
        "updated_date": "2025-08-19T08:10:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaolu Hou",
            "Bing Ma",
            "Jiaxiang Cheng",
            "Xuhua Ren",
            "Kai Yu",
            "Wenyue Li",
            "Tianxiang Zheng",
            "Qinglin Lu"
        ],
        "tldr": "The paper introduces PersonaVlog, a multi-agent collaboration framework using MLLMs for personalized and stylized Vlog generation with iterative self-correction, and includes a new evaluation benchmark.",
        "tldr_zh": "该论文介绍了PersonaVlog，一个使用多模态大型语言模型的多智能体协作框架，用于生成具有迭代自我校正功能的个性化和风格化的Vlog，并包含一个新的评估基准。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model",
        "summary": "Referring Video Object Segmentation (RVOS) aims to segment specific objects\nin a video according to textual descriptions. We observe that recent RVOS\napproaches often place excessive emphasis on feature extraction and temporal\nmodeling, while relatively neglecting the design of the segmentation head. In\nfact, there remains considerable room for improvement in segmentation head\ndesign. To address this, we propose a Temporal-Conditional Referring Video\nObject Segmentation model, which innovatively integrates existing segmentation\nmethods to effectively enhance boundary segmentation capability. Furthermore,\nour model leverages a text-to-video diffusion model for feature extraction. On\ntop of this, we remove the traditional noise prediction module to avoid the\nrandomness of noise from degrading segmentation accuracy, thereby simplifying\nthe model while improving performance. Finally, to overcome the limited feature\nextraction capability of the VAE, we design a Temporal Context Mask Refinement\n(TCMR) module, which significantly improves segmentation quality without\nintroducing complex designs. We evaluate our method on four public RVOS\nbenchmarks, where it consistently achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2508.13584v1",
        "published_date": "2025-08-19T07:36:04+00:00",
        "updated_date": "2025-08-19T07:36:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruixin Zhang",
            "Jiaqing Fan",
            "Yifan Liao",
            "Qian Qiao",
            "Fanzhang Li"
        ],
        "tldr": "This paper introduces a novel RVOS model leveraging a simplified text-to-video diffusion model and a Temporal Context Mask Refinement module to improve segmentation accuracy and achieve state-of-the-art performance on RVOS benchmarks.",
        "tldr_zh": "本文提出了一种新的RVOS模型，该模型利用简化的文本到视频扩散模型和时间上下文掩码细化模块来提高分割精度，并在RVOS基准测试中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models",
        "summary": "Facial Emotion Recognition (FER) is crucial for applications such as\nhuman-computer interaction and mental health diagnostics. This study presents\nthe first empirical comparison of open-source Vision-Language Models (VLMs),\nincluding Phi-3.5 Vision and CLIP, against traditional deep learning models\nVGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset,\nwhich contains 35,887 low-resolution grayscale images across seven emotion\nclasses. To address the mismatch between VLM training assumptions and the noisy\nnature of FER data, we introduce a novel pipeline that integrates GFPGAN-based\nimage restoration with FER evaluation. Results show that traditional models,\nparticularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly\noutperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting\nthe limitations of VLMs in low-quality visual tasks. In addition to performance\nevaluation using precision, recall, F1-score, and accuracy, we provide a\ndetailed computational cost analysis covering preprocessing, training,\ninference, and evaluation phases, offering practical insights for deployment.\nThis work underscores the need for adapting VLMs to noisy environments and\nprovides a reproducible benchmark for future research in emotion recognition.",
        "url": "http://arxiv.org/abs/2508.13524v1",
        "published_date": "2025-08-19T05:33:10+00:00",
        "updated_date": "2025-08-19T05:33:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Vamsi Krishna Mulukutla",
            "Sai Supriya Pavarala",
            "Srinivasa Raju Rudraraju",
            "Sridevi Bonthu"
        ],
        "tldr": "This paper compares open-source VLMs against traditional deep learning models for facial emotion recognition (FER) on the FER-2013 dataset, finding that traditional models outperform VLMs in this low-quality image task.",
        "tldr_zh": "本文比较了开源的视觉语言模型与传统深度学习模型在 FER-2013 数据集上的面部表情识别 (FER) 性能，发现传统模型在这个低质量图像任务中优于视觉语言模型。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency",
        "summary": "Despite the fast progress of deep learning, one standing challenge is the gap\nof the observed training samples and the underlying true distribution. There\nare multiple reasons for the causing of this gap e.g. sampling bias, noise etc.\nIn the era of foundation models, we show that when leveraging the off-the-shelf\n(vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the\ngeometric shapes of the resulting feature distributions exhibit remarkable\ntransferability across domains and datasets. To verify its practical\nusefulness, we embody our geometric knowledge-guided distribution calibration\nframework in two popular and challenging settings: federated learning and\nlong-tailed recognition. In the federated setting, we devise a technique of\nacquiring the global geometric shape under privacy constraints, then leverage\nthis knowledge to generate new samples for clients, in the aim of bridging the\ngap between local and global observations. In long-tailed learning, it utilizes\nthe geometric knowledge transferred from sample-rich categories to recover the\ntrue distribution for sample-scarce tail classes. Comprehensive experiments\nshow that our proposed geometric knowledge-guided distribution calibration\neffectively overcomes information deficits caused by data heterogeneity and\nsample imbalance, with boosted performance across benchmarks.",
        "url": "http://arxiv.org/abs/2508.13518v1",
        "published_date": "2025-08-19T05:22:59+00:00",
        "updated_date": "2025-08-19T05:22:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yanbiao Ma",
            "Wei Dai",
            "Bowei Liu",
            "Jiayi Chen",
            "Wenke Huang",
            "Guancheng Wan",
            "Zhiwu Lu",
            "Junchi Yan"
        ],
        "tldr": "This paper introduces a geometric knowledge-guided distribution calibration framework leveraging vision foundation models to address data heterogeneity and sample imbalance in federated learning and long-tailed recognition tasks, demonstrating improved performance by bridging the gap between biased observed distributions and the underlying true distribution.",
        "tldr_zh": "该论文提出了一种基于几何知识引导的分布校准框架，利用视觉基础模型解决联邦学习和长尾识别任务中的数据异构性和样本不平衡问题。通过弥合有偏观测分布与潜在真实分布之间的差距，该方法能够提升性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Revisiting MLLM Token Technology through the Lens of Classical Visual Coding",
        "summary": "Classical visual coding and Multimodal Large Language Model (MLLM) token\ntechnology share the core objective - maximizing information fidelity while\nminimizing computational cost. Therefore, this paper reexamines MLLM token\ntechnology, including tokenization, token compression, and token reasoning,\nthrough the established principles of long-developed visual coding area. From\nthis perspective, we (1) establish a unified formulation bridging token\ntechnology and visual coding, enabling a systematic, module-by-module\ncomparative analysis; (2) synthesize bidirectional insights, exploring how\nvisual coding principles can enhance MLLM token techniques' efficiency and\nrobustness, and conversely, how token technology paradigms can inform the\ndesign of next-generation semantic visual codecs; (3) prospect for promising\nfuture research directions and critical unsolved challenges. In summary, this\nstudy presents the first comprehensive and structured technology comparison of\nMLLM token and visual coding, paving the way for more efficient multimodal\nmodels and more powerful visual codecs simultaneously.",
        "url": "http://arxiv.org/abs/2508.13460v1",
        "published_date": "2025-08-19T02:36:44+00:00",
        "updated_date": "2025-08-19T02:36:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinming Liu",
            "Junyan Lin",
            "Yuntao Wei",
            "Kele Shao",
            "Keda Tao",
            "Jianguo Huang",
            "Xudong Yang",
            "Zhibo Chen",
            "Huan Wang",
            "Xin Jin"
        ],
        "tldr": "This paper draws parallels between MLLM token technology and classical visual coding, offering a comparative analysis to improve both domains by leveraging insights from each other.",
        "tldr_zh": "本文将多模态大语言模型(MLLM)的token技术与经典视觉编码进行类比，通过对比分析，利用彼此的见解来改进这两个领域。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation",
        "summary": "We investigate to what extent Multimodal Large Language Models (MLLMs) can\naccurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg},\n180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning\ncapabilities to detect rotational cues and contextualize spatial relationships\nwithin images, regardless of their orientation. To evaluate MLLMs on these\nabilities, we introduce RotBench -- a 350-image manually-filtered benchmark\ncomprising lifestyle, portrait, and landscape images. Despite the relatively\nsimple nature of this task, we show that several state-of-the-art open and\nproprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably\nidentify rotation in input images. Providing models with auxiliary information\n-- including captions, depth maps, and more -- or using chain-of-thought\nprompting offers only small and inconsistent improvements. Our results indicate\nthat most models are able to reliably identify right-side-up (0{\\deg}) images,\nwhile certain models are able to identify upside-down (180{\\deg}) images. None\ncan reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing\nthe image rotated in different orientations leads to moderate performance gains\nfor reasoning models, while a modified setup using voting improves the\nperformance of weaker models. We further show that fine-tuning does not improve\nmodels' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite\nsubstantially improving the identification of 180{\\deg} images. Together, these\nresults reveal a significant gap between MLLMs' spatial reasoning capabilities\nand human perception in identifying rotation.",
        "url": "http://arxiv.org/abs/2508.13968v1",
        "published_date": "2025-08-19T15:58:25+00:00",
        "updated_date": "2025-08-19T15:58:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Tianyi Niu",
            "Jaemin Cho",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "tldr": "The paper introduces RotBench, a benchmark to evaluate MLLMs' ability to identify image rotation. Results show that current state-of-the-art models struggle with this task, indicating a gap in spatial reasoning.",
        "tldr_zh": "该论文介绍了RotBench，一个用于评估MLLM识别图像旋转能力的基准。结果表明，目前最先进的模型在这项任务上表现不佳，表明在空间推理方面存在差距。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup",
        "summary": "Recent vision-language models (e.g., CLIP) have demonstrated remarkable\nclass-generalizable ability to unseen classes in few-shot anomaly segmentation\n(FSAS), leveraging supervised prompt learning or fine-tuning on seen classes.\nHowever, their cross-category generalization largely depends on prior knowledge\nof real seen anomaly samples. In this paper, we propose a novel framework,\nnamely DictAS, which enables a unified model to detect visual anomalies in\nunseen object categories without any retraining on the target data, only\nemploying a few normal reference images as visual prompts. The insight behind\nDictAS is to transfer dictionary lookup capabilities to the FSAS task for\nunseen classes via self-supervised learning, instead of merely memorizing the\nnormal and abnormal feature patterns from the training set. Specifically,\nDictAS mainly consists of three components: (1) **Dictionary Construction** -\nto simulate the index and content of a real dictionary using features from\nnormal reference images. (2) **Dictionary Lookup** - to retrieve queried region\nfeatures from the dictionary via a sparse lookup strategy. When a query feature\ncannot be retrieved, it is classified as an anomaly. (3) **Query Discrimination\nRegularization**- to enhance anomaly discrimination by making abnormal features\nharder to retrieve from the dictionary. To achieve this, Contrastive Query\nConstraint and Text Alignment Constraint are further proposed. Extensive\nexperiments on seven public industrial and medical datasets demonstrate that\nDictAS consistently outperforms state-of-the-art FSAS methods.",
        "url": "http://arxiv.org/abs/2508.13560v1",
        "published_date": "2025-08-19T06:38:56+00:00",
        "updated_date": "2025-08-19T06:38:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhen Qu",
            "Xian Tao",
            "Xinyi Gong",
            "ShiChen Qu",
            "Xiaopei Zhang",
            "Xingang Wang",
            "Fei Shen",
            "Zhengtao Zhang",
            "Mukesh Prasad",
            "Guiguang Ding"
        ],
        "tldr": "The paper introduces DictAS, a novel framework for class-generalizable few-shot anomaly segmentation that uses a dictionary lookup mechanism with self-supervised learning, achieving state-of-the-art performance without retraining on target data.",
        "tldr_zh": "该论文介绍了一种名为DictAS的新框架，用于类泛化的少样本异常分割。该框架使用带有自监督学习的字典查找机制，无需在目标数据上重新训练即可达到最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]