[
    {
        "title": "ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models",
        "summary": "Advances in generative models have led to AI-generated images visually\nindistinguishable from authentic ones. Despite numerous studies on detecting\nAI-generated images with classifiers, a gap persists between such methods and\nhuman cognitive forensic analysis. We present ForenX, a novel method that not\nonly identifies the authenticity of images but also provides explanations that\nresonate with human thoughts. ForenX employs the powerful multimodal large\nlanguage models (MLLMs) to analyze and interpret forensic cues. Furthermore, we\novercome the limitations of standard MLLMs in detecting forgeries by\nincorporating a specialized forensic prompt that directs the MLLMs attention to\nforgery-indicative attributes. This approach not only enhance the\ngeneralization of forgery detection but also empowers the MLLMs to provide\nexplanations that are accurate, relevant, and comprehensive. Additionally, we\nintroduce ForgReason, a dataset dedicated to descriptions of forgery evidences\nin AI-generated images. Curated through collaboration between an LLM-based\nagent and a team of human annotators, this process provides refined data that\nfurther enhances our model's performance. We demonstrate that even limited\nmanual annotations significantly improve explanation quality. We evaluate the\neffectiveness of ForenX on two major benchmarks. The model's explainability is\nverified by comprehensive subjective evaluations.",
        "url": "http://arxiv.org/abs/2508.01402v1",
        "published_date": "2025-08-02T15:21:26+00:00",
        "updated_date": "2025-08-02T15:21:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chuangchuang Tan",
            "Jinglu Wang",
            "Xiang Ming",
            "Renshuai Tao",
            "Yunchao Wei",
            "Yao Zhao",
            "Yan Lu"
        ],
        "tldr": "The paper introduces ForenX, a method using multimodal large language models (MLLMs) with a specialized forensic prompt and a new dataset (ForgReason) to detect and explain AI-generated image forgeries in a way that aligns with human reasoning.",
        "tldr_zh": "该论文介绍了一种名为ForenX的方法，该方法使用多模态大型语言模型(MLLMs)和一个专门的取证提示以及一个新的数据集(ForgReason)来检测和解释AI生成的图像伪造，其方式与人类推理相一致。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using Vision-Language Models",
        "summary": "It is of crucial importance to assess damages promptly and accurately in\nhumanitarian assistance and disaster response (HADR). Current deep learning\napproaches struggle to generalize effectively due to the imbalance of data\nclasses, scarcity of moderate damage examples, and human inaccuracy in pixel\nlabeling during HADR situations. To accommodate for these limitations and\nexploit state-of-the-art techniques in vision-language models (VLMs) to fuse\nimagery with human knowledge understanding, there is an opportunity to generate\na diversified set of image-based damage data effectively. Our initial\nexperimental results suggest encouraging data generation quality, which\ndemonstrates an improvement in classifying scenes with different levels of\nstructural damage to buildings, roads, and infrastructures.",
        "url": "http://arxiv.org/abs/2508.01380v1",
        "published_date": "2025-08-02T14:22:25+00:00",
        "updated_date": "2025-08-02T14:22:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jie Wei",
            "Erika Ardiles-Cruz",
            "Aleksey Panasyuk",
            "Erik Blasch"
        ],
        "tldr": "This paper proposes a method to generate damage data using vision-language models to address data imbalance and inaccuracy issues in HADR, showing promising results in damage classification.",
        "tldr_zh": "该论文提出了一种利用视觉-语言模型生成灾害数据的方案，以解决人道主义援助与灾害响应（HADR）中数据不平衡和不准确的问题，并在灾害分类中显示出良好的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework",
        "summary": "Image forgery localization aims to precisely identify tampered regions within\nimages, but it commonly depends on costly pixel-level annotations. To alleviate\nthis annotation burden, weakly supervised image forgery localization (WSIFL)\nhas emerged, yet existing methods still achieve limited localization\nperformance as they mainly exploit intra-image consistency clues and lack\nexternal semantic guidance to compensate for weak supervision. In this paper,\nwe propose ViLaCo, a vision-language collaborative reasoning framework that\nintroduces auxiliary semantic supervision distilled from pre-trained\nvision-language models (VLMs), enabling accurate pixel-level localization using\nonly image-level labels. Specifically, ViLaCo first incorporates semantic\nknowledge through a vision-language feature modeling network, which jointly\nextracts textual and visual priors using pre-trained VLMs. Next, an adaptive\nvision-language reasoning network aligns textual semantics and visual features\nthrough mutual interactions, producing semantically aligned representations.\nSubsequently, these representations are passed into dual prediction heads,\nwhere the coarse head performs image-level classification and the fine head\ngenerates pixel-level localization masks, thereby bridging the gap between weak\nsupervision and fine-grained localization. Moreover, a contrastive patch\nconsistency module is introduced to cluster tampered features while separating\nauthentic ones, facilitating more reliable forgery discrimination. Extensive\nexperiments on multiple public datasets demonstrate that ViLaCo substantially\noutperforms existing WSIFL methods, achieving state-of-the-art performance in\nboth detection and localization accuracy.",
        "url": "http://arxiv.org/abs/2508.01338v1",
        "published_date": "2025-08-02T12:14:29+00:00",
        "updated_date": "2025-08-02T12:14:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziqi Sheng",
            "Junyan Wu",
            "Wei Lu",
            "Jiantao Zhou"
        ],
        "tldr": "This paper introduces ViLaCo, a vision-language collaborative reasoning framework for weakly-supervised image forgery localization, utilizing pre-trained VLMs to achieve state-of-the-art performance with only image-level labels.",
        "tldr_zh": "本文介绍了一种名为ViLaCo的视觉-语言协作推理框架，用于弱监督图像伪造定位，利用预训练的VLMs仅使用图像级别的标签即可实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification",
        "summary": "Multiple Instance Learning (MIL) is the leading approach for whole slide\nimage (WSI) classification, enabling efficient analysis of gigapixel pathology\nslides. Recent work has introduced vision-language models (VLMs) into MIL\npipelines to incorporate medical knowledge through text-based class\ndescriptions rather than simple class names. However, when these methods rely\non large language models (LLMs) to generate clinical descriptions or use\nfixed-length prompts to represent complex pathology concepts, the limited token\ncapacity of VLMs often constrains the expressiveness and richness of the\nencoded class information. Additionally, descriptions generated solely by LLMs\nmay lack domain grounding and fine-grained medical specificity, leading to\nsuboptimal alignment with visual features. To address these challenges, we\npropose a vision-language MIL framework with two key contributions: (1) A\ngrounded multi-agent description generation system that leverages curated\npathology textbooks and agent specialization (e.g., morphology, spatial\ncontext) to produce accurate and diverse clinical descriptions; (2) A text\nencoding strategy using a list of descriptions rather than a single prompt,\ncapturing fine-grained and complementary clinical signals for better alignment\nwith visual features. Integrated into a VLM-MIL pipeline, our approach shows\nimproved performance over single-prompt class baselines and achieves results\ncomparable to state-of-the-art models, as demonstrated on renal and lung cancer\ndatasets.",
        "url": "http://arxiv.org/abs/2508.01293v1",
        "published_date": "2025-08-02T09:59:39+00:00",
        "updated_date": "2025-08-02T09:59:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ngoc Bui Lam Quang",
            "Nam Le Nguyen Binh",
            "Thanh-Huy Nguyen",
            "Le Thien Phuc Nguyen",
            "Quan Nguyen",
            "Ulas Bagci"
        ],
        "tldr": "This paper introduces a novel vision-language MIL framework called GMAT for WSI classification that uses a grounded multi-agent system to generate diverse and accurate clinical descriptions, and a text encoding strategy using multiple descriptions to capture fine-grained clinical signals, achieving comparable performance to state-of-the-art models.",
        "tldr_zh": "本文介绍了一种新的视觉语言MIL框架GMAT，用于WSI分类。该框架使用一个基于grounded multi-agent的系统来生成多样且准确的临床描述，并采用一种使用多个描述的文本编码策略来捕获细粒度的临床信号，实现了与最先进模型相当的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Enhanced Image Clustering with Cross-Modal Semantic Consistency",
        "summary": "While large language-image pre-trained models like CLIP offer powerful\ngeneric features for image clustering, existing methods typically freeze the\nencoder. This creates a fundamental mismatch between the model's task-agnostic\nrepresentations and the demands of a specific clustering task, imposing a\nceiling on performance. To break this ceiling, we propose a self-enhanced\nframework based on cross-modal semantic consistency for efficient image\nclustering. Our framework first builds a strong foundation via Cross-Modal\nSemantic Consistency and then specializes the encoder through Self-Enhancement.\nIn the first stage, we focus on Cross-Modal Semantic Consistency. By mining\nconsistency between generated image-text pairs at the instance, cluster\nassignment, and cluster center levels, we train lightweight clustering heads to\nalign with the rich semantics of the pre-trained model. This alignment process\nis bolstered by a novel method for generating higher-quality cluster centers\nand a dynamic balancing regularizer to ensure well-distributed assignments. In\nthe second stage, we introduce a Self-Enhanced fine-tuning strategy. The\nwell-aligned model from the first stage acts as a reliable pseudo-label\ngenerator. These self-generated supervisory signals are then used to feed back\nthe efficient, joint optimization of the vision encoder and clustering heads,\nunlocking their full potential. Extensive experiments on six mainstream\ndatasets show that our method outperforms existing deep clustering methods by\nsignificant margins. Notably, our ViT-B/32 model already matches or even\nsurpasses the accuracy of state-of-the-art methods built upon the far larger\nViT-L/14.",
        "url": "http://arxiv.org/abs/2508.01254v1",
        "published_date": "2025-08-02T08:12:57+00:00",
        "updated_date": "2025-08-02T08:12:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihan Li",
            "Wei Sun",
            "Jing Hu",
            "Jianhua Yin",
            "Jianlong Wu",
            "Liqiang Nie"
        ],
        "tldr": "The paper proposes a self-enhanced framework for image clustering, leveraging cross-modal semantic consistency and fine-tuning a CLIP-based encoder to achieve state-of-the-art performance on multiple datasets.",
        "tldr_zh": "该论文提出了一种自增强图像聚类框架，利用跨模态语义一致性并微调基于CLIP的编码器，从而在多个数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ODOV: Towards Open-Domain Open-Vocabulary Object Detection",
        "summary": "In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV)\nobject detection, which considers the detection model's adaptability to the\nreal world including both domain and category shifts. For this problem, we\nfirst construct a new benchmark OD-LVIS, which includes 46,949 images, covers\n18 complex real-world domains and 1,203 categories, and provides a\ncomprehensive dataset for evaluating real-world object detection. Besides, we\ndevelop a novel baseline method for ODOV detection.The proposed method first\nleverages large language models to generate the domain-agnostic text prompts\nfor category embedding. It further learns the domain embedding from the given\nimage, which, during testing, can be integrated into the category embedding to\nform the customized domain-specific category embedding for each test image. We\nprovide sufficient benchmark evaluations for the proposed ODOV detection task\nand report the results, which verify the rationale of ODOV detection, the\nusefulness of our benchmark, and the superiority of the proposed method.",
        "url": "http://arxiv.org/abs/2508.01253v1",
        "published_date": "2025-08-02T08:10:45+00:00",
        "updated_date": "2025-08-02T08:10:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yupeng Zhang",
            "Ruize Han",
            "Fangnan Zhou",
            "Song Wang",
            "Wei Feng",
            "Liang Wan"
        ],
        "tldr": "This paper introduces the Open-Domain Open-Vocabulary (ODOV) object detection problem, proposes a new benchmark dataset (OD-LVIS), and presents a baseline method using LLMs and domain embedding to address domain and category shifts in real-world object detection.",
        "tldr_zh": "该论文介绍了开放领域开放词汇 (ODOV) 对象检测问题，提出了一个新的基准数据集 (OD-LVIS)，并提出了一种使用 LLM 和领域嵌入的基线方法，以解决现实世界对象检测中的领域和类别偏移。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models",
        "summary": "Despite the great success of Large Vision Language Models (LVLMs), their high\ncomputational cost severely limits their broad applications. The computational\ncost of LVLMs mainly stems from the visual sequence of the input, which\nconsists of hundreds or even thousands of tokens. Although existing methods\nhave made progress by removing redundant tokens, they suffer from severe\nperformance degradation with high pruning rates due to the loss of visual\ninformation. In this paper, we propose an Adaptive Content Compensation Method\n(ACCM), which can effectively mitigate the visual information loss via an image\ncaption. Specifically, ACCM comprises two key components: a lightweight caption\nmodel and a selector. Firstly the caption model generates question-related\ndescriptions under the guidance of the user instruction. Then the selector\nfurther identifies a contextually appropriate caption from multiple candidates.\nLeveraging self-supervised learning, our modules could be learned efficiently\nwithout any human or automated labeling. We conduct extensive experiments\nacross seven benchmarks and the results show that ACCM significantly\noutperforms existing methods with lower FLOPs (e.g., surpassing SOTA by 20.6%\nwith 6.5% fewer FLOPs).",
        "url": "http://arxiv.org/abs/2508.01236v1",
        "published_date": "2025-08-02T07:22:08+00:00",
        "updated_date": "2025-08-02T07:22:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingyu Fu",
            "Wei Suo",
            "Ji Ma",
            "Lin Yuanbo Wu",
            "Peng Wang",
            "Yanning Zhang"
        ],
        "tldr": "This paper introduces an Adaptive Content Compensation Method (ACCM) to mitigate information loss during high pruning rates in Large Vision Language Models (LVLMs) by using a caption model and selector to compensate for pruned visual tokens, achieving better performance with lower FLOPs.",
        "tldr_zh": "该论文提出了一种自适应内容补偿方法 (ACCM)，通过使用标题模型和选择器来补偿修剪后的视觉标记，从而减轻大型视觉语言模型 (LVLM) 中高修剪率下的信息丢失，并在更少的 FLOPs 下实现更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models",
        "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
        "url": "http://arxiv.org/abs/2508.01225v1",
        "published_date": "2025-08-02T06:43:43+00:00",
        "updated_date": "2025-08-02T06:43:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinyu Chen",
            "Haotian Zhai",
            "Can Zhang",
            "Xiupeng Shi",
            "Ruirui Li"
        ],
        "tldr": "This paper introduces a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) method and its improved version MCP++ to enhance the zero-shot test-time generalization performance of Vision-Language Models by improving intra-class compactness and prediction calibration.",
        "tldr_zh": "本文介绍了一种多缓存增强的基于原型的测试时自适应（MCP）方法及其改进版本MCP++，通过提高类内紧凑性和预测校准，以增强视觉语言模型的零样本测试时泛化性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
        "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.",
        "url": "http://arxiv.org/abs/2508.01151v1",
        "published_date": "2025-08-02T02:23:20+00:00",
        "updated_date": "2025-08-02T02:23:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yu Lei",
            "Jinbin Bai",
            "Qingyu Shi",
            "Aosong Feng",
            "Kaidong Yu"
        ],
        "tldr": "The paper introduces Personalized Safety Alignment (PSA), a framework for text-to-image diffusion models that incorporates user-specific safety preferences, achieving improved harmful content suppression and alignment with user constraints.",
        "tldr_zh": "该论文介绍了个性化安全对齐 (PSA)，这是一种文本到图像扩散模型的框架，它结合了用户特定的安全偏好，从而提高了有害内容的抑制并与用户约束对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation",
        "summary": "We present ROVI, a high-quality synthetic dataset for instance-grounded\ntext-to-image generation, created by labeling 1M curated web images. Our key\ninnovation is a strategy called re-captioning, focusing on the pre-detection\nstage, where a VLM (Vision-Language Model) generates comprehensive visual\ndescriptions that are then processed by an LLM (Large Language Model) to\nextract a flat list of potential categories for OVDs (Open-Vocabulary\nDetectors) to detect. This approach yields a global prompt inherently linked to\ninstance annotations while capturing secondary visual elements humans typically\noverlook. Evaluations show that ROVI exceeds existing detection datasets in\nimage quality and resolution while containing two orders of magnitude more\ncategories with an open-vocabulary nature. For demonstrative purposes, a\ntext-to-image model GLIGEN trained on ROVI significantly outperforms\nstate-of-the-art alternatives in instance grounding accuracy, prompt fidelity,\nand aesthetic quality. Our dataset and reproducible pipeline are available at\nhttps://github.com/CihangPeng/ROVI.",
        "url": "http://arxiv.org/abs/2508.01008v1",
        "published_date": "2025-08-01T18:19:51+00:00",
        "updated_date": "2025-08-01T18:19:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cihang Peng",
            "Qiming Hou",
            "Zhong Ren",
            "Kun Zhou"
        ],
        "tldr": "The paper introduces ROVI, a large-scale, high-quality synthetic dataset for instance-grounded text-to-image generation, created using a novel re-captioning strategy involving VLMs and LLMs. Experiments show significant improvements in instance grounding, prompt fidelity, and aesthetic quality when training a text-to-image model on ROVI.",
        "tldr_zh": "该论文介绍了ROVI，一个大规模、高质量的合成数据集，用于实例级文本到图像生成。它使用了一种新颖的重述策略，涉及视觉语言模型（VLM）和大型语言模型（LLM）。实验表明，在ROVI上训练文本到图像模型时，实例基础、提示保真度和美学质量均得到显著提高。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video-based Vehicle Surveillance in the Wild: License Plate, Make, and Model Recognition with Self Reflective Vision-Language Models",
        "summary": "Automatic license plate recognition (ALPR) and vehicle make and model\nrecognition underpin intelligent transportation systems, supporting law\nenforcement, toll collection, and post-incident investigation. Applying these\nmethods to videos captured by handheld smartphones or non-static\nvehicle-mounted cameras presents unique challenges compared to fixed\ninstallations, including frequent camera motion, varying viewpoints,\nocclusions, and unknown road geometry. Traditional ALPR solutions, dependent on\nspecialized hardware and handcrafted OCR pipelines, often degrade under these\nconditions. Recent advances in large vision-language models (VLMs) enable\ndirect recognition of textual and semantic attributes from arbitrary imagery.\nThis study evaluates the potential of VLMs for ALPR and makes and models\nrecognition using monocular videos captured with handheld smartphones and\nnon-static mounted cameras. The proposed license plate recognition pipeline\nfilters to sharp frames, then sends a multimodal prompt to a VLM using several\nprompt strategies. Make and model recognition pipeline runs the same VLM with a\nrevised prompt and an optional self-reflection module. In the self-reflection\nmodule, the model contrasts the query image with a reference from a 134-class\ndataset, correcting mismatches. Experiments on a smartphone dataset collected\non the campus of the University of Texas at Austin, achieve top-1 accuracies of\n91.67% for ALPR and 66.67% for make and model recognition. On the public\nUFPR-ALPR dataset, the approach attains 83.05% and 61.07%, respectively. The\nself-reflection module further improves results by 5.72% on average for make\nand model recognition. These findings demonstrate that VLMs provide a\ncost-effective solution for scalable, in-motion traffic video analysis.",
        "url": "http://arxiv.org/abs/2508.01387v1",
        "published_date": "2025-08-02T14:34:19+00:00",
        "updated_date": "2025-08-02T14:34:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pouya Parsa",
            "Keya Li",
            "Kara M. Kockelman",
            "Seongjin Choi"
        ],
        "tldr": "This paper explores the use of Vision-Language Models (VLMs) for license plate, make, and model recognition from monocular videos captured by handheld smartphones and non-static mounted cameras, showing promising results with a self-reflection module to improve accuracy.",
        "tldr_zh": "本文探讨了使用视觉语言模型（VLMs）从手持智能手机和非静态车载摄像头拍摄的单目视频中进行车牌、车型和型号识别，结果显示有希望，并使用自反思模块来提高准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation",
        "summary": "Text-to-image (T2I) models have demonstrated remarkable generative\ncapabilities but remain vulnerable to producing not-safe-for-work (NSFW)\ncontent, such as violent or explicit imagery. While recent moderation efforts\nhave introduced soft prompt-guided tuning by appending defensive tokens to the\ninput, these approaches often rely on large-scale curated image-text datasets\nand apply static, one-size-fits-all defenses at inference time. However, this\nresults not only in high computational cost and degraded benign image quality,\nbut also in limited adaptability to the diverse and nuanced safety requirements\nof real-world prompts. To address these challenges, we propose PromptSafe, a\ngated prompt tuning framework that combines a lightweight, text-only supervised\nsoft embedding with an inference-time gated control network. Instead of\ntraining on expensive image-text datasets, we first rewrite unsafe prompts into\nsemantically aligned but safe alternatives using an LLM, constructing an\nefficient text-only training corpus. Based on this, we optimize a universal\nsoft prompt that repels unsafe and attracts safe embeddings during the\ndiffusion denoising process. To avoid over-suppressing benign prompts, we\nintroduce a gated mechanism that adaptively adjusts the defensive strength\nbased on estimated prompt toxicity, thereby aligning defense intensity with\nprompt risk and ensuring strong protection for harmful inputs while preserving\nbenign generation quality. Extensive experiments across multiple benchmarks and\nT2I models show that PromptSafe achieves a SOTA unsafe generation rate (2.36%),\nwhile preserving high benign fidelity. Furthermore, PromptSafe demonstrates\nstrong generalization to unseen harmful categories, robust transferability\nacross diffusion model architectures, and resilience under adaptive adversarial\nattacks, highlighting its practical value for safe and scalable deployment.",
        "url": "http://arxiv.org/abs/2508.01272v1",
        "published_date": "2025-08-02T09:09:40+00:00",
        "updated_date": "2025-08-02T09:09:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zonglei Jing",
            "Xiao Yang",
            "Xiaoqian Li",
            "Siyuan Liang",
            "Aishan Liu",
            "Mingchuan Zhang",
            "Xianglong Liu"
        ],
        "tldr": "PromptSafe introduces a gated prompt tuning framework to mitigate the generation of NSFW content in text-to-image models. It uses a lightweight, text-only supervised soft embedding and a gated control network for adaptable defense based on prompt toxicity, demonstrating strong performance and generalization.",
        "tldr_zh": "PromptSafe 提出了一种门控 prompt 调优框架，旨在缓解文本到图像模型中 NSFW 内容的生成。它采用轻量级的纯文本监督软嵌入和门控控制网络，基于 prompt 毒性进行自适应防御，并展示出强大的性能和泛化能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SGCap: Decoding Semantic Group for Zero-shot Video Captioning",
        "summary": "Zero-shot video captioning aims to generate sentences for describing videos\nwithout training the model on video-text pairs, which remains underexplored.\nExisting zero-shot image captioning methods typically adopt a text-only\ntraining paradigm, where a language decoder reconstructs single-sentence\nembeddings obtained from CLIP. However, directly extending them to the video\ndomain is suboptimal, as applying average pooling over all frames neglects\ntemporal dynamics. To address this challenge, we propose a Semantic Group\nCaptioning (SGCap) method for zero-shot video captioning. In particular, it\ndevelops the Semantic Group Decoding (SGD) strategy to employ multi-frame\ninformation while explicitly modeling inter-frame temporal relationships.\nFurthermore, existing zero-shot captioning methods that rely on cosine\nsimilarity for sentence retrieval and reconstruct the description supervised by\na single frame-level caption, fail to provide sufficient video-level\nsupervision. To alleviate this, we introduce two key components, including the\nKey Sentences Selection (KSS) module and the Probability Sampling Supervision\n(PSS) module. The two modules construct semantically-diverse sentence groups\nthat models temporal dynamics and guide the model to capture inter-sentence\ncausal relationships, thereby enhancing its generalization ability to video\ncaptioning. Experimental results on several benchmarks demonstrate that SGCap\nsignificantly outperforms previous state-of-the-art zero-shot alternatives and\neven achieves performance competitive with fully supervised ones. Code is\navailable at https://github.com/mlvccn/SGCap_Video.",
        "url": "http://arxiv.org/abs/2508.01270v1",
        "published_date": "2025-08-02T09:05:45+00:00",
        "updated_date": "2025-08-02T09:05:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Pan",
            "Ping Li",
            "Wenxiao Wang"
        ],
        "tldr": "The paper introduces SGCap, a novel approach for zero-shot video captioning using Semantic Group Decoding, Key Sentence Selection, and Probability Sampling Supervision to model temporal dynamics and inter-sentence relationships, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了SGCap，一种新的零样本视频字幕方法，它使用语义组解码、关键句子选择和概率抽样监督来建模时间动态和句子间的关系，并取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning",
        "summary": "A core problem of Embodied AI is to learn object manipulation from\nobservation, as humans do. To achieve this, it is important to localize 3D\nobject affordance areas through observation such as images (3D affordance\ngrounding) and understand their functionalities (affordance classification).\nPrevious attempts usually tackle these two tasks separately, leading to\ninconsistent predictions due to lacking proper modeling of their dependency. In\naddition, these methods typically only ground the incomplete affordance areas\ndepicted in images, failing to predict the full potential affordance areas, and\noperate at a fixed scale, resulting in difficulty in coping with affordances\nsignificantly varying in scale with respect to the whole object. To address\nthese issues, we propose a novel approach that learns an affordance-aware 3D\nrepresentation and employs a stage-wise inference strategy leveraging the\ndependency between grounding and classification tasks. Specifically, we first\ndevelop a cross-modal 3D representation through efficient fusion and\nmulti-scale geometric feature propagation, enabling inference of full potential\naffordance areas at a suitable regional scale. Moreover, we adopt a simple\ntwo-stage prediction mechanism, effectively coupling grounding and\nclassification for better affordance understanding. Experiments demonstrate the\neffectiveness of our method, showing improved performance in both affordance\ngrounding and classification.",
        "url": "http://arxiv.org/abs/2508.01184v1",
        "published_date": "2025-08-02T04:14:18+00:00",
        "updated_date": "2025-08-02T04:14:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinhang Wan",
            "Dongqiang Gou",
            "Xinwang Liu",
            "En Zhu",
            "Xuming He"
        ],
        "tldr": "This paper presents a novel method for object affordance recognition and grounding by learning a cross-modal 3D representation and using a stage-wise inference strategy to couple grounding and classification tasks, improving performance in both areas.",
        "tldr_zh": "本文提出了一种新的物体可供性识别和定位方法，通过学习跨模态3D表示，并采用阶段性推理策略来耦合定位和分类任务，从而提高了两方面的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding",
        "summary": "Recent advancements in 3D scene understanding have made significant strides\nin enabling interaction with scenes using open-vocabulary queries, particularly\nfor VR/AR and robotic applications. Nevertheless, existing methods are hindered\nby rigid offline pipelines and the inability to provide precise 3D object-level\nunderstanding given open-ended queries. In this paper, we present\nOpenGS-Fusion, an innovative open-vocabulary dense mapping framework that\nimproves semantic modeling and refines object-level understanding.\nOpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed\nDistance Field to facilitate lossless fusion of semantic features on-the-fly.\nFurthermore, we introduce a novel multimodal language-guided approach named\nMLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D\nobjects by adaptively adjusting similarity thresholds, achieving an improvement\n17\\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments\ndemonstrate that our method outperforms existing methods in 3D object\nunderstanding and scene reconstruction quality, as well as showcasing its\neffectiveness in language-guided scene interaction. The code is available at\nhttps://young-bit.github.io/opengs-fusion.github.io/ .",
        "url": "http://arxiv.org/abs/2508.01150v1",
        "published_date": "2025-08-02T02:22:36+00:00",
        "updated_date": "2025-08-02T02:22:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dianyi Yang",
            "Xihan Wang",
            "Yu Gao",
            "Shiyang Liu",
            "Bohan Ren",
            "Yufeng Yue",
            "Yi Yang"
        ],
        "tldr": "OpenGS-Fusion introduces a novel framework for open-vocabulary dense mapping that uses 3D Gaussian Splatting and a Truncated Signed Distance Field for on-the-fly semantic feature fusion, coupled with a multimodal language-guided thresholding approach for refined object segmentation.",
        "tldr_zh": "OpenGS-Fusion 提出了一种新颖的开放词汇密集映射框架，该框架使用 3D 高斯溅射和截断符号距离场来实现即时语义特征融合，并结合多模态语言引导阈值方法来改进对象分割。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Diagnostic Accuracy of Open-Source Vision-Language Models on Diverse Medical Imaging Tasks",
        "summary": "This retrospective study evaluated five VLMs (Qwen2.5, Phi-4, Gemma3,\nLlama3.2, and Mistral3.1) using the MedFMC dataset. This dataset includes\n22,349 images from 7,461 patients encompassing chest radiography (19 disease\nmulti-label classifications), colon pathology (tumor detection), endoscopy\n(colorectal lesion identification), neonatal jaundice assessment (skin\ncolor-based treatment necessity), and retinal fundoscopy (5-point diabetic\nretinopathy grading). Diagnostic accuracy was compared in three experimental\nsettings: visual input only, multimodal input, and chain-of-thought reasoning.\nModel accuracy was assessed against ground truth labels, with statistical\ncomparisons using bootstrapped confidence intervals (p<.05). Qwen2.5 achieved\nthe highest accuracy for chest radiographs (90.4%) and endoscopy images\n(84.2%), significantly outperforming the other models (p<.001). In colon\npathology, Qwen2.5 (69.0%) and Phi-4 (69.6%) performed comparably (p=.41), both\nsignificantly exceeding other VLMs (p<.001). Similarly, for neonatal jaundice\nassessment, Qwen2.5 (58.3%) and Phi-4 (58.1%) showed comparable leading\naccuracies (p=.93) significantly exceeding their counterparts (p<.001). All\nmodels struggled with retinal fundoscopy; Qwen2.5 and Gemma3 achieved the\nhighest, albeit modest, accuracies at 18.6% (comparable, p=.99), significantly\nbetter than other tested models (p<.001). Unexpectedly, multimodal input\nreduced accuracy for some models and modalities, and chain-of-thought reasoning\nprompts also failed to improve accuracy. The open-source VLMs demonstrated\npromising diagnostic capabilities, particularly in chest radiograph\ninterpretation. However, performance in complex domains such as retinal\nfundoscopy was limited, underscoring the need for further development and\ndomain-specific adaptation before widespread clinical application.",
        "url": "http://arxiv.org/abs/2508.01016v1",
        "published_date": "2025-08-01T18:28:37+00:00",
        "updated_date": "2025-08-01T18:28:37+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Gustav Müller-Franzes",
            "Debora Jutz",
            "Jakob Nikolas Kather",
            "Christiane Kuhl",
            "Sven Nebelung",
            "Daniel Truhn"
        ],
        "tldr": "This study benchmarked five open-source VLMs on the MedFMC dataset across five medical imaging tasks, finding that Qwen2.5 performed best overall, particularly in chest radiography, but performance varied significantly by task and input modality.",
        "tldr_zh": "该研究在MedFMC数据集上评估了五个开源VLM在五个医学影像任务中的表现，发现Qwen2.5总体表现最佳，尤其是在胸部X光片方面，但性能因任务和输入模态而异。",
        "relevance_score": 8,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Evading Data Provenance in Deep Neural Networks",
        "summary": "Modern over-parameterized deep models are highly data-dependent, with large\nscale general-purpose and domain-specific datasets serving as the bedrock for\nrapid advancements. However, many datasets are proprietary or contain sensitive\ninformation, making unrestricted model training problematic. In the open world\nwhere data thefts cannot be fully prevented, Dataset Ownership Verification\n(DOV) has emerged as a promising method to protect copyright by detecting\nunauthorized model training and tracing illicit activities. Due to its\ndiversity and superior stealth, evading DOV is considered extremely\nchallenging. However, this paper identifies that previous studies have relied\non oversimplistic evasion attacks for evaluation, leading to a false sense of\nsecurity. We introduce a unified evasion framework, in which a teacher model\nfirst learns from the copyright dataset and then transfers task-relevant yet\nidentifier-independent domain knowledge to a surrogate student using an\nout-of-distribution (OOD) dataset as the intermediary. Leveraging\nVision-Language Models and Large Language Models, we curate the most\ninformative and reliable subsets from the OOD gallery set as the final transfer\nset, and propose selectively transferring task-oriented knowledge to achieve a\nbetter trade-off between generalization and evasion effectiveness. Experiments\nacross diverse datasets covering eleven DOV methods demonstrate our approach\nsimultaneously eliminates all copyright identifiers and significantly\noutperforms nine state-of-the-art evasion attacks in both generalization and\neffectiveness, with moderate computational overhead. As a proof of concept, we\nreveal key vulnerabilities in current DOV methods, highlighting the need for\nlong-term development to enhance practicality.",
        "url": "http://arxiv.org/abs/2508.01074v1",
        "published_date": "2025-08-01T21:13:45+00:00",
        "updated_date": "2025-08-01T21:13:45+00:00",
        "categories": [
            "cs.CV",
            "cs.CR"
        ],
        "authors": [
            "Hongyu Zhu",
            "Sichu Liang",
            "Wenwen Wang",
            "Zhuomeng Zhang",
            "Fangqi Li",
            "Shi-Lin Wang"
        ],
        "tldr": "The paper introduces a novel evasion framework for Dataset Ownership Verification (DOV) in deep learning models, leveraging transfer learning and large language models to outperform existing evasion techniques while maintaining generalization performance.",
        "tldr_zh": "该论文介绍了一种新颖的深度学习模型数据集所有权验证(DOV)逃逸框架，利用迁移学习和大型语言模型，在保持泛化性能的同时，优于现有的逃逸技术。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]