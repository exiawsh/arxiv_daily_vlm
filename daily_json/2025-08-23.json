[
    {
        "title": "A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer",
        "summary": "The non-invasive assessment of increasingly incidentally discovered renal\nmasses is a critical challenge in urologic oncology, where diagnostic\nuncertainty frequently leads to the overtreatment of benign or indolent tumors.\nIn this study, we developed and validated RenalCLIP using a dataset of 27,866\nCT scans from 8,809 patients across nine Chinese medical centers and the public\nTCIA cohort, a visual-language foundation model for characterization, diagnosis\nand prognosis of renal mass. The model was developed via a two-stage\npre-training strategy that first enhances the image and text encoders with\ndomain-specific knowledge before aligning them through a contrastive learning\nobjective, to create robust representations for superior generalization and\ndiagnostic precision. RenalCLIP achieved better performance and superior\ngeneralizability across 10 core tasks spanning the full clinical workflow of\nkidney cancer, including anatomical assessment, diagnostic classification, and\nsurvival prediction, compared with other state-of-the-art general-purpose CT\nfoundation models. Especially, for complicated task like recurrence-free\nsurvival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726,\nrepresenting a substantial improvement of approximately 20% over the leading\nbaselines. Furthermore, RenalCLIP's pre-training imparted remarkable data\nefficiency; in the diagnostic classification task, it only needs 20% training\ndata to achieve the peak performance of all baseline models even after they\nwere fully fine-tuned on 100% of the data. Additionally, it achieved superior\nperformance in report generation, image-text retrieval and zero-shot diagnosis\ntasks. Our findings establish that RenalCLIP provides a robust tool with the\npotential to enhance diagnostic accuracy, refine prognostic stratification, and\npersonalize the management of patients with kidney cancer.",
        "url": "http://arxiv.org/abs/2508.16569v1",
        "published_date": "2025-08-22T17:48:19+00:00",
        "updated_date": "2025-08-22T17:48:19+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yuhui Tao",
            "Zhongwei Zhao",
            "Zilong Wang",
            "Xufang Luo",
            "Feng Chen",
            "Kang Wang",
            "Chuanfu Wu",
            "Xue Zhang",
            "Shaoting Zhang",
            "Jiaxi Yao",
            "Xingwei Jin",
            "Xinyang Jiang",
            "Yifan Yang",
            "Dongsheng Li",
            "Lili Qiu",
            "Zhiqiang Shao",
            "Jianming Guo",
            "Nengwang Yu",
            "Shuo Wang",
            "Ying Xiong"
        ],
        "tldr": "The paper introduces RenalCLIP, a disease-centric vision-language foundation model for kidney cancer diagnosis, prognosis, and characterization using CT scans and associated text, demonstrating superior performance and generalizability compared to general-purpose models.",
        "tldr_zh": "该论文介绍了一种以疾病为中心的视觉-语言基础模型 RenalCLIP，用于利用 CT 扫描和相关文本进行肾癌诊断、预后和特征分析，与通用模型相比，表现出卓越的性能和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation",
        "summary": "Foundation models (FMs) exhibit remarkable generalization but require\nadaptation to downstream tasks, particularly in privacy-sensitive applications.\nDue to data privacy regulations, cloud-based FMs cannot directly access private\nedge data, limiting their adaptation. Federated learning (FL) provides a\nprivacy-aware alternative, but existing FL approaches overlook the constraints\nimposed by edge devices -- namely, limited computational resources and the\nscarcity of labeled data. To address these challenges, we introduce Practical\nSemi-Supervised Federated Learning (PSSFL), where edge devices hold only\nunlabeled, low-resolution data, while the server has limited labeled,\nhigh-resolution data. In this setting, we propose the Federated Mixture of\nExperts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox\ntackles computational and resolution mismatch challenges via a sparse\nMixture-of-Experts architecture, employing a spatial router to align features\nacross resolutions and a Soft-Mixture strategy to stabilize semi-supervised\nlearning. We take object detection as a case study, and experiments on\nreal-world autonomous driving datasets demonstrate that FedMox effectively\nadapts FMs under PSSFL, significantly improving performance with constrained\nmemory costs on edge devices. Our work paves the way for scalable and\nprivacy-preserving FM adaptation in federated scenarios.",
        "url": "http://arxiv.org/abs/2508.16568v1",
        "published_date": "2025-08-22T17:47:02+00:00",
        "updated_date": "2025-08-22T17:47:02+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Guangyu Sun",
            "Jingtao Li",
            "Weiming Zhuang",
            "Chen Chen",
            "Chen Chen",
            "Lingjuan Lyu"
        ],
        "tldr": "This paper introduces Practical Semi-Supervised Federated Learning (PSSFL) and Federated Mixture of Experts (FedMox) to adapt foundation models in federated settings with limited edge resources and scarce labeled data, achieving significant performance improvements in object detection for autonomous driving.",
        "tldr_zh": "该论文介绍了实用半监督联邦学习 (PSSFL) 和联邦混合专家 (FedMox)，以在资源有限的边缘设备和标记数据稀缺的联邦环境中调整基础模型，并在自动驾驶的目标检测方面取得了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Modular Embedding Recomposition for Incremental Learning",
        "summary": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth.",
        "url": "http://arxiv.org/abs/2508.16463v1",
        "published_date": "2025-08-22T15:25:40+00:00",
        "updated_date": "2025-08-22T15:25:40+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Aniello Panariello",
            "Emanuele Frascaroli",
            "Pietro Buzzega",
            "Lorenzo Bonicelli",
            "Angelo Porrello",
            "Simone Calderara"
        ],
        "tldr": "This paper introduces MoDER, a modular framework that enhances the zero-shot capabilities of VLMs in continual learning by training and composing textual experts for unseen classes, showing effectiveness across several datasets.",
        "tldr_zh": "本文介绍了一种名为MoDER的模块化框架，通过训练和组合文本专家来增强VLM在持续学习中的零样本能力，从而改进对未见类别的分类，并在多个数据集上展示了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
        "summary": "Video large language models (Vid-LLMs) have shown strong capabilities in\nunderstanding video content. However, their reliance on dense video token\nrepresentations introduces substantial memory and computational overhead in\nboth prefilling and decoding. To mitigate the information loss of recent video\ntoken reduction methods and accelerate the decoding stage of Vid-LLMs\nlosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)\nframework tailored for Vid-LLMs that incorporates staged video token pruning.\nBuilding on our novel finding that the draft model's speculation exhibits low\nsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,\nenabling efficient speculation without sacrificing accuracy. To achieve this,\nit performs a two-stage pruning process: Stage I selects highly informative\ntokens guided by attention signals from the verifier (target model), while\nStage II prunes remaining redundant ones in a spatially uniform manner.\nExtensive experiments on four video understanding benchmarks demonstrate the\neffectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$\ndecoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for\nQwen2.5-VL-32B.",
        "url": "http://arxiv.org/abs/2508.16201v1",
        "published_date": "2025-08-22T08:23:09+00:00",
        "updated_date": "2025-08-22T08:23:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yicheng Ji",
            "Jun Zhang",
            "Heming Xia",
            "Jinpeng Chen",
            "Lidan Shou",
            "Gang Chen",
            "Huan Li"
        ],
        "tldr": "The paper introduces SpecVLM, a training-free speculative decoding framework for video LLMs that uses verifier-guided token pruning to achieve significant decoding speedups without sacrificing accuracy.",
        "tldr_zh": "该论文介绍了 SpecVLM，一种用于视频 LLM 的免训练推测解码框架，该框架使用验证器引导的 token 剪枝来实现显著的解码加速，而不会牺牲准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection",
        "summary": "Pre-trained Vision-Language Models (VLMs) have recently shown promise in\ndetecting anomalies. However, previous approaches are fundamentally limited by\ntheir reliance on human-designed prompts and the lack of accessible anomaly\nsamples, leading to significant gaps in context-specific anomaly understanding.\nIn this paper, we propose \\textbf{A}daptive \\textbf{P}rompt \\textbf{T}uning\nwith semantic alignment for anomaly detection (APT), a groundbreaking prior\nknowledge-free, few-shot framework and overcomes the limitations of traditional\nprompt-based approaches. APT uses self-generated anomaly samples with noise\nperturbations to train learnable prompts that capture context-dependent\nanomalies in different scenarios. To prevent overfitting to synthetic noise, we\npropose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively\naligns the prompts with general anomaly semantics while incorporating diverse\nsynthetic anomaly. Our system not only advances pixel-wise anomaly detection,\nbut also achieves state-of-the-art performance on multiple benchmark datasets\nwithout requiring prior knowledge for prompt crafting, establishing a robust\nand versatile solution for real-world anomaly detection.",
        "url": "http://arxiv.org/abs/2508.16157v1",
        "published_date": "2025-08-22T07:26:56+00:00",
        "updated_date": "2025-08-22T07:26:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pi-Wei Chen",
            "Jerry Chun-Wei Lin",
            "Wei-Han Chen",
            "Jia Ji",
            "Zih-Ching Chen",
            "Feng-Hao Yeh",
            "Chao-Chun Chen"
        ],
        "tldr": "The paper introduces APT, a novel few-shot anomaly detection framework that utilizes self-generated anomaly samples and adaptive prompt tuning with semantic alignment to overcome the limitations of human-designed prompts in VLMs.",
        "tldr_zh": "该论文介绍了一种新颖的少样本异常检测框架APT，它利用自生成的异常样本和自适应提示调整与语义对齐来克服VLMs中人工设计的提示的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification",
        "summary": "Vision-language models (VLMs) have shown considerable potential in digital\npathology, yet their effectiveness remains limited for fine-grained,\ndisease-specific classification tasks such as distinguishing between glomerular\nsubtypes. The subtle morphological variations among these subtypes, combined\nwith the difficulty of aligning visual patterns with precise clinical\nterminology, make automated diagnosis in renal pathology particularly\nchallenging. In this work, we explore how large pretrained VLMs can be\neffectively adapted to perform fine-grained glomerular classification, even in\nscenarios where only a small number of labeled examples are available. In this\nwork, we introduce Glo-VLMs, a systematic framework designed to explore the\nadaptation of VLMs to fine-grained glomerular classification in\ndata-constrained settings. Our approach leverages curated pathology images\nalongside clinical text prompts to facilitate joint image-text representation\nlearning for nuanced renal pathology subtypes. By assessing various VLMs\narchitectures and adaptation strategies under a few-shot learning paradigm, we\nexplore how both the choice of method and the amount of labeled data impact\nmodel performance in clinically relevant scenarios. To ensure a fair\ncomparison, we evaluate all models using standardized multi-class metrics,\naiming to clarify the practical requirements and potential of large pretrained\nmodels for specialized clinical research applications. As a result, fine-tuning\nthe VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with\nonly 8 shots per class, demonstrating that even with highly limited\nsupervision, foundation models can be effectively adapted for fine-grained\nmedical image classification.",
        "url": "http://arxiv.org/abs/2508.15960v1",
        "published_date": "2025-08-21T21:05:44+00:00",
        "updated_date": "2025-08-21T21:05:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenhao Guo",
            "Rachit Saluja",
            "Tianyuan Yao",
            "Quan Liu",
            "Yuankai Huo",
            "Benjamin Liechty",
            "David J. Pisapia",
            "Kenji Ikemura",
            "Mert R. Sabuncu",
            "Yihe Yang",
            "Ruining Deng"
        ],
        "tldr": "The paper introduces Glo-VLMs, a framework for adapting vision-language models to fine-grained glomerular classification in renal pathology with limited labeled data, achieving promising results in few-shot learning scenarios.",
        "tldr_zh": "该论文介绍了 Glo-VLMs，一个用于在有限标记数据的情况下，将视觉语言模型应用于肾脏病理学中细粒度肾小球分类的框架，并在少样本学习场景中取得了有希望的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structuring GUI Elements through Vision Language Models: Towards Action Space Generation",
        "summary": "Multimodal large language models (MLLMs) have emerged as pivotal tools in\nenhancing human-computer interaction. In this paper we focus on the application\nof MLLMs in the field of graphical user interface (GUI) elements structuring,\nwhere they assist in processing user instructions based on screen contents.\nDespite the promise of MLLMs, their performance in precisely generating UI\nelement coordinates, a critical aspect of GUI understanding, is hindered by the\nnature of next-token prediction training. This challenge arises from the\nsemantic void surrounding numerical UI coordinates in language representation\nspaces, necessitating a substantial and diverse dataset to bolster visual\nmodule capabilities. To address these limitations, we introduce an\nIoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our\napproach involves a novel pipeline for IoU-based coordinate sampling to augment\nthe training data, which considers the proximity to ground truth coordinates.\nThis data augmentation strategy is then employed to fine-tune MLLMs under the\nIAML paradigm, which is designed to mitigate the exposure bias problem inherent\nin traditional maximum likelihood estimation. Through extensive experiments, we\ndemonstrate the superior performance of our IAML training approach over\ntraditional training paradigms.",
        "url": "http://arxiv.org/abs/2508.16271v1",
        "published_date": "2025-08-22T10:14:15+00:00",
        "updated_date": "2025-08-22T10:14:15+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yi Xu",
            "Yesheng Zhang",
            "jiajia Liu",
            "Jingdong Chen"
        ],
        "tldr": "This paper introduces an IoU-Augmented Maximum Likelihood (IAML) training paradigm to improve MLLM performance in generating precise UI element coordinates by addressing the semantic void of numerical coordinates. The approach uses IoU-based coordinate sampling for data augmentation and fine-tunes MLLMs to mitigate exposure bias.",
        "tldr_zh": "本文提出了一种IoU增强的最大似然（IAML）训练范式，通过解决数值坐标的语义空白，来提高MLLM在生成精确UI元素坐标方面的性能。 该方法采用基于IoU的坐标采样进行数据增强，并对MLLM进行微调以减轻暴露偏差。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation",
        "summary": "We present an Audio-Visual Language Model (AVLM) for expressive speech\ngeneration by integrating full-face visual cues into a pre-trained expressive\nspeech model. We explore multiple visual encoders and multimodal fusion\nstrategies during pre-training to identify the most effective integration\napproach. Subsequent fine-tuning on emotion recognition and expressive dialogue\ntasks yields substantial gains over speech-only baselines (e.g., +5 F1 in\nemotion recognition). AVLM highlights the value of expressive visual\ninformation in guiding speech generation and offers a foundation for end-to-end\nmultimodal conversational systems.",
        "url": "http://arxiv.org/abs/2508.16188v1",
        "published_date": "2025-08-22T08:08:45+00:00",
        "updated_date": "2025-08-22T08:08:45+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Weiting Tan",
            "Jiachen Lian",
            "Hirofumi Inaguma",
            "Paden Tomasello",
            "Philipp Koehn",
            "Xutai Ma"
        ],
        "tldr": "This paper introduces an Audio-Visual Language Model (AVLM) that leverages visual cues to improve expressive speech generation, demonstrating significant gains in emotion recognition and expressive dialogue tasks compared to speech-only models.",
        "tldr_zh": "本文介绍了一种音频-视觉语言模型（AVLM），该模型利用视觉线索来改进表达性语音生成，与仅语音模型相比，在情感识别和表达性对话任务中表现出显著的提升。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution",
        "summary": "The rich textual information of large vision-language models (VLMs) combined\nwith the powerful generative prior of pre-trained text-to-image (T2I) diffusion\nmodels has achieved impressive performance in single-image super-resolution\n(SISR). However, existing methods still face significant challenges in\ngenerating clear and accurate regional details, particularly in scenarios\ninvolving multiple objects. This challenge primarily stems from a lack of\nfine-grained regional descriptions and the models' insufficient ability to\ncapture complex prompts. To address these limitations, we propose a Regional\nAttention Guided Super-Resolution (RAGSR) method that explicitly extracts\nlocalized fine-grained information and effectively encodes it through a novel\nregional attention mechanism, enabling both enhanced detail and overall\nvisually coherent SR results. Specifically, RAGSR localizes object regions in\nan image and assigns fine-grained caption to each region, which are formatted\nas region-text pairs as textual priors for T2I models. A regional guided\nattention is then leveraged to ensure that each region-text pair is properly\nconsidered in the attention process while preventing unwanted interactions\nbetween unrelated region-text pairs. By leveraging this attention mechanism,\nour approach offers finer control over the integration of text and image\ninformation, thereby effectively overcoming limitations faced by traditional\nSISR techniques. Experimental results on benchmark datasets demonstrate that\nour approach exhibits superior performance in generating perceptually authentic\nvisual details while maintaining contextual consistency compared to existing\napproaches.",
        "url": "http://arxiv.org/abs/2508.16158v1",
        "published_date": "2025-08-22T07:28:34+00:00",
        "updated_date": "2025-08-22T07:28:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haodong He",
            "Yancheng Bai",
            "Rui Lan",
            "Xu Duan",
            "Lei Sun",
            "Xiangxiang Chu",
            "Gui-Song Xia"
        ],
        "tldr": "The paper introduces RAGSR, a Regional Attention Guided Super-Resolution method, to improve detail generation in single-image super-resolution by using fine-grained regional descriptions and a novel regional attention mechanism.",
        "tldr_zh": "该论文介绍了RAGSR，一种区域注意力引导的超分辨率方法，通过使用细粒度的区域描述和一种新的区域注意力机制来提高单图像超分辨率中的细节生成。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation",
        "summary": "Sign Language (SL) enables two-way communication for the deaf and\nhard-of-hearing community, yet many sign languages remain under-resourced in\nthe AI space. Sign Language Instruction Generation (SLIG) produces step-by-step\ntextual instructions that enable non-SL users to imitate and learn SL gestures,\npromoting two-way interaction. We introduce BdSLIG, the first Bengali SLIG\ndataset, used to evaluate Vision Language Models (VLMs) (i) on under-resourced\nSLIG tasks, and (ii) on long-tail visual concepts, as Bengali SL is unlikely to\nappear in the VLM pre-training data. To enhance zero-shot performance, we\nintroduce Sign Parameter-Infused (SPI) prompting, which integrates standard SL\nparameters, like hand shape, motion, and orientation, directly into the textual\nprompts. Subsuming standard sign parameters into the prompt makes the\ninstructions more structured and reproducible than free-form natural text from\nvanilla prompting. We envision that our work would promote inclusivity and\nadvancement in SL learning systems for the under-resourced communities.",
        "url": "http://arxiv.org/abs/2508.16076v1",
        "published_date": "2025-08-22T04:11:28+00:00",
        "updated_date": "2025-08-22T04:11:28+00:00",
        "categories": [
            "cs.HC",
            "cs.CV"
        ],
        "authors": [
            "Md Tariquzzaman",
            "Md Farhan Ishmam",
            "Saiyma Sittul Muna",
            "Md Kamrul Hasan",
            "Hasan Mahmud"
        ],
        "tldr": "The paper introduces a new Bengali Sign Language Instruction Generation (BdSLIG) dataset and a Sign Parameter-Infused (SPI) prompting method to improve zero-shot performance of VLMs in generating sign language instructions, especially for under-resourced languages.",
        "tldr_zh": "该论文介绍了一个新的孟加拉语手语教学生成（BdSLIG）数据集，并提出了一种手语参数注入（SPI）的提示方法，以提高视觉语言模型在生成手语指令方面的零样本性能，特别是对于资源不足的语言。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]