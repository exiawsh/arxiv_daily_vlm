[
    {
        "title": "MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning",
        "summary": "Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.",
        "url": "http://arxiv.org/abs/2601.07107v1",
        "published_date": "2026-01-12T00:11:10+00:00",
        "updated_date": "2026-01-12T00:11:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Meng Lu",
            "Yuxing Lu",
            "Yuchen Zhuang",
            "Megan Mullins",
            "Yang Xie",
            "Guanghua Xiao",
            "Charles Fleming",
            "Wenqi Shi",
            "Xuan Wang"
        ],
        "tldr": "The paper introduces MedVistaGym, a scalable training environment for medical image analysis using tool-integrated reinforcement learning, enabling VLMs to perform multi-step reasoning with tools and achieve significant performance gains on medical VQA tasks.",
        "tldr_zh": "该论文介绍了MedVistaGym，一个可扩展的训练环境，用于通过工具集成的强化学习进行医学图像分析，使VLM能够使用工具执行多步骤推理，并在医学VQA任务上实现显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression",
        "summary": "Autonomous driving increasingly relies on Visual Question Answering (VQA) to enable vehicles to understand complex surroundings by analyzing visual inputs and textual queries. Currently, a paramount concern for VQA in this domain is the stringent requirement for fast latency and real-time processing, as delays directly impact real-world safety in this safety-critical application. However, current state-of-the-art VQA models, particularly large vision-language models (VLMs), often prioritize performance over computational efficiency. These models typically process dense patch tokens for every frame, leading to prohibitive computational costs (FLOPs) and significant inference latency, especially with long video sequences. This focus limits their practical deployment in real-time autonomous driving scenarios. To tackle this issue, we propose an efficient VLM framework for autonomous driving VQA tasks, SRC-Pipeline. It learns to compress early frame tokens into a small number of high-level tokens while retaining full patch tokens for recent frames. Experiments on autonomous driving video question answering tasks show that our approach achieves 66% FLOPs reduction while maintaining comparable performance, enabling VLMs to operate more effectively in real-time, safety-critical autonomous driving settings.",
        "url": "http://arxiv.org/abs/2601.07092v1",
        "published_date": "2026-01-11T23:25:49+00:00",
        "updated_date": "2026-01-11T23:25:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuliang Cai",
            "Dongqiangzi Ye",
            "Zitian Chen",
            "Chongruo Wu"
        ],
        "tldr": "The paper introduces SRC-Pipeline, an efficient VLM framework for autonomous driving VQA that reduces FLOPs by compressing early frame tokens, enabling real-time performance with comparable accuracy.",
        "tldr_zh": "该论文介绍了一种名为SRC-Pipeline的高效VLM框架，用于自动驾驶VQA。它通过压缩早期帧的tokens来减少FLOPs，从而在保持相当准确率的同时实现实时性能。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    }
]