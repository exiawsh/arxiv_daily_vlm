[
    {
        "title": "SAMTok: Representing Any Mask with Two Words",
        "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
        "url": "http://arxiv.org/abs/2601.16093v1",
        "published_date": "2026-01-22T16:44:09+00:00",
        "updated_date": "2026-01-22T16:44:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yikang Zhou",
            "Tao Zhang",
            "Dengxian Gong",
            "Yuanzheng Wu",
            "Ye Tian",
            "Haochen Wang",
            "Haobo Yuan",
            "Jiacong Wang",
            "Lu Qi",
            "Hao Fei",
            "Anran Wang",
            "Zhuochen Wang",
            "Yujing Wang",
            "Cheng Chen",
            "Shunping Ji",
            "Xiangtai Li"
        ],
        "tldr": "The paper introduces SAMTok, a novel mask tokenizer that converts any region mask into two tokens, enabling pixel-wise capabilities in MLLMs through standard next-token prediction and reinforcement learning without architectural changes.",
        "tldr_zh": "该论文介绍了一种名为SAMTok的新型掩码标记器，它可以将任何区域掩码转换为两个标记，从而通过标准的下一个标记预测和强化学习，在多模态大型语言模型中实现像素级能力，而无需改变架构。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models",
        "summary": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.",
        "url": "http://arxiv.org/abs/2601.16073v1",
        "published_date": "2026-01-22T16:18:02+00:00",
        "updated_date": "2026-01-22T16:18:02+00:00",
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "authors": [
            "Hanwen Zhang",
            "Qiaojin Shen",
            "Yuxi Liu",
            "Yuesheng Zhu",
            "Guibo Luo"
        ],
        "tldr": "DSFedMed is a federated learning framework using mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation, achieving improved performance and reduced communication costs.",
        "tldr_zh": "DSFedMed是一个联邦学习框架，它利用中心化基础模型和轻量级客户端模型之间的双向知识蒸馏来进行医学图像分割，实现了性能提升并降低了通信成本。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video",
        "summary": "Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.",
        "url": "http://arxiv.org/abs/2601.15780v1",
        "published_date": "2026-01-22T09:14:11+00:00",
        "updated_date": "2026-01-22T09:14:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pascal Benschop",
            "Justin Dauwels",
            "Jan van Gemert"
        ],
        "tldr": "This paper introduces a synthetic video benchmark to evaluate the situational and spatial awareness of VLMs, revealing weaknesses in understanding temporal and geometric cues. Results indicate VLMs perform poorly on tasks requiring violence detection, role binding across viewpoints, and trajectory alignment.",
        "tldr_zh": "该论文介绍了一个合成视频基准，用于评估视觉语言模型的环境和空间感知能力，揭示了其在理解时间和几何线索方面的不足。结果表明，在需要暴力检测、跨视角角色绑定和轨迹对齐的任务中，视觉语言模型的表现不佳。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework",
        "summary": "Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.",
        "url": "http://arxiv.org/abs/2601.15711v1",
        "published_date": "2026-01-22T07:33:41+00:00",
        "updated_date": "2026-01-22T07:33:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shubham Shukla",
            "Kunal Sonalkar"
        ],
        "tldr": "This paper introduces a three-tier evaluation framework for zero-shot vision-language models (VLMs) in product attribute labeling, highlighting their strengths in fine-grained classification but weaknesses in attribute applicability detection, using fashion retail as an example.",
        "tldr_zh": "本文介绍了一个用于评估零样本视觉语言模型（VLMs）在产品属性标注中的表现的三层评估框架。研究发现，VLMs在细粒度分类方面表现出色，但在属性适用性检测方面存在不足，以时尚零售为例。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
        "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
        "url": "http://arxiv.org/abs/2601.15549v1",
        "published_date": "2026-01-22T00:35:30+00:00",
        "updated_date": "2026-01-22T00:35:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ryo Fujii",
            "Hideo Saito",
            "Ryo Hachiuma"
        ],
        "tldr": "The paper introduces VIOLA, a label-efficient video in-context learning framework that uses density-uncertainty-weighted sampling for minimal expert annotation and confidence-aware retrieval and prompting to leverage unlabeled data, showing improved performance on video tasks with limited labels.",
        "tldr_zh": "该论文介绍了VIOLA，一个标签高效的视频上下文学习框架，它使用密度-不确定性加权采样来实现最少的专家标注，并使用置信度感知的检索和提示来利用未标记数据，在标签有限的情况下，在视频任务上表现出更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection",
        "summary": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.",
        "url": "http://arxiv.org/abs/2601.15453v1",
        "published_date": "2026-01-21T20:35:51+00:00",
        "updated_date": "2026-01-21T20:35:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Morteza Poudineh",
            "Marc Lalonde"
        ],
        "tldr": "The paper proposes a deviation-guided prompt learning framework (DevPrompt) for few-normal shot anomaly detection, enhancing separability between normal and abnormal prompts using learnable context vectors and deviation-based scoring with Top-K MIL.",
        "tldr_zh": "该论文提出了一种基于偏差引导的提示学习框架 (DevPrompt)，用于少样本正常图像异常检测，通过可学习的上下文向量和基于偏差的评分与 Top-K MIL 来增强正常和异常提示之间的可分离性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation",
        "summary": "Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure",
        "url": "http://arxiv.org/abs/2601.15408v1",
        "published_date": "2026-01-21T19:19:41+00:00",
        "updated_date": "2026-01-21T19:19:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Pablo Messina",
            "Andrés Villa",
            "Juan León Alcázar",
            "Karen Sánchez",
            "Carlos Hinojosa",
            "Denis Parra",
            "Álvaro Soto",
            "Bernard Ghanem"
        ],
        "tldr": "The paper introduces CURE, a curriculum learning framework for improving grounding accuracy and reliability in medical vision-language models for radiology report generation, demonstrating improved IoU, CXRFEScore, and reduced hallucinations without additional data.",
        "tldr_zh": "该论文介绍了CURE，一个课程学习框架，旨在提高医学视觉语言模型在放射学报告生成中的grounding准确性和可靠性，展示了IoU的提升、CXRFEScore的改进以及幻觉的减少，且无需额外数据。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Understanding Best Practices for Quantization of Vision-Language Models",
        "summary": "Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.",
        "url": "http://arxiv.org/abs/2601.15287v1",
        "published_date": "2026-01-21T18:59:51+00:00",
        "updated_date": "2026-01-21T18:59:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gautom Das",
            "Vincent La",
            "Ethan Lau",
            "Abhinav Shrivastava",
            "Matthew Gwilliam"
        ],
        "tldr": "This paper investigates the effects of various quantization techniques on vision-language models, finding that both vision and language components are important and that aggressive quantization of the language model is effective. The code is publicly available.",
        "tldr_zh": "本文研究了各种量化技术对视觉语言模型的影响，发现视觉和语言组件都很重要，并且对语言模型进行积极量化是有效的。代码已公开。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Iterative Refinement Improves Compositional Image Generation",
        "summary": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/",
        "url": "http://arxiv.org/abs/2601.15286v1",
        "published_date": "2026-01-21T18:59:40+00:00",
        "updated_date": "2026-01-21T18:59:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Shantanu Jaiswal",
            "Mihir Prabhudesai",
            "Nikash Bhardwaj",
            "Zheyang Qin",
            "Amir Zadeh",
            "Chuan Li",
            "Katerina Fragkiadaki",
            "Deepak Pathak"
        ],
        "tldr": "The paper introduces an iterative refinement strategy for text-to-image generation, using a vision-language model as a critic to guide progressive improvements, showing significant gains in compositional image generation tasks.",
        "tldr_zh": "该论文提出了一种用于文本到图像生成的迭代改进策略，使用视觉语言模型作为评论员来指导渐进式改进，在组合图像生成任务中表现出显著的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
        "summary": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.",
        "url": "http://arxiv.org/abs/2601.16065v1",
        "published_date": "2026-01-22T16:02:56+00:00",
        "updated_date": "2026-01-22T16:02:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Chenyang Li",
            "Jieyuan Liu",
            "Bin Li",
            "Bo Gao",
            "Yilin Yuan",
            "Yangfan He",
            "Yuchen Li",
            "Jingqun Tang"
        ],
        "tldr": "This paper introduces a Distracting Token Pruning (DTP) framework for Vision-Language Action models to improve task success rates by dynamically pruning distracting image tokens, demonstrating improved performance on the SIMPLER benchmark.",
        "tldr_zh": "该论文介绍了一种用于视觉-语言动作模型的分散Token剪枝 (DTP) 框架，通过动态剪枝分散注意力的图像Token来提高任务成功率，并在 SIMPLER 基准测试中展示了性能改进。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
        "summary": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2601.16007v1",
        "published_date": "2026-01-22T14:33:01+00:00",
        "updated_date": "2026-01-22T14:33:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chak-Wing Mak",
            "Guanyu Zhu",
            "Boyi Zhang",
            "Hongji Li",
            "Xiaowei Chi",
            "Kevin Zhang",
            "Yichen Wu",
            "Yangfan He",
            "Chun-Kai Fan",
            "Wentao Lu",
            "Kuangzhi Ge",
            "Xinyu Fang",
            "Hongyang He",
            "Kuan Lu",
            "Tianxiang Xu",
            "Li Zhang",
            "Yongxin Ni",
            "Youhua Li",
            "Shanghang Zhang"
        ],
        "tldr": "The paper introduces PhysicsMind, a new benchmark to evaluate the physical reasoning abilities of VLMs and video world models, finding that current models often fail to adhere to basic physics principles.",
        "tldr_zh": "该论文介绍了PhysicsMind，一个新的基准，用于评估VLMs和视频世界模型的物理推理能力，发现当前的模型通常未能遵守基本的物理定律。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs",
        "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.",
        "url": "http://arxiv.org/abs/2601.15698v1",
        "published_date": "2026-01-22T06:56:27+00:00",
        "updated_date": "2026-01-22T06:56:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingyu Yu",
            "Lana Liu",
            "Zhehao Zhao",
            "Wei Wang",
            "Sujuan Qin"
        ],
        "tldr": "This paper introduces a novel framework, Beyond Visual Safety (BVS), to jailbreak MLLMs into generating harmful images through a reconstruction-then-generation strategy, achieving a high success rate against GPT-5.",
        "tldr_zh": "该论文介绍了一种名为Beyond Visual Safety (BVS) 的新型框架，通过重建然后生成的策略，对多模态大型语言模型进行越狱，使其生成有害图像，并在GPT-5上取得了很高的成功率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling",
        "summary": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.",
        "url": "http://arxiv.org/abs/2601.15664v1",
        "published_date": "2026-01-22T05:23:20+00:00",
        "updated_date": "2026-01-22T05:23:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongyang Wei",
            "Hongbo Liu",
            "Zidong Wang",
            "Yi Peng",
            "Baixin Xu",
            "Size Wu",
            "Xuying Zhang",
            "Xianglong He",
            "Zexiang Liu",
            "Peiyu Wang",
            "Xuchen Song",
            "Yangguang Li",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "tldr": "Skywork UniPic 3.0 is a unified multimodal framework for single and multi-image composition, focusing on HOI and achieving SOTA results through a novel sequence-modeling training paradigm and inference acceleration techniques. The code, models and dataset are publicly available.",
        "tldr_zh": "Skywork UniPic 3.0 是一个统一的多模态框架，用于单图像和多图像合成，侧重于 HOI，并通过一种新的序列建模训练范式和推理加速技术实现了 SOTA 结果。代码、模型和数据集公开可用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition",
        "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.",
        "url": "http://arxiv.org/abs/2601.15406v1",
        "published_date": "2026-01-21T19:17:21+00:00",
        "updated_date": "2026-01-21T19:17:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hatef Otroshi Shahreza",
            "Anjith George",
            "Sébastien Marcel"
        ],
        "tldr": "This paper benchmarks the performance of current open-source MLLMs on heterogeneous face recognition (HFR) tasks, finding they underperform compared to classical methods, especially in cross-spectral conditions, highlighting the need for rigorous biometric evaluation.",
        "tldr_zh": "本文对当前开源的多模态大型语言模型（MLLM）在异构人脸识别（HFR）任务上的性能进行了基准测试，发现它们与经典方法相比表现不佳，尤其是在跨光谱条件下，强调了在人脸识别系统中部署 MLLM 时需要进行严格的生物特征评估。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]