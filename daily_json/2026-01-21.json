[
    {
        "title": "Revisiting Multi-Task Visual Representation Learning",
        "summary": "Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity \"expert\" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves \"best-of-both-worlds\" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.",
        "url": "http://arxiv.org/abs/2601.13886v1",
        "published_date": "2026-01-20T11:59:19+00:00",
        "updated_date": "2026-01-20T11:59:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shangzhe Di",
            "Zhonghua Zhai",
            "Weidi Xie"
        ],
        "tldr": "This paper introduces MTV, a multi-task visual pretraining framework that combines vision-language, self-supervised, and dense spatial objectives using pseudo-labels to achieve a \"best-of-both-worlds\" performance in visual representation learning.",
        "tldr_zh": "本文介绍了MTV，一个多任务视觉预训练框架，它结合了视觉-语言、自监督和密集空间目标，并使用伪标签来实现视觉表征学习中“两全其美”的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models",
        "summary": "Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.",
        "url": "http://arxiv.org/abs/2601.14188v1",
        "published_date": "2026-01-20T17:45:24+00:00",
        "updated_date": "2026-01-20T17:45:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liang Shi",
            "Wei Li",
            "Kevin M Beussman",
            "Lin Chen",
            "Yun Fu"
        ],
        "tldr": "The paper introduces IIR-VLM, a vision-language model enhanced for in-context instance-level recognition by integrating pre-trained ILR expert models, enabling one-shot learning and improved performance in recognizing diverse instances.",
        "tldr_zh": "该论文介绍了IIR-VLM，一种增强的视觉-语言模型，通过集成预训练的ILR专家模型来实现上下文中的实例级识别，从而实现一次性学习并提高识别各种实例的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
        "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.",
        "url": "http://arxiv.org/abs/2601.14133v1",
        "published_date": "2026-01-20T16:30:07+00:00",
        "updated_date": "2026-01-20T16:30:07+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bin Yu",
            "Shijie Lian",
            "Xiaopeng Lin",
            "Yuliang Wei",
            "Zhaolong Shen",
            "Changti Wu",
            "Yuzhuo Miao",
            "Xinming Wang",
            "Bailing Wang",
            "Cong Huang",
            "Kai Chen"
        ],
        "tldr": "TwinBrainVLA introduces a novel architecture with asymmetric mixture-of-transformers to address the catastrophic forgetting problem in VLA models by separating general semantic understanding from embodied proprioception, leading to improved robotic manipulation performance.",
        "tldr_zh": "TwinBrainVLA 引入了一种新颖的架构，采用非对称混合Transformer来解决VLA模型中的灾难性遗忘问题，通过分离通用语义理解和具身本体感受，从而提高机器人操作性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning",
        "summary": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.",
        "url": "http://arxiv.org/abs/2601.14084v1",
        "published_date": "2026-01-20T15:44:57+00:00",
        "updated_date": "2026-01-20T15:44:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Abdurrahim Yilmaz",
            "Ozan Erdem",
            "Ece Gokyayla",
            "Ayda Acar",
            "Burc Bugra Dagtas",
            "Dilara Ilhan Erdil",
            "Gulsum Gencoglan",
            "Burak Temelkuran"
        ],
        "tldr": "The paper introduces DermaBench, a new clinician-annotated dermatology VQA benchmark dataset designed to evaluate the visual understanding, language grounding, and clinical reasoning capabilities of vision-language models in dermatology.",
        "tldr_zh": "该论文介绍了一个新的临床医生标注的皮肤病学视觉问答基准数据集DermaBench，旨在评估视觉语言模型在皮肤病学中的视觉理解、语言基础和临床推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
        "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.",
        "url": "http://arxiv.org/abs/2601.13976v1",
        "published_date": "2026-01-20T13:54:10+00:00",
        "updated_date": "2026-01-20T13:54:10+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jing Zuo",
            "Lingzhou Mu",
            "Fan Jiang",
            "Chengcheng Ma",
            "Mu Xu",
            "Yonggang Qi"
        ],
        "tldr": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation (VLN) that leverages a compact latent space for imagined visual tokens, achieving real-time performance with improved success rates and efficiency compared to explicit Chain-of-Thought (CoT) methods.",
        "tldr_zh": "FantasyVLN 提出了一种统一的隐式推理框架，用于视觉语言导航 (VLN)，该框架利用紧凑的潜在空间来表示想象的视觉标记，与显式思维链 (CoT) 方法相比，实现了实时性能，并提高了成功率和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning",
        "summary": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.",
        "url": "http://arxiv.org/abs/2601.13942v1",
        "published_date": "2026-01-20T13:18:18+00:00",
        "updated_date": "2026-01-20T13:18:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongbo Bai",
            "Yujin Zhou",
            "Yile Wu",
            "Chi-Min Chan",
            "Pengcheng Wen",
            "Kunhao Pan",
            "Sirui Han",
            "Yike Guo"
        ],
        "tldr": "The paper introduces Glance-or-Gaze (GoG), a reinforcement learning based framework for LMMs that adaptively focuses visual search by selectively attending to relevant image regions, outperforming existing methods on knowledge-intensive visual queries. It uses a two-stage training approach combining supervised fine-tuning and reinforcement learning.",
        "tldr_zh": "该论文介绍了一种基于强化学习的框架 Glance-or-Gaze (GoG)，用于 LMMs，通过选择性地关注相关的图像区域来自适应地聚焦视觉搜索，在知识密集型视觉查询方面优于现有方法。它采用了一种两阶段训练方法，结合了监督式微调和强化学习。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring",
        "summary": "While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.",
        "url": "http://arxiv.org/abs/2601.13879v1",
        "published_date": "2026-01-20T11:45:38+00:00",
        "updated_date": "2026-01-20T11:45:38+00:00",
        "categories": [
            "cs.MM",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Dongxu Zhang",
            "Yiding Sun",
            "Cheng Tan",
            "Wenbiao Yan",
            "Ning Yang",
            "Jihua Zhu",
            "Hiajun Zhang"
        ],
        "tldr": "The paper introduces V-Skip, a novel token pruning method for multimodal LLMs that avoids visual amnesia by considering visual anchors, achieving significant speedup with minimal accuracy loss.",
        "tldr_zh": "该论文介绍了V-Skip，一种新颖的多模态LLM token剪枝方法，通过考虑视觉锚点来避免视觉失忆，实现了显著的加速，且精度损失极小。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes",
        "summary": "Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.",
        "url": "http://arxiv.org/abs/2601.13839v1",
        "published_date": "2026-01-20T10:50:46+00:00",
        "updated_date": "2026-01-20T10:50:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aisha Al-Mohannadi",
            "Ayisha Firoz",
            "Yin Yang",
            "Muhammad Imran",
            "Ferda Ofli"
        ],
        "tldr": "The paper introduces DisasterVQA, a new VQA dataset for disaster scenes, highlighting the challenges faced by current VLM models in this complex, safety-critical domain and providing a benchmark for future research.",
        "tldr_zh": "该论文介绍了DisasterVQA，一个用于灾难场景的新VQA数据集，强调了当前VLM模型在这个复杂、安全关键领域中面临的挑战，并为未来的研究提供了一个基准。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval",
        "summary": "Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.",
        "url": "http://arxiv.org/abs/2601.13797v1",
        "published_date": "2026-01-20T09:57:04+00:00",
        "updated_date": "2026-01-20T09:57:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gabriele Serussi",
            "David Vainshtein",
            "Jonathan Kouchly",
            "Dotan Di Castro",
            "Chaim Baskin"
        ],
        "tldr": "The paper introduces PREGEN, a computationally efficient framework for Composed Video Retrieval (CoVR) that leverages a frozen, pre-trained VLM and a lightweight encoder, achieving state-of-the-art results without VLM fine-tuning.",
        "tldr_zh": "该论文介绍了PREGEN，一个计算高效的组合视频检索（CoVR）框架，它利用冻结的预训练VLM和一个轻量级编码器，在不进行VLM微调的情况下实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search",
        "summary": "Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.",
        "url": "http://arxiv.org/abs/2601.13719v1",
        "published_date": "2026-01-20T08:23:29+00:00",
        "updated_date": "2026-01-20T08:23:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR"
        ],
        "authors": [
            "Xinlei Yin",
            "Xiulian Peng",
            "Xiao Li",
            "Zhiwei Xiong",
            "Yan Lu"
        ],
        "tldr": "HAVEN, a novel framework, addresses long video understanding by integrating audiovisual entity cohesion, hierarchical video indexing, and agentic search, achieving state-of-the-art results on LVBench, especially in reasoning.",
        "tldr_zh": "HAVEN是一个新的框架，通过整合视听实体凝聚力、分层视频索引和代理搜索来解决长视频理解问题，在LVBench上取得了最先进的成果，尤其是在推理方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs",
        "summary": "Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.",
        "url": "http://arxiv.org/abs/2601.13707v1",
        "published_date": "2026-01-20T08:04:18+00:00",
        "updated_date": "2026-01-20T08:04:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yujin Jo",
            "Sangyoon Bae",
            "Taesup Kim"
        ],
        "tldr": "This paper introduces Attention-space Contrastive Guidance (ACG), a computationally efficient method to mitigate hallucinations in LVLMs by contrasting visually grounded and language-only representations within the attention layers, achieving state-of-the-art faithfulness and reducing latency compared to prior methods.",
        "tldr_zh": "本文介绍了一种名为注意力空间对比引导 (ACG) 的计算高效方法，通过在注意力层中对比视觉引导和纯语言表征，来减轻 LVLM 中的幻觉，从而实现最先进的忠实度，并降低了与先前方法相比的延迟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles",
        "summary": "Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.",
        "url": "http://arxiv.org/abs/2601.13705v1",
        "published_date": "2026-01-20T08:02:04+00:00",
        "updated_date": "2026-01-20T08:02:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maria Lymperaiou",
            "Vasileios Karampinis",
            "Giorgos Filandrianos",
            "Angelos Vlachos",
            "Chrysoula Zerva",
            "Athanasios Voulodimos"
        ],
        "tldr": "This survey paper analyzes visual puzzle reasoning in Large Vision-Language Models (LVLMs), categorizing benchmarks by reasoning mechanisms and highlighting limitations like brittle generalization and entanglement between perception and reasoning, suggesting future research directions.",
        "tldr_zh": "这篇综述论文分析了大型视觉语言模型（LVLMs）中的视觉谜题推理，通过推理机制对基准进行分类，并强调了脆性泛化以及感知和推理之间的纠缠等局限性，提出了未来的研究方向。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Test-time Inference for Visual Grounding",
        "summary": "Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.",
        "url": "http://arxiv.org/abs/2601.13633v1",
        "published_date": "2026-01-20T06:07:02+00:00",
        "updated_date": "2026-01-20T06:07:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guanqi Zhan",
            "Changye Li",
            "Zhijian Liu",
            "Yao Lu",
            "Yi Wu",
            "Song Han",
            "Ligeng Zhu"
        ],
        "tldr": "The paper introduces EGM, a method to scale test-time computation for smaller VLMs to improve their visual grounding performance and efficiency, achieving comparable or better results than larger models with significantly reduced latency.",
        "tldr_zh": "该论文介绍了一种名为EGM的方法，通过扩展小型VLM的测试时计算来提高其视觉定位性能和效率，从而以显著降低的延迟实现与大型模型相当甚至更好的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models",
        "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.",
        "url": "http://arxiv.org/abs/2601.13622v1",
        "published_date": "2026-01-20T05:44:33+00:00",
        "updated_date": "2026-01-20T05:44:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Donghee Lee",
            "Rui Cai",
            "Zhe Zhao"
        ],
        "tldr": "CARPE is a model-agnostic framework for LVLMs that adaptively weights visual and textual modalities by prioritizing image representations or language model reasoning based on context, improving performance on both image classification and vision-language benchmarks.",
        "tldr_zh": "CARPE是一个模型无关的框架，用于大型视觉语言模型（LVLM），它通过根据上下文确定优先使用图像表征或语言模型推理，自适应地权衡视觉和文本模态，从而提高图像分类和视觉语言基准测试的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
        "summary": "Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.",
        "url": "http://arxiv.org/abs/2601.13606v1",
        "published_date": "2026-01-20T05:11:44+00:00",
        "updated_date": "2026-01-20T05:11:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zheng Liu",
            "Honglin Lin",
            "Chonghan Qin",
            "Xiaoyang Wang",
            "Xin Gao",
            "Yu Li",
            "Mengzhang Cai",
            "Yun Zhu",
            "Zhanping Zhong",
            "Qizhi Pei",
            "Zhuoshi Pan",
            "Xiaoran Shang",
            "Bin Cui",
            "Conghui He",
            "Wentao Zhang",
            "Lijun Wu"
        ],
        "tldr": "The paper introduces ChartVerse, a framework for synthesizing complex charts and reliable reasoning data to improve VLM chart reasoning capabilities, achieving state-of-the-art performance with an 8B model.",
        "tldr_zh": "本文介绍了ChartVerse，一个用于合成复杂图表和可靠推理数据的框架，旨在提高VLM的图表推理能力，并通过一个8B模型实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion",
        "summary": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.",
        "url": "http://arxiv.org/abs/2601.14056v1",
        "published_date": "2026-01-20T15:13:43+00:00",
        "updated_date": "2026-01-20T15:13:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Andrea Rigo",
            "Luca Stornaiuolo",
            "Weijie Wang",
            "Mauro Martino",
            "Bruno Lepri",
            "Nicu Sebe"
        ],
        "tldr": "POCI-Diff is a diffusion-based method for text-to-image generation with 3D layout control, enabling consistent object placement and editing without warping artifacts by using blended latent diffusion and conditioning on reference images via IP-Adapter.",
        "tldr_zh": "POCI-Diff是一种基于扩散模型的文本到图像生成方法，通过混合潜在扩散和使用IP-Adapter对参考图像进行条件化，实现了具有3D布局控制，并能保持一致的对象放置和编辑，且没有扭曲伪影。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model",
        "summary": "Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.",
        "url": "http://arxiv.org/abs/2601.14052v1",
        "published_date": "2026-01-20T15:06:10+00:00",
        "updated_date": "2026-01-20T15:06:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Xu",
            "Yanlin Liu",
            "Zizhao Tong",
            "Jiaze Li",
            "Kexue Fu",
            "Yuyang Zhang",
            "Longxiang Gao",
            "Shuaiguang Li",
            "Xingyu Li",
            "Yanran Xu",
            "Changwei Wang"
        ],
        "tldr": "The paper proposes a multimodal approach (MM-OOD) using MLLMs to improve out-of-distribution detection in images, showing improvements on multimodal datasets like Food-101 and ImageNet-1K.",
        "tldr_zh": "该论文提出了一种使用MLLM的多模态方法 (MM-OOD)，用于提高图像中的分布外检测，并在Food-101和ImageNet-1K等多模态数据集上表现出改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology",
        "summary": "While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.",
        "url": "http://arxiv.org/abs/2601.14044v1",
        "published_date": "2026-01-20T15:00:15+00:00",
        "updated_date": "2026-01-20T15:00:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyu Wu",
            "Pucheng Han",
            "Hualong Zhang",
            "Naigeng Wu",
            "Keze Wang"
        ],
        "tldr": "The paper introduces Weather-R1, a logically consistent VLM for meteorology, along with a new benchmark (WeatherQA) and a novel reinforcement fine-tuning method (LoCo-RFT) to address the domain and reasoning faithfulness gaps in applying VLMs to meteorology. Weather-R1 outperforms existing methods, including the base model Qwen2.5-VL-32B.",
        "tldr_zh": "该论文介绍了 Weather-R1，一个逻辑一致的气象领域 VLM，以及一个新的基准（WeatherQA）和一个新的强化微调方法（LoCo-RFT），旨在解决将 VLM 应用于气象学中的领域和推理忠实性差距。Weather-R1 的性能优于现有方法，包括基础模型 Qwen2.5-VL-32B。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs",
        "summary": "Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \\textbf{HyperWalker}, a \\textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \\textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \\textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \\textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker",
        "url": "http://arxiv.org/abs/2601.13919v1",
        "published_date": "2026-01-20T12:48:09+00:00",
        "updated_date": "2026-01-20T12:48:09+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yuezhe Yang",
            "Hao Wang",
            "Yige Peng",
            "Jinman Kim",
            "Lei Bi"
        ],
        "tldr": "The paper introduces HyperWalker, a deep diagnosis framework using dynamic hypergraphs and reinforcement learning to integrate EHR data and X-ray images for improved clinical diagnosis, achieving state-of-the-art results on MRG and medical VQA tasks.",
        "tldr_zh": "该论文介绍了 HyperWalker，一个深度诊断框架，它使用动态超图和强化学习来整合 EHR 数据和 X 射线图像，以改进临床诊断，并在 MRG 和医学 VQA 任务上实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3",
        "summary": "Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.",
        "url": "http://arxiv.org/abs/2601.13895v1",
        "published_date": "2026-01-20T12:25:41+00:00",
        "updated_date": "2026-01-20T12:25:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xu Zhang",
            "Danyang Li",
            "Yingjie Xia",
            "Xiaohang Dong",
            "Hualong Yu",
            "Jianye Wang",
            "Qicheng Li"
        ],
        "tldr": "The paper introduces OmniOVCD, a novel framework for Open-Vocabulary Change Detection (OVCD) that leverages SAM 3's capabilities to achieve state-of-the-art performance on multiple benchmarks by synergistically fusing and decoupling SAM 3's outputs.",
        "tldr_zh": "该论文介绍了OmniOVCD，一个新颖的开放词汇变化检测（OVCD）框架，利用SAM 3的功能，通过协同融合和解耦SAM 3的输出来在多个基准测试中实现最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
        "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).",
        "url": "http://arxiv.org/abs/2601.13836v1",
        "published_date": "2026-01-20T10:47:20+00:00",
        "updated_date": "2026-01-20T10:47:20+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Qian Chen",
            "Jinlan Fu",
            "Changsong Li",
            "See-Kiong Ng",
            "Xipeng Qiu"
        ],
        "tldr": "The paper introduces FutureOmni, a new benchmark for evaluating multimodal LLMs' ability to forecast future events from audio-visual cues. It also proposes an instruction-tuning dataset and training strategy (OFF) to improve future forecasting capabilities.",
        "tldr_zh": "本文介绍了FutureOmni，一个新的基准，用于评估多模态LLM从视听线索预测未来事件的能力。此外，它还提出了一种指令调优数据集和训练策略（OFF），以提高未来预测能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation",
        "summary": "Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.",
        "url": "http://arxiv.org/abs/2601.13440v1",
        "published_date": "2026-01-19T22:55:30+00:00",
        "updated_date": "2026-01-19T22:55:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohit Kakda",
            "Mirudula Shri Muthukumaran",
            "Uttapreksha Patel",
            "Lawrence Swaminathan Xavier Prince"
        ],
        "tldr": "This paper analyzes various VLM-based approaches for anomaly classification and segmentation, providing insights into their performance and limitations on benchmark datasets.",
        "tldr_zh": "本文分析了各种基于VLM的异常分类和分割方法，并提供了它们在基准数据集上的性能和局限性的见解。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics",
        "summary": "Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.",
        "url": "http://arxiv.org/abs/2601.13401v1",
        "published_date": "2026-01-19T21:14:34+00:00",
        "updated_date": "2026-01-19T21:14:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Peter A. Massih",
            "Eric Cosatto"
        ],
        "tldr": "This paper introduces SQuID, a new dataset for quantitative spatial reasoning with satellite imagery, and QVLM, a code-generation VLM architecture designed to preserve pixel-level information, achieving improved accuracy compared to traditional VLMs.",
        "tldr_zh": "本文介绍了一个新的用于定量空间推理的卫星图像数据集SQuID，以及一种代码生成VLM架构QVLM，该架构旨在保留像素级信息，与传统VLM相比，实现了更高的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Organ-Aware Attention Improves CT Triage and Classification",
        "summary": "There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.",
        "url": "http://arxiv.org/abs/2601.13385v1",
        "published_date": "2026-01-19T20:37:45+00:00",
        "updated_date": "2026-01-19T20:37:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lavsen Dahal",
            "Yubraj Bhandari",
            "Geoffrey D. Rubin",
            "Joseph Y. Lo"
        ],
        "tldr": "This paper introduces ORACLE-CT, an organ-aware attention mechanism for improving CT triage and classification, achieving state-of-the-art supervised performance on chest and abdomen CT datasets.",
        "tldr_zh": "本文介绍了ORACLE-CT，一种器官感知注意力机制，用于改进CT分诊和分类，并在胸部和腹部CT数据集上实现了最先进的监督性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]