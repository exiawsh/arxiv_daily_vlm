[
    {
        "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks",
        "summary": "Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
        "url": "http://arxiv.org/abs/2510.25760v1",
        "published_date": "2025-10-29T17:55:43+00:00",
        "updated_date": "2025-10-29T17:55:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Zheng",
            "Zihao Dongfang",
            "Lutao Jiang",
            "Boyuan Zheng",
            "Yulong Guo",
            "Zhenquan Zhang",
            "Giuliano Albanese",
            "Runyi Yang",
            "Mengjiao Ma",
            "Zixin Zhang",
            "Chenfei Liao",
            "Dingcheng Zhen",
            "Yuanhuiyi Lyu",
            "Yuqian Fu",
            "Bin Ren",
            "Linfeng Zhang",
            "Danda Pani Paudel",
            "Nicu Sebe",
            "Luc Van Gool",
            "Xuming Hu"
        ],
        "tldr": "This survey paper reviews multimodal spatial reasoning tasks in the era of large models, categorizing recent advances in MLLMs and introducing open benchmarks for evaluation across various spatial reasoning tasks, including 2D, 3D, embodied AI, and emerging modalities like audio and egocentric video.",
        "tldr_zh": "这篇综述论文回顾了大型模型时代下的多模态空间推理任务，对MLLM的最新进展进行了分类，并介绍了用于评估的开放基准，涵盖各种空间推理任务，包括2D、3D、具身人工智能以及音频和以自我为中心的视频等新兴模态。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers",
        "summary": "Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.",
        "url": "http://arxiv.org/abs/2510.25372v1",
        "published_date": "2025-10-29T10:42:56+00:00",
        "updated_date": "2025-10-29T10:42:56+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "M Yashwanth",
            "Sharannya Ghosh",
            "Aditay Tripathi",
            "Anirban Chakraborty"
        ],
        "tldr": "The paper introduces PEP-FedPT, a federated learning framework for visual prompt tuning of ViTs that uses class-contextualized mixed prompts (CCMP) based on class prototypes to achieve both generalization and personalization without client-dependent trainable parameters.",
        "tldr_zh": "该论文介绍了PEP-FedPT，一个用于ViT视觉提示调优的联邦学习框架，它使用基于类原型的情境化混合提示 (CCMP)，在不使用客户端依赖的可训练参数的情况下，实现泛化和个性化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation",
        "summary": "We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based\nframework for open-vocabulary object-part instance segmentation. Given an\nimage, LangHOPS can jointly detect and segment hierarchical object and part\ninstances from open-vocabulary candidate categories. Unlike prior approaches\nthat rely on heuristic or learnable visual grouping, our approach grounds\nobject-part hierarchies in language space. It integrates the MLLM into the\nobject-part parsing pipeline to leverage its rich knowledge and reasoning\ncapabilities, and link multi-granularity concepts within the hierarchies. We\nevaluate LangHOPS across multiple challenging scenarios, including in-domain\nand cross-dataset object-part instance segmentation, and zero-shot semantic\nsegmentation. LangHOPS achieves state-of-the-art results, surpassing previous\nmethods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on\nthe PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K\n(zero-shot). Ablation studies further validate the effectiveness of the\nlanguage-grounded hierarchy and MLLM driven part query refinement strategy. The\ncode will be released here.",
        "url": "http://arxiv.org/abs/2510.25263v1",
        "published_date": "2025-10-29T08:21:59+00:00",
        "updated_date": "2025-10-29T08:21:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Miao",
            "Jan-Nico Zaech",
            "Xi Wang",
            "Fabien Despinoy",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ],
        "tldr": "LangHOPS is a novel MLLM-based framework for open-vocabulary object-part instance segmentation, grounding object-part hierarchies in language space and achieving state-of-the-art results.",
        "tldr_zh": "LangHOPS是一个新颖的基于MLLM的开放词汇对象部件实例分割框架，它将对象部件层次结构建立在语言空间中，并取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Test-Time Adaptive Object Detection with Foundation Model",
        "summary": "In recent years, test-time adaptive object detection has attracted increasing\nattention due to its unique advantages in online domain adaptation, which\naligns more closely with real-world application scenarios. However, existing\napproaches heavily rely on source-derived statistical characteristics while\nmaking the strong assumption that the source and target domains share an\nidentical category space. In this paper, we propose the first foundation\nmodel-powered test-time adaptive object detection method that eliminates the\nneed for source data entirely and overcomes traditional closed-set limitations.\nSpecifically, we design a Multi-modal Prompt-based Mean-Teacher framework for\nvision-language detector-driven test-time adaptation, which incorporates text\nand visual prompt tuning to adapt both language and vision representation\nspaces on the test data in a parameter-efficient manner. Correspondingly, we\npropose a Test-time Warm-start strategy tailored for the visual prompts to\neffectively preserve the representation capability of the vision branch.\nFurthermore, to guarantee high-quality pseudo-labels in every test batch, we\nmaintain an Instance Dynamic Memory (IDM) module that stores high-quality\npseudo-labels from previous test samples, and propose two novel\nstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM's\nhigh-quality instances for enhancing original predictions and hallucinating\nimages without available pseudo-labels, respectively. Extensive experiments on\ncross-corruption and cross-dataset benchmarks demonstrate that our method\nconsistently outperforms previous state-of-the-art methods, and can adapt to\narbitrary cross-domain and cross-category target data. Code is available at\nhttps://github.com/gaoyingjay/ttaod_foundation.",
        "url": "http://arxiv.org/abs/2510.25175v1",
        "published_date": "2025-10-29T05:19:38+00:00",
        "updated_date": "2025-10-29T05:19:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingjie Gao",
            "Yanan Zhang",
            "Zhi Cai",
            "Di Huang"
        ],
        "tldr": "This paper introduces a novel test-time adaptive object detection method powered by foundation models, eliminating the need for source data and overcoming closed-set limitations through a multi-modal prompt-based mean-teacher framework and instance dynamic memory.",
        "tldr_zh": "本文提出了一种基于基础模型的新的测试时自适应目标检测方法，通过多模态提示均值教师框架和实例动态记忆，消除了对源数据的需求并克服了封闭集限制。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning",
        "summary": "We present a transformer-based multimodal framework for generating clinically\nrelevant captions for MRI scans. Our system combines a DEiT-Small vision\ntransformer as an image encoder, MediCareBERT for caption embedding, and a\ncustom LSTM-based decoder. The architecture is designed to semantically align\nimage and textual embeddings, using hybrid cosine-MSE loss and contrastive\ninference via vector similarity. We benchmark our method on the MultiCaRe\ndataset, comparing performance on filtered brain-only MRIs versus general MRI\nimages against state-of-the-art medical image captioning methods including\nBLIP, R2GenGPT, and recent transformer-based approaches. Results show that\nfocusing on domain-specific data improves caption accuracy and semantic\nalignment. Our work proposes a scalable, interpretable solution for automated\nmedical image reporting.",
        "url": "http://arxiv.org/abs/2510.25164v1",
        "published_date": "2025-10-29T04:49:20+00:00",
        "updated_date": "2025-10-29T04:49:20+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yogesh Thakku Suresh",
            "Vishwajeet Shivaji Hogale",
            "Luca-Alexandru Zamfira",
            "Anandavardhana Hegde"
        ],
        "tldr": "This paper presents a transformer-based vision-language model (VLM) for medical image captioning, achieving improved accuracy and semantic alignment on MRI scans by focusing on domain-specific data. The model combines DEiT, MediCareBERT, and an LSTM decoder with hybrid loss and contrastive inference.",
        "tldr_zh": "本文提出了一种基于Transformer的视觉-语言模型（VLM），用于医学图像描述，通过关注特定领域数据，在MRI扫描上实现了更高的准确性和语义对齐。该模型结合了DEiT、MediCareBERT和LSTM解码器，采用混合损失和对比推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments",
        "summary": "Zero-shot scene understanding in real-world settings presents major\nchallenges due to the complexity and variability of natural scenes, where\nmodels must recognize new objects, actions, and contexts without prior labeled\nexamples. This work proposes a vision-language integration framework that\nunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models\n(e.g., GPT-based architectures) to achieve semantic alignment between visual\nand textual modalities. The goal is to enable robust zero-shot comprehension of\nscenes by leveraging natural language as a bridge to generalize over unseen\ncategories and contexts. Our approach develops a unified model that embeds\nvisual inputs and textual prompts into a shared space, followed by multimodal\nfusion and reasoning layers for contextual interpretation. Experiments on\nVisual Genome, COCO, ADE20K, and custom real-world datasets demonstrate\nsignificant gains over state-of-the-art zero-shot models in object recognition,\nactivity detection, and scene captioning. The proposed system achieves up to\n18% improvement in top-1 accuracy and notable gains in semantic coherence\nmetrics, highlighting the effectiveness of cross-modal alignment and language\ngrounding in enhancing generalization for real-world scene understanding.",
        "url": "http://arxiv.org/abs/2510.25070v1",
        "published_date": "2025-10-29T01:16:21+00:00",
        "updated_date": "2025-10-29T01:16:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manjunath Prasad Holenarasipura Rajiv",
            "B. M. Vidyavathi"
        ],
        "tldr": "This paper presents a vision-language integration framework using pre-trained visual encoders and large language models for zero-shot scene understanding in real-world environments, achieving significant improvements in object recognition, activity detection, and scene captioning.",
        "tldr_zh": "本文提出了一种视觉-语言集成框架，利用预训练的视觉编码器和大型语言模型，用于现实环境中零样本场景理解，并在物体识别、活动检测和场景描述方面取得了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DRIP: Dynamic patch Reduction via Interpretable Pooling",
        "summary": "Recently, the advances in vision-language models, including contrastive\npretraining and instruction tuning, have greatly pushed the frontier of\nmultimodal AI. However, owing to the large-scale and hence expensive\npretraining, the efficiency concern has discouraged researchers from attempting\nto pretrain a vision language model from scratch. In this work, we propose\nDynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the\ninput images and dynamically merges tokens in the deeper layers of a visual\nencoder. Our results on both ImageNet training from scratch and CLIP\ncontrastive pretraining demonstrate a significant GFLOP reduction while\nmaintaining comparable classification/zero-shot performance. To further\nvalidate our proposed method, we conduct continual pretraining on a large\nbiology dataset, extending its impact into scientific domains.",
        "url": "http://arxiv.org/abs/2510.25067v1",
        "published_date": "2025-10-29T01:10:28+00:00",
        "updated_date": "2025-10-29T01:10:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yusen Peng",
            "Sachin Kumar"
        ],
        "tldr": "The paper introduces DRIP, a method for dynamically reducing the computational cost of vision encoders in VLMs by merging tokens in deeper layers, demonstrating GFLOP reduction with comparable performance and potential for scientific domains.",
        "tldr_zh": "该论文介绍了DRIP，一种通过合并视觉编码器深层中的 tokens 来动态降低视觉语言模型计算成本的方法。实验表明，该方法在减少 GFLOP 的同时，保持了相当的性能，并具有应用于科学领域的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models",
        "summary": "Breast cancer remains the most commonly diagnosed malignancy among women in\nthe developed world. Early detection through mammography screening plays a\npivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)\nsystems have shown promise in assisting radiologists, existing approaches face\ncritical limitations in clinical deployment - particularly in handling the\nnuanced interpretation of multi-modal data and feasibility due to the\nrequirement of prior clinical history. This study introduces a novel framework\nthat synergistically combines visual features from 2D mammograms with\nstructured textual descriptors derived from easily accessible clinical metadata\nand synthesized radiological reports through innovative tokenization modules.\nOur proposed methods in this study demonstrate that strategic integration of\nconvolutional neural networks (ConvNets) with language representations achieves\nsuperior performance to vision transformer-based models while handling\nhigh-resolution images and enabling practical deployment across diverse\npopulations. By evaluating it on multi-national cohort screening mammograms,\nour multi-modal approach achieves superior performance in cancer detection and\ncalcification identification compared to unimodal baselines, with particular\nimprovements. The proposed method establishes a new paradigm for developing\nclinically viable VLM-based CAD systems that effectively leverage imaging data\nand contextual patient information through effective fusion mechanisms.",
        "url": "http://arxiv.org/abs/2510.25051v1",
        "published_date": "2025-10-29T00:37:18+00:00",
        "updated_date": "2025-10-29T00:37:18+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shunjie-Fabian Zheng",
            "Hyeonjun Lee",
            "Thijs Kooi",
            "Ali Diba"
        ],
        "tldr": "This paper introduces a novel vision-language model (VLM) for breast cancer detection, integrating mammogram images with clinical text data, achieving superior performance and practical deployability compared to unimodal baselines and vision transformers.",
        "tldr_zh": "本文介绍了一种用于乳腺癌检测的新型视觉语言模型 (VLM)，该模型集成了乳房 X 光片图像和临床文本数据，与单模基线和视觉转换器相比，实现了卓越的性能和实际可部署性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning",
        "summary": "Pressure ulcers (PUs) are a serious and prevalent healthcare concern.\nAccurate classification of PU severity (Stages I-IV) is essential for proper\ntreatment but remains challenging due to subtle visual distinctions and\nsubjective interpretation, leading to variability among clinicians. Prior\nAI-based approaches using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs) achieved promising accuracy but offered limited\ninterpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal\nmodel), a fine-tuned multimodal large language model (MLLM) with an agentic\nself-reflection mechanism for pressure ulcer severity classification. Inspired\nby clinician-style diagnostic reassessment, FT-ARM iteratively refines its\npredictions by reasoning over visual features and encoded clinical knowledge\nfrom text, enhancing both accuracy and consistency. On the publicly available\nPressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,\nachieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based\nmodels by +4%. Unlike earlier CNN/ViT studies that relied solely on offline\nevaluations, FT-ARM is designed and tested for live inference, reflecting\nreal-time deployment conditions. Furthermore, it produces clinically grounded\nnatural-language explanations, improving interpretability and trust. By\nintegrating fine-tuning and reflective reasoning across multimodal inputs,\nFT-ARM advances the reliability, transparency, and clinical applicability of\nautomated wound assessment systems, addressing the critical need for consistent\nand explainable PU staging to support improved patient care.",
        "url": "http://arxiv.org/abs/2510.24980v1",
        "published_date": "2025-10-28T21:23:32+00:00",
        "updated_date": "2025-10-28T21:23:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Reza Saadati Fard",
            "Emmanuel Agu",
            "Palawat Busaranuvong",
            "Deepak Kumar",
            "Shefalika Gautam",
            "Bengisu Tulu",
            "Diane Strong",
            "Lorraine Loretz"
        ],
        "tldr": "The paper introduces FT-ARM, a fine-tuned multimodal LLM with agentic self-reflection, achieving 85% accuracy in pressure ulcer severity classification while providing interpretable explanations, demonstrating improved reliability and clinical applicability.",
        "tldr_zh": "该论文介绍了一种名为FT-ARM的微调多模态LLM，具有自主反思机制，在压力性溃疡严重程度分类中达到85%的准确率，同时提供可解释的说明，展示了改进的可靠性和临床适用性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Instance-Level Composed Image Retrieval",
        "summary": "The progress of composed image retrieval (CIR), a popular research direction\nin image retrieval, where a combined visual and textual query is used, is held\nback by the absence of high-quality training and evaluation data. We introduce\na new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an\ninstance-level class definition. The goal is to retrieve images that contain\nthe same particular object as the visual query, presented under a variety of\nmodifications defined by textual queries. Its design and curation process keep\nthe dataset compact to facilitate future research, while maintaining its\nchallenge-comparable to retrieval among more than 40M random\ndistractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training\ndata, we leverage pre-trained vision-and-language models (VLMs) in a\ntraining-free approach called BASIC. The method separately estimates\nquery-image-to-image and query-text-to-image similarities, performing late\nfusion to upweight images that satisfy both queries, while down-weighting those\nthat exhibit high similarity with only one of the two. Each individual\nsimilarity is further improved by a set of components that are simple and\nintuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR\ndatasets that follow a semantic-level class definition. Project page:\nhttps://vrg.fel.cvut.cz/icir/.",
        "url": "http://arxiv.org/abs/2510.25387v1",
        "published_date": "2025-10-29T10:57:59+00:00",
        "updated_date": "2025-10-29T10:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bill Psomas",
            "George Retsinas",
            "Nikos Efthymiadis",
            "Panagiotis Filntisis",
            "Yannis Avrithis",
            "Petros Maragos",
            "Ondrej Chum",
            "Giorgos Tolias"
        ],
        "tldr": "The paper introduces a new instance-level CIR dataset (i-CIR) and a training-free method (BASIC) leveraging VLMs to achieve state-of-the-art results on this and existing datasets.",
        "tldr_zh": "该论文介绍了一个新的实例级别组合图像检索数据集 (i-CIR)，以及一种利用 VLM 的无训练方法 (BASIC)，在该数据集和现有数据集上实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection",
        "summary": "Zero-shot Human-Object Interaction detection aims to localize humans and\nobjects in an image and recognize their interaction, even when specific\nverb-object pairs are unseen during training. Recent works have shown promising\nresults using prompt learning with pretrained vision-language models such as\nCLIP, which align natural language prompts with visual features in a shared\nembedding space. However, existing approaches still fail to handle the visual\ncomplexity of interaction, including (1) intra-class visual diversity, where\ninstances of the same verb appear in diverse poses and contexts, and (2)\ninter-class visual entanglement, where distinct verbs yield visually similar\npatterns. To address these challenges, we propose VDRP, a framework for Visual\nDiversity and Region-aware Prompt learning. First, we introduce a visual\ndiversity-aware prompt learning strategy that injects group-wise visual\nvariance into the context embedding. We further apply Gaussian perturbation to\nencourage the prompts to capture diverse visual variations of a verb. Second,\nwe retrieve region-specific concepts from the human, object, and union regions.\nThese are used to augment the diversity-aware prompt embeddings, yielding\nregion-aware prompts that enhance verb-level discrimination. Experiments on the\nHICO-DET benchmark demonstrate that our method achieves state-of-the-art\nperformance under four zero-shot evaluation settings, effectively addressing\nboth intra-class diversity and inter-class visual entanglement. Code is\navailable at https://github.com/mlvlab/VDRP.",
        "url": "http://arxiv.org/abs/2510.25094v1",
        "published_date": "2025-10-29T01:58:35+00:00",
        "updated_date": "2025-10-29T01:58:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chanhyeong Yang",
            "Taehoon Song",
            "Jihwan Park",
            "Hyunwoo J. Kim"
        ],
        "tldr": "The paper introduces VDRP, a novel prompt learning framework for zero-shot HOI detection, addressing intra-class visual diversity and inter-class visual entanglement by incorporating visual diversity-aware and region-aware prompt learning strategies. It achieves state-of-the-art performance on the HICO-DET benchmark.",
        "tldr_zh": "该论文提出了VDRP，一种用于零样本HOI检测的新型prompt学习框架，通过结合视觉多样性感知和区域感知prompt学习策略，解决了类内视觉多样性和类间视觉纠缠问题。 在HICO-DET基准测试中取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models",
        "summary": "Real-time object detection has achieved substantial progress through\nmeticulously designed architectures and optimization strategies. However, the\npursuit of high-speed inference via lightweight network designs often leads to\ndegraded feature representation, which hinders further performance improvements\nand practical on-device deployment. In this paper, we propose a cost-effective\nand highly adaptable distillation framework that harnesses the rapidly evolving\ncapabilities of Vision Foundation Models (VFMs) to enhance lightweight object\ndetectors. Given the significant architectural and learning objective\ndisparities between VFMs and resource-constrained detectors, achieving stable\nand task-aligned semantic transfer is challenging. To address this, on one\nhand, we introduce a Deep Semantic Injector (DSI) module that facilitates the\nintegration of high-level representations from VFMs into the deep layers of the\ndetector. On the other hand, we devise a Gradient-guided Adaptive Modulation\n(GAM) strategy, which dynamically adjusts the intensity of semantic transfer\nbased on gradient norm ratios. Without increasing deployment and inference\noverhead, our approach painlessly delivers striking and consistent performance\ngains across diverse DETR-based models, underscoring its practical utility for\nreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art\nresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding\nspeeds of 273/169/124/78 FPS.",
        "url": "http://arxiv.org/abs/2510.25257v1",
        "published_date": "2025-10-29T08:13:17+00:00",
        "updated_date": "2025-10-29T08:13:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijun Liao",
            "Yian Zhao",
            "Xin Shan",
            "Yu Yan",
            "Chang Liu",
            "Lei Lu",
            "Xiangyang Ji",
            "Jie Chen"
        ],
        "tldr": "RT-DETRv4 uses a distillation framework with a Deep Semantic Injector and Gradient-guided Adaptive Modulation to enhance lightweight object detectors with Vision Foundation Models, achieving state-of-the-art real-time object detection performance without increasing inference overhead.",
        "tldr_zh": "RT-DETRv4 采用一种蒸馏框架，结合深度语义注入器（Deep Semantic Injector）和梯度引导自适应调制（Gradient-guided Adaptive Modulation），利用视觉基础模型（Vision Foundation Models）增强轻量级目标检测器，在不增加推理开销的情况下，实现了最先进的实时目标检测性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8",
        "summary": "Developing a highly accurate automatic license plate recognition system\n(ALPR) is challenging due to environmental factors such as lighting, rain, and\ndust. Additional difficulties include high vehicle speeds, varying camera\nangles, and low-quality or low-resolution images. ALPR is vital in traffic\ncontrol, parking, vehicle tracking, toll collection, and law enforcement\napplications. This paper proposes a deep learning strategy using YOLOv8 for\nlicense plate detection and recognition tasks. This method seeks to enhance the\nperformance of the model using datasets from Ontario, Quebec, California, and\nNew York State. It achieved an impressive recall rate of 94% on the dataset\nfrom the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and\n91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised\nlearning framework, combining a small set of manually labeled data with\npseudo-labels generated by Grounding DINO to train our detection model.\nGrounding DINO, a powerful vision-language model, automatically annotates many\nimages with bounding boxes for license plates, thereby minimizing the reliance\non labor-intensive manual labeling. By integrating human-verified and\nmodel-generated annotations, we can scale our dataset efficiently while\nmaintaining label quality, which significantly enhances the training process\nand overall model performance. Furthermore, it reports character error rates\nfor both datasets, providing additional insight into system performance.",
        "url": "http://arxiv.org/abs/2510.25032v1",
        "published_date": "2025-10-28T23:21:00+00:00",
        "updated_date": "2025-10-28T23:21:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zahra Ebrahimi Vargoorani",
            "Amir Mohammad Ghoreyshi",
            "Ching Yee Suen"
        ],
        "tldr": "This paper proposes a semi-supervised license plate recognition system using YOLOv8 and Grounding DINO for pseudo-labeling, achieving good recall rates on two datasets while reducing manual labeling effort.",
        "tldr_zh": "本文提出了一种基于YOLOv8和Grounding DINO的半监督车牌识别系统，利用伪标签来减少人工标注工作，并在两个数据集上取得了良好的召回率。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving",
        "summary": "Assessing scenario coverage is crucial for evaluating the robustness of\nautonomous agents, yet existing methods rely on expensive human annotations or\ncomputationally intensive Large Vision-Language Models (LVLMs). These\napproaches are impractical for large-scale deployment due to cost and\nefficiency constraints. To address these shortcomings, we propose SCOUT\n(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate\nmodel designed to predict scenario coverage labels directly from an agent's\nlatent sensor representations. SCOUT is trained through a distillation process,\nlearning to approximate LVLM-generated coverage labels while eliminating the\nneed for continuous LVLM inference or human annotation. By leveraging\nprecomputed perception features, SCOUT avoids redundant computations and\nenables fast, scalable scenario coverage estimation. We evaluate our method\nacross a large dataset of real-life autonomous navigation scenarios,\ndemonstrating that it maintains high accuracy while significantly reducing\ncomputational cost. Our results show that SCOUT provides an effective and\npractical alternative for large-scale coverage analysis. While its performance\ndepends on the quality of LVLM-generated training labels, SCOUT represents a\nmajor step toward efficient scenario coverage oversight in autonomous systems.",
        "url": "http://arxiv.org/abs/2510.24949v1",
        "published_date": "2025-10-28T20:31:19+00:00",
        "updated_date": "2025-10-28T20:31:19+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Anil Yildiz",
            "Sarah M. Thornton",
            "Carl Hildebrandt",
            "Sreeja Roy-Singh",
            "Mykel J. Kochenderfer"
        ],
        "tldr": "The paper introduces SCOUT, a lightweight surrogate model that uses knowledge distillation to predict scenario coverage labels in autonomous driving, offering a computationally efficient alternative to LVLMs and human annotation for large-scale deployment.",
        "tldr_zh": "本文介绍SCOUT，一种轻量级代理模型，利用知识蒸馏来预测自动驾驶中的场景覆盖标签，为大规模部署提供了计算效率高于LVLM和人工标注的替代方案。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]