[
    {
        "title": "Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis",
        "summary": "Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.",
        "url": "http://arxiv.org/abs/2511.08087v1",
        "published_date": "2025-11-11T10:43:39+00:00",
        "updated_date": "2025-11-12T01:39:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aditi Singhania",
            "Krutik Malani",
            "Riddhi Dhawan",
            "Arushi Jain",
            "Garv Tandon",
            "Nippun Sharma",
            "Souymodip Chakraborty",
            "Vineet Batra",
            "Ankit Phogat"
        ],
        "tldr": "This paper introduces a hierarchical VLM-based framework, Beyond the Pixels, for evaluating identity preservation in generative models by decomposing identity assessment into feature-level transformations, validated on state-of-the-art models and a new benchmark.",
        "tldr_zh": "本文介绍了一种基于VLM的分层框架，名为Beyond the Pixels，通过将身份评估分解为特征级转换，来评估生成模型中的身份保留，该框架已在最先进的模型和新基准上进行了验证。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Remodeling Semantic Relationships in Vision-Language Fine-Tuning",
        "summary": "Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.",
        "url": "http://arxiv.org/abs/2511.08238v1",
        "published_date": "2025-11-11T13:37:13+00:00",
        "updated_date": "2025-11-12T01:48:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiangyang Wu",
            "Liu Liu",
            "Baosheng Yu",
            "Jiayan Qiu",
            "Zhenwei Shi"
        ],
        "tldr": "This paper introduces a novel vision-language fine-tuning method that improves multimodal alignment by explicitly modeling semantic relationships within images, outperforming existing methods on visual question answering and image captioning tasks.",
        "tldr_zh": "本文提出了一种新的视觉-语言微调方法，通过显式建模图像中的语义关系来改善多模态对齐，并在视觉问答和图像描述任务上优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revisiting MLLM Based Image Quality Assessment: Errors and Remedy",
        "summary": "The rapid progress of multi-modal large language models (MLLMs) has boosted the task of image quality assessment (IQA). However, a key challenge arises from the inherent mismatch between the discrete token outputs of MLLMs and the continuous nature of quality scores required by IQA tasks. This discrepancy significantly hinders the performance of MLLM-based IQA methods. Previous approaches that convert discrete token predictions into continuous scores often suffer from conversion errors. Moreover, the semantic confusion introduced by level tokens (e.g., ``good'') further constrains the performance of MLLMs on IQA tasks and degrades their original capabilities for related tasks. To tackle these problems, we provide a theoretical analysis of the errors inherent in previous approaches and, motivated by this analysis, propose a simple yet effective framework, Q-Scorer. This framework incorporates a lightweight regression module and IQA-specific score tokens into the MLLM pipeline. Extensive experiments demonstrate that Q-Scorer achieves state-of-the-art performance across multiple IQA benchmarks, generalizes well to mixed datasets, and further improves when combined with other methods.",
        "url": "http://arxiv.org/abs/2511.07812v1",
        "published_date": "2025-11-11T04:08:44+00:00",
        "updated_date": "2025-11-12T01:20:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenchen Tang",
            "Songlin Yang",
            "Bo Peng",
            "Zichuan Wang",
            "Jing Dong"
        ],
        "tldr": "This paper addresses the mismatch between discrete MLLM outputs and continuous IQA scores by proposing Q-Scorer, a framework with a regression module and IQA-specific score tokens, achieving state-of-the-art IQA performance.",
        "tldr_zh": "该论文通过提出Q-Scorer来解决离散MLLM输出与连续IQA分数之间的不匹配问题。Q-Scorer是一个具有回归模块和IQA特定分数标记的框架，可实现最先进的IQA性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards",
        "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.",
        "url": "http://arxiv.org/abs/2511.07403v1",
        "published_date": "2025-11-10T18:52:47+00:00",
        "updated_date": "2025-11-11T02:53:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Hunar Batra",
            "Haoqin Tu",
            "Hardy Chen",
            "Yuanze Lin",
            "Cihang Xie",
            "Ronald Clark"
        ],
        "tldr": "SpatialThinker is a 3D-aware MLLM trained with RL and spatial rewards to improve spatial reasoning capabilities, outperforming supervised fine-tuning, sparse RL, and even GPT-4o on spatial understanding tasks with limited data.",
        "tldr_zh": "SpatialThinker 是一个通过强化学习和空间奖励训练的 3D 感知 MLLM，旨在提高空间推理能力，在有限数据下，其在空间理解任务上优于监督微调、稀疏强化学习，甚至超过 GPT-4o。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment",
        "summary": "Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.",
        "url": "http://arxiv.org/abs/2511.08399v1",
        "published_date": "2025-11-11T16:15:15+00:00",
        "updated_date": "2025-11-12T01:57:16+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Hua Ye",
            "Hang Ding",
            "Siyuan Chen",
            "Yiyang Jiang",
            "Changyuan Zhang",
            "Xuan Zhang"
        ],
        "tldr": "This paper introduces Boundary-Aware Curriculum with Local Attention (BACL) to improve multimodal alignment by focusing on ambiguous negative pairs, achieving significant performance gains on several benchmarks.",
        "tldr_zh": "本文介绍了具有局部注意力的边界感知课程学习（BACL），通过关注模糊的负样本对来改进多模态对齐，并在多个基准测试中实现了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UI2Code$^\\text{N}$: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation",
        "summary": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code$^\\text{N}$, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code$^\\text{N}$ establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.",
        "url": "http://arxiv.org/abs/2511.08195v1",
        "published_date": "2025-11-11T13:00:09+00:00",
        "updated_date": "2025-11-12T01:46:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhen Yang",
            "Wenyi Hong",
            "Mingde Xu",
            "Xinyue Fan",
            "Weihan Wang",
            "Jiele Cheng",
            "Xiaotao Gu",
            "Jie Tang"
        ],
        "tldr": "The paper introduces UI2Code$^N$, a visual language model for interactive UI-to-code generation that unifies UI-to-code generation, UI editing, and UI polishing, achieving state-of-the-art performance among open-source models.",
        "tldr_zh": "该论文介绍了UI2Code$^N$，一个用于交互式UI到代码生成的视觉语言模型，它统一了UI到代码生成、UI编辑和UI优化，并在开源模型中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
        "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
        "url": "http://arxiv.org/abs/2511.08003v1",
        "published_date": "2025-11-11T09:07:40+00:00",
        "updated_date": "2025-11-12T01:33:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jialong Qin",
            "Xin Zou",
            "Di Lu",
            "Yibo Yan",
            "Xuming Hu"
        ],
        "tldr": "The paper introduces SharpV, a method for adaptively pruning visual tokens in VideoLLMs to reduce computational complexity and improve performance, particularly in key-value cache management.",
        "tldr_zh": "该论文介绍了SharpV，一种用于自适应修剪VideoLLM中视觉标记的方法，旨在降低计算复杂度并提高性能，尤其是在键值缓存管理方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding",
        "summary": "Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.",
        "url": "http://arxiv.org/abs/2511.08480v1",
        "published_date": "2025-11-11T17:23:02+00:00",
        "updated_date": "2025-11-12T02:00:53+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Da Li",
            "Yuxiao Luo",
            "Keping Bi",
            "Jiafeng Guo",
            "Wei Yuan",
            "Biao Yang",
            "Yan Wang",
            "Fan Yang",
            "Tingting Gao",
            "Guorui Zhou"
        ],
        "tldr": "The paper introduces CoMa, a compressed pre-training phase for Vision-Language Models (VLMs) that serves as a warm-up for contrastive learning, achieving state-of-the-art results on MMEB with improved efficiency and effectiveness.",
        "tldr_zh": "该论文介绍了一种名为CoMa的压缩预训练阶段，用于视觉语言模型（VLMs），作为对比学习的预热，在MMEB上实现了最先进的结果，并提高了效率和有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross Modal Fine-grained Alignment via Granularity-aware and Region-uncertain Modeling",
        "summary": "Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.",
        "url": "http://arxiv.org/abs/2511.07710v1",
        "published_date": "2025-11-11T00:28:11+00:00",
        "updated_date": "2025-11-12T01:12:25+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jiale Liu",
            "Haoming Zhou",
            "Yishu Zhu",
            "Bingzhi Chen",
            "Yuncheng Jiang"
        ],
        "tldr": "This paper addresses the problem of fine-grained image-text alignment by proposing a method that incorporates significance-aware modeling and region-level uncertainty modeling, achieving state-of-the-art performance on standard benchmarks.",
        "tldr_zh": "该论文通过提出一种结合了显著性感知建模和区域级不确定性建模的方法，解决了细粒度图像-文本对齐的问题，并在标准基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding",
        "summary": "Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.",
        "url": "http://arxiv.org/abs/2511.08480v1",
        "published_date": "2025-11-11T17:23:02+00:00",
        "updated_date": "2025-11-12T02:00:53+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Da Li",
            "Yuxiao Luo",
            "Keping Bi",
            "Jiafeng Guo",
            "Wei Yuan",
            "Biao Yang",
            "Yan Wang",
            "Fan Yang",
            "Tingting Gao",
            "Guorui Zhou"
        ],
        "tldr": "The paper introduces CoMa, a compressed pre-training phase for vision-language models (VLMs) that enhances their performance as embedding models through decoupling pre-training and contrastive learning, achieving state-of-the-art results with improved efficiency.",
        "tldr_zh": "该论文介绍了CoMa，一种用于视觉-语言模型（VLM）的压缩预训练阶段，通过解耦预训练和对比学习来提高其作为嵌入模型的性能，并在提高效率的同时实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization",
        "summary": "Accurately estimating the normalization term (also known as the partition function) in the contrastive loss is a central challenge for training Contrastive Language-Image Pre-training (CLIP) models. Conventional methods rely on large batches for approximation, demanding substantial computational resources. To mitigate this issue, prior works introduced per-sample normalizer estimators, which are updated at each epoch in a blockwise coordinate manner to keep track of updated encoders. However, this scheme incurs optimization error that scales with the ratio of dataset size to batch size, limiting effectiveness for large datasets or small batches. To overcome this limitation, we propose NeuCLIP, a novel and elegant optimization framework based on two key ideas: (i) $\\textbf{reformulating}$ the contrastive loss for each sample $\\textbf{via convex analysis}$ into a minimization problem with an auxiliary variable representing its log-normalizer; and (ii) $\\textbf{transforming}$ the resulting minimization over $n$ auxiliary variables (where $n$ is the dataset size) via $\\textbf{variational analysis}$ into the minimization over a compact neural network that predicts the log-normalizers. We design an alternating optimization algorithm that jointly trains the CLIP model and the auxiliary network. By employing a tailored architecture and acceleration techniques for the auxiliary network, NeuCLIP achieves more accurate normalizer estimation, leading to improved performance compared with previous methods. Extensive experiments on large-scale CLIP training, spanning datasets from millions to billions of samples, demonstrate that NeuCLIP outperforms previous methods.",
        "url": "http://arxiv.org/abs/2511.08417v1",
        "published_date": "2025-11-11T16:27:51+00:00",
        "updated_date": "2025-11-12T01:58:08+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Xiyuan Wei",
            "Chih-Jen Lin",
            "Tianbao Yang"
        ],
        "tldr": "NeuCLIP proposes a novel approach to efficiently train large-scale CLIP models by using a neural network to predict the log-normalizers in the contrastive loss, overcoming limitations of previous methods and achieving improved performance.",
        "tldr_zh": "NeuCLIP提出了一种新的方法，通过使用神经网络预测对比损失中的对数归一化器，从而高效地训练大规模CLIP模型，克服了先前方法的局限性，并提高了性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation",
        "summary": "Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\\% absolute improvement over the previous best method and more than 4$\\times$ less condensation time.",
        "url": "http://arxiv.org/abs/2511.08263v1",
        "published_date": "2025-11-11T13:55:46+00:00",
        "updated_date": "2025-11-12T01:49:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yue Min",
            "Shaobo Wang",
            "Jiaze Li",
            "Tianle Niu",
            "Junxin Fan",
            "Yongliang Miao",
            "Lijin Yang",
            "Linfeng Zhang"
        ],
        "tldr": "ImageBindDC condenses multi-modal data using ImageBind's unified feature space and a Characteristic Function loss, achieving state-of-the-art performance in data condensation for multi-modal datasets with significant improvements in both performance and condensation time.",
        "tldr_zh": "ImageBindDC利用ImageBind的统一特征空间和特征函数损失来压缩多模态数据，在多模态数据集的数据压缩方面实现了最先进的性能，并在性能和压缩时间方面都有显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2",
        "summary": "Foam formation in Wastewater Treatment Plants (WTPs) is a major challenge that can reduce treatment efficiency and increase costs. The ability to automatically examine changes in real-time with respect to the percentage of foam can be of great benefit to the plant. However, large amounts of labeled data are required to train standard Machine Learning (ML) models. The development of these systems is slow due to the scarcity and heterogeneity of labeled data. Additionally, the development is often hindered by the fact that different WTPs do not share their data due to privacy concerns. This paper proposes a new framework to address these challenges by combining Federated Learning (FL) with the state-of-the-art base model for image segmentation, Segment Anything Model 2 (SAM2). The FL paradigm enables collaborative model training across multiple WTPs without centralizing sensitive operational data, thereby ensuring privacy. The framework accelerates training convergence and improves segmentation performance even with limited local datasets by leveraging SAM2's strong pre-trained weights for initialization. The methodology involves fine-tuning SAM2 on distributed clients (edge nodes) using the Flower framework, where a central Fog server orchestrates the process by aggregating model weights without accessing private data. The model was trained and validated using various data collections, including real-world images captured at a WTPs in Granada, Spain, a synthetically generated foam dataset, and images from publicly available datasets to improve generalization. This research offers a practical, scalable, and privacy-aware solution for automatic foam tracking in WTPs. The findings highlight the significant potential of integrating large-scale foundational models into FL systems to solve real-world industrial challenges characterized by distributed and sensitive data.",
        "url": "http://arxiv.org/abs/2511.08130v1",
        "published_date": "2025-11-11T11:36:53+00:00",
        "updated_date": "2025-11-12T01:41:31+00:00",
        "categories": [
            "cs.CV",
            "cs.DC",
            "cs.LG"
        ],
        "authors": [
            "Mehmet Batuhan Duman",
            "Alejandro Carnero",
            "Cristian Martín",
            "Daniel Garrido",
            "Manuel Díaz"
        ],
        "tldr": "This paper proposes a federated learning framework using Segment Anything Model 2 (SAM2) for foam segmentation in wastewater treatment plants, addressing data scarcity and privacy concerns.",
        "tldr_zh": "本文提出了一种使用 Segment Anything Model 2 (SAM2) 的联邦学习框架，用于废水处理厂中的泡沫分割，解决了数据稀缺和隐私问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion",
        "summary": "Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.",
        "url": "http://arxiv.org/abs/2511.08075v1",
        "published_date": "2025-11-11T10:22:45+00:00",
        "updated_date": "2025-11-12T01:38:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Cameron Braunstein",
            "Mariya Toneva",
            "Eddy Ilg"
        ],
        "tldr": "This paper finds that Stable Diffusion's semantic understanding primarily comes from the CLIP text encoder, with the diffusion process acting more as a visual decoder, and semantic information degrades during the diffusion process.",
        "tldr_zh": "该论文发现 Stable Diffusion 的语义理解主要来自 CLIP 文本编码器，扩散过程更像是视觉解码器，并且语义信息在扩散过程中会退化。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ChexFract: From General to Specialized - Enhancing Fracture Description Generation",
        "summary": "Generating accurate and clinically meaningful radiology reports from chest X-ray images remains a significant challenge in medical AI. While recent vision-language models achieve strong results in general radiology report generation, they often fail to adequately describe rare but clinically important pathologies like fractures. This work addresses this gap by developing specialized models for fracture pathology detection and description. We train fracture-specific vision-language models with encoders from MAIRA-2 and CheXagent, demonstrating significant improvements over general-purpose models in generating accurate fracture descriptions. Analysis of model outputs by fracture type, location, and age reveals distinct strengths and limitations of current vision-language model architectures. We publicly release our best-performing fracture-reporting model, facilitating future research in accurate reporting of rare pathologies.",
        "url": "http://arxiv.org/abs/2511.07983v1",
        "published_date": "2025-11-11T08:47:26+00:00",
        "updated_date": "2025-11-12T01:32:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikolay Nechaev",
            "Evgeniia Przhezdzetskaia",
            "Dmitry Umerenkov",
            "Dmitry V. Dylov"
        ],
        "tldr": "The paper introduces ChexFract, a specialized vision-language model for generating accurate fracture descriptions from chest X-rays, addressing the limitations of general-purpose models in handling rare pathologies. They show improvements by fine-tuning MAIRA-2 and CheXagent for fracture reporting.",
        "tldr_zh": "该论文介绍了ChexFract，一种专门的视觉-语言模型，用于从胸部X光片生成准确的骨折描述，解决了通用模型在处理罕见病理学方面的局限性。他们通过微调MAIRA-2和CheXagent来改进骨折报告。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification",
        "summary": "Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\\times$ faster than FedAVG).",
        "url": "http://arxiv.org/abs/2511.07929v1",
        "published_date": "2025-11-11T07:26:21+00:00",
        "updated_date": "2025-11-12T01:28:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihang Wu",
            "Ahmad Chaddad"
        ],
        "tldr": "The paper introduces FedMedCLIP, a federated learning approach using CLIP for medical image classification, which addresses data heterogeneity and resource constraints by employing a masked feature adaptation module, a masked MLP classifier, and adaptive KL divergence-based distillation.",
        "tldr_zh": "该论文介绍了 FedMedCLIP，一种使用 CLIP 的联邦学习方法，用于医学图像分类，通过采用掩码特征适应模块、掩码 MLP 分类器和自适应 KL 散度蒸馏来解决数据异质性和资源约束问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring the Underwater World Segmentation without Extra Training",
        "summary": "Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \\textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \\textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \\textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.",
        "url": "http://arxiv.org/abs/2511.07923v1",
        "published_date": "2025-11-11T07:22:56+00:00",
        "updated_date": "2025-11-12T01:28:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bingyu Li",
            "Tao Huo",
            "Da Zhang",
            "Zhiyuan Zhao",
            "Junyu Gao",
            "Xuelong Li"
        ],
        "tldr": "The paper introduces AquaOV255, a large-scale underwater segmentation dataset and benchmark (UOVSBench), along with Earth2Ocean, a training-free framework for underwater open-vocabulary segmentation that transfers terrestrial VLMs.",
        "tldr_zh": "该论文介绍了AquaOV255，一个大规模水下分割数据集和基准（UOVSBench），以及Earth2Ocean，一个用于水下开放词汇分割的无训练框架，该框架迁移了陆地VLM。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views",
        "summary": "Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.",
        "url": "http://arxiv.org/abs/2511.07813v1",
        "published_date": "2025-11-11T04:13:54+00:00",
        "updated_date": "2025-11-12T01:20:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haida Feng",
            "Hao Wei",
            "Zewen Xu",
            "Haolin Wang",
            "Chade Li",
            "Yihong Wu"
        ],
        "tldr": "Sparse3DPR is a training-free framework for 3D scene understanding using sparse RGB views and LLMs, featuring hierarchical plane-enhanced scene graphs and task-adaptive subgraph extraction for improved accuracy and efficiency.",
        "tldr_zh": "Sparse3DPR是一个无需训练的3D场景理解框架，它使用稀疏的RGB视图和大语言模型，具有层级平面增强场景图和任务自适应子图提取，从而提高准确性和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics",
        "summary": "We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at https://github.com/mvrl/VectorSynth.",
        "url": "http://arxiv.org/abs/2511.07744v1",
        "published_date": "2025-11-11T01:54:45+00:00",
        "updated_date": "2025-11-12T01:14:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Cher",
            "Brian Wei",
            "Srikumar Sastry",
            "Nathan Jacobs"
        ],
        "tldr": "VectorSynth is a diffusion-based framework for generating pixel-accurate satellite images conditioned on polygonal geographic annotations with semantic attributes, enabling fine-grained, spatially grounded edits.",
        "tldr_zh": "VectorSynth是一个基于扩散的框架，用于生成像素精确的卫星图像，该图像以具有语义属性的多边形地理注释为条件，从而实现精细的、空间接地的编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection",
        "summary": "Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.",
        "url": "http://arxiv.org/abs/2511.07966v1",
        "published_date": "2025-11-11T08:27:22+00:00",
        "updated_date": "2025-11-12T01:31:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shenao Zhao",
            "Pengpeng Liang",
            "Zhoufan Yang"
        ],
        "tldr": "This paper introduces MMAssist, a multi-modal assistance approach for unsupervised domain adaptation in LiDAR-based 3D object detection, utilizing image and text features to bridge the domain gap and enhance pseudo-labels.",
        "tldr_zh": "本文介绍了一种名为MMAssist的多模态辅助方法，用于LiDAR点云3D目标检测中的无监督领域自适应，利用图像和文本特征来弥合领域差距并增强伪标签。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.",
        "url": "http://arxiv.org/abs/2511.07738v1",
        "published_date": "2025-11-11T01:42:37+00:00",
        "updated_date": "2025-11-12T01:14:01+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Donglai Xu",
            "Hongzheng Yang",
            "Yuzhi Zhao",
            "Pingping Zhang",
            "Jinpeng Chen",
            "Wenao Ma",
            "Zhijian Hou",
            "Mengyang Wu",
            "Xiaolei Li",
            "Senkang Hu",
            "Ziyi Guan",
            "Jason Chun Lok Li",
            "Lai Man Po"
        ],
        "tldr": "This paper introduces a two-stage, token-level entropy optimization method for noise-tolerant training of MLLMs using Reinforcement Learning with Verifiable Rewards (RLVR), effectively transitioning from exploration to exploitation during training and demonstrating improved performance across various models and tasks.",
        "tldr_zh": "本文提出了一种用于多模态大型语言模型（MLLMs）的噪声容忍训练的两阶段令牌级熵优化方法，该方法利用可验证奖励的强化学习（RLVR），有效地在训练期间从探索过渡到利用，并在各种模型和任务中展示了改进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers",
        "summary": "With the development of diffusion models, enhancing spatial controllability in text-to-image generation has become a vital challenge. As a representative task for addressing this challenge, layout-to-image generation aims to generate images that are spatially consistent with the given layout condition. Existing layout-to-image methods typically introduce the layout condition by integrating adapter modules into the base generative model. However, the generated images often exhibit low visual quality and stylistic inconsistency with the base model, indicating a loss of pretrained knowledge. To alleviate this issue, we construct the Layout Synthesis (LaySyn) dataset, which leverages images synthesized by the base model itself to mitigate the distribution shift from the pretraining data. Moreover, we propose the Layout Control (Laytrol) Network, in which parameters are inherited from MM-DiT to preserve the pretrained knowledge of the base model. To effectively activate the copied parameters and avoid disturbance from unstable control conditions, we adopt a dedicated initialization scheme for Laytrol. In this scheme, the layout encoder is initialized as a pure text encoder to ensure that its output tokens remain within the data domain of MM-DiT. Meanwhile, the outputs of the layout control network are initialized to zero. In addition, we apply Object-level Rotary Position Embedding to the layout tokens to provide coarse positional information. Qualitative and quantitative experiments demonstrate the effectiveness of our method.",
        "url": "http://arxiv.org/abs/2511.07934v1",
        "published_date": "2025-11-11T07:31:28+00:00",
        "updated_date": "2025-11-12T01:29:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sida Huang",
            "Siqi Huang",
            "Ping Luo",
            "Hongyuan Zhang"
        ],
        "tldr": "The paper introduces Laytrol, a method for layout-to-image generation that aims to preserve pretrained knowledge in multimodal diffusion transformers by using a dedicated initialization scheme and a synthesized dataset to mitigate distribution shift.",
        "tldr_zh": "该论文介绍了Laytrol，一种布局到图像生成的方法，旨在通过使用专门的初始化方案和一个合成数据集来缓解分布偏移，从而在多模态扩散Transformer中保留预训练知识。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion",
        "summary": "Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \\ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \\ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.",
        "url": "http://arxiv.org/abs/2511.08173v1",
        "published_date": "2025-11-11T12:37:38+00:00",
        "updated_date": "2025-11-12T01:44:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Samet Hicsonmez",
            "Abd El Rahman Shabayek",
            "Djamila Aouada"
        ],
        "tldr": "This paper introduces VLMDiff, a novel unsupervised multi-class visual anomaly detection framework that leverages Vision-Language Models (VLMs) to condition a Latent Diffusion Model (LDM), achieving improved anomaly localization and detection without per-class training.",
        "tldr_zh": "本文介绍了一种新的无监督多类视觉异常检测框架VLMDiff，该框架利用视觉-语言模型 (VLM) 来调节潜在扩散模型 (LDM)，从而在无需按类训练的情况下实现更好的异常定位和检测。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]