[
    {
        "title": "DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry",
        "summary": "Solid geometry problem solving demands spatial mathematical reasoning that\nintegrates spatial intelligence and symbolic reasoning. However, most existing\nmultimodal mathematical reasoning benchmarks focus primarily on 2D plane\ngeometry, rely on static datasets prone to data contamination and memorization,\nand evaluate models solely by final answers, overlooking the reasoning process.\nTo address these limitations, we introduce DynaSolidGeo, the first dynamic\nbenchmark for evaluating genuine spatial reasoning in Vision-Language Models\n(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo\ncontains 503 expert-curated seed questions that can, in principle, dynamically\ngenerate an unbounded number of diverse multimodal text-visual instances.\nBeyond answer accuracy, we incorporate process evaluation based on\nexpert-annotated reasoning chains to measure logical validity and causal\ncoherence. Experiments across representative open-source and closed-source VLMs\nreveal large performance gaps, severe degradation in dynamic settings, and poor\nperformance on tasks requiring high-level spatial intelligence, such as mental\nrotation and visualization. The code and dataset are available at\n\\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.",
        "url": "http://arxiv.org/abs/2510.22340v1",
        "published_date": "2025-10-25T15:49:45+00:00",
        "updated_date": "2025-10-25T15:49:45+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Changti Wu",
            "Shijie Lian",
            "Zihao Liu",
            "Lei Zhang",
            "Laurence Tianruo Yang",
            "Kai Chen"
        ],
        "tldr": "The paper introduces DynaSolidGeo, a dynamic benchmark for evaluating spatial mathematical reasoning in VLMs using solid geometry problems, addressing limitations of existing benchmarks by focusing on reasoning processes and dynamic data generation.",
        "tldr_zh": "该论文介绍了DynaSolidGeo，一个动态基准，用于评估视觉语言模型在立体几何问题中的空间数学推理能力。该基准旨在解决现有基准的局限性，着重于推理过程和动态数据生成。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language Models",
        "summary": "Large-scale and high-quality image-text pair datasets play an important role\nin developing high-performing Vision-Language Models (VLMs). In this work, we\nintroduce WAON, a large-scale and high-quality Japanese image-text pair dataset\ncontaining approximately 155 million examples, collected from Common Crawl. Our\ndataset construction pipeline employs various techniques, including filtering\nand deduplication, which have been shown to be effective in previous studies.\nTo evaluate its effectiveness, we also construct WAON-Bench, a manually curated\nbenchmark for Japanese cultural image classification, consisting of 374\nclasses. To assess the effectiveness of our dataset, we conduct experiments\nusing both WAON and the Japanese subset of ReLAION, one of the most widely used\nvision-language datasets. We fine-tune SigLIP2, a strong multilingual model, on\nboth datasets. The results demonstrate that WAON enhances model performance on\nWAON-Bench more efficiently than ReLAION and achieves higher accuracy across\nall evaluated benchmarks. Furthermore, the model fine-tuned on WAON achieves\nstate-of-the-art performance on several Japanese cultural benchmarks. We\nrelease our dataset, model, and code at https://speed1313.github.io/WAON.",
        "url": "http://arxiv.org/abs/2510.22276v1",
        "published_date": "2025-10-25T12:42:42+00:00",
        "updated_date": "2025-10-25T12:42:42+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Issa Sugiura",
            "Shuhei Kurita",
            "Yusuke Oda",
            "Daisuke Kawahara",
            "Yasuo Okabe",
            "Naoaki Okazaki"
        ],
        "tldr": "The paper introduces WAON, a large-scale (155M) Japanese image-text dataset, and shows its effectiveness in fine-tuning VLMs for Japanese cultural understanding, achieving state-of-the-art results on relevant benchmarks.",
        "tldr_zh": "该论文介绍了 WAON，一个大型 (1.55 亿) 日语图像-文本数据集，并展示了其在微调 VLM 以理解日本文化方面的有效性，在相关基准测试中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HARMONY: Hidden Activation Representations and Model Output-Aware Uncertainty Estimation for Vision-Language Models",
        "summary": "The growing deployment of Vision-Language Models (VLMs) in high-stakes\napplications such as autonomous driving and assistive technologies for visually\nimpaired individuals necessitates reliable mechanisms to assess the\ntrustworthiness of their generation. Uncertainty Estimation (UE) plays a\ncentral role in quantifying the reliability of model outputs and reducing\nunsafe generations via selective prediction. In this regard, most existing\nprobability-based UE approaches rely on output probability distributions,\naggregating token probabilities into a single uncertainty score using\npredefined functions such as length-normalization. Another line of research\nleverages model hidden representations and trains MLP-based models to predict\nuncertainty. However, these methods often fail to capture the complex\nmultimodal relationships between semantic and textual tokens and struggle to\nidentify biased probabilities often influenced by language priors. Motivated by\nthese observations, we propose a novel UE framework, HARMONY, that jointly\nleverages fused multimodal information in model activations and the output\ndistribution of the VLM to determine the reliability of responses. The key\nhypothesis of our work is that both the model's internal belief in its visual\nunderstanding, captured by its hidden representations, and the produced token\nprobabilities carry valuable reliability signals that can be jointly leveraged\nto improve UE performance, surpassing approaches that rely on only one of these\ncomponents. Experimental results on three open-ended VQA benchmarks, A-OKVQA,\nVizWiz, and PathVQA, and three state-of-the-art VLMs, LLaVa-7b, LLaVA-13b and\nInstructBLIP demonstrate that our method consistently performs on par with or\nbetter than existing approaches, achieving up to 4\\% improvement in AUROC, and\n6\\% in PRR, establishing new state of the art in uncertainty estimation for\nVLMs.",
        "url": "http://arxiv.org/abs/2510.22171v1",
        "published_date": "2025-10-25T05:45:18+00:00",
        "updated_date": "2025-10-25T05:45:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Erum Mushtaq",
            "Zalan Fabian",
            "Yavuz Faruk Bakman",
            "Anil Ramakrishna",
            "Mahdi Soltanolkotabi",
            "Salman Avestimehr"
        ],
        "tldr": "The paper introduces HARMONY, a novel uncertainty estimation framework for VLMs that leverages both hidden activations and output distributions to improve reliability assessment, achieving state-of-the-art results on VQA benchmarks.",
        "tldr_zh": "该论文介绍了一种名为HARMONY的新型VLM不确定性估计框架，该框架利用隐藏激活和输出分布来提高可靠性评估，并在VQA基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mint: A Simple Test-Time Adaptation of Vision-Language Models against Common Corruptions",
        "summary": "Pretrained vision-language models such as CLIP achieve strong zero-shot\ngeneralization but remain vulnerable to distribution shifts caused by input\ncorruptions. In this work, we investigate how corruptions affect CLIP's image\nembeddings and uncover a consistent phenomenon we term as embedding variance\ncollapse, where both intra-class and inter-class variances shrink as corruption\nseverity increases. We find that this collapse is closely tied to performance\ndegradation, with inter-class variance strongly correlated with classification\naccuracy. To explain this phenomenon, we analyze how corruptions alter the\nstructure of the embedding space. Our theoretical results suggest that the\nvisual encoder tends to encode corruption-related signals, which dilute\nclass-discriminative features and compress the representation geometry. We\nfurther show that maximizing inter-class variance, even when estimated from\npseudo-labels, can provably enhance embedding quality. Based on this insight,\nwe propose Mint, a simple test-time adaptation method that maximizes\npseudo-label-based inter-class variance on the fly using a mean accumulator and\na gradient accumulator. Mint operates effectively with small batch sizes and\nconsistently improves performance across multiple corruption benchmarks and\nCLIP architectures. Our code is available at https://github.com/baowenxuan/Mint .",
        "url": "http://arxiv.org/abs/2510.22127v1",
        "published_date": "2025-10-25T02:55:08+00:00",
        "updated_date": "2025-10-25T02:55:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Wenxuan Bao",
            "Ruxi Deng",
            "Jingrui He"
        ],
        "tldr": "The paper proposes Mint, a test-time adaptation method for CLIP that maximizes inter-class variance based on pseudo-labels to mitigate the embedding variance collapse caused by common corruptions, improving performance across various benchmarks.",
        "tldr_zh": "该论文提出了一种名为Mint的CLIP测试时自适应方法，通过最大化基于伪标签的类间方差来缓解常见腐败引起的嵌入方差坍塌，从而提高各种基准测试中的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation",
        "summary": "Vision Language Models (VLMs) achieve strong performance on many\nvision-language tasks but often struggle with spatial reasoning\\textemdash{}a\nprerequisite for many applications. Empirically, we find that a dataset\nproduced by a current training data generation pipeline has a 57.6\\% human\nvalidation rate. These rates stem from current limitations: single-image 3D\nreconstruction introduces cascading modeling errors and requires wide answer\ntolerances, while caption-based methods require hyper-detailed annotations and\nsuffer from generative hallucinations. We present GRAID, built on the key\ninsight that qualitative spatial relationships can be reliably determined from\n2D geometric primitives alone. By operating exclusively on 2D bounding boxes\nfrom standard object detectors, GRAID avoids both 3D reconstruction errors and\ngenerative hallucinations, resulting in datasets that are of higher quality\nthan existing tools that produce similar datasets as validated by human\nevaluations. We apply our framework to the BDD100k, NuImages, and Waymo\ndatasets, generating over 8.5 million high-quality VQA pairs creating questions\nspanning spatial relations, counting, ranking, and size comparisons. We\nevaluate one of the datasets and find it achieves 91.16\\% human-validated\naccuracy\\textemdash{}compared to 57.6\\% on a dataset generated by recent work.\n% or recent work Critically, we demonstrate that when trained on GRAID data,\nmodels learn spatial reasoning concepts that generalize: models fine-tuned on 6\nquestion types improve on over 10 held-out types, with accuracy gains of 47.5\\%\non BDD and 37.9\\% on NuImages for Llama 3.2B 11B, and when trained on all\nquestions types, achieve improvements on several existing benchmarks such as\nBLINK. The GRAID framework, datasets, and additional information can be found\non our \\href{https://ke7.github.io/graid/}{project page}.",
        "url": "http://arxiv.org/abs/2510.22118v1",
        "published_date": "2025-10-25T02:07:23+00:00",
        "updated_date": "2025-10-25T02:07:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Karim Elmaaroufi",
            "Liheng Lai",
            "Justin Svegliato",
            "Yutong Bai",
            "Sanjit A. Seshia",
            "Matei Zaharia"
        ],
        "tldr": "The paper introduces GRAID, a framework for generating high-quality VQA data for VLMs, focusing on improving spatial reasoning through 2D geometric primitives, achieving higher human validation rates and better generalization than existing methods. It fine-tunes existing VLMs and achieves significant performance gains on spatial reasoning tasks.",
        "tldr_zh": "该论文介绍了一种名为GRAID的框架，用于生成高质量的VQA数据，以提升视觉语言模型（VLM）的空间推理能力。该框架侧重于使用2D几何图元，相比现有方法，实现了更高的人工验证率和更好的泛化能力。通过对现有VLM进行微调，在空间推理任务上取得了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Coordinate Prediction Bias from Positional Encoding Failures",
        "summary": "Multimodal large language models (MLLMs) excel at vision-language tasks such\nas VQA and document understanding, yet precise coordinate prediction remains\nchallenging. High-resolution inputs exacerbate this difficulty by producing\nlong token sequences that weaken positional encodings and introduce directional\nbiases in coordinate outputs. We investigate this phenomenon by analyzing how\nMLLMs behave when visual positional encodings (VPEs) are deliberately perturbed\nthrough shuffling. Our analysis reveals that such perturbations induce\npredictable, non-random coordinate biases rather than random errors, suggesting\nthat models rely on internal positional priors when spatial grounding signals\nare degraded. Crucially, we observe similar directional error patterns in\nnatural high-resolution datasets, indicating that positional encoding failures\nare a key bottleneck for accurate coordinate prediction at scale. To address\nthis issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free\ntest-time method that leverages the directional nature of these biases for\ncorrection. VPSG runs auxiliary decoding with shuffled VPEs to isolate\nposition-unconditioned tendencies, then uses this as negative evidence to guide\ndigit prediction while preserving coordinate format through a lightweight\nfinite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable\nimprovements, highlighting positional encoding robustness as a critical factor\nfor spatial reasoning in MLLMs.",
        "url": "http://arxiv.org/abs/2510.22102v1",
        "published_date": "2025-10-25T00:58:47+00:00",
        "updated_date": "2025-10-25T00:58:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Xingjian Tao",
            "Yiwei Wang",
            "Yujun Cai",
            "Yihong Luo",
            "Jing Tang"
        ],
        "tldr": "The paper identifies positional encoding failures in MLLMs as a bottleneck for precise coordinate prediction, particularly at high resolutions, and proposes a training-free test-time method, VPSG, to mitigate this issue by leveraging directional biases.",
        "tldr_zh": "该论文指出MLLM中的位置编码失败是精确坐标预测的瓶颈，尤其是在高分辨率下。并提出了一种无需训练的测试时方法VPSG，通过利用方向偏差来缓解此问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation",
        "summary": "Vision language models (VLMs) often generate hallucination, i.e., content\nthat cannot be substantiated by either textual or visual inputs. Prior work\nprimarily attributes this to over-reliance on linguistic prior knowledge rather\nthan visual inputs. Some methods attempt to mitigate hallucination by\namplifying visual token attention proportionally to their attention scores.\nHowever, these methods overlook the visual attention sink problem, where\nattention is frequently misallocated to task-irrelevant visual regions, and\nneglect cross-modal fusion balance by enhancing only visual attention without\nadjusting attention to the user query. This can result in amplifying incorrect\nareas while failing to properly interpret the user query. To address these\nchallenges, we propose a simple yet effective method called Gaze Shift-Guided\nCross-modal Fusion Enhancement (GIFT). GIFT pre-computes a holistic visual\nsaliency map by tracking positive changes in visual attention, or \"gaze\nshifts\", during user query comprehension, and leverages this map to amplify\nattention to both salient visual information and the user query at each\ndecoding step. This reduces the impact of visual attention sink, as irrelevant\ntokens exhibit minimal shifts, while ensuring balanced cross-modal fusion for\nwell-integrated representation. Extensive experiments show that GIFT\neffectively mitigates hallucination in VLMs across both generative and\nclassification tasks, achieving up to 20.7% improvement over greedy decoding,\nwhile maintaining general vision-language performance with low computational\noverhead.",
        "url": "http://arxiv.org/abs/2510.22067v1",
        "published_date": "2025-10-24T23:04:26+00:00",
        "updated_date": "2025-10-24T23:04:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zheng Qi",
            "Chao Shang",
            "Evangelia Spiliopoulou",
            "Nikolaos Pappas"
        ],
        "tldr": "The paper introduces Gaze Shift-Guided Cross-modal Fusion Enhancement (GIFT), a method to mitigate hallucination in VLMs by tracking attention shifts and balancing cross-modal fusion, achieving significant improvements in generative and classification tasks.",
        "tldr_zh": "该论文介绍了一种名为Gaze Shift-Guided Cross-modal Fusion Enhancement (GIFT) 的方法，通过跟踪注意力转移和平衡跨模态融合来减少VLM中的幻觉，在生成和分类任务中取得了显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations",
        "summary": "Visualization, a domain-specific yet widely used form of imagery, is an\neffective way to turn complex datasets into intuitive insights, and its value\ndepends on whether data are faithfully represented, clearly communicated, and\naesthetically designed. However, evaluating visualization quality is\nchallenging: unlike natural images, it requires simultaneous judgment across\ndata encoding accuracy, information expressiveness, and visual aesthetics.\nAlthough multimodal large language models (MLLMs) have shown promising\nperformance in aesthetic assessment of natural images, no systematic benchmark\nexists for measuring their capabilities in evaluating visualizations. To\naddress this, we propose VisJudge-Bench, the first comprehensive benchmark for\nevaluating MLLMs' performance in assessing visualization aesthetics and\nquality. It contains 3,090 expert-annotated samples from real-world scenarios,\ncovering single visualizations, multiple visualizations, and dashboards across\n32 chart types. Systematic testing on this benchmark reveals that even the most\nadvanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human\nexperts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a\ncorrelation with human ratings of only 0.429. To address this issue, we propose\nVisJudge, a model specifically designed for visualization aesthetics and\nquality assessment. Experimental results demonstrate that VisJudge\nsignificantly narrows the gap with human judgment, reducing the MAE to 0.442 (a\n19.8% reduction) and increasing the consistency with human experts to 0.681 (a\n58.7% improvement) compared to GPT-5. The benchmark is available at\nhttps://github.com/HKUSTDial/VisJudgeBench.",
        "url": "http://arxiv.org/abs/2510.22373v1",
        "published_date": "2025-10-25T17:31:02+00:00",
        "updated_date": "2025-10-25T17:31:02+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yupeng Xie",
            "Zhiyang Zhang",
            "Yifan Wu",
            "Sirong Lu",
            "Jiayi Zhang",
            "Zhaoyang Yu",
            "Jinlin Wang",
            "Sirui Hong",
            "Bang Liu",
            "Chenglin Wu",
            "Yuyu Luo"
        ],
        "tldr": "The paper introduces VisJudge-Bench, a new benchmark for evaluating MLLMs in assessing visualization aesthetics and quality, and proposes VisJudge, a model that outperforms existing MLLMs on this benchmark.",
        "tldr_zh": "该论文介绍了VisJudge-Bench，这是一个用于评估MLLM在评估可视化美学和质量方面的新基准，并提出了VisJudge模型，该模型在该基准上优于现有的MLLM。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles",
        "summary": "In this paper, we propose Bootstrapped Language-Image Pretraining-driven\nFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a\nnovel multimodal reinforcement learning (RL) framework for autonomous\nlane-keeping (LK), in which semantic embeddings generated by a vision-language\nmodel (VLM) are directly fused with geometric states, LiDAR observations, and\nProportional-Integral-Derivative-based (PID) control feedback within the agent\nobservation space. The proposed method lets the agent learn driving rules that\nare aware of their surroundings and easy to understand by combining high-level\nscene understanding from the VLM with low-level control and spatial signals.\nOur architecture brings together semantic, geometric, and control-aware\nrepresentations to make policy learning more robust. A hybrid reward function\nthat includes semantic alignment, LK accuracy, obstacle avoidance, and speed\nregulation helps learning to be more efficient and generalizable. Our method is\ndifferent from the approaches that only use semantic models to shape rewards.\nInstead, it directly embeds semantic features into the state representation.\nThis cuts down on expensive runtime inference and makes sure that semantic\nguidance is always available. The simulation results show that the proposed\nmodel is better at LK stability and adaptability than the best vision-based and\nmultimodal RL baselines in a wide range of difficult driving situations. We\nmake our code publicly available.",
        "url": "http://arxiv.org/abs/2510.22370v1",
        "published_date": "2025-10-25T17:27:08+00:00",
        "updated_date": "2025-10-25T17:27:08+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SE"
        ],
        "authors": [
            "Seyed Ahmad Hosseini Miangoleh",
            "Amin Jalal Aghdasian",
            "Farzaneh Abdollahi"
        ],
        "tldr": "The paper introduces BLIP-FusePPO, a novel multimodal reinforcement learning framework for autonomous lane-keeping that fuses vision-language model embeddings with other state information, achieving improved performance and robustness compared to existing methods. The code is publicly available.",
        "tldr_zh": "该论文介绍了一种名为BLIP-FusePPO的新型多模态强化学习框架，用于自动车道保持。该框架将视觉-语言模型的嵌入与其它状态信息融合，与现有方法相比，实现了更高的性能和鲁棒性。代码已公开。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning",
        "summary": "Harnessing publicly available, large-scale web data, such as street view and\nsatellite imagery, urban socio-economic sensing is of paramount importance for\nachieving global sustainable development goals. With the emergence of Large\nVision-Language Models (LVLMs), new opportunities have arisen to solve this\ntask by treating it as a multi-modal perception and understanding problem.\nHowever, recent studies reveal that LVLMs still struggle with accurate and\ninterpretable socio-economic predictions from visual data. To address these\nlimitations and maximize the potential of LVLMs, we introduce\n\\textbf{CityRiSE}, a novel framework for \\textbf{R}eason\\textbf{i}ng urban\n\\textbf{S}ocio-\\textbf{E}conomic status in LVLMs through pure reinforcement\nlearning (RL). With carefully curated multi-modal data and verifiable reward\ndesign, our approach guides the LVLM to focus on semantically meaningful visual\ncues, enabling structured and goal-oriented reasoning for generalist\nsocio-economic status prediction. Experiments demonstrate that CityRiSE with\nemergent reasoning process significantly outperforms existing baselines,\nimproving both prediction accuracy and generalization across diverse urban\ncontexts, particularly for prediction on unseen cities and unseen indicators.\nThis work highlights the promise of combining RL and LVLMs for interpretable\nand generalist urban socio-economic sensing.",
        "url": "http://arxiv.org/abs/2510.22282v1",
        "published_date": "2025-10-25T12:56:46+00:00",
        "updated_date": "2025-10-25T12:56:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Tianhui Liu",
            "Hetian Pang",
            "Xin Zhang",
            "Jie Feng",
            "Yong Li",
            "Pan Hui"
        ],
        "tldr": "The paper introduces CityRiSE, a reinforcement learning framework that enhances Large Vision-Language Models (LVLMs) for predicting urban socio-economic status using multi-modal data, achieving improved accuracy and generalization, especially on unseen cities and indicators.",
        "tldr_zh": "该论文介绍了 CityRiSE，一个通过强化学习框架增强大型视觉语言模型（LVLM）用于预测城市社会经济地位的框架，它使用多模态数据，并在未见过的城市和指标上实现了更高的准确性和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction",
        "summary": "Vision-Language Models (VLMs) have shown significant progress in open-set\nchallenges. However, the limited availability of 3D datasets hinders their\neffective application in 3D scene understanding. We propose LOC, a general\nlanguage-guided framework adaptable to various occupancy networks, supporting\nboth supervised and self-supervised learning paradigms. For self-supervised\ntasks, we employ a strategy that fuses multi-frame LiDAR points for\ndynamic/static scenes, using Poisson reconstruction to fill voids, and\nassigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain\ncomprehensive voxel representations. To mitigate feature over-homogenization\ncaused by direct high-dimensional feature distillation, we introduce Densely\nContrastive Learning (DCL). DCL leverages dense voxel semantic information and\npredefined textual prompts. This efficiently enhances open-set recognition\nwithout dense pixel-level supervision, and our framework can also leverage\nexisting ground truth to further improve performance. Our model predicts dense\nvoxel features embedded in the CLIP feature space, integrating textual and\nimage pixel information, and classifies based on text and semantic similarity.\nExperiments on the nuScenes dataset demonstrate the method's superior\nperformance, achieving high-precision predictions for known classes and\ndistinguishing unknown classes without additional training data.",
        "url": "http://arxiv.org/abs/2510.22141v1",
        "published_date": "2025-10-25T03:27:19+00:00",
        "updated_date": "2025-10-25T03:27:19+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG",
            "cs.RO",
            "eess.IV"
        ],
        "authors": [
            "Yuhang Gao",
            "Xiang Xiang",
            "Sheng Zhong",
            "Guoyou Wang"
        ],
        "tldr": "The paper introduces LOC, a language-guided framework for open-set 3D occupancy prediction that leverages VLMs and contrastive learning to improve performance on the nuScenes dataset, demonstrating the ability to distinguish unknown classes without additional training data.",
        "tldr_zh": "该论文介绍了一种名为LOC的语言引导框架，用于开放场景下的3D体素占据预测。该框架利用视觉语言模型和对比学习提高了在nuScenes数据集上的性能，并且展示了在无需额外训练数据的情况下区分未知类别的能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT",
        "summary": "Vision-language models (VLMs) are increasingly used to evaluate multimodal\ncontent, including presentation slides, yet their slide-specific understanding\nremains underexplored {despite their growing role as critics in agentic,\nmodel-forward pipelines}. We introduce VLM-SlideEval, an evaluation framework\nthat probes VLMs along three axes: (1) element-level extraction from slide\nimages aligned to ground truth; (2) robustness to controlled perturbations in\ngeometry, style, and text; and (3) higher-level comprehension, such as\nrecovering a deck's narrative order from shuffled slides. Using publicly\navailable decks from Zenodo\n(https://huggingface.co/datasets/Forceless/Zenodo10K/viewer/default/pptx), we\nstandardize ground-truth element metadata from PowerPoint XML and live\nrenderings into a unified, verifiable schema. Empirically, VLMs underperform on\npixel-accurate extraction and show non-trivial agreement, fidelity, and\nconsistency under controlled perturbations, while performing better on\nsingle-slide content understanding; however, they do not reliably capture\nnarrative structure across slides. These results highlight the limits of\ncurrent VLMs for slide evaluation and motivate calibrated, critic-in-the-loop\nevaluators that drive iterative refinement and selection in agentic pipelines.",
        "url": "http://arxiv.org/abs/2510.22045v1",
        "published_date": "2025-10-24T22:06:56+00:00",
        "updated_date": "2025-10-24T22:06:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hyeonsu Kang",
            "Emily Bao",
            "Anjan Goswami"
        ],
        "tldr": "This paper introduces VLM-SlideEval, a framework for evaluating VLMs on PowerPoint slides, revealing limitations in pixel-accurate extraction, perturbation robustness, and narrative structure understanding, motivating critic-in-the-loop evaluators.",
        "tldr_zh": "本文介绍了VLM-SlideEval，一个评估VLM在PowerPoint幻灯片上表现的框架，揭示了VLM在像素级精确提取、抗扰动性和叙事结构理解方面的局限性，从而推动了闭环评论器评估器的发展。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model",
        "summary": "Using risky text prompts, such as pornography and violent prompts, to test\nthe safety of text-to-image (T2I) models is a critical task. However, existing\nrisky prompt datasets are limited in three key areas: 1) limited risky\ncategories, 2) coarse-grained annotation, and 3) low effectiveness. To address\nthese limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark\ndesigned for evaluating safety-related tasks in T2I models. Specifically, we\nfirst develop a hierarchical risk taxonomy, which consists of 6 primary\ncategories and 14 fine-grained subcategories. Building upon this taxonomy, we\nconstruct a pipeline to collect and annotate risky prompts. Finally, we obtain\n6,432 effective risky prompts, where each prompt is annotated with both\nhierarchical category labels and detailed risk reasons. Moreover, to facilitate\nthe evaluation, we propose a reason-driven risky image detection method that\nexplicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,\nwe conduct a comprehensive evaluation of eight T2I models, nine defense\nmethods, five safety filters, and five attack strategies, offering nine key\ninsights into the strengths and limitations of T2I model safety. Finally, we\ndiscuss potential applications of T2I-RiskyPrompt across various research\nfields. The dataset and code are provided in\nhttps://github.com/datar001/T2I-RiskyPrompt.",
        "url": "http://arxiv.org/abs/2510.22300v1",
        "published_date": "2025-10-25T14:00:26+00:00",
        "updated_date": "2025-10-25T14:00:26+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenyu Zhang",
            "Tairen Zhang",
            "Lanjun Wang",
            "Ruidong Chen",
            "Wenhui Li",
            "Anan Liu"
        ],
        "tldr": "The paper introduces T2I-RiskyPrompt, a new benchmark dataset for evaluating the safety of text-to-image models using a comprehensive set of risky prompts and annotations, along with an evaluation of several models, defenses, and attacks.",
        "tldr_zh": "该论文介绍了T2I-RiskyPrompt，这是一个新的基准数据集，用于评估文本到图像模型的安全性，它使用了一套全面的风险提示和注释，并评估了多个模型、防御和攻击。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning",
        "summary": "Anomaly detection in surveillance videos remains a challenging task due to\nthe diversity of abnormal events, class imbalance, and scene-dependent visual\nclutter. To address these issues, we propose a robust deep learning framework\nthat integrates human-centric preprocessing with spatio-temporal modeling for\nmulti-class anomaly classification. Our pipeline begins by applying YOLO-World\n- an open-vocabulary vision-language detector - to identify human instances in\nraw video clips, followed by ByteTrack for consistent identity-aware tracking.\nBackground regions outside detected bounding boxes are suppressed via Gaussian\nblurring, effectively reducing scene-specific distractions and focusing the\nmodel on behaviorally relevant foreground content. The refined frames are then\nprocessed by an ImageNet-pretrained InceptionV3 network for spatial feature\nextraction, and temporal dynamics are captured using a bidirectional LSTM\n(BiLSTM) for sequence-level classification. Evaluated on a five-class subset of\nthe UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our\nmethod achieves a mean test accuracy of 92.41% across three independent trials,\nwith per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation\nmetrics - including confusion matrices, ROC curves, and macro/weighted averages\n- demonstrate strong generalization and resilience to class imbalance. The\nresults confirm that foreground-focused preprocessing significantly enhances\nanomaly discrimination in real-world surveillance scenarios.",
        "url": "http://arxiv.org/abs/2510.22056v1",
        "published_date": "2025-10-24T22:38:17+00:00",
        "updated_date": "2025-10-24T22:38:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.2.10; I.4.9; I.2.6"
        ],
        "authors": [
            "Mohammad Ali Etemadi Naeen",
            "Hoda Mohammadzade",
            "Saeed Bagheri Shouraki"
        ],
        "tldr": "This paper presents a deep learning framework for anomaly detection in surveillance videos, using YOLO-World and BiLSTMs with human-centric preprocessing to achieve high accuracy on a UCF-Crime dataset subset.",
        "tldr_zh": "本文提出了一种用于监控视频中异常检测的深度学习框架，该框架使用YOLO-World和BiLSTM，并结合以人为中心的预处理，在UCF-Crime数据集子集上实现了高精度。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]