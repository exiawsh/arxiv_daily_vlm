[
    {
        "title": "PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction",
        "summary": "Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.",
        "url": "http://arxiv.org/abs/2512.10888v1",
        "published_date": "2025-12-11T18:19:00+00:00",
        "updated_date": "2025-12-11T18:19:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Brandon Smock",
            "Valerie Faucon-Morin",
            "Max Sokolov",
            "Libin Liang",
            "Tayyibah Khanam",
            "Maury Courtland"
        ],
        "tldr": "The paper introduces PubTables-v2, a large-scale dataset for full-page and multi-page table extraction, designed to benchmark and advance vision-language models for this task. They also introduce POTATR, a VLM extension for comprehensive page-level table extraction.",
        "tldr_zh": "该论文介绍了PubTables-v2，一个用于整页和多页表格提取的大规模数据集，旨在对视觉语言模型进行基准测试并推动其在该任务中的发展。他们还介绍了POTATR，一个用于全面页面级表格提取的VLM扩展。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
        "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
        "url": "http://arxiv.org/abs/2512.10867v1",
        "published_date": "2025-12-11T18:00:21+00:00",
        "updated_date": "2025-12-11T18:00:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongzhao Li",
            "Xiangzhe Kong",
            "Jiahui Su",
            "Zongyang Ma",
            "Mingze Li",
            "Songyou Li",
            "Yuelin Zhang",
            "Yu Rong",
            "Tingyang Xu",
            "Deli Zhao",
            "Wenbing Huang"
        ],
        "tldr": "The paper introduces MiSI-Bench, a benchmark for evaluating the microscopic spatial intelligence of VLMs on molecular structures, revealing that current VLMs struggle but show potential with fine-tuning, especially in spatial transformations.",
        "tldr_zh": "该论文介绍了MiSI-Bench，一个用于评估VLMs在分子结构上微观空间智能的基准，结果显示当前的VLMs表现不佳，但通过微调显示出潜力，尤其是在空间转换方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]