[
    {
        "title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts",
        "summary": "In this paper, we introduce FedMGP, a new paradigm for personalized federated\nprompt learning in vision-language models. FedMGP equips each client with\nmultiple groups of paired textual and visual prompts, enabling the model to\ncapture diverse, fine-grained semantic and instance-level cues. A diversity\nloss is introduced to drive each prompt group to specialize in distinct and\ncomplementary semantic aspects, ensuring that the groups collectively cover a\nbroader range of local characteristics. During communication, FedMGP employs a\ndynamic prompt aggregation strategy based on similarity-guided probabilistic\nsampling: each client computes the cosine similarity between its prompt groups\nand the global prompts from the previous round, then samples s groups via a\nsoftmax-weighted distribution. This soft selection mechanism preferentially\naggregates semantically aligned knowledge while still enabling exploration of\nunderrepresented patterns effectively balancing the preservation of common\nknowledge with client-specific features. Notably, FedMGP maintains parameter\nefficiency by redistributing a fixed prompt capacity across multiple groups,\nachieving state-of-the-art performance with the lowest communication parameters\namong all federated prompt learning methods. Theoretical analysis shows that\nour dynamic aggregation strategy promotes robust global representation learning\nby reinforcing shared semantics while suppressing client-specific noise.\nExtensive experiments demonstrate that FedMGP consistently outperforms prior\napproaches in both personalization and domain generalization across diverse\nfederated vision-language benchmarks. The code will be released on\nhttps://github.com/weihao-bo/FedMGP.git.",
        "url": "http://arxiv.org/abs/2511.00480v1",
        "published_date": "2025-11-01T10:15:04+00:00",
        "updated_date": "2025-11-01T10:15:04+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Weihao Bo",
            "Yanpeng Sun",
            "Yu Wang",
            "Xinyu Zhang",
            "Zechao Li"
        ],
        "tldr": "FedMGP introduces a personalized federated prompt learning method for vision-language models, using multi-group textual and visual prompts and a dynamic aggregation strategy to achieve state-of-the-art performance with low communication overhead.",
        "tldr_zh": "FedMGP 提出了一种用于视觉语言模型的个性化联邦提示学习方法，它使用多组文本和视觉提示以及动态聚合策略，以低通信开销实现最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World",
        "summary": "How far are deep models from real-world video anomaly understanding (VAU)?\nCurrent works typically emphasize on detecting unexpected occurrences deviated\nfrom normal patterns or comprehending anomalous events with interpretable\ndescriptions. However, they exhibit only a superficial comprehension of\nreal-world anomalies, with limited breadth in complex principles and subtle\ncontext that distinguish the anomalies from normalities, e.g., climbing cliffs\nwith safety gear vs. without it. To this end, we introduce CueBench, the first\nof its kind Benchmark, devoted to Context-aware video anomalies within a\nUnified Evaluation framework. We comprehensively establish an event-centric\nhierarchical taxonomy that anchors two core event types: 14 conditional and 18\nabsolute anomaly events, defined by their refined semantics from diverse\ncontexts across 174 scenes and 198 attributes. Based on this, we propose to\nunify and benchmark context-aware VAU with various challenging tasks across\nrecognition, temporal grounding, detection, and anticipation. This also serves\nas a rigorous and fair probing evaluation suite for generative-discriminative\nas well as generalized-specialized vision-language models (VLMs). To address\nthe challenges underlying CueBench, we further develop Cue-R1 based on R1-style\nreinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined\nrewards in a unified generative manner. Extensive results on CueBench reveal\nthat, existing VLMs are still far from satisfactory real-world anomaly\nunderstanding, while our Cue-R1 surpasses these state-of-the-art approaches by\nover 24% on average.",
        "url": "http://arxiv.org/abs/2511.00613v1",
        "published_date": "2025-11-01T16:29:35+00:00",
        "updated_date": "2025-11-01T16:29:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yating Yu",
            "Congqi Cao",
            "Zhaoying Wang",
            "Weihua Meng",
            "Jie Li",
            "Yuxin Li",
            "Zihao Wei",
            "Zhongpei Shen",
            "Jiajun Zhang"
        ],
        "tldr": "The paper introduces CueBench, a new benchmark for context-aware video anomaly understanding, and a novel method, Cue-R1, which outperforms existing VLMs on the benchmark.",
        "tldr_zh": "该论文介绍了 CueBench，一个新的用于上下文感知的视频异常理解的基准测试，以及一种名为 Cue-R1 的新方法，该方法在基准测试中优于现有的 VLM。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era",
        "summary": "Industrial Anomaly Detection (IAD) is critical for enhancing operational\nsafety, ensuring product quality, and optimizing manufacturing efficiency\nacross global industries. However, the IAD algorithms are severely constrained\nby the limitations of existing public benchmarks. Current datasets exhibit\nrestricted category diversity and insufficient scale, frequently resulting in\nmetric saturation and limited model transferability to real-world scenarios. To\naddress this gap, we introduce Real-IAD Variety, the largest and most diverse\nIAD benchmark, comprising 198,960 high-resolution images across 160 distinct\nobject categories. Its diversity is ensured through comprehensive coverage of\n28 industries, 24 material types, and 22 color variations. Our comprehensive\nexperimental analysis validates the benchmark's substantial challenge:\nstate-of-the-art multi-class unsupervised anomaly detection methods experience\nsignificant performance degradation when scaled from 30 to 160 categories.\nCrucially, we demonstrate that vision-language models exhibit remarkable\nrobustness to category scale-up, with minimal performance variation across\ndifferent category counts, significantly enhancing generalization capabilities\nin diverse industrial contexts. The unprecedented scale and complexity of\nReal-IAD Variety position it as an essential resource for training and\nevaluating next-generation foundation models for anomaly detection. By\nproviding this comprehensive benchmark with rigorous evaluation protocols\nacross multi-class unsupervised, multi-view, and zero-/few-shot settings, we\naim to accelerate research beyond domain-specific constraints, enabling the\ndevelopment of scalable, general-purpose anomaly detection systems. Real-IAD\nVariety will be made publicly available to facilitate innovation in this\ncritical field.",
        "url": "http://arxiv.org/abs/2511.00540v1",
        "published_date": "2025-11-01T12:58:02+00:00",
        "updated_date": "2025-11-01T12:58:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenbing Zhu",
            "Chengjie Wang",
            "Bin-Bin Gao",
            "Jiangning Zhang",
            "Guannan Jiang",
            "Jie Hu",
            "Zhenye Gan",
            "Lidong Wang",
            "Ziqing Zhou",
            "Linjie Cheng",
            "Yurui Pan",
            "Bo Peng",
            "Mingmin Chi",
            "Lizhuang Ma"
        ],
        "tldr": "The paper introduces Real-IAD Variety, a large and diverse industrial anomaly detection benchmark with 198,960 images across 160 object categories, and demonstrates the robustness of vision-language models on this dataset compared to traditional methods.",
        "tldr_zh": "该论文介绍了Real-IAD Variety，一个大型且多样化的工业异常检测基准，包含198,960张图片，涵盖160个物体类别。论文展示了视觉-语言模型在该数据集上相比传统方法的鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text-guided Fine-Grained Video Anomaly Detection",
        "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events within video\nsegments. In scenarios such as surveillance or industrial process monitoring,\nanomaly detection is of critical importance. While existing approaches are\nsemi-automated, requiring human assessment for anomaly detection, traditional\nVADs offer limited output as either normal or anomalous. We propose Text-guided\nFine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large\nVision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)\nthat performs pixel-wise visual-textual feature alignment to generate\nfine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly\nEncoder (RAE) that transforms the heatmaps into learnable textual embeddings,\nguiding the LVLM to accurately identify and localize anomalous events in\nvideos. This significantly enhances both the granularity and interactivity of\nanomaly detection. The proposed method achieving SOTA performance by\ndemonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and\n67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,\nand subjectively verified more preferable textual description on the\nShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;\nYes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for\ntargets, 78.10 for trajectories; Yes/No accuracy: 89.73%).",
        "url": "http://arxiv.org/abs/2511.00524v2",
        "published_date": "2025-11-01T11:59:23+00:00",
        "updated_date": "2025-11-05T15:46:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jihao Gu",
            "Kun Li",
            "He Wang",
            "Kaan Akşit"
        ],
        "tldr": "This paper introduces Text-guided Fine-Grained Video Anomaly Detection (T-VAD), a novel framework using LVLMs with Anomaly Heatmap Decoder and Region-aware Anomaly Encoder to enhance the granularity and interactivity of anomaly detection, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了文本引导的细粒度视频异常检测（T-VAD），这是一种新颖的框架，使用LVLM结合异常热图解码器和区域感知异常编码器，以增强异常检测的粒度和交互性，并实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation",
        "summary": "Vision language models such as CLIP have shown remarkable performance in zero\nshot classification, but remain susceptible to spurious correlations, where\nirrelevant visual features influence predictions. Existing debiasing methods\noften require access to training data and explicit group labels to perform\nfine-tuning or adjust embeddings, which limits their practicality in real-world\nsettings. Test-time methods attempt to avoid this constraint, but many still\ndepend on prior knowledge of dataset specific biases, limiting their\ngeneralizability in open set settings. In this work, we propose a test-time\ndebiasing method for ViT based CLIP models that requires no additional training\nor assumptions of bias annotations. Our approach uses a pretrained segmentation\nmodel to isolate the target visual attribute, then adjusts the non target\nregions so that their embeddings are uniformly similar to all class specific\ntext prompts. This procedure removes unintended bias signals from confounding\nvisual regions while preserving the target attribute. Experiments on Waterbirds\nand CelebA show that our method outperforms existing test-time debiasing\napproaches in both group robustness metrics and Attention IoU. These results\ndemonstrate the effectiveness of segmentation guided interventions for scalable\nand annotation free bias mitigation in vision language models.",
        "url": "http://arxiv.org/abs/2511.00523v1",
        "published_date": "2025-11-01T11:57:57+00:00",
        "updated_date": "2025-11-01T11:57:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fangyu Wu",
            "Yujun Cai"
        ],
        "tldr": "The paper introduces SegDebias, a test-time debiasing method for ViT-based CLIP models using segmentation to mitigate spurious correlations without requiring training data or bias annotations, demonstrating improved robustness on Waterbirds and CelebA datasets.",
        "tldr_zh": "该论文介绍了一种名为SegDebias的测试时去偏方法，用于基于ViT的CLIP模型，通过分割来缓解虚假相关性，无需训练数据或偏见标注，并在Waterbirds和CelebA数据集上展示了改进的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation",
        "summary": "Video generative models pretrained on large-scale datasets can produce\nhigh-quality videos, but are often conditioned on text or a single image,\nlimiting controllability and applicability. We introduce ID-Composer, a novel\nframework that addresses this gap by tackling multi-subject video generation\nfrom a text prompt and reference images. This task is challenging as it\nrequires preserving subject identities, integrating semantics across subjects\nand modalities, and maintaining temporal consistency. To faithfully preserve\nthe subject consistency and textual information in synthesized videos,\nID-Composer designs a hierarchical identity-preserving attention mechanism,\nwhich effectively aggregates features within and across subjects and\nmodalities. To effectively allow for the semantic following of user intention,\nwe introduce semantic understanding via pretrained vision-language model (VLM),\nleveraging VLM's superior semantic understanding to provide fine-grained\nguidance and capture complex interactions between multiple subjects.\nConsidering that standard diffusion loss often fails in aligning the critical\nconcepts like subject ID, we employ an online reinforcement learning phase to\ndrive the overall training objective of ID-Composer into RLVR. Extensive\nexperiments demonstrate that our model surpasses existing methods in identity\npreservation, temporal consistency, and video quality.",
        "url": "http://arxiv.org/abs/2511.00511v2",
        "published_date": "2025-11-01T11:29:14+00:00",
        "updated_date": "2025-11-04T03:11:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Panwang Pan",
            "Jingjing Zhao",
            "Yuchen Lin",
            "Chenguo Lin",
            "Chenxin Li",
            "Haopeng Li",
            "Honglei Yan",
            "Tingting Shen",
            "Yadong Mu"
        ],
        "tldr": "ID-Composer is a novel framework for multi-subject video generation from text prompts and reference images, using hierarchical identity-preserving attention and VLM-based semantic understanding, further enhanced by reinforcement learning for improved identity alignment.",
        "tldr_zh": "ID-Composer是一个新的框架，用于从文本提示和参考图像生成多主体的视频。它使用分层身份保持注意力机制和基于VLM的语义理解，并通过强化学习进一步增强，以改善身份对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning",
        "summary": "We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable\nMedical Visual Question Answering (Med-VQA) with spatial grounding. The dataset\ncontains 17,597 question-answer pairs across 4,394 images, each annotated with\nradiologist-verified bounding boxes and clinical reasoning explanations. Our\nquestion taxonomy spans six diagnostic types-Where, What, Is there, How many,\nWhich, and Yes/No-capturing diverse clinical intents. To improve reliability,\nwe construct a balanced distribution of 41.7% positive and 58.3% negative\nsamples, mitigating hallucinations in normal cases. Benchmarking with\nMedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over\nbaseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance\nreproducible and clinically grounded Med-VQA research. The dataset and\nevaluation tools are publicly available at\nhuggingface.co/datasets/Dangindev/VinDR-CXR-VQA.",
        "url": "http://arxiv.org/abs/2511.00504v1",
        "published_date": "2025-11-01T11:17:44+00:00",
        "updated_date": "2025-11-01T11:17:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hai-Dang Nguyen",
            "Ha-Hieu Pham",
            "Hao T. Nguyen",
            "Huy-Hieu Pham"
        ],
        "tldr": "The paper introduces VinDr-CXR-VQA, a large-scale, radiologist-verified chest X-ray VQA dataset with spatial grounding for explainable medical AI, showing improved performance with MedGemma-4B-it.",
        "tldr_zh": "该论文介绍了VinDr-CXR-VQA，一个大规模、放射科医生验证的胸部X光VQA数据集，具有空间定位功能，用于可解释的医学AI，并展示了MedGemma-4B-it的性能改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training",
        "summary": "The Contrastive Language-Image Pretraining (CLIP) model has significantly\nadvanced vision-language modeling by aligning image-text pairs from large-scale\nweb data through self-supervised contrastive learning. Yet, its reliance on\nuncurated Internet-sourced data exposes it to data poisoning and backdoor\nrisks. While existing studies primarily investigate image-based attacks, the\ntext modality, which is equally central to CLIP's training, remains\nunderexplored. In this work, we introduce ToxicTextCLIP, a framework for\ngenerating high-quality adversarial texts that target CLIP during the\npre-training phase. The framework addresses two key challenges: semantic\nmisalignment caused by background inconsistency with the target class, and the\nscarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively\napplies: 1) a background-aware selector that prioritizes texts with background\ncontent aligned to the target class, and 2) a background-driven augmenter that\ngenerates semantically coherent and diverse poisoned samples. Extensive\nexperiments on classification and retrieval tasks show that ToxicTextCLIP\nachieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while\nbypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be\naccessed via https://github.com/xinyaocse/ToxicTextCLIP/.",
        "url": "http://arxiv.org/abs/2511.00446v1",
        "published_date": "2025-11-01T08:25:49+00:00",
        "updated_date": "2025-11-01T08:25:49+00:00",
        "categories": [
            "cs.CV",
            "cs.CR",
            "cs.LG"
        ],
        "authors": [
            "Xin Yao",
            "Haiyang Zhao",
            "Yimin Chen",
            "Jiawei Guo",
            "Kecheng Huang",
            "Ming Zhao"
        ],
        "tldr": "This paper introduces ToxicTextCLIP, a framework for generating adversarial text to poison CLIP pre-training, demonstrating high success rates in poisoning and backdoor attacks while bypassing existing defenses.",
        "tldr_zh": "该论文介绍了ToxicTextCLIP，一个用于生成对抗性文本以毒害CLIP预训练的框架，展示了在毒害和后门攻击方面的高成功率，同时绕过了现有的防御措施。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection",
        "summary": "With the rapid development of generative models, detecting generated fake\nimages to prevent their malicious use has become a critical issue recently.\nExisting methods frame this challenge as a naive binary image classification\ntask. However, such methods focus only on visual clues, yielding trained\ndetectors susceptible to overfitting specific image patterns and incapable of\ngeneralizing to unseen models. In this paper, we address this issue from a\nmulti-modal perspective and find that fake images cannot be properly aligned\nwith corresponding captions compared to real images. Upon this observation, we\npropose a simple yet effective detector termed ITEM by leveraging the\nimage-text misalignment in a joint visual-language space as discriminative\nclues. Specifically, we first measure the misalignment of the images and\ncaptions in pre-trained CLIP's space, and then tune a MLP head to perform the\nusual detection task. Furthermore, we propose a hierarchical misalignment\nscheme that first focuses on the whole image and then each semantic object\ndescribed in the caption, which can explore both global and fine-grained local\nsemantic misalignment as clues. Extensive experiments demonstrate the\nsuperiority of our method against other state-of-the-art competitors with\nimpressive generalization and robustness on various recent generative models.",
        "url": "http://arxiv.org/abs/2511.00427v1",
        "published_date": "2025-11-01T06:51:14+00:00",
        "updated_date": "2025-11-01T06:51:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Daichi Zhang",
            "Tong Zhang",
            "Jianmin Bao",
            "Shiming Ge",
            "Sabine Süsstrunk"
        ],
        "tldr": "This paper introduces ITEM, a method for detecting fake images by leveraging image-text misalignment in a visual-language space, utilizing a hierarchical approach to explore both global and local semantic discrepancies.",
        "tldr_zh": "本文介绍了一种名为ITEM的方法，通过利用视觉-语言空间中的图像-文本不对齐来检测伪造图像，并采用分层方法来探索全局和细粒度的局部语义差异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "iFlyBot-VLA Technical Report",
        "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community",
        "url": "http://arxiv.org/abs/2511.01914v1",
        "published_date": "2025-11-01T06:24:56+00:00",
        "updated_date": "2025-11-01T06:24:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Yuan Zhang",
            "Chenyu Xue",
            "Wenjie Xu",
            "Chao Ji",
            "Jiajia wu",
            "Jia Pan"
        ],
        "tldr": "The paper introduces iFlyBot-VLA, a Vision-Language-Action model trained with a novel framework utilizing a latent action model and dual-level action representation to improve performance on robotic manipulation tasks, with plans to open-source a portion of the dataset.",
        "tldr_zh": "该论文介绍了 iFlyBot-VLA，一个视觉-语言-动作模型，它采用了一种新颖的框架进行训练，该框架利用潜在动作模型和双层动作表示来提高机器人的操作任务性能，并计划开源部分数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LGCA: Enhancing Semantic Representation via Progressive Expansion",
        "summary": "Recent advancements in large-scale pretraining in natural language processing\nhave enabled pretrained vision-language models such as CLIP to effectively\nalign images and text, significantly improving performance in zero-shot image\nclassification tasks. Subsequent studies have further demonstrated that\ncropping images into smaller regions and using large language models to\ngenerate multiple descriptions for each caption can further enhance model\nperformance. However, due to the inherent sensitivity of CLIP, random image\ncrops can introduce misinformation and bias, as many images share similar\nfeatures at small scales. To address this issue, we propose\nLocalized-Globalized Cross-Alignment (LGCA), a framework that first captures\nthe local features of an image and then repeatedly selects the most salient\nregions and expands them. The similarity score is designed to incorporate both\nthe original and expanded images, enabling the model to capture both local and\nglobal features while minimizing misinformation. Additionally, we provide a\ntheoretical analysis demonstrating that the time complexity of LGCA remains the\nsame as that of the original model prior to the repeated expansion process,\nhighlighting its efficiency and scalability. Extensive experiments demonstrate\nthat our method substantially improves zero-shot performance across diverse\ndatasets, outperforming state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2511.00419v1",
        "published_date": "2025-11-01T06:09:42+00:00",
        "updated_date": "2025-11-01T06:09:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Thanh Hieu Cao",
            "Trung Khang Tran",
            "Gia Thinh Pham",
            "Tuong Nghiem Diep",
            "Thanh Binh Nguyen"
        ],
        "tldr": "The paper introduces LGCA, a framework that enhances vision-language model performance in zero-shot image classification by selectively expanding salient image regions, mitigating misinformation from random crops, while maintaining computational efficiency.",
        "tldr_zh": "该论文介绍了LGCA框架，通过选择性地扩展显著图像区域，减轻随机裁剪带来的误导信息，同时保持计算效率，从而提高视觉-语言模型在零样本图像分类中的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks",
        "summary": "We present the first unified framework that jointly handles three\noperationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting\neach as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model\n(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT\nquality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a\nlightweight single-sample algorithm that leverages the discrepancy between\nreward and model confidence as a per-sample advantage signal. This design\nnaturally focuses updates on informative responses while eliminating group\nsampling, thereby addressing GRPO's key limitations: confidence-agnostic\nlearning, signal dilution, and prohibitive computational overhead. We also\nintroduce an \"output-to-reasoning\" strategy to construct high-fidelity SFT data\nthat ensures logical consistency with ground-truth masks. Experiments show our\nmodel matches or outperforms specialized SOTA methods and strong closed-source\nVLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for\nCoSOD, surpassing the prior best by 8.0 percentage points, despite using far\nless training data.",
        "url": "http://arxiv.org/abs/2511.00396v1",
        "published_date": "2025-11-01T04:37:01+00:00",
        "updated_date": "2025-11-01T04:37:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Long Li",
            "Shuichen Ji",
            "Ziyang Luo",
            "Nian Liu",
            "Dingwen Zhang",
            "Junwei Han"
        ],
        "tldr": "The paper presents CoT-Saliency, a unified Chain-of-Thought framework using VLMs for heterogeneous saliency tasks (SOD, CoSOD, SIS), enhanced by a novel Confidence-Guided Policy Optimization and an \"output-to-reasoning\" data construction strategy, achieving SOTA performance with less data.",
        "tldr_zh": "该论文提出了CoT-Saliency，一个统一的Chain-of-Thought框架，利用视觉语言模型处理异构显著性任务（SOD、CoSOD、SIS）。通过一种新的置信度引导策略优化和“输出到推理”的数据构建策略，该框架以更少的数据实现了SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture",
        "summary": "Accurate classification plays a pivotal role in smart agriculture, enabling\napplications such as crop monitoring, fruit recognition, and pest detection.\nHowever, conventional centralized training often requires large-scale data\ncollection, which raises privacy concerns, while standard federated learning\nstruggles with non-independent and identically distributed (non-IID) data and\nincurs high communication costs. To address these challenges, we propose a\nfederated learning framework that integrates a frozen Contrastive\nLanguage-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight\ntransformer classifier. By leveraging the strong feature extraction capability\nof the pre-trained CLIP ViT, the framework avoids training large-scale models\nfrom scratch and restricts federated updates to a compact classifier, thereby\nreducing transmission overhead significantly. Furthermore, to mitigate\nperformance degradation caused by non-IID data distribution, a small subset\n(1%) of CLIP-extracted feature representations from all classes is shared\nacross clients. These shared features are non-reversible to raw images,\nensuring privacy preservation while aligning class representation across\nparticipants. Experimental results on agricultural classification tasks show\nthat the proposed method achieve 86.6% accuracy, which is more than 4 times\nhigher compared to baseline federated learning approaches. This demonstrates\nthe effectiveness and efficiency of combining vision-language model features\nwith federated learning for privacy-preserving and scalable agricultural\nintelligence.",
        "url": "http://arxiv.org/abs/2511.00269v1",
        "published_date": "2025-10-31T21:44:52+00:00",
        "updated_date": "2025-10-31T21:44:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Long Li",
            "Jiajia Li",
            "Dong Chen",
            "Lina Pu",
            "Haibo Yao",
            "Yanbo Huang"
        ],
        "tldr": "This paper proposes FedReplay, a federated learning framework that uses a frozen CLIP ViT for feature extraction and a lightweight classifier, sharing a small subset of CLIP features across clients to address non-IID data issues in smart agriculture, achieving significant accuracy improvements.",
        "tldr_zh": "该论文提出了FedReplay，一个联邦学习框架，它使用冻结的CLIP ViT进行特征提取，并使用轻量级分类器，通过在客户端之间共享CLIP特征的一个小子集来解决智能农业中非IID数据问题，从而显著提高了准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Retrospect to Multi-prompt Learning across Vision and Language",
        "summary": "The vision community is undergoing the unprecedented progress with the\nemergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays\nas the holy grail of accessing VLMs since it enables their fast adaptation to\ndownstream tasks with limited resources. Whereas existing researches milling\naround single-prompt paradigms, rarely investigate the technical potential\nbehind their multi-prompt learning counterparts. This paper aims to provide a\nprincipled retrospect for vision-language multi-prompt learning. We extend the\nrecent constant modality gap phenomenon to learnable prompts and then, justify\nthe superiority of vision-language transfer with multi-prompt augmentation,\nempirically and theoretically. In terms of this observation, we propose an\nEnergy-based Multi-prompt Learning (EMPL) to generate multiple prompt\nembeddings by drawing instances from an energy-based distribution, which is\nimplicitly defined by VLMs. So our EMPL is not only parameter-efficient but\nalso rigorously lead to the balance between in-domain and out-of-domain\nopen-vocabulary generalization. Comprehensive experiments have been conducted\nto justify our claims and the excellence of EMPL.",
        "url": "http://arxiv.org/abs/2511.00191v1",
        "published_date": "2025-10-31T18:50:35+00:00",
        "updated_date": "2025-10-31T18:50:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ziliang Chen",
            "Xin Huang",
            "Quanlong Guan",
            "Liang Lin",
            "Weiqi Luo"
        ],
        "tldr": "This paper investigates multi-prompt learning for Vision-Language Models (VLMs), introduces Energy-based Multi-prompt Learning (EMPL) to improve generalization, and validates its effectiveness through experiments.",
        "tldr_zh": "本文研究了视觉语言模型（VLM）的多提示学习，引入了基于能量的多提示学习（EMPL）以提高泛化能力，并通过实验验证了其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection",
        "summary": "The rapid evolution of AI-generated images poses unprecedented challenges to\ninformation integrity and media authenticity. Existing detection approaches\nsuffer from fundamental limitations: traditional classifiers lack\ninterpretability and fail to generalize across evolving generative models,\nwhile vision-language models (VLMs), despite their promise, remain constrained\nto single-shot analysis and pixel-level reasoning. To address these challenges,\nwe introduce AIFo (Agent-based Image Forensics), a novel training-free\nframework that emulates human forensic investigation through multi-agent\ncollaboration. Unlike conventional methods, our framework employs a set of\nforensic tools, including reverse image search, metadata extraction,\npre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based\nagents that collect, synthesize, and reason over cross-source evidence. When\nevidence is conflicting or insufficient, a structured multi-agent debate\nmechanism allows agents to exchange arguments and reach a reliable conclusion.\nFurthermore, we enhance the framework with a memory-augmented reasoning module\nthat learns from historical cases to improve future detection accuracy. Our\ncomprehensive evaluation spans 6,000 images across both controlled laboratory\nsettings and challenging real-world scenarios, including images from modern\ngenerative platforms and diverse online sources. AIFo achieves 97.05% accuracy,\nsubstantially outperforming traditional classifiers and state-of-the-art VLMs.\nThese results demonstrate that agent-based procedural reasoning offers a new\nparadigm for more robust, interpretable, and adaptable AI-generated image\ndetection.",
        "url": "http://arxiv.org/abs/2511.00181v1",
        "published_date": "2025-10-31T18:36:49+00:00",
        "updated_date": "2025-10-31T18:36:49+00:00",
        "categories": [
            "cs.CV",
            "cs.CR"
        ],
        "authors": [
            "Mengfei Liang",
            "Yiting Qu",
            "Yukun Jiang",
            "Michael Backes",
            "Yang Zhang"
        ],
        "tldr": "The paper introduces AIFo, a novel agent-based framework for AI-generated image detection that uses LLM-based agents to coordinate forensic tools, debate conflicting evidence, and learn from historical cases, achieving state-of-the-art accuracy.",
        "tldr_zh": "该论文介绍了AIFo，一种新颖的基于代理的AI生成图像检测框架，它使用基于LLM的代理来协调取证工具、辩论冲突证据，并从历史案例中学习，从而实现了最先进的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning",
        "summary": "Multimodal code generation has garnered significant interest within the\nresearch community. Despite the notable success of recent vision-language\nmodels (VLMs) on specialized tasks like Chart-to-code generation, their\nreliance on single-task training regimens fosters a narrow paradigm that\nhinders the development of generalized \\textbf{VI}sio\\textbf{N} \\textbf{C}ode\n\\textbf{I}ntelligence. In this work, we introduce \\textbf{VinciCoder}, a\nunified multimodal code generation model that addresses this limitation via a\ntwo-stage training framework. We begin by constructing a large-scale Supervised\nFinetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving\ndirect code generation and visual-based code refinement. Subsequently, we\nintroduce a Visual Reinforcement Learning (ViRL) strategy, which employs a\ncoarse-to-fine reward mechanism to improve visual fidelity by calculating\nvisual similarity across local and global image patches. Extensive experiments\non various multimodal code generation benchmarks demonstrate that VinciCoder\nachieves state-of-the-art performance, underscoring the effectiveness of our\ncoarse-to-fine ViRL strategy. The code and model will be available at\nhttps://github.com/DocTron-hub/VinciCoder.",
        "url": "http://arxiv.org/abs/2511.00391v1",
        "published_date": "2025-11-01T04:05:26+00:00",
        "updated_date": "2025-11-01T04:05:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuanle Zhao",
            "Deyang Jiang",
            "Zhixiong Zeng",
            "Lei Chen",
            "Haibo Qiu",
            "Jing Huang",
            "Yufeng Zhong",
            "Liming Zheng",
            "Yilin Cao",
            "Lin Ma"
        ],
        "tldr": "VinciCoder is a unified multimodal code generation model that uses a two-stage training framework involving supervised finetuning and visual reinforcement learning with a coarse-to-fine reward mechanism to improve visual fidelity, achieving state-of-the-art performance.",
        "tldr_zh": "VinciCoder是一个统一的多模态代码生成模型，它使用一个两阶段训练框架，包括监督微调和视觉强化学习，并采用由粗到精的奖励机制来提高视觉保真度，从而实现最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond",
        "summary": "Multimodal Large Language Models (MLLMs) have revolutionized numerous\nresearch fields, including computer vision and affective computing. As a\npivotal challenge in this interdisciplinary domain, facial expression\nrecognition (FER) has evolved from separate, domain-specific models to more\nunified approaches. One promising avenue to unify FER tasks is converting\nconventional FER datasets into visual question-answering (VQA) formats,\nenabling the direct application of powerful generalist MLLMs for inference.\nHowever, despite the success of cutting-edge MLLMs in various tasks, their\nperformance on FER tasks remains largely unexplored. To address this gap, we\nprovide FERBench, a systematic benchmark that incorporates 20 state-of-the-art\nMLLMs across four widely used FER datasets. Our results reveal that, while\nMLLMs exhibit good classification performance, they still face significant\nlimitations in reasoning and interpretability. To this end, we introduce\npost-training strategies aimed at enhancing the facial expression reasoning\ncapabilities of MLLMs. Specifically, we curate two high-quality and large-scale\ndatasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K\nfor reinforcement learning with verifiable rewards (RLVR), respectively.\nBuilding upon them, we develop a unified and interpretable FER foundation model\ntermed UniFER-7B, which outperforms many open-sourced and closed-source\ngeneralist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).",
        "url": "http://arxiv.org/abs/2511.00389v1",
        "published_date": "2025-11-01T03:53:00+00:00",
        "updated_date": "2025-11-01T03:53:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fan Zhang",
            "Haoxuan Li",
            "Shengju Qian",
            "Xin Wang",
            "Zheng Lian",
            "Hao Wu",
            "Zhihong Zhu",
            "Yuan Gao",
            "Qiankun Li",
            "Yefeng Zheng",
            "Zhouchen Lin",
            "Pheng-Ann Heng"
        ],
        "tldr": "This paper benchmarks the performance of MLLMs on facial expression recognition (FER) tasks, identifies limitations in reasoning and interpretability, and introduces a new unified FER foundation model, UniFER-7B, trained on curated datasets with post-training strategies to improve performance.",
        "tldr_zh": "本文对MLLM在面部表情识别（FER）任务上的性能进行了基准测试，发现了其在推理和可解释性方面的局限性，并介绍了一种新的统一FER基础模型UniFER-7B，该模型通过在精选数据集上进行训练并采用后训练策略，以提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict",
        "summary": "Video moment retrieval uses a text query to locate a moment from a given\nuntrimmed video reference. Locating corresponding video moments with text\nqueries helps people interact with videos efficiently. Current solutions for\nthis task have not considered conflict within location results from different\nmodels, so various models cannot integrate correctly to produce better results.\nThis study introduces a reinforcement learning-based video moment retrieval\nmodel that can scan the whole video once to find the moment's boundary while\nproducing its locational evidence. Moreover, we proposed a multi-agent system\nframework that can use evidential learning to resolve conflicts between agents'\nlocalization output. As a side product of observing and dealing with conflicts\nbetween agents, we can decide whether a query has no corresponding moment in a\nvideo (out-of-scope) without additional training, which is suitable for\nreal-world applications. Extensive experiments on benchmark datasets show the\neffectiveness of our proposed methods compared with state-of-the-art\napproaches. Furthermore, the results of our study reveal that modeling\ncompetition and conflict of the multi-agent system is an effective way to\nimprove RL performance in moment retrieval and show the new role of evidential\nlearning in the multi-agent framework.",
        "url": "http://arxiv.org/abs/2511.00370v1",
        "published_date": "2025-11-01T02:42:36+00:00",
        "updated_date": "2025-11-01T02:42:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chaochen Wu",
            "Guan Luo",
            "Meiyun Zuo",
            "Zhitao Fan"
        ],
        "tldr": "This paper introduces a reinforcement learning-based video moment retrieval model with a multi-agent system that resolves conflicts in localization output using evidential learning, also addressing the out-of-scope query problem.",
        "tldr_zh": "本文介绍了一种基于强化学习的视频片段检索模型，该模型采用多智能体系统，利用证据学习解决定位输出中的冲突，同时解决了超出范围的查询问题。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Spot The Ball: A Benchmark for Visual Social Inference",
        "summary": "Humans excel at visual social inference, the ability to infer hidden elements\nof a scene from subtle behavioral cues such as other people's gaze, pose, and\norientation. This ability drives everyday social reasoning in humans and is\ncritical for developing more human-like AI agents. We introduce Spot The Ball,\na challenging benchmark for evaluating visual social inference in\nvision-language models (VLMs) using sports as a test domain. The task is to\nlocalize a removed sports ball from soccer, basketball, and volleyball images.\nWe present a curated evaluation set with human baselines and a scalable\npipeline for generating additional test items. We evaluate four\nstate-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting\nstrategies, finding that humans are consistently two to three times more\naccurate (20-34%) than models ($\\leq$ 17%) across all sports. Our analyses show\nthat models rely on superficial spatial heuristics--such as guessing near the\nimage center or nearby players--while humans leverage social cues like gaze\ndirection and body pose. These findings reveal a persistent human-model gap in\nvisual social reasoning and underscore the need for architectures that\nexplicitly encode structured behavioral cues to achieve robust, human-like\ninference.",
        "url": "http://arxiv.org/abs/2511.00261v1",
        "published_date": "2025-10-31T21:20:46+00:00",
        "updated_date": "2025-10-31T21:20:46+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Neha Balamurugan",
            "Sarah Wu",
            "Adam Chun",
            "Gabe Gaw",
            "Cristobal Eyzaguirre",
            "Tobias Gerstenberg"
        ],
        "tldr": "The paper introduces 'Spot The Ball,' a new benchmark for visual social inference in VLMs, revealing a significant performance gap between humans and state-of-the-art models in understanding social cues.",
        "tldr_zh": "该论文介绍了“Spot The Ball”，一个新的用于评估视觉语言模型中视觉社交推理能力的基准，揭示了人类和最先进模型在理解社交线索方面的显著性能差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CompAgent: An Agentic Framework for Visual Compliance Verification",
        "summary": "Visual compliance verification is a critical yet underexplored problem in\ncomputer vision, especially in domains such as media, entertainment, and\nadvertising where content must adhere to complex and evolving policy rules.\nExisting methods often rely on task-specific deep learning models trained on\nmanually labeled datasets, which are costly to build and limited in\ngeneralizability. While recent multi-modal large language models (MLLMs) offer\nbroad real-world knowledge and policy understanding, they struggle to reason\nover fine-grained visual details and apply structured compliance rules\neffectively on their own. In this paper, we propose CompAgent, the first\nagentic framework for visual compliance verification. CompAgent augments MLLMs\nwith a suite of visual tools - such as object detectors, face analyzers, NSFW\ndetectors, and captioning models - and introduces a planning agent that\ndynamically selects appropriate tools based on the compliance policy. A\nverification agent then integrates image, tool outputs, and policy context to\nperform multi-modal reasoning. Experiments on public benchmarks show that\nCompAgent outperforms specialized classifiers, direct MLLM prompting, and\ncurated routing baselines, achieving up to 76% F1 score and a 10% improvement\nover the state-of-the-art on the UnsafeBench dataset. Our results demonstrate\nthe effectiveness of agentic planning and tool-augmented reasoning for\nscalable, accurate, and adaptable visual compliance verification.",
        "url": "http://arxiv.org/abs/2511.00171v1",
        "published_date": "2025-10-31T18:20:06+00:00",
        "updated_date": "2025-10-31T18:20:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rahul Ghosh",
            "Baishali Chaudhury",
            "Hari Prasanna Das",
            "Meghana Ashok",
            "Ryan Razkenari",
            "Sungmin Hong",
            "Chun-Hao Liu"
        ],
        "tldr": "The paper introduces CompAgent, an agentic framework that enhances MLLMs with visual tools and planning agents for visual compliance verification, achieving state-of-the-art performance on public benchmarks.",
        "tldr_zh": "该论文介绍了CompAgent，一个agentic框架，通过视觉工具和规划代理增强MLLM，用于视觉合规性验证，并在公共基准测试中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing",
        "summary": "In entomology and ecology research, biologists often need to collect a large\nnumber of insects, among which beetles are the most common species. A common\npractice for biologists to organize beetles is to place them on trays and take\na picture of each tray. Given the images of thousands of such trays, it is\nimportant to have an automated pipeline to process the large-scale data for\nfurther research. Therefore, we develop a 3-stage pipeline to detect all the\nbeetles on each tray, sort and crop the image of each beetle, and do\nmorphological segmentation on the cropped beetles. For detection, we design an\niterative process utilizing a transformer-based open-vocabulary object detector\nand a vision-language model. For segmentation, we manually labeled 670 beetle\nimages and fine-tuned two variants of a transformer-based segmentation model to\nachieve fine-grained segmentation of beetles with relatively high accuracy. The\npipeline integrates multiple deep learning methods and is specialized for\nbeetle image processing, which can greatly improve the efficiency to process\nlarge-scale beetle data and accelerate biological research.",
        "url": "http://arxiv.org/abs/2511.00255v1",
        "published_date": "2025-10-31T20:55:33+00:00",
        "updated_date": "2025-10-31T20:55:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fangxun Liu",
            "S M Rayeed",
            "Samuel Stevens",
            "Alyson East",
            "Cheng Hsuan Chiang",
            "Colin Lee",
            "Daniel Yi",
            "Junke Yang",
            "Tejas Naik",
            "Ziyi Wang",
            "Connor Kilrain",
            "Elijah H Buckwalter",
            "Jiacheng Hou",
            "Saul Ibaven Bueno",
            "Shuheng Wang",
            "Xinyue Ma",
            "Yifan Liu",
            "Zhiyuan Tao",
            "Ziheng Zhang",
            "Eric Sokol",
            "Michael Belitz",
            "Sydne Record",
            "Charles V. Stewart",
            "Wei-Lun Chao"
        ],
        "tldr": "The paper introduces a 3-stage deep learning pipeline, BeetleFlow, for beetle image processing, incorporating detection, sorting/cropping, and morphological segmentation to improve efficiency in biological research.",
        "tldr_zh": "该论文介绍了一个三阶段的深度学习流水线BeetleFlow，用于甲虫图像处理，包括检测、排序/裁剪和形态分割，以提高生物学研究的效率。",
        "relevance_score": 3,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]