[
    {
        "title": "SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models",
        "summary": "Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal\nreasoning but often amplify safety risks under adversarial or unsafe prompts, a\nphenomenon we call the \\textit{Reasoning Tax}. Existing defenses mainly act at\nthe output level and do not constrain the reasoning process, leaving models\nexposed to implicit risks. In this paper, we propose SaFeR-VLM, a\nsafety-aligned reinforcement learning framework that embeds safety directly\ninto multimodal reasoning. The framework integrates four components: (I)\nQI-Safe-10K, a curated dataset emphasizing safety-critical and\nreasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations\nundergo reflection and correction instead of being discarded; (III) structured\nreward modeling with multi-dimensional weighted criteria and explicit penalties\nfor hallucinations and contradictions; and (IV) GRPO optimization, which\nreinforces both safe and corrected trajectories. This unified design shifts\nsafety from a passive safeguard to an active driver of reasoning, enabling\nscalable and generalizable safety-aware reasoning. SaFeR-VLM further\ndemonstrates robustness against both explicit and implicit risks, supporting\ndynamic and interpretable safety decisions beyond surface-level filtering.\nSaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and\nhelpfulness across six benchmarks, surpassing both same-scale and $>10\\times$\nlarger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.\nRemarkably, SaFeR-VLM-7B benefits from its increased scale to surpass\nGPT-5-mini and Gemini-2.5-Flash by \\num{6.47} and \\num{16.76} points\nrespectively on safety metrics, achieving this improvement without any\ndegradation in helpfulness performance. Our codes are available at\nhttps://github.com/HarveyYi/SaFeR-VLM.",
        "url": "http://arxiv.org/abs/2510.06871v1",
        "published_date": "2025-10-08T10:39:12+00:00",
        "updated_date": "2025-10-08T10:39:12+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Huahui Yi",
            "Kun Wang",
            "Qiankun Li",
            "Miao Yu",
            "Liang Lin",
            "Gongli Xi",
            "Hao Wu",
            "Xuming Hu",
            "Kang Li",
            "Yang Liu"
        ],
        "tldr": "The paper introduces SaFeR-VLM, a safety-aligned reinforcement learning framework for multimodal models that improves safety without sacrificing helpfulness, outperforming larger models on safety benchmarks.",
        "tldr_zh": "该论文介绍了SaFeR-VLM，一个用于多模态模型的安全对齐强化学习框架，它在不牺牲有用性的前提下提高了安全性，并在安全基准测试中优于更大的模型。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
        "summary": "Existing methods for extracting reward signals in Reinforcement Learning\ntypically rely on labeled data and dedicated training splits, a setup that\ncontrasts with how humans learn directly from their environment. In this work,\nwe propose TTRV to enhance vision language understanding by adapting the model\non the fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework\nby designing rewards based on the frequency of the base model's output, while\ninferring on each test sample multiple times. Further, we also propose to\ncontrol the diversity of the model's output by simultaneously rewarding the\nmodel for obtaining low entropy of the output empirical distribution. Our\napproach delivers consistent gains across both object recognition and visual\nquestion answering (VQA), with improvements of up to 52.4% and 29.8%,\nrespectively, and average boosts of 24.6% and 10.0% across 16\ndatasets.Remarkably, on image recognition, TTRV applied to InternVL 8B\nsurpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining\nhighly competitive on VQA, demonstrating that test-time reinforcement learning\ncan match or exceed the strongest proprietary models. Finally, we find many\ninteresting properties of test-time RL for VLMs: for example, even in extremely\ndata-constrained scenarios, where adaptation is performed on a single randomly\nchosen unlabeled test example, TTRV still yields non-trivial improvements of up\nto 5.5% in recognition tasks.",
        "url": "http://arxiv.org/abs/2510.06783v1",
        "published_date": "2025-10-08T09:10:31+00:00",
        "updated_date": "2025-10-08T09:10:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Akshit Singh",
            "Shyam Marjit",
            "Wei Lin",
            "Paul Gavrikov",
            "Serena Yeung-Levy",
            "Hilde Kuehne",
            "Rogerio Feris",
            "Sivan Doveh",
            "James Glass",
            "M. Jehanzeb Mirza"
        ],
        "tldr": "The paper introduces TTRV, a test-time reinforcement learning method for vision language models that enhances performance without labeled data by adapting the model during inference, achieving state-of-the-art results on object recognition and VQA tasks.",
        "tldr_zh": "该论文介绍了TTRV，一种用于视觉语言模型的测试时强化学习方法，通过在推理过程中调整模型，无需标注数据即可提高性能，并在对象识别和VQA任务上实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
        "summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios.",
        "url": "http://arxiv.org/abs/2510.07134v1",
        "published_date": "2025-10-08T15:29:17+00:00",
        "updated_date": "2025-10-08T15:29:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiahang Liu",
            "Yunpeng Qi",
            "Jiazhao Zhang",
            "Minghan Li",
            "Shaoan Wang",
            "Kui Wu",
            "Hanjing Ye",
            "Hong Zhang",
            "Zhibo Chen",
            "Fangwei Zhong",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "tldr": "TrackVLA++ introduces a novel Vision-Language-Action model that improves embodied visual tracking using spatial reasoning and temporal memory modules, achieving state-of-the-art performance and strong zero-shot generalization.",
        "tldr_zh": "TrackVLA++ 提出了一种新颖的视觉-语言-动作模型，通过空间推理和时间记忆模块来改进具身视觉跟踪，实现了最先进的性能和强大的零样本泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Addressing the ID-Matching Challenge in Long Video Captioning",
        "summary": "Generating captions for long and complex videos is both critical and\nchallenging, with significant implications for the growing fields of\ntext-to-video generation and multi-modal understanding. One key challenge in\nlong video captioning is accurately recognizing the same individuals who appear\nin different frames, which we refer to as the ID-Matching problem. Few prior\nworks have focused on this important issue. Those that have, usually suffer\nfrom limited generalization and depend on point-wise matching, which limits\ntheir overall effectiveness. In this paper, unlike previous approaches, we\nbuild upon LVLMs to leverage their powerful priors. We aim to unlock the\ninherent ID-Matching capabilities within LVLMs themselves to enhance the\nID-Matching performance of captions. Specifically, we first introduce a new\nbenchmark for assessing the ID-Matching capabilities of video captions. Using\nthis benchmark, we investigate LVLMs containing GPT-4o, revealing key insights\nthat the performance of ID-Matching can be improved through two methods: 1)\nenhancing the usage of image information and 2) increasing the quantity of\ninformation of individual descriptions. Based on these insights, we propose a\nnovel video captioning method called Recognizing Identities for Captioning\nEffectively (RICE). Extensive experiments including assessments of caption\nquality and ID-Matching performance, demonstrate the superiority of our\napproach. Notably, when implemented on GPT-4o, our RICE improves the precision\nof ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15%\nto 80% compared to baseline. RICE makes it possible to continuously track\ndifferent individuals in the captions of long videos.",
        "url": "http://arxiv.org/abs/2510.06973v1",
        "published_date": "2025-10-08T12:59:21+00:00",
        "updated_date": "2025-10-08T12:59:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhantao Yang",
            "Huangji Wang",
            "Ruili Feng",
            "Han Zhang",
            "Yuting Hu",
            "Shangwen Zhu",
            "Junyan Li",
            "Yu Liu",
            "Fan Cheng"
        ],
        "tldr": "This paper addresses the ID-Matching problem in long video captioning by leveraging LVLMs, proposing a new benchmark, and introducing a novel method (RICE) that significantly improves ID-Matching precision and recall, particularly when implemented on GPT-4o.",
        "tldr_zh": "本文解决了长视频字幕中的ID匹配问题，通过利用LVLMs，提出了一个新的基准测试，并介绍了一种新方法（RICE），该方法显著提高了ID匹配的精确率和召回率，尤其是在GPT-4o上实现时。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting",
        "summary": "We present a quantitative evaluation to understand the effect of zero-shot\nlarge-language model (LLMs) and prompting uses on chart reading tasks. We asked\nLLMs to answer 107 visualization questions to compare inference accuracies\nbetween the agentic GPT-5 and multimodal GPT-4V, for difficult image instances,\nwhere GPT-4V failed to produce correct answers. Our results show that model\narchitecture dominates the inference accuracy: GPT5 largely improved accuracy,\nwhile prompt variants yielded only small effects. Pre-registration of this work\nis available here:\nhttps://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3; the Google\nDrive materials are\nhere:https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view.",
        "url": "http://arxiv.org/abs/2510.06782v1",
        "published_date": "2025-10-08T09:09:29+00:00",
        "updated_date": "2025-10-08T09:09:29+00:00",
        "categories": [
            "cs.HC",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Kaichun Yang",
            "Jian Chen"
        ],
        "tldr": "This paper quantitatively evaluates GPT-5's ability to correct GPT-4V's chart reading errors, finding that the model architecture (GPT-5) significantly improves accuracy compared to prompt engineering. The study focuses on difficult image instances where GPT-4V initially failed.",
        "tldr_zh": "本文定量评估了GPT-5纠正GPT-4V图表阅读错误的能力，发现模型架构（GPT-5）相比提示工程显著提高了准确性。该研究重点关注GPT-4V最初失败的困难图像实例。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
        "summary": "Recent advancements in instruction-based image editing and subject-driven\ngeneration have garnered significant attention, yet both tasks still face\nlimitations in meeting practical user needs. Instruction-based editing relies\nsolely on language instructions, which often fail to capture specific editing\ndetails, making reference images necessary. Meanwhile, subject-driven\ngeneration is limited to combining concrete objects or people, overlooking\nbroader, abstract concepts. To address these challenges, we propose two novel\ntasks: multimodal instruction-based editing and generation. These tasks support\nboth text and image instructions and extend the scope to include both concrete\nand abstract concepts, greatly enhancing their practical applications. We\nintroduce DreamOmni2, tackling two primary challenges: data creation and model\nframework design. Our data synthesis pipeline consists of three steps: (1)\nusing a feature mixing method to create extraction data for both abstract and\nconcrete concepts, (2) generating multimodal instruction-based editing training\ndata using the editing and extraction models, and (3) further applying the\nextraction model to create training data for multimodal instruction-based\nediting. For the framework, to handle multi-image input, we propose an index\nencoding and position encoding shift scheme, which helps the model distinguish\nimages and avoid pixel confusion. Additionally, we introduce joint training\nwith the VLM and our generation/editing model to better process complex\ninstructions. In addition, we have proposed comprehensive benchmarks for these\ntwo new tasks to drive their development. Experiments show that DreamOmni2 has\nachieved impressive results. Models and codes will be released.",
        "url": "http://arxiv.org/abs/2510.06679v1",
        "published_date": "2025-10-08T06:07:14+00:00",
        "updated_date": "2025-10-08T06:07:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Xia",
            "Bohao Peng",
            "Yuechen Zhang",
            "Junjia Huang",
            "Jiyang Liu",
            "Jingyao Li",
            "Haoru Tan",
            "Sitong Wu",
            "Chengyao Wang",
            "Yitong Wang",
            "Xinglong Wu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "tldr": "The paper introduces two new multimodal instruction-based editing and generation tasks, proposes a data synthesis pipeline and model framework (DreamOmni2) to tackle these tasks, and provides benchmarks for evaluation, showing impressive results.",
        "tldr_zh": "本文提出了两个新的多模态指令驱动的编辑和生成任务，提出了一个数据合成流程和模型框架（DreamOmni2）来解决这些任务，并提供了评估基准，展示了令人印象深刻的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VUGEN: Visual Understanding priors for GENeration",
        "summary": "Recent advances in Vision-Language Models (VLMs) have enabled unified\nunderstanding across text and images, yet equipping these models with robust\nimage generation capabilities remains challenging. Existing approaches often\nrely on reconstruction-oriented autoencoders or complex bridging mechanisms,\nleading to misalignment between understanding and generation representations,\nor architectural complexity. In this work, we propose VUGEN, a novel framework\nthat explicitly leverages VLM's pretrained visual understanding priors for\nefficient and high-quality image generation. Our approach first transforms the\nhigh-dimensional latent space of the VLM's native vision encoder into a\nlower-dimensional, tractable distribution that maximally preserves visual\ninformation. The VLM is then trained to sample within this reduced latent\nspace, ensuring alignment with its visual understanding capabilities. Finally,\na dedicated pixel decoder maps these generated latents back to the image space.\nWe find that a VAE-free pixel diffusion decoder to be on par or better than\ncommonly used complex latent diffusion decoders that internally rely on VAE\nlatents. Extensive experiments demonstrate that VUGEN achieves superior image\ngeneration performance, improving DPG Bench from 71.17 to 74.32 and FID from\n11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding\ncapabilities.",
        "url": "http://arxiv.org/abs/2510.06529v1",
        "published_date": "2025-10-08T00:04:47+00:00",
        "updated_date": "2025-10-08T00:04:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyi Chen",
            "Théophane Vallaeys",
            "Maha Elbayad",
            "John Nguyen",
            "Jakob Verbeek"
        ],
        "tldr": "VUGEN is a novel framework that leverages VLMs' pretrained visual understanding priors for efficient and high-quality image generation by mapping the VLM's latent space to a lower-dimensional distribution and using a VAE-free pixel diffusion decoder. It improves image generation performance while preserving the VLM's original understanding capabilities.",
        "tldr_zh": "VUGEN是一个新颖的框架，它利用VLMs的预训练视觉理解先验知识进行高效和高质量的图像生成，通过将VLM的潜在空间映射到低维分布并使用无VAE像素扩散解码器。它在保留VLM原始理解能力的同时，提高了图像生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
        "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.",
        "url": "http://arxiv.org/abs/2510.07313v1",
        "published_date": "2025-10-08T17:59:08+00:00",
        "updated_date": "2025-10-08T17:59:08+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zezhong Qian",
            "Xiaowei Chi",
            "Yuming Li",
            "Shizun Wang",
            "Zhiyuan Qin",
            "Xiaozhu Ju",
            "Sirui Han",
            "Shanghang Zhang"
        ],
        "tldr": "WristWorld generates wrist-view videos from anchor views using a 4D world model, improving spatial consistency and VLA performance in robotic manipulation tasks.",
        "tldr_zh": "WristWorld使用4D世界模型从锚视图生成手腕视图视频，从而提高机器人操作任务中的空间一致性和VLA性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation",
        "summary": "Text-to-image synthesis has made remarkable progress, yet accurately\ninterpreting complex and lengthy prompts remains challenging, often resulting\nin semantic inconsistencies and missing details. Existing solutions, such as\nfine-tuning, are model-specific and require training, while prior automatic\nprompt optimization (APO) approaches typically lack systematic error analysis\nand refinement strategies, resulting in limited reliability and effectiveness.\nMeanwhile, test-time scaling methods operate on fixed prompts and on noise or\nsample numbers, limiting their interpretability and adaptability. To solve\nthese, we introduce a flexible and efficient test-time prompt optimization\nstrategy that operates directly on the input text. We propose a plug-and-play\nmulti-agent system called GenPilot, integrating error analysis,\nclustering-based adaptive exploration, fine-grained verification, and a memory\nmodule for iterative optimization. Our approach is model-agnostic,\ninterpretable, and well-suited for handling long and complex prompts.\nSimultaneously, we summarize the common patterns of errors and the refinement\nstrategy, offering more experience and encouraging further exploration.\nExperiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%\ndemonstrate the strong capability of our methods in enhancing the text and\nimage consistency and structural coherence of generated images, revealing the\neffectiveness of our test-time prompt optimization strategy. The code is\navailable at https://github.com/27yw/GenPilot.",
        "url": "http://arxiv.org/abs/2510.07217v1",
        "published_date": "2025-10-08T16:51:52+00:00",
        "updated_date": "2025-10-08T16:51:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wen Ye",
            "Zhaocheng Liu",
            "Yuwei Gui",
            "Tingyu Yuan",
            "Yunyue Su",
            "Bowen Fang",
            "Chaoyang Zhao",
            "Qiang Liu",
            "Liang Wang"
        ],
        "tldr": "The paper introduces GenPilot, a model-agnostic, test-time prompt optimization multi-agent system that iteratively refines prompts for text-to-image generation, improving consistency and coherence.",
        "tldr_zh": "该论文介绍了一种名为GenPilot的模型无关的测试时提示优化多智能体系统，该系统迭代地改进文本到图像生成的提示，从而提高一致性和连贯性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models",
        "summary": "Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable\npotential thanks to large-scale pretraining, achieving strong zero-shot\nperformance on various tasks. However, their ability to generalize in low-data\nregimes, such as few-shot learning, remains insufficiently explored. In this\nwork, we present the first structured benchmark for evaluating few-shot\nadaptation methods on RSVLMs. We conduct comprehensive experiments across ten\nremote sensing scene classification datasets, applying five widely used\nfew-shot adaptation strategies to three state-of-the-art RSVLMs with varying\nbackbones. Our findings reveal that models with similar zero-shot performance\ncan exhibit markedly different behavior under few-shot adaptation, with some\nRSVLMs being inherently more amenable to such adaptation than others. The\nvariability of performance and the absence of a clear winner among existing\nmethods highlight the need for the development of more robust methods for\nfew-shot adaptation tailored to RS. To facilitate future research, we provide a\nreproducible benchmarking framework and open-source code to systematically\nevaluate RSVLMs under few-shot conditions. The source code is publicly\navailable on Github: https://github.com/elkhouryk/fewshot_RSVLMs",
        "url": "http://arxiv.org/abs/2510.07135v1",
        "published_date": "2025-10-08T15:29:48+00:00",
        "updated_date": "2025-10-08T15:29:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Karim El Khoury",
            "Maxime Zanella",
            "Christophe De Vleeschouwer",
            "Benoit Macq"
        ],
        "tldr": "This paper introduces a benchmark for evaluating few-shot adaptation methods on Remote Sensing Vision-Language Models (RSVLMs), finding that models with similar zero-shot performance vary significantly in few-shot scenarios, highlighting the need for specialized adaptation techniques and providing a reproducible framework.",
        "tldr_zh": "本文介绍了一个用于评估遥感视觉语言模型(RSVLMs)上的少样本自适应方法的基准，发现具有相似零样本性能的模型在少样本场景下表现差异很大，强调了需要专门的自适应技术，并提供了一个可重复的框架。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Enhancing Concept Localization in CLIP-based Concept Bottleneck Models",
        "summary": "This paper addresses explainable AI (XAI) through the lens of Concept\nBottleneck Models (CBMs) that do not require explicit concept annotations,\nrelying instead on concepts extracted using CLIP in a zero-shot manner. We show\nthat CLIP, which is central in these techniques, is prone to concept\nhallucination, incorrectly predicting the presence or absence of concepts\nwithin an image in scenarios used in numerous CBMs, hence undermining the\nfaithfulness of explanations. To mitigate this issue, we introduce Concept\nHallucination Inhibition via Localized Interpretability (CHILI), a technique\nthat disentangles image embeddings and localizes pixels corresponding to target\nconcepts. Furthermore, our approach supports the generation of saliency-based\nexplanations that are more interpretable.",
        "url": "http://arxiv.org/abs/2510.07115v1",
        "published_date": "2025-10-08T15:07:16+00:00",
        "updated_date": "2025-10-08T15:07:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rémi Kazmierczak",
            "Steve Azzolin",
            "Eloïse Berthier",
            "Goran Frehse",
            "Gianni Franchi"
        ],
        "tldr": "The paper identifies and mitigates concept hallucination in CLIP-based Concept Bottleneck Models (CBMs) using a novel technique called CHILI, enhancing the faithfulness of explanations.",
        "tldr_zh": "本文提出了一种名为CHILI的新技术，用于识别和缓解基于CLIP的概念瓶颈模型(CBMs)中的概念幻觉，从而提高解释的忠实性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
        "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io .",
        "url": "http://arxiv.org/abs/2510.07077v1",
        "published_date": "2025-10-08T14:38:25+00:00",
        "updated_date": "2025-10-08T14:38:25+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kento Kawaharazuka",
            "Jihoon Oh",
            "Jun Yamada",
            "Ingmar Posner",
            "Yuke Zhu"
        ],
        "tldr": "This paper provides a comprehensive review of Vision-Language-Action (VLA) models for robotics, covering their architecture, training, datasets, and hardware considerations to facilitate real-world deployment.",
        "tldr_zh": "本文全面回顾了用于机器人的视觉-语言-动作（VLA）模型，涵盖了它们的架构、训练、数据集和硬件考量，以促进实际部署。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking",
        "summary": "Multimodal retrieval still leans on embedding-based models like CLIP for fast\nvector search over pre-computed image embeddings. Yet, unlike text retrieval,\nwhere joint-encoder rerankers are standard, comparable vision--language\nrerankers are largely absent. We find that seminal joint encoders such as BLIP\nare severely bottlenecked by an expensive visual feature-extraction stage,\npreventing practical deployment at scale. Motivated by this bottleneck, we\nintroduce EDJE, an Efficient Discriminative Joint Encoder that precomputes\nvision tokens offline and compresses them via a lightweight attention-based\nadapter, so online inference runs only a compact joint encoder over a small set\nof visual tokens plus the text. EDJE preserves strong retrieval performance\nwhile drastically reducing storage and online compute, enabling high-throughput\ninference. Specifically, EDJE processes 50k image--text pairs/second while\nrequiring 49kB of disk storage per image, matching prior art on Flickr\n(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints\nwill be made publicly available shortly.",
        "url": "http://arxiv.org/abs/2510.06820v1",
        "published_date": "2025-10-08T09:46:09+00:00",
        "updated_date": "2025-10-08T09:46:09+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mitchell Keren Taraday",
            "Shahaf Wagner",
            "Chaim Baskin"
        ],
        "tldr": "The paper introduces EDJE, an efficient vision-language reranker that addresses the computational bottleneck of existing joint encoders by precomputing and compressing visual features, enabling high-throughput inference with reduced storage.",
        "tldr_zh": "本文介绍了一种高效的视觉-语言重排序器 EDJE，通过预计算和压缩视觉特征来解决现有联合编码器的计算瓶颈，从而以减少的存储实现高吞吐量推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering",
        "summary": "Knowledge-based Visual Question Answering (KVQA) requires models to ground\nentities in images and reason over factual knowledge. We study its\nimplicit-knowledge variant, IK-KVQA, where a multimodal large language model\n(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs\nlack explicit reasoning supervision and produce inconsistent justifications,\nand generalize poorly after standard supervised fine-tuning (SFT). We present\nStaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises\nstructured traces - dual symbolic relation paths plus path-grounded\nnatural-language explanations - so that reasoning becomes transparent and\nverifiable. With one open-source MLLM, StaR-KVQA constructs and selects\npath-grounded reasoning traces to form a trace-enriched dataset, then\nfine-tunes via structured self-distillation to align generation with\nsupervision; no external retrievers, verifiers, or curated knowledge bases\n(KBs) are used, traces are built offline, and inference is a single\nautoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and\ninterpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over\nthe strongest baseline while exhibiting robust cross-domain generalization.",
        "url": "http://arxiv.org/abs/2510.06638v1",
        "published_date": "2025-10-08T04:37:53+00:00",
        "updated_date": "2025-10-08T04:37:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhihao Wen",
            "Wenkang Wei",
            "Yuan Fang",
            "Xingtong Yu",
            "Hui Zhang",
            "Weicheng Zhu",
            "Xin Zhang"
        ],
        "tldr": "The paper introduces StaR-KVQA, a method for improving implicit-knowledge visual question answering in MLLMs by supervising structured reasoning traces, achieving better accuracy and interpretability without external knowledge retrieval.",
        "tldr_zh": "该论文介绍了StaR-KVQA，一种通过监督结构化推理轨迹来改进MLLM中隐式知识视觉问答的方法，无需外部知识检索即可实现更高的准确性和可解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers",
        "summary": "This paper presents Pixel-Perfect Depth, a monocular depth estimation model\nbased on pixel-space diffusion generation that produces high-quality,\nflying-pixel-free point clouds from estimated depth maps. Current generative\ndepth estimation models fine-tune Stable Diffusion and achieve impressive\nperformance. However, they require a VAE to compress depth maps into latent\nspace, which inevitably introduces \\textit{flying pixels} at edges and details.\nOur model addresses this challenge by directly performing diffusion generation\nin the pixel space, avoiding VAE-induced artifacts. To overcome the high\ncomplexity associated with pixel-space generation, we introduce two novel\ndesigns: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which\nincorporate semantic representations from vision foundation models into DiT to\nprompt the diffusion process, thereby preserving global semantic consistency\nwhile enhancing fine-grained visual details; and 2) Cascade DiT Design that\nprogressively increases the number of tokens to further enhance efficiency and\naccuracy. Our model achieves the best performance among all published\ngenerative models across five benchmarks, and significantly outperforms all\nother models in edge-aware point cloud evaluation.",
        "url": "http://arxiv.org/abs/2510.07316v1",
        "published_date": "2025-10-08T17:59:33+00:00",
        "updated_date": "2025-10-08T17:59:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gangwei Xu",
            "Haotong Lin",
            "Hongcheng Luo",
            "Xianqi Wang",
            "Jingfeng Yao",
            "Lianghui Zhu",
            "Yuechuan Pu",
            "Cheng Chi",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Sida Peng",
            "Xin Yang"
        ],
        "tldr": "This paper introduces Pixel-Perfect Depth, a monocular depth estimation model using Semantics-Prompted Diffusion Transformers in pixel space, avoiding VAE artifacts and achieving state-of-the-art performance on multiple benchmarks.",
        "tldr_zh": "本文介绍了Pixel-Perfect Depth，一种使用像素空间中语义提示扩散变换器的单眼深度估计模型，避免了VAE伪影，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]