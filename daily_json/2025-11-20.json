[
    {
        "title": "AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning",
        "summary": "With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.",
        "url": "http://arxiv.org/abs/2511.15578v1",
        "published_date": "2025-11-19T16:09:38+00:00",
        "updated_date": "2025-11-19T16:09:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Urjitkumar Patel",
            "Fang-Chun Yeh",
            "Chinmay Gondhalekar"
        ],
        "tldr": "The paper introduces AVATAAR, a modular framework that combines global and local video context with a Pre Retrieval Thinking Agent and a Rethink Module for improved long-form Video Question Answering, demonstrating significant gains on the CinePile benchmark.",
        "tldr_zh": "该论文介绍了AVATAAR，一个模块化框架，它结合了全局和局部视频上下文，以及一个预检索思考代理和一个反思模块，以改进长视频问答，并在CinePile基准测试中展示了显著的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome",
        "summary": "Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\\% in the gene-expression prediction task and avg. 26.93\\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.",
        "url": "http://arxiv.org/abs/2511.15464v1",
        "published_date": "2025-11-19T14:22:23+00:00",
        "updated_date": "2025-11-19T14:22:23+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dabin Jeong",
            "Amirhossein Vahidi",
            "Ciro Ramírez-Suástegui",
            "Marie Moullet",
            "Kevin Ly",
            "Mohammad Vali Sanian",
            "Sebastian Birk",
            "Yinshui Chang",
            "Adam Boxall",
            "Daniyal Jafree",
            "Lloyd Steele",
            "Vijaya Baskar MS",
            "Muzlifah Haniffa",
            "Mohammad Lotfollahi"
        ],
        "tldr": "The paper introduces Sigmma, a multi-modal contrastive learning framework that aligns histopathology images and spatial transcriptomic data at multiple scales using graph representations to capture cell-cell interactions, demonstrating improved performance in gene expression prediction and cross-modal retrieval.",
        "tldr_zh": "该论文介绍了 Sigmma，一个多模态对比学习框架，它使用图表示在多个尺度上对齐组织病理学图像和空间转录组数据以捕获细胞间相互作用，并在基因表达预测和跨模态检索方面表现出改进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models",
        "summary": "Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.",
        "url": "http://arxiv.org/abs/2511.15411v1",
        "published_date": "2025-11-19T13:08:25+00:00",
        "updated_date": "2025-11-19T13:08:25+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Wenlun Zhang",
            "Yunshan Zhong",
            "Zihao Ding",
            "Xinyu Li",
            "Kentaro Yoshioka"
        ],
        "tldr": "This paper introduces D4C, a data-free quantization framework tailored for CLIP models, addressing performance degradation issues caused by insufficient semantic content and low intra-image diversity in synthesized samples. It shows significant performance improvements on various benchmarks.",
        "tldr_zh": "本文介绍了D4C，一个为CLIP模型量身定制的无数据量化框架，解决了因合成样本中语义内容不足和图像内多样性低而导致的性能下降问题。该方法在各种基准测试中表现出显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IPR-1: Interactive Physical Reasoner",
        "summary": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.",
        "url": "http://arxiv.org/abs/2511.15407v1",
        "published_date": "2025-11-19T13:04:44+00:00",
        "updated_date": "2025-11-19T13:04:44+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Mingyu Zhang",
            "Lifeng Zhuo",
            "Tianxi Tan",
            "Guocan Xie",
            "Xian Nie",
            "Yan Li",
            "Renjie Zhao",
            "Zizhu He",
            "Ziyu Wang",
            "Jiting Cai",
            "Yong-Lu Li"
        ],
        "tldr": "The paper introduces IPR, an Interactive Physical Reasoner, that learns physical and causal reasoning through interaction in a diverse game environment, achieving performance comparable to GPT-5 and improving with more experience.",
        "tldr_zh": "该论文介绍了IPR，一种交互式物理推理器，它通过在不同的游戏环境中交互来学习物理和因果推理，实现了与GPT-5相当的性能，并随着经验的积累而不断提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration",
        "summary": "Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.",
        "url": "http://arxiv.org/abs/2511.15351v1",
        "published_date": "2025-11-19T11:22:13+00:00",
        "updated_date": "2025-11-19T11:22:13+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yifu Guo",
            "Zishan Xu",
            "Zhiyuan Yao",
            "Yuquan Lu",
            "Jiaye Lin",
            "Sen Hu",
            "Zhenheng Tang",
            "Yingchao Li",
            "Huacan Wang",
            "Ronghao Chen"
        ],
        "tldr": "The paper introduces Octopus, a multimodal agentic reasoning framework with six core capabilities, designed to address the limitations of existing models in dynamically adapting to real-world tasks, and presents a benchmark to evaluate it.",
        "tldr_zh": "该论文介绍了Octopus，一个具有六种核心能力的多模态Agentic推理框架，旨在解决现有模型在动态适应实际任务中的局限性，并提出了一个用于评估它的基准。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception",
        "summary": "In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.",
        "url": "http://arxiv.org/abs/2511.15279v1",
        "published_date": "2025-11-19T09:42:08+00:00",
        "updated_date": "2025-11-19T09:42:08+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jiashu Yang",
            "Yifan Han",
            "Yucheng Xie",
            "Ning Guo",
            "Wenzhao Lian"
        ],
        "tldr": "The paper introduces EyeVLA, a robotic eyeball system that integrates vision-language models with active perception, enabling robots to acquire detailed visual information across wide spatial extents based on instructions.",
        "tldr_zh": "该论文介绍了一种名为EyeVLA的机器人眼球系统，该系统集成了视觉-语言模型和主动感知，使机器人能够根据指令在广泛的空间范围内获取详细的视觉信息。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning",
        "summary": "We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.",
        "url": "http://arxiv.org/abs/2511.15242v1",
        "published_date": "2025-11-19T08:55:23+00:00",
        "updated_date": "2025-11-19T08:55:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Shen",
            "Jiahe Qian",
            "Zhangtianyi Chen",
            "Yuanhao He",
            "Juexiao Zhou"
        ],
        "tldr": "SkinGPT-R1 is a dermatology-focused vision language model that uses adapter-only dual distillation and a novel dermatology chain-of-thought dataset (DermCoT) to achieve state-of-the-art results in dermatology reasoning and classification tasks.",
        "tldr_zh": "SkinGPT-R1是一个专注于皮肤科的视觉语言模型，它使用仅适配器的双重蒸馏和新的皮肤科思维链数据集(DermCoT)，在皮肤科推理和分类任务中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset",
        "summary": "The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.",
        "url": "http://arxiv.org/abs/2511.15186v1",
        "published_date": "2025-11-19T07:17:19+00:00",
        "updated_date": "2025-11-19T07:17:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Geon Choi",
            "Hangyul Yoon",
            "Hyunju Shin",
            "Hyunki Park",
            "Sang Hoon Seo",
            "Eunho Yang",
            "Edward Choi"
        ],
        "tldr": "The paper introduces MIMIC-ILS, a large-scale instruction-answer dataset for CXR lesion segmentation, and ROSALIA, a vision-language model fine-tuned on it for instruction-guided segmentation and explanation of chest X-ray lesions.",
        "tldr_zh": "该论文介绍了MIMIC-ILS，一个用于胸部X光病灶分割的大规模指令-答案数据集，以及ROSALIA，一个基于此数据集微调的视觉-语言模型，用于胸部X光病灶的指令引导分割和解释。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance",
        "summary": "Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.",
        "url": "http://arxiv.org/abs/2511.15164v1",
        "published_date": "2025-11-19T06:29:15+00:00",
        "updated_date": "2025-11-19T06:29:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Songze Li",
            "Mingyu Gao",
            "Tonghua Su",
            "Xu-Yao Zhang",
            "Zhongjie Wang"
        ],
        "tldr": "This paper introduces a novel method, Dynamic Gradient Guidance, to mitigate catastrophic forgetting in multimodal continual instruction tuning by approximating missing gradients from old tasks during new task learning, achieving state-of-the-art performance without model expansion.",
        "tldr_zh": "本文提出了一种名为动态梯度引导的新方法，通过近似新任务学习期间旧任务缺失的梯度，来缓解多模态持续指令调整中的灾难性遗忘问题，无需模型扩展即可实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer",
        "summary": "Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.",
        "url": "http://arxiv.org/abs/2511.15090v1",
        "published_date": "2025-11-19T04:03:54+00:00",
        "updated_date": "2025-11-19T04:03:54+00:00",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wenhan Yu",
            "Wang Chen",
            "Guanqiang Qi",
            "Weikang Li",
            "Yang Li",
            "Lei Sha",
            "Deguo Xia",
            "Jizhou Huang"
        ],
        "tldr": "The paper introduces BBox DocVQA, a large-scale bounding box grounded dataset for Document Visual Question Answering, aimed at improving spatial reasoning and interpretability of VLMs, demonstrating its effectiveness through benchmarking and fine-tuning experiments.",
        "tldr_zh": "该论文介绍了BBox DocVQA，一个大规模的基于边界框的文档视觉问答数据集，旨在提高视觉语言模型的空间推理能力和可解释性。通过基准测试和微调实验，验证了该数据集的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
        "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
        "url": "http://arxiv.org/abs/2511.14993v1",
        "published_date": "2025-11-19T00:23:22+00:00",
        "updated_date": "2025-11-19T00:23:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Vladimir Arkhipkin",
            "Vladimir Korviakov",
            "Nikolai Gerasimenko",
            "Denis Parkhomenko",
            "Viacheslav Vasilev",
            "Alexey Letunovskiy",
            "Maria Kovaleva",
            "Nikolai Vaulin",
            "Ivan Kirillov",
            "Lev Novitskiy",
            "Denis Koposov",
            "Nikita Kiselev",
            "Alexander Varlamov",
            "Dmitrii Mikhailov",
            "Vladimir Polovnikov",
            "Andrey Shutkin",
            "Ilya Vasiliev",
            "Julia Agafonova",
            "Anastasiia Kargapoltseva",
            "Anna Dmitrienko",
            "Anastasia Maltseva",
            "Anna Averchenkova",
            "Olga Kim",
            "Tatiana Nikulina",
            "Denis Dimitrov"
        ],
        "tldr": "Kandinsky 5.0 introduces a family of foundation models for high-resolution image and video generation, featuring various model sizes and incorporating data curation, training optimizations, and quality enhancement techniques, with open-source code and checkpoints released for the research community.",
        "tldr_zh": "Kandinsky 5.0 介绍了一系列用于高分辨率图像和视频生成的基础模型，具有不同的模型尺寸，并结合了数据管理、训练优化和质量增强技术，同时为研究社区发布了开源代码和检查点。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis",
        "summary": "The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.\n  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.",
        "url": "http://arxiv.org/abs/2511.14900v1",
        "published_date": "2025-11-18T20:38:36+00:00",
        "updated_date": "2025-11-18T20:38:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Zehao Liu",
            "Wejieying Ren",
            "Jipeng Zhang",
            "Tianxiang Zhao",
            "Jingxi Zhu",
            "Xiaoting Li",
            "Vasant G. Honavar"
        ],
        "tldr": "Skin-R1 is a novel dermatological VLM that combines textbook-based reasoning with reinforcement learning to improve diagnostic accuracy by addressing data heterogeneity, lack of grounded rationales, and limited scalability.",
        "tldr_zh": "Skin-R1 是一种新型皮肤科视觉语言模型，结合了基于教科书的推理和强化学习，通过解决数据异构性、缺乏可靠的推理基础以及有限的可扩展性等问题，来提高诊断准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning",
        "summary": "We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.",
        "url": "http://arxiv.org/abs/2511.14760v1",
        "published_date": "2025-11-18T18:59:30+00:00",
        "updated_date": "2025-11-18T18:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Tian",
            "Mingfei Gao",
            "Haiming Gang",
            "Jiasen Lu",
            "Zhe Gan",
            "Yinfei Yang",
            "Zuxuan Wu",
            "Afshin Dehghan"
        ],
        "tldr": "UniGen-1.5 is a unified MLLM that enhances image understanding, generation, and editing through a unified RL strategy with shared reward models and an edit instruction alignment stage, achieving state-of-the-art performance.",
        "tldr_zh": "UniGen-1.5是一个统一的多模态大型语言模型，通过统一的强化学习策略，利用共享奖励模型和编辑指令对齐阶段，增强了图像理解、生成和编辑能力，达到了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision Large Language Models Are Good Noise Handlers in Engagement Analysis",
        "summary": "Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.",
        "url": "http://arxiv.org/abs/2511.14749v1",
        "published_date": "2025-11-18T18:50:26+00:00",
        "updated_date": "2025-11-18T18:50:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexander Vedernikov",
            "Puneet Kumar",
            "Haoyu Chen",
            "Tapio Seppänen",
            "Xiaobai Li"
        ],
        "tldr": "This paper uses Vision Large Language Models (VLMs) to refine noisy labels in video engagement datasets by extracting behavioral cues and employing a curriculum learning strategy with soft label refinement, achieving state-of-the-art results on engagement benchmarks.",
        "tldr_zh": "本文利用视觉大型语言模型（VLMs），通过提取行为线索并结合软标签细化的课程学习策略，来改进视频互动数据集中嘈杂的标签，并在互动基准测试中取得了最先进的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Evaluation of Russian-language Architectures",
        "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
        "url": "http://arxiv.org/abs/2511.15552v1",
        "published_date": "2025-11-19T15:43:53+00:00",
        "updated_date": "2025-11-19T15:43:53+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Artem Chervyakov",
            "Ulyana Isaeva",
            "Anton Emelyanov",
            "Artem Safin",
            "Maria Tikhonova",
            "Alexander Kharitonov",
            "Yulia Lyakh",
            "Petr Surovtsev",
            "Denis Shevelev Vildan Saburov",
            "Vasily Konovalov",
            "Elisei Rykov",
            "Ivan Sviridov",
            "Amina Miftakhova",
            "Ilseyar Alimova",
            "Alexander Panchenko",
            "Alexander Kapitanov",
            "Alena Fenogenova"
        ],
        "tldr": "The paper introduces Mera Multi, a new open multimodal evaluation framework for Russian-language architectures, including 18 new datasets and tasks, along with baseline results and a methodology for preventing benchmark leakage.",
        "tldr_zh": "该论文介绍了一个新的俄语多模态评估框架Mera Multi，包括18个新的数据集和任务，以及基线结果和一个防止基准泄露的方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-Text Guided Few-Shot Semantic Segmentation",
        "summary": "Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.",
        "url": "http://arxiv.org/abs/2511.15515v1",
        "published_date": "2025-11-19T15:09:19+00:00",
        "updated_date": "2025-11-19T15:09:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiang Jiao",
            "Bin Yan",
            "Yi Yang",
            "Mengrui Shi",
            "Qiang Zhang"
        ],
        "tldr": "The paper introduces MTGNet, a novel dual-branch framework for few-shot semantic segmentation that leverages multiple textual prompts and cross-modal interaction to enhance segmentation performance, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "该论文介绍了MTGNet，一种用于少样本语义分割的新型双分支框架，该框架利用多个文本提示和跨模态交互来增强分割性能，并在标准基准测试中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models",
        "summary": "3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.",
        "url": "http://arxiv.org/abs/2511.15311v1",
        "published_date": "2025-11-19T10:22:22+00:00",
        "updated_date": "2025-11-19T10:22:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mehran Tamjidi",
            "Hamidreza Dastmalchi",
            "Mohammadreza Alimoradijazi",
            "Ali Cheraghian",
            "Aijun An",
            "Morteza Saberi"
        ],
        "tldr": "The paper introduces Uni-Adapter, a training-free test-time adaptation method for 3D Vision-Language Foundation Models that uses dynamic prototype learning and entropy-weighted aggregation to improve performance in noisy or out-of-distribution scenarios.",
        "tldr_zh": "该论文介绍了Uni-Adapter，一种针对3D视觉-语言基础模型的免训练测试时自适应方法，它使用动态原型学习和熵加权聚合来提高在嘈杂或分布外场景中的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language",
        "summary": "We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.",
        "url": "http://arxiv.org/abs/2511.15308v1",
        "published_date": "2025-11-19T10:19:45+00:00",
        "updated_date": "2025-11-19T10:19:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yan Xia",
            "Letian Shi",
            "Yilin Di",
            "Joao F. Henriques",
            "Daniel Cremers"
        ],
        "tldr": "The paper introduces Text2Loc++, a novel neural network for localizing 3D point cloud submaps from natural language descriptions, along with a new city-scale dataset and several techniques to improve performance and generalization.",
        "tldr_zh": "该论文介绍了Text2Loc++，一种用于从自然语言描述中定位3D点云子地图的新型神经网络，并提出了一个新的城市规模数据集和多种技术来提高性能和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Physics-Based Benchmarking Metrics for Multimodal Synthetic Images",
        "summary": "Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.",
        "url": "http://arxiv.org/abs/2511.15204v1",
        "published_date": "2025-11-19T07:52:20+00:00",
        "updated_date": "2025-11-19T07:52:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kishor Datta Gupta",
            "Marufa Kamal",
            "Md. Mahfuzur Rahman",
            "Fahad Rahman",
            "Mohd Ariful Haque",
            "Sunzida Siddique"
        ],
        "tldr": "This paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric that combines LLMs with reasoning, knowledge-based mapping and VLMs to improve semantic and structural accuracy in evaluating synthetic images.",
        "tldr_zh": "该论文提出了一种物理约束的多模态数据评估（PCMDE）指标，该指标结合了LLM与推理、基于知识的映射和VLM，以提高评估合成图像时的语义和结构准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation",
        "summary": "Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.",
        "url": "http://arxiv.org/abs/2511.15118v1",
        "published_date": "2025-11-19T04:41:43+00:00",
        "updated_date": "2025-11-19T04:41:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jin Wang",
            "Bingfeng Zhang",
            "Jian Pang",
            "Weifeng Liu",
            "Baodi Liu",
            "Honglong Chen"
        ],
        "tldr": "This paper proposes an Unbiased Semantic Decoding (USD) strategy for few-shot segmentation with SAM, leveraging CLIP to extract target information from both support and query sets. It enhances SAM features using CLIP's semantic alignment and generates target-focused prompt embeddings.",
        "tldr_zh": "本文提出了一种用于小样本分割的无偏语义解码（USD）策略，该策略利用CLIP从支持集和查询集中提取目标信息。它使用CLIP的语义对齐来增强SAM特征，并生成以目标为中心的提示嵌入。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models",
        "summary": "Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.",
        "url": "http://arxiv.org/abs/2511.15098v1",
        "published_date": "2025-11-19T04:13:36+00:00",
        "updated_date": "2025-11-19T04:13:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Duo Li",
            "Zuhao Yang",
            "Xiaoqin Zhang",
            "Ling Shao",
            "Shijian Lu"
        ],
        "tldr": "The paper investigates visual token redundancy in discrete diffusion-based multimodal large language models (dMLLMs), finding that redundancy exists in from-scratch models handling long-answer tasks and proposing different pruning strategies based on model architecture to improve efficiency.",
        "tldr_zh": "本文研究了离散扩散多模态大型语言模型（dMLLM）中的视觉令牌冗余问题，发现冗余存在于处理长答案任务的从零开始训练的模型中，并提出了基于模型架构的不同剪枝策略以提高效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Evaluating Multimodal Large Language Models on Vertically Written Japanese Text",
        "summary": "Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.",
        "url": "http://arxiv.org/abs/2511.15059v1",
        "published_date": "2025-11-19T03:04:22+00:00",
        "updated_date": "2025-11-19T03:04:22+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Keito Sasagawa",
            "Shuhei Kurita",
            "Daisuke Kawahara"
        ],
        "tldr": "This paper evaluates the performance of existing Multimodal Large Language Models (MLLMs) on vertically written Japanese text, finding that they perform worse compared to horizontally written text, and demonstrates performance improvement through fine-tuning on a newly generated synthetic dataset.",
        "tldr_zh": "本文评估了现有的多模态大型语言模型（MLLM）在垂直书写的日语文本上的表现，发现它们比水平书写的文本表现更差，并通过在一个新生成的合成数据集上进行微调来展示性能的提升。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space",
        "summary": "In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.",
        "url": "http://arxiv.org/abs/2511.15046v1",
        "published_date": "2025-11-19T02:37:03+00:00",
        "updated_date": "2025-11-19T02:37:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Panqi Yang",
            "Haodong Jing",
            "Nanning Zheng",
            "Yongqiang Ma"
        ],
        "tldr": "The paper introduces UniHOI, a unified framework for jointly modeling HOI detection and generation using a unified token space and a semi-supervised learning paradigm, achieving state-of-the-art performance in both tasks.",
        "tldr_zh": "该论文介绍了UniHOI，一个统一的框架，通过统一的token空间和半监督学习范式，联合建模HOI检测和生成，并在两项任务中都实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding",
        "summary": "As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.",
        "url": "http://arxiv.org/abs/2511.14901v1",
        "published_date": "2025-11-18T20:39:15+00:00",
        "updated_date": "2025-11-18T20:39:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenshi Li",
            "Weikang Yu",
            "Dilxat Muhtar",
            "Xueliang Zhang",
            "Pengfeng Xiao",
            "Pedram Ghamisi",
            "Xiao Xiang Zhu"
        ],
        "tldr": "The paper introduces FarSLIP, a novel fine-grained vision-language pretraining framework for remote sensing, addressing limitations of CLIP in capturing fine-grained details by using a new multi-granularity dataset and patch-to-patch distillation to align local and global visual cues while preserving semantic coherence.",
        "tldr_zh": "该论文介绍了FarSLIP，一种用于遥感的新型细粒度视觉语言预训练框架，通过使用新的多粒度数据集和patch-to-patch蒸馏来对齐局部和全局视觉线索，同时保持语义连贯性，从而解决了CLIP在捕获细粒度细节方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization",
        "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.",
        "url": "http://arxiv.org/abs/2511.14899v1",
        "published_date": "2025-11-18T20:37:52+00:00",
        "updated_date": "2025-11-18T20:37:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Gilo",
            "Or Litany"
        ],
        "tldr": "The paper introduces InstructMix2Mix, a framework for consistent multi-view image editing from sparse views using a multi-view diffusion model, addressing limitations of existing methods in maintaining cross-view consistency during instruction-based editing.",
        "tldr_zh": "该论文介绍了InstructMix2Mix，一个用于从稀疏视图进行一致的多视图图像编辑的框架，它使用多视图扩散模型，解决了现有方法在基于指令的编辑过程中保持跨视图一致性方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers",
        "summary": "Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\\%p (avg. 1.78\\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.",
        "url": "http://arxiv.org/abs/2511.15369v1",
        "published_date": "2025-11-19T11:56:16+00:00",
        "updated_date": "2025-11-19T11:56:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Gihwan Kim",
            "Jemin Lee",
            "Hyungshin Kim"
        ],
        "tldr": "The paper introduces IPTQ-ViT, a novel post-training quantization (PTQ) framework for fully integer-only Vision Transformers that achieves comparable accuracy and latency to QAT methods without retraining, using optimized approximation functions and a unified selection metric.",
        "tldr_zh": "该论文介绍了一种名为IPTQ-ViT 的新型训练后量化 (PTQ) 框架，用于全整数视觉Transformer，无需重新训练即可实现与量化感知训练 (QAT) 方法相当的准确性和延迟，并使用了优化的近似函数和统一的选择指标。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]