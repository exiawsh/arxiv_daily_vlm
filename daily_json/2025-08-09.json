[
    {
        "title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment",
        "summary": "Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.",
        "url": "http://arxiv.org/abs/2508.06434v1",
        "published_date": "2025-08-08T16:23:05+00:00",
        "updated_date": "2025-08-08T16:23:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shengzhu Yang",
            "Jiawei Du",
            "Shuai Lu",
            "Weihang Zhang",
            "Ningli Wang",
            "Huiqi Li"
        ],
        "tldr": "The paper introduces CLIPin, a non-contrastive plug-in for CLIP models that enhances multimodal semantic alignment, addressing the challenge of weak supervision and low content diversity in image-text datasets.",
        "tldr_zh": "该论文介绍了CLIPin，一个用于CLIP模型的非对比式插件，旨在增强多模态语义对齐，解决了图像-文本数据集中弱监督和低内容多样性的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning",
        "summary": "The integration of prompt tuning with multimodal learning has shown\nsignificant generalization abilities for various downstream tasks. Despite\nadvancements, existing methods heavily depend on massive modality-specific\nlabeled data (e.g., video, audio, and image), or are customized for a single\nmodality. In this study, we present Text as Any-Modality by Consistent Prompt\nTuning (TaAM-CPT), a scalable approach for constructing a general\nrepresentation model toward unlimited modalities using solely text data.\nTaAM-CPT comprises modality prompt pools, text construction, and\nmodality-aligned text encoders from pre-trained models, which allows for\nextending new modalities by simply adding prompt pools and modality-aligned\ntext encoders. To harmonize the learning across different modalities, TaAM-CPT\ndesigns intra- and inter-modal learning objectives, which can capture category\ndetails within modalities while maintaining semantic consistency across\ndifferent modalities. Benefiting from its scalable architecture and pre-trained\nmodels, TaAM-CPT can be seamlessly extended to accommodate unlimited\nmodalities. Remarkably, without any modality-specific labeled data, TaAM-CPT\nachieves leading results on diverse datasets spanning various modalities,\nincluding video classification, image classification, and audio classification.\nThe code is available at https://github.com/Jinx630/TaAM-CPT.",
        "url": "http://arxiv.org/abs/2508.06382v1",
        "published_date": "2025-08-08T15:13:05+00:00",
        "updated_date": "2025-08-08T15:13:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Wu",
            "Feng Yu",
            "Yang Yang",
            "Jianfeng Lu"
        ],
        "tldr": "The paper introduces TaAM-CPT, a novel approach for zero-shot classification across unlimited modalities using solely text data and consistent prompt tuning, achieving state-of-the-art results without modality-specific labeled data.",
        "tldr_zh": "该论文提出了一种名为TaAM-CPT的新方法，它仅使用文本数据和一致的提示调优来实现跨无限模态的零样本分类，并在没有特定模态标签数据的情况下取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models",
        "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.",
        "url": "http://arxiv.org/abs/2508.06350v1",
        "published_date": "2025-08-08T14:30:05+00:00",
        "updated_date": "2025-08-08T14:30:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingxian Chen",
            "Jiahui Liu",
            "Ruifan Di",
            "Yanwei Li",
            "Chirui Chang",
            "Shizhen Zhao",
            "Wilton W. T. Fok",
            "Xiaojuan Qi",
            "Yik-Chung Wu"
        ],
        "tldr": "The paper introduces VA-GPT, a new MLLM architecture with Spatial and Temporal Effective Token selection modules, designed to improve video anomaly detection and localization, outperforming existing methods on various benchmarks.",
        "tldr_zh": "该论文介绍了一种名为VA-GPT的新型MLLM架构，它具有空间和时间有效令牌选择模块，旨在改进视频异常检测和定位，并在各种基准测试中优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields",
        "summary": "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy.",
        "url": "http://arxiv.org/abs/2508.06301v1",
        "published_date": "2025-08-08T13:24:57+00:00",
        "updated_date": "2025-08-08T13:24:57+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC"
        ],
        "authors": [
            "Junhyeog Yun",
            "Minui Hong",
            "Gunhee Kim"
        ],
        "tldr": "The paper introduces FedMeNF, a privacy-preserving federated meta-learning approach for neural fields that enables fast optimization and robust reconstruction, addressing data privacy and computational limitations in traditional federated meta-learning.",
        "tldr_zh": "该论文介绍了FedMeNF，一种用于神经场的隐私保护联邦元学习方法，它实现了快速优化和稳健的重建，解决了传统联邦元学习中的数据隐私和计算限制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning",
        "summary": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.",
        "url": "http://arxiv.org/abs/2508.06259v1",
        "published_date": "2025-08-08T12:26:20+00:00",
        "updated_date": "2025-08-08T12:26:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.2.10"
        ],
        "authors": [
            "Zhangquan Chen",
            "Ruihui Zhao",
            "Chuwei Luo",
            "Mingze Sun",
            "Xinlei Yu",
            "Yangyang Kang",
            "Ruqi Huang"
        ],
        "tldr": "SIFThinker is a new \"think-with-images\" framework that enhances multimodal large language models' spatial and fine-grained visual reasoning via depth-enhanced bounding boxes and natural language interleaving, introducing a novel training paradigm and dataset.",
        "tldr_zh": "SIFThinker是一种新的“图像思考”框架，通过深度增强的边界框和自然语言交错，增强了多模态大型语言模型在空间和细粒度视觉推理方面的能力，并引入了一种新的训练范式和数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning",
        "summary": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.",
        "url": "http://arxiv.org/abs/2508.06202v1",
        "published_date": "2025-08-08T10:32:38+00:00",
        "updated_date": "2025-08-08T10:32:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chang Che",
            "Ziqi Wang",
            "Pengwan Yang",
            "Qi Wang",
            "Hui Ma",
            "Zenglin Shi"
        ],
        "tldr": "The paper introduces LoRA in LoRA (LiLoRA), a parameter-efficient architecture expansion method for Continual Visual Instruction Tuning (CVIT) in MLLMs that reduces parameter overhead and mitigates catastrophic forgetting.",
        "tldr_zh": "该论文介绍了LoRA in LoRA (LiLoRA)，这是一种用于MLLM中持续视觉指令调整 (CVIT) 的参数高效架构扩展方法，可减少参数开销并缓解灾难性遗忘。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation",
        "summary": "We present VISTAR, a user-centric, multi-dimensional benchmark for\ntext-to-image (T2I) evaluation that addresses the limitations of existing\nmetrics. VISTAR introduces a two-tier hybrid paradigm: it employs\ndeterministic, scriptable metrics for physically quantifiable attributes (e.g.,\ntext rendering, lighting) and a novel Hierarchical Weighted P/N Questioning\n(HWPQ) scheme that uses constrained vision-language models to assess abstract\nsemantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study\nwith 120 experts, we defined seven user roles and nine evaluation angles to\nconstruct the benchmark, which comprises 2,845 prompts validated by over 15,000\nhuman pairwise comparisons. Our metrics achieve high human alignment (>75%),\nwith the HWPQ scheme reaching 85.9% accuracy on abstract semantics,\nsignificantly outperforming VQA baselines. Comprehensive evaluation of\nstate-of-the-art models reveals no universal champion, as role-weighted scores\nreorder rankings and provide actionable guidance for domain-specific\ndeployment. All resources are publicly released to foster reproducible T2I\nassessment.",
        "url": "http://arxiv.org/abs/2508.06152v1",
        "published_date": "2025-08-08T09:15:02+00:00",
        "updated_date": "2025-08-08T09:15:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyuan Jiang",
            "Ruoxi Sun",
            "Ying Cao",
            "Yuqi Xu",
            "Xinran Zhang",
            "Junyan Guo",
            "ChengSheng Deng"
        ],
        "tldr": "The paper introduces VISTAR, a new benchmark for text-to-image evaluation that incorporates user roles and diverse evaluation angles using both deterministic metrics and a novel Hierarchical Weighted P/N Questioning scheme, showing strong human alignment and enabling domain-specific model assessment.",
        "tldr_zh": "该论文介绍了一个新的文本到图像评估基准 VISTAR，它结合了用户角色和不同的评估角度，使用确定性指标和一种新的分层加权 P/N 提问方案，表现出良好的人类一致性，并支持特定领域的模型评估。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Text-guided Visual Prompt DINO for Generic Segmentation",
        "summary": "Recent advancements in multimodal vision models have highlighted limitations\nin late-stage feature fusion and suboptimal query selection for hybrid prompts\nopen-world segmentation, alongside constraints from caption-derived\nvocabularies. To address these challenges, we propose Prompt-DINO, a\ntext-guided visual Prompt DINO framework featuring three key innovations.\nFirst, we introduce an early fusion mechanism that unifies text/visual prompts\nand backbone features at the initial encoding stage, enabling deeper\ncross-modal interactions to resolve semantic ambiguities. Second, we design\norder-aligned query selection for DETR-based architectures, explicitly\noptimizing the structural alignment between text and visual queries during\ndecoding to enhance semantic-spatial consistency. Third, we develop a\ngenerative data engine powered by the Recognize Anything via Prompting (RAP)\nmodel, which synthesizes 0.5B diverse training instances through a dual-path\ncross-verification pipeline, reducing label noise by 80.5% compared to\nconventional approaches. Extensive experiments demonstrate that Prompt-DINO\nachieves state-of-the-art performance on open-world detection benchmarks while\nsignificantly expanding semantic coverage beyond fixed-vocabulary constraints.\nOur work establishes a new paradigm for scalable multimodal detection and data\ngeneration in open-world scenarios. Data&Code are available at\nhttps://github.com/WeChatCV/WeVisionOne.",
        "url": "http://arxiv.org/abs/2508.06146v1",
        "published_date": "2025-08-08T09:09:30+00:00",
        "updated_date": "2025-08-08T09:09:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuchen Guan",
            "Chong Sun",
            "Canmiao Fu",
            "Zhipeng Huang",
            "Chun Yuan",
            "Chen Li"
        ],
        "tldr": "Prompt-DINO is a text-guided visual prompt framework for open-world segmentation that introduces early fusion, order-aligned query selection, and a generative data engine to achieve state-of-the-art performance and expanded semantic coverage.",
        "tldr_zh": "Prompt-DINO是一个用于开放世界分割的文本引导视觉提示框架，它引入了早期融合、顺序对齐的查询选择以及生成式数据引擎，以实现最先进的性能并扩展语义覆盖范围。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning",
        "summary": "We propose SC-Captioner, a reinforcement learning framework that enables the\nself-correcting capability of image caption models. Our crucial technique lies\nin the design of the reward function to incentivize accurate caption\ncorrections. Specifically, the predicted and reference captions are decomposed\ninto object, attribute, and relation sets using scene-graph parsing algorithms.\nWe calculate the set difference between sets of initial and self-corrected\ncaptions to identify added and removed elements. These elements are matched\nagainst the reference sets to calculate correctness bonuses for accurate\nrefinements and mistake punishments for wrong additions and removals, thereby\nforming the final reward. For image caption quality assessment, we propose a\nset of metrics refined from CAPTURE that alleviate its incomplete precision\nevaluation and inefficient relation matching problems. Furthermore, we collect\na fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K\ndiverse images from COCO dataset. Experiments show that applying SC-Captioner\non large visual-language models can generate better image captions across\nvarious scenarios, significantly outperforming the direct preference\noptimization training strategy.",
        "url": "http://arxiv.org/abs/2508.06125v1",
        "published_date": "2025-08-08T08:45:52+00:00",
        "updated_date": "2025-08-08T08:45:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lin Zhang",
            "Xianfang Zeng",
            "Kangcong Li",
            "Gang Yu",
            "Tao Chen"
        ],
        "tldr": "The paper introduces SC-Captioner, a reinforcement learning framework for self-correcting image captioning models, using a reward function based on scene-graph parsing and a refined set of evaluation metrics with a new dataset, demonstrating improvements over direct preference optimization.",
        "tldr_zh": "该论文提出了SC-Captioner，一个使用强化学习框架的自校正图像描述模型，通过基于场景图解析的奖励函数和改进的评估指标以及一个新的数据集，展示了优于直接偏好优化的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation",
        "summary": "Accurate and efficient Video Quality Assessment (VQA) has long been a key\nresearch challenge. Current mainstream VQA methods typically improve\nperformance by pretraining on large-scale classification datasets (e.g.,\nImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this\nstrategy presents two significant challenges: (1) merely transferring semantic\nknowledge learned from pretraining is insufficient for VQA, as video quality\ndepends on multiple factors (e.g., semantics, distortion, motion, aesthetics);\n(2) pretraining on large-scale datasets demands enormous computational\nresources, often dozens or even hundreds of times greater than training\ndirectly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown\nremarkable generalization capabilities across a wide range of visual tasks, and\nhave begun to demonstrate promising potential in quality assessment. In this\nwork, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP\nenhances both visual and textual representations through a Shared Cross-Modal\nAdapter (SCMA), which contains only a minimal number of trainable parameters\nand is the only component that requires training. This design significantly\nreduces computational cost. In addition, we introduce a set of five learnable\nquality-level prompts to guide the VLMs in perceiving subtle quality\nvariations, thereby further enhancing the model's sensitivity to video quality.\nFurthermore, we investigate the impact of different frame sampling strategies\non VQA performance, and find that frame-difference-based sampling leads to\nbetter generalization performance across datasets. Extensive experiments\ndemonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.",
        "url": "http://arxiv.org/abs/2508.06092v1",
        "published_date": "2025-08-08T07:36:01+00:00",
        "updated_date": "2025-08-08T07:36:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yachun Mi",
            "Yu Li",
            "Yanting Li",
            "Shixin Sun",
            "Chen Hui",
            "Tong Zhang",
            "Yuanyuan Liu",
            "Chenyue Song",
            "Shaohui Liu"
        ],
        "tldr": "The paper introduces Q-CLIP, a novel VLM-based framework for video quality assessment that uses a cross-modal adapter and quality-level prompts to improve performance while significantly reducing computational cost.",
        "tldr_zh": "该论文介绍了Q-CLIP，一种新颖的基于VLM的视频质量评估框架，它使用跨模态适配器和质量级别提示来提高性能，同时显著降低计算成本。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance",
        "summary": "Vision-language models (VLMs) have achieved impressive performance on\nmultimodal reasoning tasks such as visual question answering (VQA), but their\ninference cost remains a significant challenge due to the large number of\nvision tokens processed during the prefill stage. Existing pruning methods\noften rely on directly using the attention patterns or static text prompt\nguidance, failing to exploit the dynamic internal signals generated during\ninference. To address these issues, we propose AdaptInfer, a plug-and-play\nframework for adaptive vision token pruning in VLMs. First, we introduce a\nfine-grained, dynamic text-guided pruning mechanism that reuses layer-wise\ntext-to-text attention maps to construct soft priors over text-token\nimportance, allowing more informed scoring of vision tokens at each stage.\nSecond, we perform an offline analysis of cross-modal attention shifts and\nidentify consistent inflection locations in inference, which inspire us to\npropose a more principled and efficient pruning schedule. Our method is\nlightweight and plug-and-play, also generalizable across multi-modal tasks.\nExperimental results have verified the effectiveness of the proposed method.\nFor example, it reduces CUDA latency by 61.3\\% while maintaining an average\naccuracy of 92.9\\% on vanilla LLaVA-1.5-7B. Under the same token budget,\nAdaptInfer surpasses SOTA in accuracy.",
        "url": "http://arxiv.org/abs/2508.06084v1",
        "published_date": "2025-08-08T07:27:26+00:00",
        "updated_date": "2025-08-08T07:27:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weichen Zhang",
            "Zhui Zhu",
            "Ningbo Li",
            "Kebin Liu",
            "Yunhao Liu"
        ],
        "tldr": "The paper introduces AdaptInfer, a plug-and-play framework for adaptive vision token pruning in VLMs that uses dynamic text-guided pruning and a principled pruning schedule to reduce inference cost while maintaining accuracy.",
        "tldr_zh": "该论文介绍AdaptInfer，一个VLM中用于自适应视觉token剪枝的即插即用框架，它使用动态文本引导的剪枝和有原则的剪枝计划，以降低推理成本，同时保持准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DreamVE: Unified Instruction-based Image and Video Editing",
        "summary": "Instruction-based editing holds vast potential due to its simple and\nefficient interactive editing format. However, instruction-based editing,\nparticularly for video, has been constrained by limited training data,\nhindering its practical application. To this end, we introduce DreamVE, a\nunified model for instruction-based image and video editing. Specifically, We\npropose a two-stage training strategy: first image editing, then video editing.\nThis offers two main benefits: (1) Image data scales more easily, and models\nare more efficient to train, providing useful priors for faster and better\nvideo editing training. (2) Unifying image and video generation is natural and\naligns with current trends. Moreover, we present comprehensive training data\nsynthesis pipelines, including collage-based and generative model-based data\nsynthesis. The collage-based data synthesis combines foreground objects and\nbackgrounds to generate diverse editing data, such as object manipulation,\nbackground changes, and text modifications. It can easily generate billions of\naccurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE\non extensive collage-based data to achieve strong performance in key editing\ntypes and enhance generalization and transfer capabilities. However,\ncollage-based data lacks some attribute editing cases, leading to a relative\ndrop in performance. In contrast, the generative model-based pipeline, despite\nbeing hard to scale up, offers flexibility in handling attribute editing cases.\nTherefore, we use generative model-based data to further fine-tune DreamVE.\nBesides, we design an efficient and powerful editing framework for DreamVE. We\nbuild on the SOTA T2V model and use a token concatenation with early drop\napproach to inject source image guidance, ensuring strong consistency and\neditability. The codes and models will be released.",
        "url": "http://arxiv.org/abs/2508.06080v1",
        "published_date": "2025-08-08T07:20:30+00:00",
        "updated_date": "2025-08-08T07:20:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Xia",
            "Jiyang Liu",
            "Yuechen Zhang",
            "Bohao Peng",
            "Ruihang Chu",
            "Yitong Wang",
            "Xinglong Wu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "tldr": "DreamVE is a unified model for instruction-based image and video editing, trained using a two-stage approach and comprehensive synthetic data pipelines to overcome limitations of existing methods, with code and models to be released.",
        "tldr_zh": "DreamVE是一个统一的基于指令的图像和视频编辑模型，通过两阶段训练方法和全面的合成数据流程训练，克服了现有方法的局限性，代码和模型即将发布。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) typically replace the predefined image\nplaceholder token (<image>) in textual instructions with visual features from\nan image encoder, forming the input to a backbone Large Language Model (LLM).\nHowever, the large number of vision tokens significantly increases the context\nlength, leading to high computational overhead and inference latency. While\nprevious efforts mitigate this by selecting only important visual features or\nleveraging learnable queries to reduce token count, they often compromise\nperformance or introduce substantial extra costs. In response, we propose\nFourier-VLM, a simple yet efficient method that compresses visual\nrepresentations in the frequency domain. Our approach is motivated by the\nobservation that vision features output from the vision encoder exhibit\nconcentrated energy in low-frequency components. Leveraging this, we apply a\nlow-pass filter to the vision features using a two-dimentional Discrete Cosine\nTransform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier\nTransform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$,\nminimizing the extra computational cost while introducing no additional\nparameters. Extensive experiments across various image-based benchmarks\ndemonstrate that Fourier-VLM achieves competitive performance with strong\ngeneralizability across both LLaVA and Qwen-VL architectures. Crucially, it\nreduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%\ncompared to LLaVA-v1.5, highlighting the superior efficiency and practicality.",
        "url": "http://arxiv.org/abs/2508.06038v1",
        "published_date": "2025-08-08T05:49:42+00:00",
        "updated_date": "2025-08-08T05:49:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huanyu Wang",
            "Jushi Kai",
            "Haoli Bai",
            "Lu Hou",
            "Bo Jiang",
            "Ziwei He",
            "Zhouhan Lin"
        ],
        "tldr": "The paper introduces Fourier-VLM, a method to compress visual representations in Vision-Language Models (VLMs) by leveraging the Discrete Cosine Transform (DCT) in the frequency domain, achieving significant FLOPs reduction and speedup during inference.",
        "tldr_zh": "该论文介绍了 Fourier-VLM，一种通过在频域中使用离散余弦变换 (DCT) 压缩视觉语言模型 (VLM) 中视觉表示的方法，从而在推理过程中显著减少 FLOPs 并提高速度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation",
        "summary": "The fragmentation between high-level task semantics and low-level geometric\nfeatures remains a persistent challenge in robotic manipulation. While\nvision-language models (VLMs) have shown promise in generating affordance-aware\nvisual representations, the lack of semantic grounding in canonical spaces and\nreliance on manual annotations severely limit their ability to capture dynamic\nsemantic-affordance relationships. To address these, we propose Primitive-Aware\nSemantic Grounding (PASG), a closed-loop framework that introduces: (1)\nAutomatic primitive extraction through geometric feature aggregation, enabling\ncross-category detection of keypoints and axes; (2) VLM-driven semantic\nanchoring that dynamically couples geometric primitives with functional\naffordances and task-relevant description; (3) A spatial-semantic reasoning\nbenchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's\neffectiveness in practical robotic manipulation tasks across diverse scenarios,\nachieving performance comparable to manual annotations. PASG achieves a\nfiner-grained semantic-affordance understanding of objects, establishing a\nunified paradigm for bridging geometric primitives with task semantics in\nrobotic manipulation.",
        "url": "http://arxiv.org/abs/2508.05976v1",
        "published_date": "2025-08-08T03:23:33+00:00",
        "updated_date": "2025-08-08T03:23:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhihao Zhu",
            "Yifan Zheng",
            "Siyu Pan",
            "Yaohui Jin",
            "Yao Mu"
        ],
        "tldr": "The paper introduces PASG, a closed-loop framework that uses automatic geometric primitive extraction and VLM-driven semantic anchoring to bridge the gap between high-level task semantics and low-level geometric features in robotic manipulation, achieving performance comparable to manual annotations.",
        "tldr_zh": "该论文介绍了PASG，一个闭环框架，它使用自动几何基元提取和VLM驱动的语义锚定来弥合机器人操作中高级任务语义和低级几何特征之间的差距，其性能与手动标注相当。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents",
        "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.",
        "url": "http://arxiv.org/abs/2508.05954v1",
        "published_date": "2025-08-08T02:38:47+00:00",
        "updated_date": "2025-08-08T02:38:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Han Lin",
            "Jaemin Cho",
            "Amir Zadeh",
            "Chuan Li",
            "Mohit Bansal"
        ],
        "tldr": "Bifrost-1 bridges multimodal LLMs and diffusion models using patch-level CLIP embeddings, enabling efficient and high-fidelity controllable image generation while preserving MLLM reasoning capabilities. It achieves comparable or better performance than existing methods with substantially lower training costs.",
        "tldr_zh": "Bifrost-1使用patch-level CLIP嵌入连接多模态LLM和扩散模型，从而以高效的方式实现高保真、可控的图像生成，同时保留MLLM的推理能力。相比现有方法，该方法在大幅降低训练成本的同时，实现了相当甚至更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing",
        "summary": "3D scene generation plays a crucial role in gaming, artistic creation,\nvirtual reality and many other domains. However, current 3D scene design still\nrelies heavily on extensive manual effort from creators, and existing automated\nmethods struggle to generate open-domain scenes or support flexible editing. As\na result, generating 3D worlds directly from text has garnered increasing\nattention. In this paper, we introduce HOLODECK 2.0, an advanced\nvision-language-guided framework for 3D world generation with support for\ninteractive scene editing based on human feedback. HOLODECK 2.0 can generate\ndiverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and\ncyberpunk styles) that exhibit high semantic fidelity to fine-grained input\ndescriptions, suitable for both indoor and open-domain environments. HOLODECK\n2.0 leverages vision-language models (VLMs) to identify and parse the objects\nrequired in a scene and generates corresponding high-quality assets via\nstate-of-the-art 3D generative models. It then iteratively applies spatial\nconstraints derived from the VLMs to achieve semantically coherent and\nphysically plausible layouts. Human evaluations and CLIP-based assessments\ndemonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely\naligned with detailed textual descriptions, consistently outperforming\nbaselines across indoor and open-domain scenarios. Additionally, we provide\nediting capabilities that flexibly adapt to human feedback, supporting layout\nrefinement and style-consistent object edits. Finally, we present a practical\napplication of HOLODECK 2.0 in procedural game modeling, generating visually\nrich and immersive environments, potentially boosting efficiency.",
        "url": "http://arxiv.org/abs/2508.05899v1",
        "published_date": "2025-08-07T23:23:07+00:00",
        "updated_date": "2025-08-07T23:23:07+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Zixuan Bian",
            "Ruohan Ren",
            "Yue Yang",
            "Chris Callison-Burch"
        ],
        "tldr": "HOLODECK 2.0 is a vision-language-guided framework for generating and editing diverse 3D scenes from text, outperforming baselines in both quality and semantic fidelity, with applications in procedural game modeling.",
        "tldr_zh": "HOLODECK 2.0是一个视觉-语言引导的框架，用于从文本生成和编辑多样化的3D场景，在质量和语义保真度方面优于基线，并在程序化游戏建模中具有应用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments",
        "summary": "Driver visual attention prediction is a critical task in autonomous driving\nand human-computer interaction (HCI) research. Most prior studies focus on\nestimating attention allocation at a single moment in time, typically using\nstatic RGB images such as driving scene pictures. In this work, we propose a\nvision-language framework that models the changing landscape of drivers' gaze\nthrough natural language, using few-shot and zero-shot learning on single RGB\nimages. We curate and refine high-quality captions from the BDD-A dataset using\nhuman-in-the-loop feedback, then fine-tune LLaVA to align visual perception\nwith attention-centric scene understanding. Our approach integrates both\nlow-level cues and top-down context (e.g., route semantics, risk anticipation),\nenabling language-based descriptions of gaze behavior. We evaluate performance\nacross training regimes (few shot, and one-shot) and introduce domain-specific\nmetrics for semantic alignment and response diversity. Results show that our\nfine-tuned model outperforms general-purpose VLMs in attention shift detection\nand interpretability. To our knowledge, this is among the first attempts to\ngenerate driver visual attention allocation and shifting predictions in natural\nlanguage, offering a new direction for explainable AI in autonomous driving.\nOur approach provides a foundation for downstream tasks such as behavior\nforecasting, human-AI teaming, and multi-agent coordination.",
        "url": "http://arxiv.org/abs/2508.05852v1",
        "published_date": "2025-08-07T21:01:43+00:00",
        "updated_date": "2025-08-07T21:01:43+00:00",
        "categories": [
            "cs.CV",
            "I.5.4"
        ],
        "authors": [
            "Kaiser Hamid",
            "Khandakar Ashrafi Akbar",
            "Nade Liang"
        ],
        "tldr": "The paper introduces VISTA, a vision-language framework fine-tuned on LLaVA to predict driver visual attention shifts in dynamic environments using natural language, outperforming general VLMs and enabling explainable AI in autonomous driving.",
        "tldr_zh": "该论文介绍了VISTA，一个在LLaVA上微调的视觉语言框架，用于预测动态环境中驾驶员视觉注意力转移，使用自然语言，优于通用VLM，并实现自动驾驶中可解释的AI。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation",
        "summary": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba",
        "url": "http://arxiv.org/abs/2508.06453v1",
        "published_date": "2025-08-08T16:54:06+00:00",
        "updated_date": "2025-08-08T16:54:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruida Cheng",
            "Tejas Sudharshan Mathai",
            "Pritam Mukherjee",
            "Benjamin Hou",
            "Qingqing Zhu",
            "Zhiyong Lu",
            "Matthew McAuliffe",
            "Ronald M. Summers"
        ],
        "tldr": "This paper introduces Text-Swin-UMamba, a novel architecture integrating text from radiology reports into a Swin-UMamba model for improved lesion segmentation, outperforming existing image-based and LLM-driven methods on the DeepLesion dataset.",
        "tldr_zh": "本文介绍了一种名为 Text-Swin-UMamba 的新架构，该架构将放射学报告中的文本整合到 Swin-UMamba 模型中，以改进病灶分割。在DeepLesion数据集上，该模型优于现有的基于图像和LLM驱动的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation",
        "summary": "Recent unsupervised domain adaptation (UDA) methods have shown great success\nin addressing classical domain shifts (e.g., synthetic-to-real), but they still\nsuffer under complex shifts (e.g. geographical shift), where both the\nbackground and object appearances differ significantly across domains. Prior\nworks showed that the language modality can help in the adaptation process,\nexhibiting more robustness to such complex shifts. In this paper, we introduce\nTRUST, a novel UDA approach that exploits the robustness of the language\nmodality to guide the adaptation of a vision model. TRUST generates\npseudo-labels for target samples from their captions and introduces a novel\nuncertainty estimation strategy that uses normalised CLIP similarity scores to\nestimate the uncertainty of the generated pseudo-labels. Such estimated\nuncertainty is then used to reweight the classification loss, mitigating the\nadverse effects of wrong pseudo-labels obtained from low-quality captions. To\nfurther increase the robustness of the vision model, we propose a multimodal\nsoft-contrastive learning loss that aligns the vision and language feature\nspaces, by leveraging captions to guide the contrastive training of the vision\nmodel on target images. In our contrastive loss, each pair of images acts as\nboth a positive and a negative pair and their feature representations are\nattracted and repulsed with a strength proportional to the similarity of their\ncaptions. This solution avoids the need for hardly determining positive and\nnegative pairs, which is critical in the UDA setting. Our approach outperforms\nprevious methods, setting the new state-of-the-art on classical (DomainNet) and\ncomplex (GeoNet) domain shifts. The code will be available upon acceptance.",
        "url": "http://arxiv.org/abs/2508.06452v1",
        "published_date": "2025-08-08T16:51:44+00:00",
        "updated_date": "2025-08-08T16:51:44+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mattia Litrico",
            "Mario Valerio Giuffrida",
            "Sebastiano Battiato",
            "Devis Tuia"
        ],
        "tldr": "This paper introduces TRUST, a UDA method that uses the robustness of language modality (captions) to guide vision model adaptation, particularly effective in complex domain shifts, using pseudo-labeling and multimodal soft-contrastive learning.",
        "tldr_zh": "该论文介绍了TRUST，一种利用语言模态（字幕）的鲁棒性来引导视觉模型适应的UDA方法，尤其是在复杂的领域迁移中有效，它使用伪标签和多模态软对比学习。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding",
        "summary": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.",
        "url": "http://arxiv.org/abs/2508.06317v1",
        "published_date": "2025-08-08T13:47:00+00:00",
        "updated_date": "2025-08-08T13:47:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Hu",
            "Zixu Cheng",
            "Shaogang Gong",
            "Isabel Guan",
            "Jianye Hao",
            "Jun Wang",
            "Kun Shao"
        ],
        "tldr": "The paper introduces Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain video temporal grounding without target labels, using a small number of unlabelled videos to adapt a model trained on labelled data, achieving real-time performance.",
        "tldr_zh": "该论文提出了一种不确定性量化的Rollout策略自适应(URPA)方法，用于无目标标签的跨领域视频时间定位。该方法仅使用少量未标记的视频来调整在标记数据上训练的模型，从而实现实时性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models",
        "summary": "In the rapidly evolving landscape of Multimodal Large Language Models\n(MLLMs), the safety concerns of their outputs have earned significant\nattention. Although numerous datasets have been proposed, they may become\noutdated with MLLM advancements and are susceptible to data contamination\nissues. To address these problems, we propose \\textbf{SDEval}, the\n\\textit{first} safety dynamic evaluation framework to controllably adjust the\ndistribution and complexity of safety benchmarks. Specifically, SDEval mainly\nadopts three dynamic strategies: text, image, and text-image dynamics to\ngenerate new samples from original benchmarks. We first explore the individual\neffects of text and image dynamics on model safety. Then, we find that\ninjecting text dynamics into images can further impact safety, and conversely,\ninjecting image dynamics into text also leads to safety risks. SDEval is\ngeneral enough to be applied to various existing safety and even capability\nbenchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and\ncapability benchmarks, MMBench and MMVet, show that SDEval significantly\ninfluences safety evaluation, mitigates data contamination, and exposes safety\nlimitations of MLLMs. Code is available at https://github.com/hq-King/SDEval",
        "url": "http://arxiv.org/abs/2508.06142v1",
        "published_date": "2025-08-08T09:01:56+00:00",
        "updated_date": "2025-08-08T09:01:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanqing Wang",
            "Yuan Tian",
            "Mingyu Liu",
            "Zhenhao Zhang",
            "Xiangyang Zhu"
        ],
        "tldr": "The paper introduces SDEval, a dynamic evaluation framework for MLLM safety, which addresses data contamination and outdated benchmarks by dynamically generating new samples via text, image, and combined text-image manipulations. Experiments show it can reveal safety limitations of MLLMs.",
        "tldr_zh": "该论文介绍了SDEval，一个用于MLLM安全性的动态评估框架，通过文本、图像和文本-图像组合操作动态生成新样本，从而解决数据污染和基准过时的问题。实验表明它可以揭示MLLM的安全限制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction",
        "summary": "Text-guided image editing involves modifying a source image based on a\nlanguage instruction and, typically, requires changes to only small local\nregions. However, existing approaches generate the entire target image rather\nthan selectively regenerate only the intended editing areas. This results in\n(1) unnecessary computational costs and (2) a bias toward reconstructing\nnon-editing regions, which compromises the quality of the intended edits. To\nresolve these limitations, we propose to formulate image editing as Next\nEditing-token Prediction (NEP) based on autoregressive image generation, where\nonly regions that need to be edited are regenerated, thus avoiding unintended\nmodification to the non-editing areas. To enable any-region editing, we propose\nto pre-train an any-order autoregressive text-to-image (T2I) model. Once\ntrained, it is capable of zero-shot image editing and can be easily adapted to\nNEP for image editing, which achieves a new state-of-the-art on widely used\nimage editing benchmarks. Moreover, our model naturally supports test-time\nscaling (TTS) through iteratively refining its generation in a zero-shot\nmanner. The project page is: https://nep-bigai.github.io/",
        "url": "http://arxiv.org/abs/2508.06044v1",
        "published_date": "2025-08-08T06:06:34+00:00",
        "updated_date": "2025-08-08T06:06:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huimin Wu",
            "Xiaojian Ma",
            "Haozhe Zhao",
            "Yanpeng Zhao",
            "Qing Li"
        ],
        "tldr": "The paper introduces NEP, a novel autoregressive image editing approach that predicts the next editing token, focusing regeneration only on the regions that need modification, achieving state-of-the-art results on image editing benchmarks.",
        "tldr_zh": "该论文介绍了一种名为NEP的新型自回归图像编辑方法，该方法通过预测下一个编辑token，只关注需要修改的区域进行再生，并在图像编辑基准测试中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment",
        "summary": "In this paper, we present our solution for the semi-supervised learning track\n(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the\nprinciple that \"more is better,\" to construct a robust Mixture of Experts (MoE)\nemotion recognition system. Our approach integrates a diverse range of input\nmodalities as independent experts, including novel signals such as knowledge\nfrom large Vision-Language Models (VLMs) and temporal Action Unit (AU)\ninformation. To effectively utilize unlabeled data, we introduce a\nconsensus-based pseudo-labeling strategy, generating high-quality labels from\nthe agreement between a baseline model and Gemini, which are then used in a\ntwo-stage training paradigm. Finally, we employ a multi-expert voting ensemble\ncombined with a rule-based re-ranking process to correct prediction bias and\nbetter align the outputs with human preferences. Evaluated on the MER2025-SEMI\nchallenge dataset, our method achieves an F1-score of 0.8772 on the test set,\nranking 2nd in the track. Our code is available at\nhttps://github.com/zhuyjan/MER2025-MRAC25.",
        "url": "http://arxiv.org/abs/2508.06036v1",
        "published_date": "2025-08-08T05:44:26+00:00",
        "updated_date": "2025-08-08T05:44:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jun Xie",
            "Yingjian Zhu",
            "Feng Chen",
            "Zhenghao Zhang",
            "Xiaohui Fan",
            "Hongzhu Yi",
            "Xinming Wang",
            "Chen Yu",
            "Yue Bi",
            "Zhaoran Zhao",
            "Xiongjun Guan",
            "Zhepeng Wang"
        ],
        "tldr": "This paper proposes a Mixture of Experts (MoE) framework for emotion recognition, integrating multiple modalities and a consensus-based pseudo-labeling strategy, achieving competitive results in the MER2025-SEMI challenge. It leverages VLMs as one of the expert modalities.",
        "tldr_zh": "该论文提出了一个用于情感识别的混合专家（MoE）框架，集成了多种模态和一个基于共识的伪标签策略，在 MER2025-SEMI 挑战赛中取得了有竞争力的结果。它利用 VLM 作为专家模态之一。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow",
        "summary": "We propose a fast text-guided image editing method called InstantEdit based\non the RectifiedFlow framework, which is structured as a few-step editing\nprocess that preserves critical content while following closely to textual\ninstructions. Our approach leverages the straight sampling trajectories of\nRectifiedFlow by introducing a specialized inversion strategy called PerRFI. To\nmaintain consistent while editable results for RectifiedFlow model, we further\npropose a novel regeneration method, Inversion Latent Injection, which\neffectively reuses latent information obtained during inversion to facilitate\nmore coherent and detailed regeneration. Additionally, we propose a\nDisentangled Prompt Guidance technique to balance editability with detail\npreservation, and integrate a Canny-conditioned ControlNet to incorporate\nstructural cues and suppress artifacts. Evaluation on the PIE image editing\ndataset demonstrates that InstantEdit is not only fast but also achieves better\nqualitative and quantitative results compared to state-of-the-art few-step\nediting methods.",
        "url": "http://arxiv.org/abs/2508.06033v1",
        "published_date": "2025-08-08T05:38:17+00:00",
        "updated_date": "2025-08-08T05:38:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Gong",
            "Zhen Zhu",
            "Minjia Zhang"
        ],
        "tldr": "InstantEdit is a fast text-guided image editing method based on RectifiedFlow, introducing PerRFI, Inversion Latent Injection, and Disentangled Prompt Guidance to improve editing quality and speed while preserving content.",
        "tldr_zh": "InstantEdit是一种基于RectifiedFlow的快速文本引导图像编辑方法，引入了PerRFI、Inversion Latent Injection和Disentangled Prompt Guidance，以提高编辑质量和速度，同时保留内容。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates",
        "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
        "url": "http://arxiv.org/abs/2508.05898v1",
        "published_date": "2025-08-07T23:11:33+00:00",
        "updated_date": "2025-08-07T23:11:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hamidreza Dastmalchi",
            "Aijun An",
            "Ali cheraghian"
        ],
        "tldr": "The paper introduces ETTA, an efficient test-time adaptation method for VLMs that uses a recursive updating module and adaptive ensemble to improve accuracy under distribution shifts with minimal overhead.",
        "tldr_zh": "该论文介绍了ETTA，一种用于VLM的高效测试时自适应方法，它使用递归更新模块和自适应集成来提高分布偏移下的准确性，且开销最小。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction",
        "summary": "This paper presents a novel approach that integrates vision foundation models\nwith reinforcement learning to enhance object interaction capabilities in\nsimulated environments. By combining the Segment Anything Model (SAM) and\nYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the\nAI2-THOR simulation environment, we enable the agent to perceive and interact\nwith objects more effectively. Our comprehensive experiments, conducted across\nfour diverse indoor kitchen settings, demonstrate significant improvements in\nobject interaction success rates and navigation efficiency compared to a\nbaseline agent without advanced perception. The results show a 68% increase in\naverage cumulative reward, a 52.5% improvement in object interaction success\nrate, and a 33% increase in navigation efficiency. These findings highlight the\npotential of integrating foundation models with reinforcement learning for\ncomplex robotic tasks, paving the way for more sophisticated and capable\nautonomous agents.",
        "url": "http://arxiv.org/abs/2508.05838v1",
        "published_date": "2025-08-07T20:29:01+00:00",
        "updated_date": "2025-08-07T20:29:01+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY",
            "68T07, 68T40, 90C40, 93E35",
            "I.2.6; I.2.9; I.2.10"
        ],
        "authors": [
            "Ahmad Farooq",
            "Kamran Iqbal"
        ],
        "tldr": "The paper integrates SAM and YOLOv5 with a PPO agent in AI2-THOR, showing improved object interaction and navigation. It claims a significant improvement over baseline agents.",
        "tldr_zh": "该论文将SAM和YOLOv5与AI2-THOR中的PPO智能体结合，展示了物体交互和导航的改进。它声称比基线智能体有了显著的改进。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning",
        "summary": "Video quality assessment (VQA) aims to objectively quantify perceptual\nquality degradation in alignment with human visual perception. Despite recent\nadvances, existing VQA models still suffer from two critical limitations:\n\\textit{poor generalization to out-of-distribution (OOD) videos} and\n\\textit{limited explainability}, which restrict their applicability in\nreal-world scenarios. To address these challenges, we propose\n\\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large\nmultimodal models (LMMs) with reinforcement learning to jointly model video\nquality understanding and scoring, emulating human perceptual decision-making.\nSpecifically, we adopt group relative policy optimization (GRPO), a rule-guided\nreinforcement learning algorithm that enables reasoning over video quality\nunder score-level supervision, and introduce three VQA-specific rewards: (1) a\n\\textbf{bell-shaped regression reward} that increases rapidly as the prediction\nerror decreases and becomes progressively less sensitive near the ground truth;\n(2) a \\textbf{pairwise ranking reward} that guides the model to correctly\ndetermine the relative quality between video pairs; and (3) a \\textbf{temporal\nconsistency reward} that encourages the model to prefer temporally coherent\nvideos over their perturbed counterparts. Extensive experiments demonstrate\nthat VQAThinker achieves state-of-the-art performance on both in-domain and OOD\nVQA benchmarks, showing strong generalization for video quality scoring.\nFurthermore, evaluations on video quality understanding tasks validate its\nsuperiority in distortion attribution and quality description compared to\nexisting explainable VQA models and LMMs. These findings demonstrate that\nreinforcement learning offers an effective pathway toward building\ngeneralizable and explainable VQA models solely with score-level supervision.",
        "url": "http://arxiv.org/abs/2508.06051v1",
        "published_date": "2025-08-08T06:16:23+00:00",
        "updated_date": "2025-08-08T06:16:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linhan Cao",
            "Wei Sun",
            "Weixia Zhang",
            "Xiangyang Zhu",
            "Jun Jia",
            "Kaiwei Zhang",
            "Dandan Zhu",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "tldr": "The paper introduces VQAThinker, a reinforcement learning-based VQA framework using large multimodal models that improves generalization and explainability for video quality assessment, achieving state-of-the-art results on both in-domain and out-of-domain benchmarks.",
        "tldr_zh": "该论文介绍了VQAThinker，一个基于强化学习的VQA框架，它利用大型多模态模型来提高视频质量评估的泛化性和可解释性，并在领域内和领域外基准测试中取得了最先进的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities",
        "summary": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.",
        "url": "http://arxiv.org/abs/2508.06342v1",
        "published_date": "2025-08-08T14:15:58+00:00",
        "updated_date": "2025-08-08T14:15:58+00:00",
        "categories": [
            "cs.CV",
            "cs.SI"
        ],
        "authors": [
            "Kieran Elrod",
            "Katherine Flanigan",
            "Mario Bergés"
        ],
        "tldr": "This paper explores using street view imagery and a multimodal LLM to analyze urban sociability, finding correlations between inferred social interactions and built environment variables.",
        "tldr_zh": "本文探讨了使用街景图像和多模态LLM来分析城市社交行为，发现推断出的社交互动与建筑环境变量之间的相关性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]