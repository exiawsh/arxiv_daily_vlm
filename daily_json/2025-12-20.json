[
    {
        "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
        "summary": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
        "url": "http://arxiv.org/abs/2512.16921v1",
        "published_date": "2025-12-18T18:59:57+00:00",
        "updated_date": "2025-12-18T18:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qihao Liu",
            "Chengzhi Mao",
            "Yaojie Liu",
            "Alan Yuille",
            "Wen-Sheng Chu"
        ],
        "tldr": "The paper introduces AuditDM, a reinforcement learning-based framework that audits MLLMs by generating challenging questions and counterfactual images to uncover model weaknesses, which can then be used for model rectification, demonstrating significant improvements across various benchmarks.",
        "tldr_zh": "该论文介绍了一种名为AuditDM的基于强化学习的框架，通过生成具有挑战性的问题和反事实图像来审计多模态大型语言模型 (MLLM)，以发现模型弱点，进而用于模型修正，并在各种基准测试中展示了显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
        "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
        "url": "http://arxiv.org/abs/2512.16918v1",
        "published_date": "2025-12-18T18:59:55+00:00",
        "updated_date": "2025-12-18T18:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaoyang Wang",
            "Kaituo Feng",
            "Dongyang Chen",
            "Zhongyu Wang",
            "Zhixun Li",
            "Sicheng Gao",
            "Meng Meng",
            "Xu Zhou",
            "Manyuan Zhang",
            "Yuzhang Shang",
            "Xiangyu Yue"
        ],
        "tldr": "AdaTooler-V is a multimodal large language model (MLLM) that adaptively determines when to use visual tools, improving performance and reducing inference overhead by using reinforcement learning to adjust reward scales and introducing new datasets for training. It outperforms existing methods, including GPT-4o and Gemini 1.5 Pro, on several benchmarks.",
        "tldr_zh": "AdaTooler-V 是一种多模态大型语言模型 (MLLM)，它自适应地确定何时使用视觉工具，通过使用强化学习来调整奖励比例并引入新的训练数据集，从而提高性能并减少推理开销。在多个基准测试中，它优于包括 GPT-4o 和 Gemini 1.5 Pro 在内的现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
        "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
        "url": "http://arxiv.org/abs/2512.16909v1",
        "published_date": "2025-12-18T18:59:03+00:00",
        "updated_date": "2025-12-18T18:59:03+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yuanchen Ju",
            "Yongyuan Liang",
            "Yen-Jen Wang",
            "Nandiraju Gireesh",
            "Yuanliang Ju",
            "Seungjae Lee",
            "Qiao Gu",
            "Elvis Hsieh",
            "Furong Huang",
            "Koushil Sreenath"
        ],
        "tldr": "The paper introduces MomaGraph, a unified scene representation for mobile manipulators that integrates spatial-functional relationships, part-level interactive elements, and object states. It also contributes a large-scale dataset and benchmark, along with a vision-language model, MomaGraph-R1, for task-oriented scene graph prediction and task planning.",
        "tldr_zh": "该论文介绍了MomaGraph，一种用于移动机械臂的统一场景表示，集成了空间-功能关系、零件级交互元素和对象状态。它还贡献了一个大型数据集和基准测试，以及一个视觉-语言模型MomaGraph-R1，用于面向任务的场景图预测和任务规划。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization",
        "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io",
        "url": "http://arxiv.org/abs/2512.16906v1",
        "published_date": "2025-12-18T18:58:42+00:00",
        "updated_date": "2025-12-18T18:58:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyan Cong",
            "Haotian Yang",
            "Angtian Wang",
            "Yizhi Wang",
            "Yiding Yang",
            "Canyu Zhang",
            "Chongyang Ma"
        ],
        "tldr": "The paper introduces VIVA, a framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization to improve generalization and editing quality compared to existing diffusion-based methods.",
        "tldr_zh": "该论文介绍了 VIVA，一个基于指令的视频编辑框架，它利用 VLM 指导的编码和奖励优化来提高与现有基于扩散的方法相比的泛化能力和编辑质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
        "summary": "Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.",
        "url": "http://arxiv.org/abs/2512.16905v1",
        "published_date": "2025-12-18T18:57:58+00:00",
        "updated_date": "2025-12-18T18:57:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaixin Ding",
            "Yang Zhou",
            "Xi Chen",
            "Miao Yang",
            "Jiarong Ou",
            "Rui Chen",
            "Xin Tao",
            "Hengshuang Zhao"
        ],
        "tldr": "The paper introduces Alchemist, a meta-gradient-based framework for efficient text-to-image model training through data selection, demonstrating improved visual quality and performance with less data.",
        "tldr_zh": "该论文介绍了一种名为Alchemist的基于元梯度的框架，通过数据选择来提高文本到图像模型训练的效率，并证明使用更少的数据可以提高视觉质量和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation",
        "summary": "Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.",
        "url": "http://arxiv.org/abs/2512.16891v1",
        "published_date": "2025-12-18T18:52:18+00:00",
        "updated_date": "2025-12-18T18:52:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Haichao Zhang",
            "Yao Lu",
            "Lichen Wang",
            "Yunzhe Li",
            "Daiwei Chen",
            "Yunpeng Xu",
            "Yun Fu"
        ],
        "tldr": "The paper introduces LinkedOut, a novel VLLM-based video representation method for video recommendation that extracts knowledge-aware tokens directly from raw frames, achieving state-of-the-art results on standard benchmarks while addressing the limitations of directly applying VLLMs to recommendation tasks.",
        "tldr_zh": "本文介绍LinkedOut，一种新颖的基于VLLM的视频表示方法，用于视频推荐。该方法直接从原始帧中提取知识感知的tokens，在标准基准测试中取得了最先进的结果，同时解决了直接将VLLM应用于推荐任务的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
        "summary": "Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io",
        "url": "http://arxiv.org/abs/2512.16864v1",
        "published_date": "2025-12-18T18:34:23+00:00",
        "updated_date": "2025-12-18T18:34:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyuan Qu",
            "Lei Ke",
            "Xiaohang Zhan",
            "Longxiang Tang",
            "Yuqi Liu",
            "Bohao Peng",
            "Bei Yu",
            "Dong Yu",
            "Jiaya Jia"
        ],
        "tldr": "The paper introduces RePlan, a vision-language framework for instruction-based image editing that uses reasoning-guided region planning and a diffusion editor to handle complex instructions and scenes, achieving superior performance on a new benchmark called IV-Edit.",
        "tldr_zh": "该论文介绍了RePlan，一个基于指令的图像编辑视觉语言框架，它使用推理引导的区域规划和扩散编辑器来处理复杂的指令和场景，并在名为IV-Edit的新基准测试上取得了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Radiology Report Generation with Layer-Wise Anatomical Attention",
        "summary": "Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.",
        "url": "http://arxiv.org/abs/2512.16841v1",
        "published_date": "2025-12-18T18:17:57+00:00",
        "updated_date": "2025-12-18T18:17:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Emmanuel D. Muñiz-De-León",
            "Jorge A. Rosales-de-Golferichs",
            "Ana S. Muñoz-Rodríguez",
            "Alejandro I. Trejo-Castro",
            "Eduardo de Avila-Armenta",
            "Antonio Martínez-Torteya"
        ],
        "tldr": "This paper introduces a compact image-to-text architecture for generating chest X-ray reports, using a frozen DINOv3 ViT encoder and a GPT-2 decoder with layer-wise anatomical attention based on lung and heart segmentation masks, achieving significant performance gains on the MIMIC-CXR dataset.",
        "tldr_zh": "该论文介绍了一种紧凑的图像到文本架构，用于生成胸部X光报告，使用冻结的DINOv3 ViT编码器和GPT-2解码器，并结合基于肺和心脏分割掩码的分层解剖注意力，在MIMIC-CXR数据集上实现了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]