[
    {
        "title": "Dynamic Reflections: Probing Video Representations with Text Alignment",
        "summary": "The alignment of representations from different modalities has recently been\nshown to provide insights on the structural similarities and downstream\ncapabilities of different encoders across diverse data types. While significant\nprogress has been made in aligning images with text, the temporal nature of\nvideo data remains largely unexplored in this context. In this work, we conduct\nthe first comprehensive study of video-text representation alignment, probing\nthe capabilities of modern video and language encoders. Our findings reveal\nseveral key insights. First, we demonstrate that cross-modal alignment highly\ndepends on the richness of both visual (static images vs. multi-frame videos)\nand text (single caption vs. a collection) data provided at test time,\nespecially when using state-of-the-art video encoders. We propose parametric\ntest-time scaling laws that capture this behavior and show remarkable\npredictive power against empirical observations. Secondly, we investigate the\ncorrelation between semantic alignment and performance on both semantic and\nnon-semantic downstream tasks, providing initial evidence that strong alignment\nagainst text encoders may be linked to general-purpose video representation and\nunderstanding. Finally, we correlate temporal reasoning with cross-modal\nalignment providing a challenging test-bed for vision and language models.\nOverall, our work introduces video-text alignment as an informative zero-shot\nway to probe the representation power of different encoders for spatio-temporal\ndata. Project page can be found at https://video-prh.github.io/",
        "url": "http://arxiv.org/abs/2511.02767v1",
        "published_date": "2025-11-04T17:52:14+00:00",
        "updated_date": "2025-11-04T17:52:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tyler Zhu",
            "Tengda Han",
            "Leonidas Guibas",
            "Viorica Pătrăucean",
            "Maks Ovsjanikov"
        ],
        "tldr": "This paper introduces a comprehensive study of video-text representation alignment, revealing insights into the capabilities of video and language encoders and their correlation with downstream task performance and temporal reasoning.",
        "tldr_zh": "本文对视频-文本表征对齐进行了全面研究，揭示了视频和语言编码器的能力，以及它们与下游任务性能和时间推理的相关性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models",
        "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
        "url": "http://arxiv.org/abs/2511.02650v1",
        "published_date": "2025-11-04T15:17:06+00:00",
        "updated_date": "2025-11-04T15:17:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianfan Peng",
            "Yuntao Du",
            "Pengzhou Ji",
            "Shijie Dong",
            "Kailin Jiang",
            "Mingchuan Ma",
            "Yijun Tian",
            "Jinhe Bi",
            "Qian Li",
            "Wei Du",
            "Feng Xiao",
            "Lizhen Cui"
        ],
        "tldr": "The paper introduces UniPruneBench, a unified benchmark for evaluating visual token pruning methods in LMMs, revealing insights about pruning strategies and their impact on performance and efficiency across various tasks and models.",
        "tldr_zh": "该论文介绍了 UniPruneBench，一个统一的基准测试，用于评估 LMM 中视觉 token 剪枝方法，揭示了关于剪枝策略及其对各种任务和模型性能与效率的影响的见解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniChange: Unifying Change Detection with Multimodal Large Language Model",
        "summary": "Change detection (CD) is a fundamental task for monitoring and analyzing land\ncover dynamics. While recent high performance models and high quality datasets\nhave significantly advanced the field, a critical limitation persists. Current\nmodels typically acquire limited knowledge from single-type annotated data and\ncannot concurrently leverage diverse binary change detection (BCD) and semantic\nchange detection (SCD) datasets. This constraint leads to poor generalization\nand limited versatility. The recent advancements in Multimodal Large Language\nModels (MLLMs) introduce new possibilities for a unified CD framework. We\nleverage the language priors and unification capabilities of MLLMs to develop\nUniChange, the first MLLM-based unified change detection model. UniChange\nintegrates generative language abilities with specialized CD functionalities.\nOur model successfully unifies both BCD and SCD tasks through the introduction\nof three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange\nutilizes text prompts to guide the identification of change categories,\neliminating the reliance on predefined classification heads. This design allows\nUniChange to effectively acquire knowledge from multi-source datasets, even\nwhen their class definitions conflict. Experiments on four public benchmarks\n(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,\nachieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,\nsurpassing all previous methods. The code is available at\nhttps://github.com/Erxucomeon/UniChange.",
        "url": "http://arxiv.org/abs/2511.02607v1",
        "published_date": "2025-11-04T14:31:06+00:00",
        "updated_date": "2025-11-04T14:31:06+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Xu Zhang",
            "Danyang Li",
            "Xiaohang Dong",
            "Tianhao Wu",
            "Hualong Yu",
            "Jianye Wang",
            "Qicheng Li",
            "Xiang Li"
        ],
        "tldr": "The paper introduces UniChange, a novel MLLM-based unified change detection model that achieves state-of-the-art performance on multiple datasets by unifying binary and semantic change detection tasks with special tokens and text prompts.",
        "tldr_zh": "本文介绍了一种名为UniChange的新型基于MLLM的统一变化检测模型，该模型通过使用特殊标记和文本提示统一二元和语义变化检测任务，从而在多个数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
        "summary": "Complex chart understanding tasks demand advanced visual recognition and\nreasoning capabilities from multimodal large language models (MLLMs). However,\ncurrent research provides limited coverage of complex chart scenarios and\ncomputation-intensive reasoning tasks prevalent in real-world applications.\nThis study proposes an automated multi-stage code-driven pipeline for\nsystematically generating visual reasoning datasets to address these\nlimitations. The pipeline integrates retrieval-augmented generation (RAG) to\nretrieve professional chart templates and employs chain-of-thought (CoT)\nstrategies to generate reasoning codes that simulate real data distributions,\nthereby driving chart rendering and question-related statistical computations.\nThrough model-based evaluation, the pipeline enhances chart diversity and data\nquality. Using this framework, we construct ChartM$^3$, a multi-dimensional and\nmulti-step dataset containing 38K charts and 142K Q&A pairs for training, along\nwith 2,871 high-quality evaluation samples for enabling practical performance\nassessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)\nexperiments demonstrate that our dataset significantly improves reasoning\ncapabilities and cross-domain generalization performance, enabling smaller\nmodels to achieve performance comparable to larger-scale models in complex\nchart comprehension.",
        "url": "http://arxiv.org/abs/2511.02415v1",
        "published_date": "2025-11-04T09:45:34+00:00",
        "updated_date": "2025-11-04T09:45:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Duo Xu",
            "Hao Cheng",
            "Xin Lin",
            "Zhen Xie",
            "Hao Wang"
        ],
        "tldr": "The paper introduces ChartM$^3$, a large-scale, multi-dimensional chart understanding dataset generated through a novel code-driven pipeline, demonstrating improved reasoning and generalization capabilities for smaller VLM models.",
        "tldr_zh": "该论文介绍了ChartM$^3$，一个大规模、多维的图表理解数据集，通过一种新颖的代码驱动管道生成，展示了小型VLM模型在推理和泛化能力方面的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning",
        "summary": "Large-scale chemical reaction datasets are crucial for AI research in\nchemistry. However, existing chemical reaction data often exist as images\nwithin papers, making them not machine-readable and unusable for training\nmachine learning models. In response to this challenge, we propose the\nRxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP).\nOur framework reformulates the traditional coordinate prediction driven parsing\nprocess into an image captioning problem, which Large Vision-Language Models\n(LVLMs) handle naturally. We introduce a strategy termed \"BBox and Index as\nVisual Prompt\" (BIVP), which uses our state-of-the-art molecular detector,\nMolYOLO, to pre-draw molecular bounding boxes and indices directly onto the\ninput image. This turns the downstream parsing into a natural-language\ndescription problem. Extensive experiments show that the BIVP strategy\nsignificantly improves structural extraction quality while simplifying model\ndesign. We further construct the RxnCaption-11k dataset, an order of magnitude\nlarger than prior real-world literature benchmarks, with a balanced test subset\nacross four layout archetypes. Experiments demonstrate that RxnCaption-VL\nachieves state-of-the-art performance on multiple metrics. We believe our\nmethod, dataset, and models will advance structured information extraction from\nchemical literature and catalyze broader AI applications in chemistry. We will\nrelease data, models, and code on GitHub.",
        "url": "http://arxiv.org/abs/2511.02384v1",
        "published_date": "2025-11-04T09:08:44+00:00",
        "updated_date": "2025-11-04T09:08:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahe Song",
            "Chuang Wang",
            "Bowen Jiang",
            "Yinfan Wang",
            "Hao Zheng",
            "Xingjian Wei",
            "Chengjin Liu",
            "Junyuan Gao",
            "Yubin Wang",
            "Lijun Wu",
            "Jiang Wu",
            "Qian Yu",
            "Conghui He"
        ],
        "tldr": "This paper introduces RxnCaption, a framework that reformulates chemical Reaction Diagram Parsing as a visual prompt guided captioning problem, leveraging Large Vision-Language Models and a new large-scale dataset, RxnCaption-11k, to achieve state-of-the-art performance.",
        "tldr_zh": "该论文提出了 RxnCaption 框架，将化学反应图解析重新定义为视觉提示引导的图像描述问题，利用大型视觉语言模型和一个新的大规模数据集 RxnCaption-11k，实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning",
        "summary": "In human cognition, there exist numerous thought processes that are tacit and\nbeyond verbal expression, enabling us to understand and interact with the world\nin multiple ways. However, contemporary Vision-Language Models (VLMs) remain\nconstrained to reasoning within the discrete and rigid space of linguistic\ntokens, thereby bottlenecking the rich, high-dimensional nature of visual\nperception. To bridge this gap, we propose CoCoVa (Chain of Continuous\nVision-Language Thought), a novel framework for vision-language model that\nleverages continuous cross-modal reasoning for diverse vision-language tasks.\nThe core of CoCoVa is an iterative reasoning cycle, where a novel Latent\nQ-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a\nchain of latent thought vectors through cross-modal fusion. To focus this\nprocess, a token selection mechanism dynamically identifies salient visual\nregions, mimicking attentional focus. To ensure these latent thoughts remain\ngrounded, we train the model with a multi-task objective that combines\ncontrastive learning and diffusion-based reconstruction, enforcing alignment\nbetween latent representations and both visual and textual modalities.\nEvaluations show CoCoVa improves accuracy and token efficiency over strong\nbaselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B\nmodels on almost all benchmarks. When scaled to 7B LLM backbones, it remains\ncompetitive with state-of-the-art models. Qualitative analysis validates that\nlearned latent space captures interpretable and structured reasoning patterns,\nhighlighting the potential of CoCoVa to bridge the representational gap between\ndiscrete language processing and the continuous nature of visual understanding.",
        "url": "http://arxiv.org/abs/2511.02360v1",
        "published_date": "2025-11-04T08:28:46+00:00",
        "updated_date": "2025-11-04T08:28:46+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jizheng Ma",
            "Xiaofei Zhou",
            "Yanlong Song",
            "Han Yan"
        ],
        "tldr": "The paper introduces CoCoVa, a novel VLM framework that uses continuous latent space reasoning and a Latent Q-Former for improved cross-modal understanding and performance, achieving comparable results to larger models with better token efficiency.",
        "tldr_zh": "该论文介绍了CoCoVa，一种新型VLM框架，它使用连续潜在空间推理和潜在Q-Former来提高跨模态理解和性能，并以更高的token效率实现了与更大模型相当的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning",
        "summary": "We introduce SAIL-RL, a reinforcement learning (RL) post-training framework\nthat enhances the reasoning capabilities of multimodal large language models\n(MLLMs) by teaching them when and how to think. Existing approaches are limited\nby outcome-only supervision, which rewards correct answers without ensuring\nsound reasoning, and by uniform thinking strategies, which often lead to\noverthinking on simple tasks and underthinking on complex ones. SAIL-RL\naddresses these challenges with a dual reward system: the Thinking Reward,\nwhich evaluates reasoning quality through factual grounding, logical coherence,\nand answer consistency, and the Judging Reward, which adaptively determines\nwhether deep reasoning or direct answering is appropriate. Experiments on the\nstate-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal\nunderstanding benchmarks at both 4B and 8B scales, achieving competitive\nperformance against commercial closed-source models such as GPT-4o, and\nsubstantially reduces hallucinations, establishing it as a principled framework\nfor building more reliable and adaptive MLLMs. The code will be available at\nhttps://github.com/BytedanceDouyinContent/SAIL-RL.",
        "url": "http://arxiv.org/abs/2511.02280v1",
        "published_date": "2025-11-04T05:34:06+00:00",
        "updated_date": "2025-11-04T05:34:06+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Fangxun Shu",
            "Yongjie Ye",
            "Yue Liao",
            "Zijian Kang",
            "Weijie Yin",
            "Jiacong Wang",
            "Xiao Liang",
            "Shuicheng Yan",
            "Chao Feng"
        ],
        "tldr": "SAIL-RL is a reinforcement learning framework that enhances MLLM reasoning by using a dual-reward system to teach models when and how to think, improving performance and reducing hallucinations.",
        "tldr_zh": "SAIL-RL是一个强化学习框架，通过双重奖励系统来增强多模态大语言模型的推理能力，教导模型何时以及如何思考，从而提高性能并减少幻觉。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models",
        "summary": "Understanding and predicting emotion from videos has gathered significant\nattention in recent studies, driven by advancements in video large language\nmodels (VideoLLMs). While advanced methods have made progress in video emotion\nanalysis, the intrinsic nature of emotions poses significant challenges.\nEmotions are characterized by dynamic and cues-dependent properties, making it\ndifficult to understand complex and evolving emotional states with reasonable\nrationale. To tackle these challenges, we propose a novel affective cues-guided\nreasoning framework that unifies fundamental attribute perception, expression\nanalysis, and high-level emotional understanding in a stage-wise manner. At the\ncore of our approach is a family of video emotion foundation models (VidEmo),\nspecifically designed for emotion reasoning and instruction-following. These\nmodels undergo a two-stage tuning process: first, curriculum emotion learning\nfor injecting emotion knowledge, followed by affective-tree reinforcement\nlearning for emotion reasoning. Moreover, we establish a foundational data\ninfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)\nconsisting of 2.1M diverse instruction-based samples. Emo-CFG includes\nexplainable emotional question-answering, fine-grained captions, and associated\nrationales, providing essential resources for advancing emotion understanding\ntasks. Experimental results demonstrate that our approach achieves competitive\nperformance, setting a new milestone across 15 face perception tasks.",
        "url": "http://arxiv.org/abs/2511.02712v1",
        "published_date": "2025-11-04T16:31:09+00:00",
        "updated_date": "2025-11-04T16:31:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhicheng Zhang",
            "Weicheng Wang",
            "Yongjie Zhu",
            "Wenyu Qin",
            "Pengfei Wan",
            "Di Zhang",
            "Jufeng Yang"
        ],
        "tldr": "The paper introduces VidEmo, a video emotion foundation model, along with a new dataset (Emo-CFG) and a novel affective-tree reasoning framework for improved video emotion analysis, demonstrating competitive performance across various face perception tasks.",
        "tldr_zh": "该论文介绍了VidEmo，一个视频情感基础模型，以及一个新的数据集(Emo-CFG)和一个新颖的情感树推理框架，用于改进视频情感分析，并在各种面部感知任务中表现出竞争性的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework",
        "summary": "Medical Report Generation (MRG) is a key part of modern medical diagnostics,\nas it automatically generates reports from radiological images to reduce\nradiologists' burden. However, reliable MRG models for lesion description face\nthree main challenges: insufficient domain knowledge understanding, poor\ntext-visual entity embedding alignment, and spurious correlations from\ncross-modal biases. Previous work only addresses single challenges, while this\npaper tackles all three via a novel hierarchical task decomposition approach,\nproposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into\nlow-, mid-, and high-level tasks: 1) Low-level: align medical entity features\nwith spatial locations to enhance domain knowledge for visual encoders; 2)\nMid-level: use Prefix Language Modeling (text) and Masked Image Modeling\n(images) to boost cross-modal alignment via mutual guidance; 3) High-level: a\ncross-modal causal intervention module (via front-door intervention) to reduce\nconfounders and improve interpretability. Extensive experiments confirm\nHTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA)\nMRG methods. Code will be made public upon paper acceptance.",
        "url": "http://arxiv.org/abs/2511.02271v1",
        "published_date": "2025-11-04T05:24:52+00:00",
        "updated_date": "2025-11-04T05:24:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yucheng Song",
            "Yifan Ge",
            "Junhao Li",
            "Zhining Liao",
            "Zhifang Liao"
        ],
        "tldr": "This paper introduces a Hierarchical Task Structure-based Cross-Modal Causal Intervention Framework (HTSC-CIF) for medical report generation, addressing domain knowledge, cross-modal alignment, and spurious correlation challenges.",
        "tldr_zh": "该论文介绍了一种基于分层任务结构的跨模态因果干预框架(HTSC-CIF)，用于医学报告生成，解决了领域知识、跨模态对齐和虚假相关性的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models",
        "summary": "In this technical report, we introduce a framework to address Grounded Video\nQuestion Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The\nGVQA task demands robust multimodal models capable of complex reasoning over\nvideo content, grounding the resulting answers visually, and tracking the\nreferenced objects temporally. To achieve this capability, our proposed\napproach decomposes the GVQA task into a three-stage pipeline: (1) Video\nReasoning \\& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key\ncontribution is the introduction of a trigger moment, derived from our proposed\nCORTEX prompt, which pinpoints the single most visible frame of a target object\nto serve as a robust anchor for grounding and tracking. To this end, we achieve\nthe HOTA score of 0.4968, which marks a significant improvement over the\nprevious year's winning score of 0.2704 on GVQA task.",
        "url": "http://arxiv.org/abs/2511.02182v1",
        "published_date": "2025-11-04T01:50:19+00:00",
        "updated_date": "2025-11-04T01:50:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinhwan Seo",
            "Yoonki Cho",
            "Junhyug Noh",
            "Sung-eui Yoon"
        ],
        "tldr": "This paper introduces a three-stage pipeline for Grounded Video Question Answering (GVQA) that utilizes a 'trigger moment' derived from a CORTEX prompt to enhance spatio-temporal grounding and tracking, achieving a significant improvement over previous results on the GVQA task.",
        "tldr_zh": "本文介绍了一个用于Grounded Video Question Answering (GVQA) 的三阶段流程，该流程利用从CORTEX提示中获得的“触发时刻”来增强时空定位和跟踪，并在GVQA任务上取得了比以往结果显着提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images",
        "summary": "The detection of Protected Health Information (PHI) in medical imaging is\ncritical for safeguarding patient privacy and ensuring compliance with\nregulatory frameworks. Traditional detection methodologies predominantly\nutilize Optical Character Recognition (OCR) models in conjunction with named\nentity recognition. However, recent advancements in Large Multimodal Model\n(LMM) present new opportunities for enhanced text extraction and semantic\nanalysis. In this study, we systematically benchmark three prominent closed and\nopen-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing\ntwo distinct pipeline configurations: one dedicated to text analysis alone and\nanother integrating both OCR and semantic analysis. Our results indicate that\nLMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to\nconventional models like EasyOCR. However, this improvement in OCR performance\ndoes not consistently correlate with enhanced overall PHI detection accuracy.\nThe strongest performance gains are observed on test cases with complex imprint\npatterns. In scenarios where text regions are well readable with sufficient\ncontrast, and strong LMMs are employed for text analysis after OCR, different\npipeline configurations yield similar results. Furthermore, we provide\nempirically grounded recommendations for LMM selection tailored to specific\noperational constraints and propose a deployment strategy that leverages\nscalable and modular infrastructure.",
        "url": "http://arxiv.org/abs/2511.02014v1",
        "published_date": "2025-11-03T19:39:13+00:00",
        "updated_date": "2025-11-03T19:39:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tuan Truong",
            "Guillermo Jimenez Perez",
            "Pedro Osorio",
            "Matthias Lenga"
        ],
        "tldr": "This paper benchmarks several Large Multimodal Models (LMMs) for detecting Protected Health Information (PHI) in medical images, finding superior OCR performance but inconsistent gains in overall PHI detection accuracy, especially with complex imprint patterns.",
        "tldr_zh": "该论文对几种大型多模态模型 (LMM) 在医学图像中检测受保护健康信息 (PHI) 的能力进行了基准测试，发现其 OCR 性能优越，但整体 PHI 检测准确性提升并不稳定，尤其是在复杂印记模式下。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art",
        "summary": "The ability to connect visual patterns with the processes that form them\nrepresents one of the deepest forms of visual understanding. Textures of clouds\nand waves, the growth of cities and forests, or the formation of materials and\nlandscapes are all examples of patterns emerging from underlying mechanisms. We\npresent the Scitextures dataset, a large-scale collection of textures and\nvisual patterns from all domains of science, tech, and art, along with the\nmodels and code that generate these images. Covering over 1,200 different\nmodels and 100,000 images of patterns and textures from physics, chemistry,\nbiology, sociology, technology, mathematics, and art, this dataset offers a way\nto explore the connection between the visual patterns that shape our world and\nthe mechanisms that produce them. Created by an agentic AI pipeline that\nautonomously collects and implements models in standardized form, we use\nSciTextures to evaluate the ability of leading AI models to link visual\npatterns to the models and code that generate them, and to identify different\npatterns that emerged from the same process. We also test AIs ability to infer\nand recreate the mechanisms behind visual patterns by providing a natural image\nof a real-world pattern and asking the AI to identify, model, and code the\nmechanism that formed the pattern, then run this code to generate a simulated\nimage that is compared to the real image. These benchmarks show that\nvision-language models (VLMs) can understand and simulate the physical system\nbeyond a visual pattern. The dataset and code are available at:\nhttps://zenodo.org/records/17485502",
        "url": "http://arxiv.org/abs/2511.01817v1",
        "published_date": "2025-11-03T18:22:11+00:00",
        "updated_date": "2025-11-03T18:22:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sagi Eppel",
            "Alona Strugatski"
        ],
        "tldr": "The paper introduces SciTextures, a large-scale dataset of textures and visual patterns across science, tech, and art, along with associated models and code, and benchmarks vision-language models on tasks like linking visual patterns to their generating mechanisms and recreating mechanisms from real-world images.",
        "tldr_zh": "该论文介绍了 SciTextures，一个大规模的跨科学、技术和艺术的纹理和视觉模式数据集，以及相关的模型和代码。论文还对视觉语言模型进行了基准测试，任务包括将视觉模式与其生成机制联系起来，以及从真实世界的图像中重建机制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics",
        "summary": "Video Understanding, Scene Interpretation and Commonsense Reasoning are\nhighly challenging tasks enabling the interpretation of visual information,\nallowing agents to perceive, interact with and make rational decisions in its\nenvironment. Large Language Models (LLMs) and Visual Language Models (VLMs)\nhave shown remarkable advancements in these areas in recent years, enabling\ndomain-specific applications as well as zero-shot open vocabulary tasks,\ncombining multiple domains. However, the required computational complexity\nposes challenges for their application on edge devices and in the context of\nMobile Robotics, especially considering the trade-off between accuracy and\ninference time. In this paper, we investigate the capabilities of\nstate-of-the-art VLMs for the task of Scene Interpretation and Action\nRecognition, with special regard to small VLMs capable of being deployed to\nedge devices in the context of Mobile Robotics. The proposed pipeline is\nevaluated on a diverse dataset consisting of various real-world cityscape,\non-campus and indoor scenarios. The experimental evaluation discusses the\npotential of these small models on edge devices, with particular emphasis on\nchallenges, weaknesses, inherent model biases and the application of the gained\ninformation. Supplementary material is provided via the following repository:\nhttps://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/",
        "url": "http://arxiv.org/abs/2511.02427v1",
        "published_date": "2025-11-04T09:58:29+00:00",
        "updated_date": "2025-11-04T09:58:29+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Nicolas Schuler",
            "Lea Dewald",
            "Nick Baldig",
            "Jürgen Graf"
        ],
        "tldr": "This paper evaluates the performance of small VLMs on edge devices for scene interpretation and action recognition in mobile robotics, highlighting challenges and biases in real-world scenarios.",
        "tldr_zh": "本文评估了小型 VLM 在移动机器人中边缘设备上进行场景解释和动作识别的性能，强调了真实场景中的挑战和偏差。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution",
        "summary": "Retrieval-Augmented Generation (RAG) has become a robust framework for\nenhancing Large Language Models (LLMs) with external knowledge. Recent advances\nin RAG have investigated graph based retrieval for intricate reasoning;\nhowever, the influence of prompt design on enhancing the retrieval and\nreasoning process is still considerably under-examined. In this paper, we\npresent a prompt-driven GraphRAG framework that underscores the significance of\nprompt formulation in facilitating entity extraction, fact selection, and\npassage reranking for multi-hop question answering. Our approach creates a\nsymbolic knowledge graph from text data by encoding entities and factual\nrelationships as structured facts triples. We use LLMs selectively during\nonline retrieval to perform semantic filtering and answer generation. We also\nuse entity-guided graph traversal through Personalized PageRank (PPR) to\nsupport efficient, scalable retrieval based on the knowledge graph we built.\nOur system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,\nwith F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,\nrespectively. These results show that prompt design is an important part of\nimproving retrieval accuracy and response quality. This research lays the\ngroundwork for more efficient and comprehensible multi-hop question-answering\nsystems, highlighting the importance of prompt-aware graph reasoning.",
        "url": "http://arxiv.org/abs/2511.01802v1",
        "published_date": "2025-11-03T18:00:56+00:00",
        "updated_date": "2025-11-03T18:00:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tejas Sarnaik",
            "Manan Shah",
            "Ravi Hegde"
        ],
        "tldr": "This paper introduces PROPEX-RAG, a prompt-driven GraphRAG framework that improves multi-hop question answering by emphasizing prompt design for entity extraction, fact selection, and passage reranking. It achieves state-of-the-art results on HotpotQA and 2WikiMultiHopQA.",
        "tldr_zh": "该论文介绍了PROPEX-RAG，一个由提示驱动的GraphRAG框架，通过强调提示设计来改进多跳问答，用于实体提取、事实选择和段落重排序。 它在HotpotQA和2WikiMultiHopQA上取得了最先进的成果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]