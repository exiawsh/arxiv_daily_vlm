[
    {
        "title": "LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit",
        "summary": "Large Vision-Language Models (VLMs) exhibit impressive multi-modal\ncapabilities but suffer from prohibitive computational and memory demands, due\nto their long visual token sequences and massive parameter sizes. To address\nthese issues, recent works have proposed training-free compression methods.\nHowever, existing efforts often suffer from three major limitations: (1)\nCurrent approaches do not decompose techniques into comparable modules,\nhindering fair evaluation across spatial and temporal redundancy. (2)\nEvaluation confined to simple single-turn tasks, failing to reflect performance\nin realistic scenarios. (3) Isolated use of individual compression techniques,\nwithout exploring their joint potential. To overcome these gaps, we introduce\nLLMC+, a comprehensive VLM compression benchmark with a versatile,\nplug-and-play toolkit. LLMC+ supports over 20 algorithms across five\nrepresentative VLM families and enables systematic study of token-level and\nmodel-level compression. Our benchmark reveals that: (1) Spatial and temporal\nredundancies demand distinct technical strategies. (2) Token reduction methods\ndegrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)\nCombining token and model compression achieves extreme compression with minimal\nperformance loss. We believe LLMC+ will facilitate fair evaluation and inspire\nfuture research in efficient VLM. Our code is available at\nhttps://github.com/ModelTC/LightCompress.",
        "url": "http://arxiv.org/abs/2508.09981v1",
        "published_date": "2025-08-13T17:54:49+00:00",
        "updated_date": "2025-08-13T17:54:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengtao Lv",
            "Bilang Zhang",
            "Yang Yong",
            "Ruihao Gong",
            "Yushi Huang",
            "Shiqiao Gu",
            "Jiajun Wu",
            "Yumeng Shi",
            "Jinyang Guo",
            "Wenya Wang"
        ],
        "tldr": "The paper introduces LLMC+, a VLM compression benchmark and toolkit addressing limitations in existing compression methods by providing modular evaluation, realistic task scenarios, and combined compression techniques, achieving significant compression with minimal performance loss.",
        "tldr_zh": "该论文介绍了LLMC+，一个VLM压缩基准测试和工具包，通过提供模块化评估、真实的任务场景和组合压缩技术，解决了现有压缩方法的局限性，实现了显著的压缩且性能损失最小。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification",
        "summary": "Recent advances in histopathology vision-language foundation models (VLFMs)\nhave shown promise in addressing data scarcity for whole slide image (WSI)\nclassification via zero-shot adaptation. However, these methods remain\noutperformed by conventional multiple instance learning (MIL) approaches\ntrained on large datasets, motivating recent efforts to enhance VLFM-based WSI\nclassification through fewshot learning paradigms. While existing few-shot\nmethods improve diagnostic accuracy with limited annotations, their reliance on\nconventional classifier designs introduces critical vulnerabilities to data\nscarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)\ncomprising two core components: (1) a meta-learner that automatically optimizes\na classifier configuration from a mixture of candidate classifiers and (2) a\nclassifier bank housing diverse candidate classifiers to enable a holistic\npathological interpretation. Extensive experiments demonstrate that MOC\noutperforms prior arts in multiple few-shot benchmarks. Notably, on the\nTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art\nfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,\noffering a critical advancement for clinical deployments where diagnostic\ntraining data is severely limited. Code is available at\nhttps://github.com/xmed-lab/MOC.",
        "url": "http://arxiv.org/abs/2508.09967v1",
        "published_date": "2025-08-13T17:32:42+00:00",
        "updated_date": "2025-08-13T17:32:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianqi Xiang",
            "Yi Li",
            "Qixiang Zhang",
            "Xiaomeng Li"
        ],
        "tldr": "The paper introduces a Meta-Optimized Classifier (MOC) for few-shot whole slide image classification, which leverages a meta-learner and a diverse classifier bank to improve diagnostic accuracy, demonstrating significant performance gains over existing methods in limited data scenarios.",
        "tldr_zh": "本文介绍了一种用于小样本全切片图像分类的元优化分类器（MOC），该分类器利用元学习器和多样化的分类器库来提高诊断准确性，并在有限数据场景中展示了相对于现有方法的显著性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis",
        "summary": "Progress in AI for automated nutritional analysis is critically hampered by\nthe lack of standardized evaluation methodologies and high-quality, real-world\nbenchmark datasets. To address this, we introduce three primary contributions.\nFirst, we present the January Food Benchmark (JFB), a publicly available\ncollection of 1,000 food images with human-validated annotations. Second, we\ndetail a comprehensive benchmarking framework, including robust metrics and a\nnovel, application-oriented overall score designed to assess model performance\nholistically. Third, we provide baseline results from both general-purpose\nVision-Language Models (VLMs) and our own specialized model,\njanuary/food-vision-v1. Our evaluation demonstrates that the specialized model\nachieves an Overall Score of 86.2, a 12.1-point improvement over the\nbest-performing general-purpose configuration. This work offers the research\ncommunity a valuable new evaluation dataset and a rigorous framework to guide\nand benchmark future developments in automated nutritional analysis.",
        "url": "http://arxiv.org/abs/2508.09966v1",
        "published_date": "2025-08-13T17:32:40+00:00",
        "updated_date": "2025-08-13T17:32:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Amir Hosseinian",
            "Ashkan Dehghani Zahedani",
            "Umer Mansoor",
            "Noosheen Hashemi",
            "Mark Woodward"
        ],
        "tldr": "The paper introduces the January Food Benchmark (JFB), a public dataset and evaluation suite for multimodal food analysis, along with a comprehensive benchmarking framework and baseline results.",
        "tldr_zh": "该论文介绍了 January Food Benchmark (JFB)，一个用于多模态食物分析的公共数据集和评估套件，以及一个全面的基准测试框架和基线结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models",
        "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
        "url": "http://arxiv.org/abs/2508.09945v1",
        "published_date": "2025-08-13T17:00:44+00:00",
        "updated_date": "2025-08-13T17:00:44+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Lingjie Jiang",
            "Shaohan Huang",
            "Xun Wu",
            "Yixia Li",
            "Dongdong Zhang",
            "Furu Wei"
        ],
        "tldr": "VisCodex is a unified framework that merges vision and coding language models to generate code from multimodal inputs, achieving state-of-the-art performance among open-source MLLMs; they also introduce a new Multimodal Coding Dataset (MCD) and a benchmark InfiBench-V.",
        "tldr_zh": "VisCodex是一个统一框架，它融合了视觉和编码语言模型，可以从多模态输入生成代码，并在开源 MLLM 中实现了最先进的性能。他们还引入了一个新的多模态编码数据集 (MCD) 和一个基准 InfiBench-V。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video",
        "summary": "This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation.",
        "url": "http://arxiv.org/abs/2508.09818v1",
        "published_date": "2025-08-13T13:54:16+00:00",
        "updated_date": "2025-08-13T13:54:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rajan Das Gupta",
            "Md Yeasin Rahat",
            "Nafiz Fahad",
            "Abir Ahmed",
            "Liew Tze Hui"
        ],
        "tldr": "The paper introduces ViMoNet, a multimodal vision-language framework that utilizes both motion and video data for improved human behavior understanding, along with a new dataset and benchmark.",
        "tldr_zh": "该论文介绍了一种多模态视觉-语言框架ViMoNet，它利用运动和视频数据来更好地理解人类行为，并提供了一个新的数据集和基准测试。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Evolution of Low-Level and Texture Human-CLIP Alignment",
        "summary": "During the training of multi-modal models like CLIP, we observed an\nintriguing phenomenon: the correlation with low-level human image quality\nassessments peaks in the early epochs before gradually declining. This study\ninvestigates this observation and seeks to understand its causes through two\nkey factors: shape-texture bias alignment and classification accuracy drop\nunder noise. Our findings suggest that CLIP initially learn low-level visual\nfeatures, enhancing its alignment with low-level human perception but also\nincreasing its sensitivity to noise and its texture bias. As training\nprogresses, the model shifts toward more abstract shape-based representations,\nimproving noise robustness but reducing alignment with low-level human\nperception. These results suggest that these factors shared an underlying\nlearning mechanism and provide new insights into optimizing the trade-off\nbetween perceptual alignment and robustness in vision-language models.",
        "url": "http://arxiv.org/abs/2508.09814v1",
        "published_date": "2025-08-13T13:47:34+00:00",
        "updated_date": "2025-08-13T13:47:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pablo Hernández-Cámara",
            "Jose Manuel Jaén-Lorites",
            "Jorge Vila-Tomás",
            "Jesus Malo",
            "Valero Laparra"
        ],
        "tldr": "The paper investigates how CLIP models initially align with low-level human perception before shifting towards shape-based representations during training, highlighting a trade-off between perceptual alignment and robustness.",
        "tldr_zh": "本文研究了CLIP模型在训练过程中如何从最初与人类低级感知对齐，转变为基于形状的表示，强调了感知对齐和鲁棒性之间的权衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations",
        "summary": "Existing video recommender systems rely primarily on user-defined metadata or\non low-level visual and acoustic signals extracted by specialised encoders.\nThese low-level features describe what appears on the screen but miss deeper\nsemantics such as intent, humour, and world knowledge that make clips resonate\nwith viewers. For example, is a 30-second clip simply a singer on a rooftop, or\nan ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such\ndistinctions are critical to personalised recommendations yet remain invisible\nto traditional encoding pipelines. In this paper, we introduce a simple,\nrecommendation system-agnostic zero-finetuning framework that injects\nhigh-level semantics into the recommendation pipeline by prompting an\noff-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip\ninto a rich natural-language description (e.g. \"a superhero parody with\nslapstick fights and orchestral stabs\"), bridging the gap between raw content\nand user intent. We use MLLM output with a state-of-the-art text encoder and\nfeed it into standard collaborative, content-based, and generative\nrecommenders. On the MicroLens-100K dataset, which emulates user interactions\nwith TikTok-style videos, our framework consistently surpasses conventional\nvideo, audio, and metadata features in five representative models. Our findings\nhighlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to\nbuild more intent-aware video recommenders.",
        "url": "http://arxiv.org/abs/2508.09789v1",
        "published_date": "2025-08-13T13:19:31+00:00",
        "updated_date": "2025-08-13T13:19:31+00:00",
        "categories": [
            "cs.IR",
            "cs.CV"
        ],
        "authors": [
            "Marco De Nadai",
            "Andreas Damianou",
            "Mounia Lalmas"
        ],
        "tldr": "This paper introduces a method to enhance video recommendations by using MLLMs to generate rich, semantic descriptions of video clips, which are then used in standard recommender systems, outperforming traditional feature-based approaches.",
        "tldr_zh": "本文介绍了一种通过使用多模态大型语言模型（MLLM）生成丰富的视频语义描述来增强视频推荐的方法。 这些描述被用于标准的推荐系统中，并优于传统的基于特征的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models",
        "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE.",
        "url": "http://arxiv.org/abs/2508.09779v1",
        "published_date": "2025-08-13T13:00:05+00:00",
        "updated_date": "2025-08-13T13:00:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dianyi Wang",
            "Siyuan Wang",
            "Zejun Li",
            "Yikun Wang",
            "Yitong Li",
            "Duyu Tang",
            "Xiaoyu Shen",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "tldr": "The paper introduces MoIIE, a Mixture of Experts architecture for LVLMs that uses intra- and inter-modality experts with a two-stage training strategy, achieving competitive performance with fewer activated parameters.",
        "tldr_zh": "该论文介绍了MoIIE，一种用于LVLM的混合专家架构，该架构使用内部和跨模态专家，并采用两阶段训练策略，以更少的激活参数实现了有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory",
        "summary": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent",
        "url": "http://arxiv.org/abs/2508.09736v1",
        "published_date": "2025-08-13T12:03:03+00:00",
        "updated_date": "2025-08-13T12:03:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lin Long",
            "Yichen He",
            "Wentao Ye",
            "Yiyuan Pan",
            "Yuan Lin",
            "Hang Li",
            "Junbo Zhao",
            "Wei Li"
        ],
        "tldr": "The paper introduces M3-Agent, a multimodal agent with long-term memory capable of processing visual and auditory inputs, building semantic knowledge, and performing reasoning tasks. They also present M3-Bench, a new long-video question answering benchmark for evaluating such agents.",
        "tldr_zh": "该论文介绍了一种名为M3-Agent的多模态智能体，它具有长期记忆，能够处理视觉和听觉输入，构建语义知识并执行推理任务。他们还提出了M3-Bench，这是一个新的长视频问答基准，用于评估此类智能体。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation",
        "summary": "The rapid growth of multimodal medical imaging data presents significant\nstorage and transmission challenges, particularly in resource-constrained\nclinical settings. We propose NEURAL, a novel framework that addresses this by\nusing semantics-guided data compression. Our approach repurposes\ncross-attention scores between the image and its radiological report from a\nfine-tuned generative vision-language model to structurally prune chest X-rays,\npreserving only diagnostically critical regions. This process transforms the\nimage into a highly compressed, graph representation. This unified graph-based\nrepresentation fuses the pruned visual graph with a knowledge graph derived\nfrom the clinical report, creating a universal data structure that simplifies\ndownstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for\npneumonia detection, NEURAL achieves a 93.4-97.7\\% reduction in image data size\nwhile maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming\nother baseline models that use uncompressed data. By creating a persistent,\ntask-agnostic data asset, NEURAL resolves the trade-off between data size and\nclinical utility, enabling efficient workflows and teleradiology without\nsacrificing performance. Our NEURAL code is available at\nhttps://github.com/basiralab/NEURAL.",
        "url": "http://arxiv.org/abs/2508.09715v1",
        "published_date": "2025-08-13T11:08:09+00:00",
        "updated_date": "2025-08-13T11:08:09+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Devvrat Joshi",
            "Islem Rekik"
        ],
        "tldr": "The paper introduces NEURAL, a framework for compressing chest X-rays using attention scores from a vision-language model to prune non-critical regions, creating a unified graph representation for efficient clinical evaluation while maintaining high diagnostic performance.",
        "tldr_zh": "本文介绍了一个名为NEURAL的框架，该框架利用视觉-语言模型的注意力分数来压缩胸部X光片，修剪非关键区域，从而创建一个统一的图表示，以实现高效的临床评估，同时保持较高的诊断性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs",
        "summary": "Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer\nfrom hallucinations, i.e., generating content inconsistent with input or\nestablished world knowledge, which correspond to faithfulness and factuality\nhallucinations, respectively. Prior studies primarily evaluate faithfulness\nhallucination at a coarse level (e.g., object-level) and lack fine-grained\nanalysis. Additionally, existing benchmarks rely on costly manual curation or\nreused public datasets, raising concerns about scalability and data leakage. To\naddress these limitations, we propose an automated data construction pipeline\nthat produces scalable, controllable, and diverse evaluation data. We also\ndesign a hierarchical hallucination induction framework with input\nperturbations to simulate realistic noisy scenarios. Integrating these designs,\nwe construct SHALE, a Scalable HALlucination Evaluation benchmark designed to\nassess both faithfulness and factuality hallucinations via a fine-grained\nhallucination categorization scheme. SHALE comprises over 30K image-instruction\npairs spanning 12 representative visual perception aspects for faithfulness and\n6 knowledge domains for factuality, considering both clean and noisy scenarios.\nExtensive experiments on over 20 mainstream LVLMs reveal significant factuality\nhallucinations and high sensitivity to semantic perturbations.",
        "url": "http://arxiv.org/abs/2508.09584v1",
        "published_date": "2025-08-13T07:58:01+00:00",
        "updated_date": "2025-08-13T07:58:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bei Yan",
            "Zhiyuan Chen",
            "Yuecong Min",
            "Jie Zhang",
            "Jiahao Wang",
            "Xiaozhen Wang",
            "Shiguang Shan"
        ],
        "tldr": "The paper introduces SHALE, a scalable benchmark for fine-grained evaluation of hallucinations in LVLMs, addressing limitations of existing benchmarks regarding scalability, data leakage, and coarse-grained analysis.",
        "tldr_zh": "该论文介绍了SHALE，一个可扩展的基准，用于对LVLM中的幻觉进行细粒度评估，解决了现有基准在可扩展性、数据泄露和粗粒度分析方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
        "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.",
        "url": "http://arxiv.org/abs/2508.09456v1",
        "published_date": "2025-08-13T03:22:19+00:00",
        "updated_date": "2025-08-13T03:22:19+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.CR"
        ],
        "authors": [
            "Junxian Li",
            "Beining Xu",
            "Di Zhang"
        ],
        "tldr": "This paper introduces IAG, a novel input-aware backdoor attack on Vision-Language Models (VLMs) for visual grounding, demonstrating its feasibility and effectiveness in manipulating grounding behavior while maintaining stealthiness.",
        "tldr_zh": "本文介绍了一种名为IAG的新型输入感知后门攻击方法，该方法针对视觉语言模型（VLM）的视觉定位任务，展示了其在操纵定位行为同时保持隐蔽性的可行性和有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.",
        "url": "http://arxiv.org/abs/2508.09987v1",
        "published_date": "2025-08-13T17:59:28+00:00",
        "updated_date": "2025-08-13T17:59:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Junyan Ye",
            "Dongzhi Jiang",
            "Zihao Wang",
            "Leqi Zhu",
            "Zhenghao Hu",
            "Zilong Huang",
            "Jun He",
            "Zhiyuan Yan",
            "Jinghua Yu",
            "Hongsheng Li",
            "Conghui He",
            "Weijia Li"
        ],
        "tldr": "This paper introduces Echo-4o, a GPT-4o-generated synthetic image dataset, to enhance open-source image generation models by addressing limitations in real-world datasets and providing cleaner supervision. It also proposes new benchmarks for evaluating image generation, showing improved performance and transferability.",
        "tldr_zh": "该论文介绍了Echo-4o，一个由GPT-4o生成的合成图像数据集，旨在通过解决真实世界数据集的局限性和提供更清晰的监督来增强开源图像生成模型。 该论文还提出了用于评估图像生成的新基准，并显示了改进的性能和可转移性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning",
        "summary": "Large language models (LLM) in natural language processing (NLP) have\ndemonstrated great potential for in-context learning (ICL) -- the ability to\nleverage a few sets of example prompts to adapt to various tasks without having\nto explicitly update the model weights. ICL has recently been explored for\ncomputer vision tasks with promising early outcomes. These approaches involve\nspecialized training and/or additional data that complicate the process and\nlimit its generalizability. In this work, we show that off-the-shelf Stable\nDiffusion models can be repurposed for visual in-context learning (V-ICL).\nSpecifically, we formulate an in-place attention re-computation within the\nself-attention layers of the Stable Diffusion architecture that explicitly\nincorporates context between the query and example prompts. Without any\nadditional fine-tuning, we show that this repurposed Stable Diffusion model is\nable to adapt to six different tasks: foreground segmentation, single object\ndetection, semantic segmentation, keypoint detection, edge detection, and\ncolorization. For example, the proposed approach improves the mean intersection\nover union (mIoU) for the foreground segmentation task on Pascal-5i dataset by\n8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,\nrespectively. Additionally, we show that the proposed method is able to\neffectively leverage multiple prompts through ensembling to infer the task\nbetter and further improve the performance.",
        "url": "http://arxiv.org/abs/2508.09949v1",
        "published_date": "2025-08-13T17:08:22+00:00",
        "updated_date": "2025-08-13T17:08:22+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Trevine Oorloff",
            "Vishwanath Sindagi",
            "Wele Gedara Chaminda Bandara",
            "Ali Shafahi",
            "Amin Ghiasi",
            "Charan Prakash",
            "Reza Ardekani"
        ],
        "tldr": "This paper demonstrates that off-the-shelf Stable Diffusion models can be adapted for visual in-context learning across various computer vision tasks without fine-tuning by reformulating attention mechanisms, achieving improvements in tasks like foreground segmentation.",
        "tldr_zh": "本文展示了现成的Stable Diffusion模型可以通过重新设计注意力机制来适应各种计算机视觉任务中的视觉上下文学习，无需微调，并在前景分割等任务中取得了改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization",
        "summary": "Visual geo-localization for drones faces critical degradation under weather\nperturbations, \\eg, rain and fog, where existing methods struggle with two\ninherent limitations: 1) Heavy reliance on limited weather categories that\nconstrain generalization, and 2) Suboptimal disentanglement of entangled\nscene-weather features through pseudo weather categories. We present\nWeatherPrompt, a multi-modality learning paradigm that establishes\nweather-invariant representations through fusing the image embedding with the\ntext context. Our framework introduces two key contributions: First, a\nTraining-free Weather Reasoning mechanism that employs off-the-shelf large\nmulti-modality models to synthesize multi-weather textual descriptions through\nhuman-like reasoning. It improves the scalability to unseen or complex weather,\nand could reflect different weather strength. Second, to better disentangle the\nscene and weather feature, we propose a multi-modality framework with the\ndynamic gating mechanism driven by the text embedding to adaptively reweight\nand fuse visual features across modalities. The framework is further optimized\nby the cross-modal objectives, including image-text contrastive learning and\nimage-text matching, which maps the same scene with different weather\nconditions closer in the respresentation space. Extensive experiments validate\nthat, under diverse weather conditions, our method achieves competitive recall\nrates compared to state-of-the-art drone geo-localization methods. Notably, it\nimproves Recall@1 by +13.37\\% under night conditions and by 18.69\\% under fog\nand snow conditions.",
        "url": "http://arxiv.org/abs/2508.09560v1",
        "published_date": "2025-08-13T07:28:41+00:00",
        "updated_date": "2025-08-13T07:28:41+00:00",
        "categories": [
            "cs.CV",
            "cs.RO",
            "I.4.10"
        ],
        "authors": [
            "Jiahao Wen",
            "Hang Yu",
            "Zhedong Zheng"
        ],
        "tldr": "The paper introduces WeatherPrompt, a multi-modal learning framework for all-weather drone visual geo-localization that leverages textual descriptions of weather conditions to improve performance under challenging conditions like rain, fog, and night. It achieves significant improvements in recall rates compared to existing methods.",
        "tldr_zh": "本文提出了一种名为WeatherPrompt的多模态学习框架，用于全天候无人机视觉地理定位。该框架利用天气状况的文本描述，提升在雨、雾和夜晚等恶劣条件下的性能。与现有方法相比，该方法在召回率方面取得了显著提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation",
        "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to follow natural language instructions through free-form 3D spaces.\nExisting VLN-CE approaches typically use a two-stage waypoint planning\nframework, where a high-level waypoint predictor generates the navigable\nwaypoints, and then a navigation planner suggests the intermediate goals in the\nhigh-level action space. However, this two-stage decomposition framework\nsuffers from: (1) global sub-optimization due to the proxy objective in each\nstage, and (2) a performance bottleneck caused by the strong reliance on the\nquality of the first-stage predicted waypoints. To address these limitations,\nwe propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE\npolicy that unifies the traditional two stages, i.e. waypoint generation and\nplanning, into a single diffusion policy. Notably, DifNav employs a conditional\ndiffusion policy to directly model multi-modal action distributions over future\nactions in continuous navigation space, eliminating the need for a waypoint\npredictor while enabling the agent to capture multiple possible\ninstruction-following behaviors. To address the issues of compounding error in\nimitation learning and enhance spatial reasoning in long-horizon navigation\ntasks, we employ DAgger for online policy training and expert trajectory\naugmentation, and use the aggregated data to further fine-tune the policy. This\napproach significantly improves the policy's robustness and its ability to\nrecover from error states. Extensive experiments on benchmark datasets\ndemonstrate that, even without a waypoint predictor, the proposed method\nsubstantially outperforms previous state-of-the-art two-stage waypoint-based\nmodels in terms of navigation performance. Our code is available at:\nhttps://github.com/Tokishx/DifNav.",
        "url": "http://arxiv.org/abs/2508.09444v1",
        "published_date": "2025-08-13T02:51:43+00:00",
        "updated_date": "2025-08-13T02:51:43+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haoxiang Shi",
            "Xiang Deng",
            "Zaijing Li",
            "Gongwei Chen",
            "Yaowei Wang",
            "Liqiang Nie"
        ],
        "tldr": "This paper introduces DAgger Diffusion Navigation (DifNav), an end-to-end VLN-CE policy that uses a conditional diffusion policy and DAgger for online training to overcome the limitations of two-stage waypoint planning frameworks, achieving state-of-the-art results.",
        "tldr_zh": "本文提出了DAgger Diffusion Navigation (DifNav)，一个端到端的VLN-CE策略，该策略使用条件扩散策略和DAgger进行在线训练，以克服两阶段航点规划框架的局限性，并实现了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
        "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
        "url": "http://arxiv.org/abs/2508.09262v1",
        "published_date": "2025-08-12T18:05:33+00:00",
        "updated_date": "2025-08-12T18:05:33+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dongwoo Kang",
            "Akhil Perincherry",
            "Zachary Coalson",
            "Aiden Gabriel",
            "Stefan Lee",
            "Sanghyun Hong"
        ],
        "tldr": "This paper introduces an input-adaptive navigation method for VLN to improve efficiency by selectively processing panoramic views, using importance-based early exits, and caching previously seen views, achieving a 2x reduction in computation.",
        "tldr_zh": "本文提出了一种用于VLN的输入自适应导航方法，通过选择性地处理全景视图、使用基于重要性的提前退出以及缓存先前看过的视图来提高效率，实现了2倍的计算量减少。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model",
        "summary": "Parotid gland lesion segmentation is essential for the treatment of parotid\ngland diseases. However, due to the variable size and complex lesion\nboundaries, accurate parotid gland lesion segmentation remains challenging.\nRecently, the Segment Anything Model (SAM) fine-tuning has shown remarkable\nperformance in the field of medical image segmentation. Nevertheless, SAM's\ninteraction segmentation model relies heavily on precise lesion prompts\n(points, boxes, masks, etc.), which are very difficult to obtain in real-world\napplications. Besides, current medical image segmentation methods are\nautomatically generated, ignoring the domain knowledge of medical experts when\nperforming segmentation. To address these limitations, we propose the parotid\ngland segment anything model (PG-SAM), an expert diagnosis text-guided SAM\nincorporating expert domain knowledge for cross-sequence parotid gland lesion\nsegmentation. Specifically, we first propose an expert diagnosis report guided\nprompt generation module that can automatically generate prompt information\ncontaining the prior domain knowledge to guide the subsequent lesion\nsegmentation process. Then, we introduce a cross-sequence attention module,\nwhich integrates the complementary information of different modalities to\nenhance the segmentation effect. Finally, the multi-sequence image features and\ngenerated prompts are feed into the decoder to get segmentation result.\nExperimental results demonstrate that PG-SAM achieves state-of-the-art\nperformance in parotid gland lesion segmentation across three independent\nclinical centers, validating its clinical applicability and the effectiveness\nof diagnostic text for enhancing image segmentation in real-world clinical\nsettings.",
        "url": "http://arxiv.org/abs/2508.09645v1",
        "published_date": "2025-08-13T09:25:29+00:00",
        "updated_date": "2025-08-13T09:25:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongyuan Wu",
            "Chuan-Xian Ren",
            "Yu Wang",
            "Xiaohua Ban",
            "Jianning Xiao",
            "Xiaohui Duan"
        ],
        "tldr": "The paper proposes PG-SAM, an expert diagnosis text-guided Segment Anything Model (SAM) for parotid gland lesion segmentation, incorporating expert domain knowledge for improved performance and clinical applicability.",
        "tldr_zh": "该论文提出了一种专家诊断文本引导的 Segment Anything 模型 (SAM) PG-SAM，用于腮腺病变分割，结合专家领域知识以提高性能和临床适用性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]