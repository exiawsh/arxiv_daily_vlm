[
    {
        "title": "VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models",
        "summary": "Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\\% on Kimi-VL and 3.09\\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.",
        "url": "http://arxiv.org/abs/2602.01037v1",
        "published_date": "2026-02-01T05:53:09+00:00",
        "updated_date": "2026-02-01T05:53:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guangshuo Qin",
            "Zhiteng Li",
            "Zheng Chen",
            "Weihang Zhang",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "tldr": "This paper introduces VEQ, a modality-adaptive quantization framework for MoE VLMs, addressing the heterogeneity between vision/language tokens and experts, achieving SOTA performance in W3A16 configuration.",
        "tldr_zh": "本文介绍了VEQ，一种针对MoE VLM的模态自适应量化框架，解决了视觉/语言tokens和专家之间的异构性问题，并在W3A16配置下实现了SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis",
        "summary": "Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.\n  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.",
        "url": "http://arxiv.org/abs/2602.01127v1",
        "published_date": "2026-02-01T09:56:25+00:00",
        "updated_date": "2026-02-01T09:56:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Matej Suchanek",
            "Klara Janouskova",
            "Ondrej Vasatko",
            "Jiri Matas"
        ],
        "tldr": "The paper introduces Koo-Fu CLIP, a supervised adaptation method for CLIP models using Fukunaga-Koontz Linear Discriminant Analysis to improve class separability and reduce dimensionality, achieving significant accuracy gains on ImageNet benchmarks.",
        "tldr_zh": "该论文介绍了Koo-Fu CLIP，一种使用Fukunaga-Koontz线性判别分析的CLIP模型的监督自适应方法，以提高类别可分离性和降低维度，在ImageNet基准测试中取得了显著的准确性提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
        "summary": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.",
        "url": "http://arxiv.org/abs/2602.01081v1",
        "published_date": "2026-02-01T07:56:10+00:00",
        "updated_date": "2026-02-01T07:56:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haitao Zhang",
            "Yingying Wang",
            "Jiaxiang Wang",
            "Haote Xu",
            "Hongyang Zhang",
            "Yirong Chen",
            "Yue Huang",
            "Xinghao Ding"
        ],
        "tldr": "The paper introduces MedAD-38K, a new large-scale benchmark for Medical Anomaly Detection, and MedAD-R1, a model using Consistency-Reinforced Policy Optimization, achieving SOTA performance with interpretable reasoning.",
        "tldr_zh": "该论文介绍了MedAD-38K，一个新的大规模医学异常检测基准，以及MedAD-R1，一个使用一致性强化策略优化的模型，通过可解释的推理实现了SOTA性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
        "summary": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.",
        "url": "http://arxiv.org/abs/2602.01047v1",
        "published_date": "2026-02-01T06:12:05+00:00",
        "updated_date": "2026-02-01T06:12:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinrong Chen",
            "Xu Chu",
            "Yingmin Qiu",
            "Hengyuan Zhang",
            "Jing Xiong",
            "Shiyu Tang",
            "Shuai Liu",
            "Shaokang Yang",
            "Cheng Yang",
            "Hayden Kwok-Hay So",
            "Ngai Wong"
        ],
        "tldr": "The paper introduces Residual Decoding (ResDec), a training-free method to mitigate hallucinations in Large Vision-Language Models by leveraging historical information and internal reasoning mechanisms to improve visual grounding and reduce object hallucinations.",
        "tldr_zh": "该论文介绍了一种名为残差解码 (ResDec) 的免训练方法，通过利用历史信息和内部推理机制来减轻大型视觉语言模型中的幻觉，从而提高视觉基础并减少对象幻觉。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \\href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.",
        "url": "http://arxiv.org/abs/2602.01025v1",
        "published_date": "2026-02-01T05:18:47+00:00",
        "updated_date": "2026-02-01T05:18:47+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Kaiyuan Cui",
            "Yige Li",
            "Yutao Wu",
            "Xingjun Ma",
            "Sarah Erfani",
            "Christopher Leckie",
            "Hanxun Huang"
        ],
        "tldr": "This paper introduces UltraBreak, a novel framework for generating universal and transferable jailbreak attacks on Vision-Language Models (VLMs) by combining vision-level regularization and semantically guided textual supervision to improve transferability and effectiveness compared to existing methods.",
        "tldr_zh": "本文介绍了一种名为UltraBreak的新框架，通过结合视觉层面的正则化和语义引导的文本监督，生成针对视觉语言模型（VLM）的通用且可转移的越狱攻击，从而提高了可转移性和有效性，优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning",
        "summary": "Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.",
        "url": "http://arxiv.org/abs/2602.01004v1",
        "published_date": "2026-02-01T03:57:45+00:00",
        "updated_date": "2026-02-01T03:57:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihao Zhao",
            "Shengting Cao",
            "Muchao Ye"
        ],
        "tldr": "The paper introduces SRVAU-R1, a reflection-aware learning framework to enhance video anomaly understanding (VAU) by incorporating self-reflection and self-correction in multi-modal large language model (MLLM) reasoning, demonstrating improvements in anomaly localization and reasoning quality.",
        "tldr_zh": "该论文介绍了SRVAU-R1，一种反射感知学习框架，通过在多模态大型语言模型（MLLM）推理中加入自我反思和自我纠正来增强视频异常理解（VAU），在异常定位和推理质量方面均有改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models",
        "summary": "Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \\textit{either} vision-encoder saliency (broad but query-agnostic) \\textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \\emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \\textbf{ConsensusDrop}, a training-free framework that derives a \\emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.",
        "url": "http://arxiv.org/abs/2602.00946v1",
        "published_date": "2026-02-01T00:28:55+00:00",
        "updated_date": "2026-02-01T00:28:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dhruv Parikh",
            "Haoyang Fan",
            "Rajgopal Kannan",
            "Viktor Prasanna"
        ],
        "tldr": "ConsensusDrop is a training-free framework that fuses visual and cross-modal saliency to efficiently prune visual tokens in VLMs, outperforming existing methods by achieving a better accuracy-efficiency trade-off.",
        "tldr_zh": "ConsensusDrop是一个无需训练的框架，融合了视觉和跨模态显著性，可以高效地剪枝VLMs中的视觉token。该方法优于现有方法，实现了更好的准确率-效率权衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom",
        "summary": "Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.",
        "url": "http://arxiv.org/abs/2602.01334v1",
        "published_date": "2026-02-01T17:00:50+00:00",
        "updated_date": "2026-02-01T17:00:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yan Ma",
            "Weiyu Zhang",
            "Tianle Li",
            "Linge Du",
            "Xuyang Shen",
            "Pengfei Liu"
        ],
        "tldr": "This paper investigates whether performance gains in vision tool-use RL are due to improved tool use or intrinsic capabilities. It finds that intrinsic learning dominates, while tool-use RL primarily reduces tool-induced harm, suggesting the models learn to coexist with tools rather than master them.",
        "tldr_zh": "本文研究了视觉工具使用强化学习中性能的提升是由于工具使用的改进还是模型自身能力的发展。研究发现，性能提升主要归功于模型自身能力的提升，而工具使用强化学习主要减少了工具引入的负面影响，表明模型学会了与工具共存，而不是掌握它们。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling",
        "summary": "Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory",
        "url": "http://arxiv.org/abs/2602.01306v1",
        "published_date": "2026-02-01T16:07:30+00:00",
        "updated_date": "2026-02-01T16:07:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayushman Sarkar",
            "Zhenyu Yu",
            "Mohd Yamani Idna Idris"
        ],
        "tldr": "DeCorStory is a training-free method for improving consistency in text-to-image storytelling by decorrelating prompt embeddings, addressing issues like color leakage and identity drift in existing methods.",
        "tldr_zh": "DeCorStory是一种无需训练的方法，通过解耦提示嵌入来提高文本到图像故事叙述的一致性，解决了现有方法中颜色泄漏和身份漂移等问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "StoryState: Agent-Based State Control for Consistent and Editable Storybooks",
        "summary": "Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState",
        "url": "http://arxiv.org/abs/2602.01305v1",
        "published_date": "2026-02-01T16:06:10+00:00",
        "updated_date": "2026-02-01T16:06:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayushman Sarkar",
            "Zhenyu Yu",
            "Wei Tang",
            "Chu Chen",
            "Kangning Cui",
            "Mohd Yamani Idna Idris"
        ],
        "tldr": "The paper introduces StoryState, an agent-based system for generating and editing consistent storybooks by maintaining an explicit story state, improving visual consistency and editability compared to existing methods.",
        "tldr_zh": "本文介绍了 StoryState，一个基于代理的系统，通过维护显式的故事状态来生成和编辑一致的绘本，与现有方法相比，提高了视觉一致性和可编辑性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation",
        "summary": "Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory",
        "url": "http://arxiv.org/abs/2602.01303v1",
        "published_date": "2026-02-01T16:04:40+00:00",
        "updated_date": "2026-02-01T16:04:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayushman Sarkar",
            "Zhenyu Yu",
            "Chu Chen",
            "Wei Tang",
            "Kangning Cui",
            "Mohd Yamani Idna Idris"
        ],
        "tldr": "ReDiStory is a training-free framework that improves visual story generation by disentangling and decorrelating text embeddings into identity-related and frame-specific components, enhancing identity consistency across frames.",
        "tldr_zh": "ReDiStory是一个免训练框架，通过将文本嵌入解耦为身份相关和帧特定成分并使它们去相关，从而改进视觉故事生成，增强跨帧的身份一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Interaction-Consistent Object Removal via MLLM-Based Reasoning",
        "summary": "Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.",
        "url": "http://arxiv.org/abs/2602.01298v1",
        "published_date": "2026-02-01T15:55:35+00:00",
        "updated_date": "2026-02-01T15:55:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ching-Kai Huang",
            "Wen-Chieh Lin",
            "Yan-Cen Lee"
        ],
        "tldr": "This paper introduces Interaction-Consistent Object Removal (ICOR), a new problem in image editing where objects are removed along with related interaction evidence using a multimodal large language model (MLLM) based framework called REORM, and provides a new benchmark ICOREval for evaluation.",
        "tldr_zh": "本文介绍了一种新的图像编辑问题，即交互一致的对象移除（ICOR），其中使用基于多模态大型语言模型（MLLM）的框架REORM移除对象以及相关的交互证据，并提供了一个新的基准ICOREval用于评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
        "summary": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.",
        "url": "http://arxiv.org/abs/2602.01200v1",
        "published_date": "2026-02-01T12:43:11+00:00",
        "updated_date": "2026-02-01T12:43:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Lai",
            "Zihang Jiang",
            "Kun Zhang",
            "Qingsong Yao",
            "Rongsheng Wang",
            "Zhiyang He",
            "Xiaodong Tao",
            "Wei Wei",
            "Shaohua Kevin Zhou"
        ],
        "tldr": "Med3D-R1 is a reinforcement learning framework that enhances 3D vision-language models for medical abnormality diagnosis by improving clinical reasoning and achieving state-of-the-art results on CT-RATE and RAD-ChestCT benchmarks.",
        "tldr_zh": "Med3D-R1是一个强化学习框架，通过改善临床推理来增强用于医学异常诊断的3D视觉-语言模型，并在CT-RATE和RAD-ChestCT基准测试中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models",
        "summary": "Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.",
        "url": "http://arxiv.org/abs/2602.01163v1",
        "published_date": "2026-02-01T11:30:03+00:00",
        "updated_date": "2026-02-01T11:30:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chunliang Hua",
            "Zeyuan Yang",
            "Lei Zhang",
            "Jiayang Sun",
            "Fengwen Chen",
            "Chunlan Zeng",
            "Xiao Hu"
        ],
        "tldr": "This paper introduces a multimodal large language model (MLLM) framework for UAV emergency landing site assessment using remote sensing imagery and point-of-interest data, demonstrating improved risk identification and interpretable justifications compared to geometric baselines. They also release a new benchmark dataset, ELSS.",
        "tldr_zh": "本文提出了一种基于遥感图像和兴趣点数据的多模态大型语言模型（MLLM）框架，用于无人机紧急着陆点评估。该框架展示了相比于几何基线方法，在风险识别和可解释性方面的改进。此外，他们还发布了一个新的基准数据集ELSS。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction",
        "summary": "Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.",
        "url": "http://arxiv.org/abs/2602.01046v1",
        "published_date": "2026-02-01T06:05:44+00:00",
        "updated_date": "2026-02-01T06:05:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiawei Lin",
            "Shizhao Sun",
            "Danqing Huang",
            "Ting Liu",
            "Ji Li",
            "Jiang Bian"
        ],
        "tldr": "The paper introduces ReLayout, a framework for versatile and structure-preserving design layout editing using relation-aware design reconstruction (RADR) and a multi-modal large language model, trained without triplet data.",
        "tldr_zh": "该论文介绍了ReLayout，一个通用且能保留结构的版面设计编辑框架，它利用关系感知的版面重建（RADR）和一个多模态大型语言模型，无需三元组数据进行训练。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]