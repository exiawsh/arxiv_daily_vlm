[
    {
        "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
        "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
        "url": "http://arxiv.org/abs/2510.05094v1",
        "published_date": "2025-10-06T17:57:59+00:00",
        "updated_date": "2025-10-06T17:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziqi Huang",
            "Ning Yu",
            "Gordon Chen",
            "Haonan Qiu",
            "Paul Debevec",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces VChain, a framework that uses multimodal models to inject visual reasoning into video generation by generating keyframes that guide the tuning of a pre-trained video generator, leading to improved video quality.",
        "tldr_zh": "该论文介绍了VChain，一种利用多模态模型将视觉推理注入视频生成中的框架，它通过生成关键帧来指导预训练视频生成器的调整，从而提高视频质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals",
        "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.",
        "url": "http://arxiv.org/abs/2510.05091v1",
        "published_date": "2025-10-06T17:56:55+00:00",
        "updated_date": "2025-10-06T17:56:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Le Zhuo",
            "Songhao Han",
            "Yuandong Pu",
            "Boxiang Qiu",
            "Sayak Paul",
            "Yue Liao",
            "Yihao Liu",
            "Jie Shao",
            "Xi Chen",
            "Si Liu",
            "Hongsheng Li"
        ],
        "tldr": "The paper introduces a new dataset, model, and benchmark (StructBench) for generating and editing structured visuals (charts, diagrams, etc.) with improved factual accuracy, addressing a gap in existing visual generation models. They also present StructScore, a new metric for factuality evaluation.",
        "tldr_zh": "本文介绍了一个新的数据集、模型和基准测试 (StructBench)，用于生成和编辑结构化视觉内容（图表、示意图等），并提高了事实准确性，解决了现有视觉生成模型中的一个不足。 他们还提出了StructScore，一种用于评估事实性的新指标。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models",
        "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
        "url": "http://arxiv.org/abs/2510.05034v1",
        "published_date": "2025-10-06T17:10:44+00:00",
        "updated_date": "2025-10-06T17:10:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunlong Tang",
            "Jing Bi",
            "Pinxin Liu",
            "Zhenyu Pan",
            "Zhangyun Tan",
            "Qianxiang Shen",
            "Jiani Liu",
            "Hang Hua",
            "Junjia Guo",
            "Yunzhong Xiao",
            "Chao Huang",
            "Zhiyuan Wang",
            "Susan Liang",
            "Xinyi Liu",
            "Yizhi Song",
            "Yuhe Nie",
            "Jia-Xing Zhong",
            "Bozheng Li",
            "Daiqing Qi",
            "Ziyun Zeng",
            "Ali Vosoughi",
            "Luchuan Song",
            "Zeliang Zhang",
            "Daiki Shimada",
            "Han Liu",
            "Jiebo Luo",
            "Chenliang Xu"
        ],
        "tldr": "This survey paper comprehensively examines post-training methodologies for Video-LMMs, focusing on supervised fine-tuning, reinforcement learning, and test-time scaling, aiming to provide a unified framework for advancing Video-LMM capabilities.",
        "tldr_zh": "这篇综述论文全面考察了视频大语言模型（Video-LMMs）的后训练方法，重点关注监督微调、强化学习和测试时缩放，旨在为提升视频大语言模型的能力提供一个统一的框架。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Representations inside the Language Model",
        "summary": "Despite interpretability work analyzing VIT encoders and transformer\nactivations, we don't yet understand why Multimodal Language Models (MLMs)\nstruggle on perception-heavy tasks. We offer an under-studied perspective by\nexamining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and\nLlama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the\nflow of visual information through the language model, finding that image value\ntokens encode sufficient information to perform several perception-heavy tasks\nzero-shot: segmentation, semantic correspondence, temporal correspondence, and\nreferring expression detection. We find that while the language model does\naugment the visual information received from the projection of input visual\nencodings-which we reveal correlates with overall MLM perception capability-it\ncontains less visual information on several tasks than the equivalent visual\nencoder (SigLIP) that has not undergone MLM finetuning. Further, we find that\nthe visual information corresponding to input-agnostic image key tokens in\nlater layers of language models contains artifacts which reduce perception\ncapability of the overall MLM. Next, we discuss controlling visual information\nin the language model, showing that adding a text prefix to the image input\nimproves perception capabilities of visual representations. Finally, we reveal\nthat if language models were able to better control their visual information,\ntheir perception would significantly improve; e.g., in 33.3% of Art Style\nquestions in the BLINK benchmark, perception information present in the\nlanguage model is not surfaced to the output! Our findings reveal insights into\nthe role of key-value tokens in multimodal systems, paving the way for deeper\nmechanistic interpretability of MLMs and suggesting new directions for training\ntheir visual encoder and language model components.",
        "url": "http://arxiv.org/abs/2510.04819v1",
        "published_date": "2025-10-06T14:01:39+00:00",
        "updated_date": "2025-10-06T14:01:39+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Benlin Liu",
            "Amita Kamath",
            "Madeleine Grunde-McLaughlin",
            "Winson Han",
            "Ranjay Krishna"
        ],
        "tldr": "This paper investigates how Multimodal Language Models (MLMs) process visual information within the language model, identifying bottlenecks and suggesting improvements for perception-heavy tasks. It finds that visual information is present but not always effectively utilized, and that input-agnostic key tokens can hinder performance.",
        "tldr_zh": "本文研究了多模态语言模型（MLM）如何在语言模型中处理视觉信息，识别瓶颈并提出改进感知密集型任务的建议。研究发现视觉信息存在，但并非总是有效利用，并且与输入无关的关键令牌会阻碍性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior",
        "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process\ninvolving changes in magnification and movement between fields. Although recent\npathology foundation models are strong, practical agentic systems that decide\nwhat field to examine next, adjust magnification, and deliver explainable\ndiagnoses are still lacking. The blocker is data: scalable, clinically aligned\nsupervision of expert viewing behavior that is tacit and experience-based, not\nwritten in textbooks or online, and therefore absent from large language model\ntraining. We introduce the AI Session Recorder, which works with standard WSI\nviewers to unobtrusively record routine navigation and convert the viewer logs\ninto standardized behavioral commands (inspect or peek at discrete\nmagnifications) and bounding boxes. A lightweight human-in-the-loop review\nturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired\n\"where to look\" and \"why it matters\" supervision produced at roughly six times\nlower labeling time. Using this behavioral data, we build Pathologist-o3, a\ntwo-stage agent that first proposes regions of interest and then performs\nbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,\nit achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the\nstate-of-the-art OpenAI o3 model and generalizing across backbones. To our\nknowledge, this constitutes one of the first behavior-grounded agentic systems\nin pathology. Turning everyday viewer logs into scalable, expert-validated\nsupervision, our framework makes agentic pathology practical and establishes a\npath to human-aligned, upgradeable clinical AI.",
        "url": "http://arxiv.org/abs/2510.04587v1",
        "published_date": "2025-10-06T08:44:04+00:00",
        "updated_date": "2025-10-06T08:44:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Wang",
            "Ruiming Wu",
            "Charles Herndon",
            "Yihang Liu",
            "Shunsuke Koga",
            "Jeanne Shen",
            "Zhi Huang"
        ],
        "tldr": "The paper introduces Pathology-CoT, a system that learns from expert pathologist behavior recorded during WSI diagnosis to create an agentic AI system, achieving strong performance in lymph-node metastasis detection.",
        "tldr_zh": "该论文介绍了 Pathology-CoT，一个通过学习专家病理学家在WSI诊断期间的行为记录来创建自主AI系统的系统，并在淋巴结转移检测中取得了出色的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Conditional Representation Learning for Customized Tasks",
        "summary": "Conventional representation learning methods learn a universal representation\nthat primarily captures dominant semantics, which may not always align with\ncustomized downstream tasks. For instance, in animal habitat analysis,\nresearchers prioritize scene-related features, whereas universal embeddings\nemphasize categorical semantics, leading to suboptimal results. As a solution,\nexisting approaches resort to supervised fine-tuning, which however incurs high\ncomputational and annotation costs. In this paper, we propose Conditional\nRepresentation Learning (CRL), aiming to extract representations tailored to\narbitrary user-specified criteria. Specifically, we reveal that the semantics\nof a space are determined by its basis, thereby enabling a set of descriptive\nwords to approximate the basis for a customized feature space. Building upon\nthis insight, given a user-specified criterion, CRL first employs a large\nlanguage model (LLM) to generate descriptive texts to construct the semantic\nbasis, then projects the image representation into this conditional feature\nspace leveraging a vision-language model (VLM). The conditional representation\nbetter captures semantics for the specific criterion, which could be utilized\nfor multiple customized tasks. Extensive experiments on classification and\nretrieval tasks demonstrate the superiority and generality of the proposed CRL.\nThe code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.",
        "url": "http://arxiv.org/abs/2510.04564v1",
        "published_date": "2025-10-06T08:00:59+00:00",
        "updated_date": "2025-10-06T08:00:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Honglin Liu",
            "Chao Sun",
            "Peng Hu",
            "Yunfan Li",
            "Xi Peng"
        ],
        "tldr": "The paper introduces Conditional Representation Learning (CRL), a method that leverages LLMs and VLMs to generate customized image representations based on user-specified criteria, avoiding expensive fine-tuning.",
        "tldr_zh": "该论文介绍了条件表示学习 (CRL)，这是一种利用 LLM 和 VLM 根据用户指定的标准生成定制图像表示的方法，从而避免了昂贵的微调。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Post-training quantization of vision encoders needs prefixing registers",
        "summary": "Transformer-based vision encoders -- such as CLIP -- are central to\nmultimodal intelligence, powering applications from autonomous web agents to\nrobotic control. Since these applications often demand real-time processing of\nmassive visual data, reducing the inference cost of vision encoders is\ncritical. Post-training quantization offers a practical path, but remains\nchallenging even at 8-bit precision due to massive-scale activations (i.e.,\noutliers). In this work, we propose $\\textit{RegCache}$, a training-free\nalgorithm to mitigate outliers in vision encoders, enabling quantization with\nsignificantly smaller accuracy drops. The proposed RegCache introduces\noutlier-prone yet semantically meaningless prefix tokens to the target vision\nencoder, which prevents other tokens from having outliers. Notably, we observe\nthat outliers in vision encoders behave differently from those in language\nmodels, motivating two technical innovations: middle-layer prefixing and token\ndeletion. Experiments show that our method consistently improves the accuracy\nof quantized models across both text-supervised and self-supervised vision\nencoders.",
        "url": "http://arxiv.org/abs/2510.04547v1",
        "published_date": "2025-10-06T07:27:46+00:00",
        "updated_date": "2025-10-06T07:27:46+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Seunghyeon Kim",
            "Jinho Kim",
            "Taesun Yeom",
            "Wonpyo Park",
            "Kyuyeun Kim",
            "Jaeho Lee"
        ],
        "tldr": "The paper introduces RegCache, a training-free algorithm that uses prefix tokens to mitigate outlier activations in vision encoders, thereby improving the accuracy of post-training quantization for these models.",
        "tldr_zh": "本文介绍了一种名为 RegCache 的免训练算法，该算法通过使用前缀 tokens 来减轻视觉编码器中的异常激活，从而提高这些模型后训练量化的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models",
        "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models.",
        "url": "http://arxiv.org/abs/2510.04477v1",
        "published_date": "2025-10-06T04:26:39+00:00",
        "updated_date": "2025-10-06T04:26:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Soo Yong Kim",
            "Suin Cho",
            "Vincent-Daniel Yun",
            "Gyeongyeon Hwang"
        ],
        "tldr": "The paper introduces MedCLM, a pipeline that generates medical VQA data with Chain-of-Thought reasoning and proposes an integrated CoT-Curriculum strategy, achieving state-of-the-art performance on medical VQA benchmarks.",
        "tldr_zh": "该论文介绍了MedCLM，一个利用链式思考推理生成医学VQA数据的管道，并提出了一个集成的CoT课程策略，在医学VQA基准测试上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering",
        "summary": "Effectively applying Vision-Language Models (VLMs) to Video Question\nAnswering (VideoQA) hinges on selecting a concise yet comprehensive set of\nframes, as processing entire videos is computationally infeasible. However,\ncurrent frame selection methods face a critical trade-off: approaches relying\non lightweight similarity models, such as CLIP, often fail to capture the\nnuances of complex queries, resulting in inaccurate similarity scores that\ncannot reflect the authentic query-frame relevance, which further undermines\nframe selection. Meanwhile, methods that leverage a VLM for deeper analysis\nachieve higher accuracy but incur prohibitive computational costs. To address\nthese limitations, we propose A.I.R., a training-free approach for Adaptive,\nIterative, and Reasoning-based frame selection. We leverage a powerful VLM to\nperform deep, semantic analysis on complex queries, and this analysis is\ndeployed within a cost-effective iterative loop that processes only a small\nbatch of the most high-potential frames at a time. Extensive experiments on\nvarious VideoQA benchmarks demonstrate that our approach outperforms existing\nframe selection methods, significantly boosts the performance of the foundation\nVLM, and achieves substantial gains in computational efficiency over other\nVLM-based techniques.",
        "url": "http://arxiv.org/abs/2510.04428v1",
        "published_date": "2025-10-06T01:51:13+00:00",
        "updated_date": "2025-10-06T01:51:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhao Zou",
            "Shengji Jin",
            "Andong Deng",
            "Youpeng Zhao",
            "Jun Wang",
            "Chen Chen"
        ],
        "tldr": "The paper introduces A.I.R., a training-free, adaptive, iterative, and reasoning-based frame selection method for VideoQA that leverages a VLM in an iterative loop to achieve better performance and computational efficiency.",
        "tldr_zh": "该论文提出了一种名为A.I.R.的免训练、自适应、迭代和基于推理的帧选择方法，用于视频问答。该方法在一个迭代循环中利用视觉语言模型，以实现更好的性能和计算效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting",
        "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI\ncommunity, owing to their impressive abilities gained from training on\nlarge-scale vision-language data from the Web. These models have demonstrated\nstrong performance across diverse tasks, including image understanding, video\nunderstanding, complex visual reasoning, and embodied AI. Despite these\nnoteworthy successes, a fundamental question remains: Can VLMs count objects\ncorrectly? In this paper, we introduce a simple yet effective benchmark,\nVLMCountBench, designed under a minimalist setting with only basic geometric\nshapes (e.g., triangles, circles) and their compositions, focusing exclusively\non counting tasks without interference from other factors. We adopt strict\nindependent variable control and systematically study the effects of simple\nproperties such as color, size, and prompt refinement in a controlled ablation.\nOur empirical results reveal that while VLMs can count reliably when only one\nshape type is present, they exhibit substantial failures when multiple shape\ntypes are combined (i.e., compositional counting). This highlights a\nfundamental empirical limitation of current VLMs and motivates important\ndirections for future research.",
        "url": "http://arxiv.org/abs/2510.04401v1",
        "published_date": "2025-10-06T00:11:24+00:00",
        "updated_date": "2025-10-06T00:11:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xuyang Guo",
            "Zekai Huang",
            "Zhenmei Shi",
            "Zhao Song",
            "Jiahao Zhang"
        ],
        "tldr": "This paper introduces VLMCountBench, a benchmark exposing the failures of current Vision-Language Models in compositional counting tasks involving simple geometric shapes, highlighting a key limitation of VLMs.",
        "tldr_zh": "本文介绍了VLMCountBench，该基准测试揭示了当前视觉语言模型在涉及简单几何形状的组合计数任务中的失败，突出了VLM的一个关键局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder",
        "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.",
        "url": "http://arxiv.org/abs/2510.05081v1",
        "published_date": "2025-10-06T17:51:04+00:00",
        "updated_date": "2025-10-06T17:51:04+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ronen Kamenetsky",
            "Sara Dorfman",
            "Daniel Garibi",
            "Roni Paiss",
            "Or Patashnik",
            "Daniel Cohen-Or"
        ],
        "tldr": "This paper introduces SAEdit, a method for disentangled and continuous image editing by manipulating text embeddings at the token level using a Sparse Autoencoder to identify semantically isolated dimensions, making it model-agnostic and applicable to various image synthesis backbones.",
        "tldr_zh": "该论文介绍了SAEdit，一种通过使用稀疏自动编码器在token级别操作文本嵌入来实现解耦和连续图像编辑的方法，该方法识别语义隔离的维度，使其模型无关，并适用于各种图像合成骨干。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering",
        "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.",
        "url": "http://arxiv.org/abs/2510.04514v1",
        "published_date": "2025-10-06T06:05:36+00:00",
        "updated_date": "2025-10-06T06:05:36+00:00",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.CL",
            "cs.CV",
            "stat.ME"
        ],
        "authors": [
            "Rachneet Kaur",
            "Nishan Srishankar",
            "Zhen Zeng",
            "Sumitra Ganesh",
            "Manuela Veloso"
        ],
        "tldr": "ChartAgent introduces a novel agentic framework for visually grounded reasoning in chart question answering, achieving state-of-the-art accuracy by explicitly performing visual reasoning directly within the chart's spatial domain.",
        "tldr_zh": "ChartAgent 引入了一种新的代理框架，用于在图表问题解答中进行视觉基础推理，通过直接在图表的空间域内执行视觉推理，实现了最先进的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery",
        "summary": "Vision-Language Models (VLMs) have achieved significant progress in\nmultimodal understanding tasks, demonstrating strong capabilities particularly\nin general tasks such as image captioning and visual reasoning. However, when\ndealing with specialized cultural heritage domains like 3D vase artifacts,\nexisting models face severe data scarcity issues and insufficient domain\nknowledge limitations. Due to the lack of targeted training data, current VLMs\nstruggle to effectively handle such culturally significant specialized tasks.\nTo address these challenges, we propose the VaseVQA-3D dataset, which serves as\nthe first 3D visual question answering dataset for ancient Greek pottery\nanalysis, collecting 664 ancient Greek vase 3D models with corresponding\nquestion-answer data and establishing a complete data construction pipeline. We\nfurther develop the VaseVLM model, enhancing model performance in vase artifact\nanalysis through domain-adaptive training. Experimental results validate the\neffectiveness of our approach, where we improve by 12.8% on R@1 metrics and by\n6.6% on lexical similarity compared with previous state-of-the-art on the\nVaseVQA-3D dataset, significantly improving the recognition and understanding\nof 3D vase artifacts, providing new technical pathways for digital heritage\npreservation research.",
        "url": "http://arxiv.org/abs/2510.04479v1",
        "published_date": "2025-10-06T04:28:39+00:00",
        "updated_date": "2025-10-06T04:28:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nonghai Zhang",
            "Zeyu Zhang",
            "Jiazi Wang",
            "Yang Zhao",
            "Hao Tang"
        ],
        "tldr": "The paper introduces VaseVQA-3D, a new 3D visual question answering dataset for ancient Greek pottery, along with a domain-adapted VLM, VaseVLM, demonstrating improved performance in this specialized cultural heritage domain.",
        "tldr_zh": "本文介绍了 VaseVQA-3D，这是一个用于古希腊陶器的新的 3D 视觉问答数据集，以及一个领域自适应的 VLM，VaseVLM，展示了在这个专业的文化遗产领域中改进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]