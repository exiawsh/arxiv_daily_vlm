[
    {
        "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.",
        "url": "http://arxiv.org/abs/2508.00553v1",
        "published_date": "2025-08-01T11:48:11+00:00",
        "updated_date": "2025-08-01T11:48:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jizhihui Liu",
            "Feiyi Du",
            "Guangdao Zhu",
            "Niu Lian",
            "Jun Li",
            "Bin Chen"
        ],
        "tldr": "HiPrune is a training-free token pruning method for VLMs that leverages hierarchical attention to selectively retain informative tokens, achieving significant FLOPs and latency reduction with minimal accuracy loss.",
        "tldr_zh": "HiPrune 是一种针对视觉语言模型（VLM）的免训练令牌剪枝方法，它利用分层注意力选择性地保留信息丰富的令牌，从而在最小的精度损失下显著降低 FLOPs 和延迟。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images",
        "summary": "Clinical decision-making relies heavily on understanding relative positions\nof anatomical structures and anomalies. Therefore, for Vision-Language Models\n(VLMs) to be applicable in clinical practice, the ability to accurately\ndetermine relative positions on medical images is a fundamental prerequisite.\nDespite its importance, this capability remains highly underexplored. To\naddress this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,\nLlama3.2, Pixtral, and JanusPro, and find that all models fail at this\nfundamental task. Inspired by successful approaches in computer vision, we\ninvestigate whether visual prompts, such as alphanumeric or colored markers\nplaced on anatomical structures, can enhance performance. While these markers\nprovide moderate improvements, results remain significantly lower on medical\nimages compared to observations made on natural images. Our evaluations suggest\nthat, in medical imaging, VLMs rely more on prior anatomical knowledge than on\nactual image content for answering relative position questions, often leading\nto incorrect conclusions. To facilitate further research in this area, we\nintroduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,\ndesigned to systematically evaluate the capability to identify relative\npositions in medical images.",
        "url": "http://arxiv.org/abs/2508.00549v1",
        "published_date": "2025-08-01T11:44:06+00:00",
        "updated_date": "2025-08-01T11:44:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Wolf",
            "Heiko Hillenhagen",
            "Billurvan Taskin",
            "Alex Bäuerle",
            "Meinrad Beer",
            "Michael Götz",
            "Timo Ropinski"
        ],
        "tldr": "This paper evaluates the performance of state-of-the-art VLMs on identifying relative positions in medical images, finding that they perform poorly and tend to rely on prior anatomical knowledge rather than image content. They introduce a new benchmark dataset, MIRP, to facilitate further research in this area.",
        "tldr_zh": "该论文评估了最先进的视觉语言模型在识别医学图像中相对位置方面的性能，发现它们的表现很差，并且倾向于依赖先前的解剖学知识而不是图像内容。他们引入了一个新的基准数据集MIRP，以促进该领域的研究。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AutoDebias: Automated Framework for Debiasing Text-to-Image Models",
        "summary": "Text-to-Image (T2I) models generate high-quality images from text prompts but\noften exhibit unintended social biases, such as gender or racial stereotypes,\neven when these attributes are not mentioned. Existing debiasing methods work\nwell for simple or well-known cases but struggle with subtle or overlapping\nbiases. We propose AutoDebias, a framework that automatically identifies and\nmitigates harmful biases in T2I models without prior knowledge of specific bias\ntypes. Specifically, AutoDebias leverages vision-language models to detect\nbiased visual patterns and constructs fairness guides by generating inclusive\nalternative prompts that reflect balanced representations. These guides drive a\nCLIP-guided training process that promotes fairer outputs while preserving the\noriginal model's image quality and diversity. Unlike existing methods,\nAutoDebias effectively addresses both subtle stereotypes and multiple\ninteracting biases. We evaluate the framework on a benchmark covering over 25\nbias scenarios, including challenging cases where multiple biases occur\nsimultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and\nreduces biased outputs from 90% to negligible levels, while preserving the\nvisual fidelity of the original model.",
        "url": "http://arxiv.org/abs/2508.00445v1",
        "published_date": "2025-08-01T09:05:45+00:00",
        "updated_date": "2025-08-01T09:05:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyi Cai",
            "Mohammad Mahdinur Rahman",
            "Mingkang Dong",
            "Jie Li",
            "Muxin Pu",
            "Zhili Fang",
            "Yinan Peng",
            "Hanjun Luo",
            "Yang Liu"
        ],
        "tldr": "AutoDebias is a framework that automatically identifies and mitigates harmful biases in text-to-image models by leveraging vision-language models to detect biased patterns and generate fairness guides for CLIP-guided training.",
        "tldr_zh": "AutoDebias是一个框架，它通过利用视觉语言模型来检测有偏见的模式，并生成公平性指导，用于CLIP指导的训练，从而自动识别和减轻文本到图像模型中的有害偏见。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents",
        "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store\nsimulation for benchmarking embodied agents against human performance in\nshopping tasks. Addressing a gap in retail-specific sim environments for\nembodied agent training, Sari Sandbox features over 250 interactive grocery\nitems across three store configurations, controlled via an API. It supports\nboth virtual reality (VR) for human interaction and a vision language model\n(VLM)-powered embodied agent. We also introduce SariBench, a dataset of\nannotated human demonstrations across varied task difficulties. Our sandbox\nenables embodied agents to navigate, inspect, and manipulate retail items,\nproviding baselines against human performance. We conclude with benchmarks,\nperformance analysis, and recommendations for enhancing realism and\nscalability. The source code can be accessed via\nhttps://github.com/upeee/sari-sandbox-env.",
        "url": "http://arxiv.org/abs/2508.00400v1",
        "published_date": "2025-08-01T08:01:38+00:00",
        "updated_date": "2025-08-01T08:01:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Janika Deborah Gajo",
            "Gerarld Paul Merales",
            "Jerome Escarcha",
            "Brenden Ashley Molina",
            "Gian Nartea",
            "Emmanuel G. Maminta",
            "Juan Carlos Roldan",
            "Rowel O. Atienza"
        ],
        "tldr": "The paper introduces Sari Sandbox, a photorealistic 3D retail store simulation environment for training and benchmarking embodied agents, along with SariBench, a dataset of human demonstrations for shopping tasks.",
        "tldr_zh": "该论文介绍了Sari Sandbox，一个逼真的3D零售店模拟环境，用于训练和评估具身智能体，以及SariBench，一个包含人类购物任务演示的数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "iSafetyBench: A video-language benchmark for safety in industrial environment",
        "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\ngeneralization across diverse video understanding tasks under zero-shot\nsettings. However, their capabilities in high-stakes industrial domains-where\nrecognizing both routine operations and safety-critical anomalies is\nessential-remain largely underexplored. To address this gap, we introduce\niSafetyBench, a new video-language benchmark specifically designed to evaluate\nmodel performance in industrial environments across both normal and hazardous\nscenarios. iSafetyBench comprises 1,100 video clips sourced from real-world\nindustrial settings, annotated with open-vocabulary, multi-label action tags\nspanning 98 routine and 67 hazardous action categories. Each clip is paired\nwith multiple-choice questions for both single-label and multi-label\nevaluation, enabling fine-grained assessment of VLMs in both standard and\nsafety-critical contexts. We evaluate eight state-of-the-art video-language\nmodels under zero-shot conditions. Despite their strong performance on existing\nvideo benchmarks, these models struggle with iSafetyBench-particularly in\nrecognizing hazardous activities and in multi-label scenarios. Our results\nreveal significant performance gaps, underscoring the need for more robust,\nsafety-aware multimodal models for industrial applications. iSafetyBench\nprovides a first-of-its-kind testbed to drive progress in this direction. The\ndataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.",
        "url": "http://arxiv.org/abs/2508.00399v1",
        "published_date": "2025-08-01T07:55:53+00:00",
        "updated_date": "2025-08-01T07:55:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Raiyaan Abdullah",
            "Yogesh Singh Rawat",
            "Shruti Vyas"
        ],
        "tldr": "The paper introduces iSafetyBench, a new video-language benchmark for evaluating VLMs in industrial environments, highlighting their shortcomings in recognizing hazardous activities and multi-label scenarios.",
        "tldr_zh": "该论文介绍了iSafetyBench，一个新的视频语言基准，用于评估VLM在工业环境中的表现，强调了它们在识别危险活动和多标签场景中的不足。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning",
        "summary": "Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,\nhas showcased remarkable effectiveness in improving the task-specific\ntransferability of vision-language models. This paper delves into a previously\noverlooked information asymmetry issue in PT, where the visual modality mostly\nconveys more context than the object-oriented textual modality.\nCorrespondingly, coarsely aligning these two modalities could result in the\nbiased attention, driving the model to merely focus on the context area. To\naddress this, we propose DAPT, an effective PT framework based on an intuitive\ndecouple-before-align concept. First, we propose to explicitly decouple the\nvisual modality into the foreground and background representation via\nexploiting coarse-and-fine visual segmenting cues, and then both of these\ndecoupled patterns are aligned with the original foreground texts and the\nhand-crafted background classes, thereby symmetrically strengthening the modal\nalignment. To further enhance the visual concentration, we propose a visual\npull-push regularization tailored for the foreground-background patterns,\ndirecting the original visual representation towards unbiased attention on the\nregion-of-interest object. We demonstrate the power of architecture-free DAPT\nthrough few-shot learning, base-to-novel generalization, and data-efficient\nlearning, all of which yield superior performance across prevailing benchmarks.\nOur code will be released at https://github.com/Ferenas/DAPT.",
        "url": "http://arxiv.org/abs/2508.00395v1",
        "published_date": "2025-08-01T07:46:00+00:00",
        "updated_date": "2025-08-01T07:46:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fei Zhang",
            "Tianfei Zhou",
            "Jiangchao Yao",
            "Ya Zhang",
            "Ivor W. Tsang",
            "Yanfeng Wang"
        ],
        "tldr": "The paper introduces DAPT, a prompt tuning framework that decouples visual modality into foreground and background representations before alignment with text, addressing information asymmetry in vision-language models. It demonstrates improved performance in few-shot learning and generalization.",
        "tldr_zh": "该论文介绍了一种名为DAPT的提示调优框架，该框架在与文本对齐之前，将视觉模态解耦为前景和背景表示，从而解决了视觉语言模型中的信息不对称问题。它在少样本学习和泛化方面表现出更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning",
        "summary": "We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.\nOur approach tackles the challenge of interleaved multimodal reasoning across\ndiverse datasets and task formats by employing a dual-agent system: a\nlanguage-based PromptEngineer, which generates context-aware, task-specific\nprompts, and a VisionReasoner, a large vision-language model (LVLM) responsible\nfor final inference. The framework is fully automated, modular, and\ntraining-free, enabling generalization across classification, question\nanswering, and free-form generation tasks involving one or multiple input\nimages. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE\nChallenge (Track A), covering a broad spectrum of visual reasoning tasks\nincluding document QA, visual comparison, dialogue-based understanding, and\nscene-level inference. Our results demonstrate that LVLMs can effectively\nreason over multiple images when guided by informative prompts. Notably, Claude\n3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%\naccuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how\ndesign choices-such as model selection, shot count, and input length-influence\nthe reasoning performance of different LVLMs.",
        "url": "http://arxiv.org/abs/2508.00356v1",
        "published_date": "2025-08-01T06:39:15+00:00",
        "updated_date": "2025-08-01T06:39:15+00:00",
        "categories": [
            "cs.CV",
            "cs.MA",
            "I.2; I.2.7"
        ],
        "authors": [
            "Angelos Vlachos",
            "Giorgos Filandrianos",
            "Maria Lymperaiou",
            "Nikolaos Spanos",
            "Ilias Mitsouras",
            "Vasileios Karampinis",
            "Athanasios Voulodimos"
        ],
        "tldr": "The paper introduces a collaborative agent-based framework (Analyze-Prompt-Reason) with a PromptEngineer and VisionReasoner for multi-image vision-language reasoning, achieving strong results on diverse datasets by leveraging informative prompts for LVLMs.",
        "tldr_zh": "本文提出了一种协作式 Agent 框架 (Analyze-Prompt-Reason)，其中包含一个 PromptEngineer 和一个 VisionReasoner，用于多图像视觉语言推理。通过利用信息丰富的提示，该框架在各种数据集上取得了优异的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models",
        "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.",
        "url": "http://arxiv.org/abs/2508.00319v1",
        "published_date": "2025-08-01T05:02:26+00:00",
        "updated_date": "2025-08-01T05:02:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sunghyun Park",
            "Seokeon Choi",
            "Hyoungwoo Park",
            "Sungrack Yun"
        ],
        "tldr": "This paper introduces \"personalization guidance,\" a method for improving the trade-off between subject fidelity and text editability in personalized text-to-image diffusion models by leveraging an unlearned weak model and dynamic weight interpolation.",
        "tldr_zh": "本文提出了一种名为“个性化引导”的方法，通过利用未学习的弱模型和动态权重插值，改善个性化文本到图像扩散模型中主体保真度和文本可编辑性之间的权衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models",
        "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.",
        "url": "http://arxiv.org/abs/2508.00260v1",
        "published_date": "2025-08-01T02:08:09+00:00",
        "updated_date": "2025-08-01T02:08:09+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Hyundong Jin",
            "Hyung Jin Chang",
            "Eunwoo Kim"
        ],
        "tldr": "This paper proposes a continual learning framework for generative vision-language models (VLMs) that uses a mixture of instruction-grounded visual projectors to improve instruction following and mitigate catastrophic forgetting.",
        "tldr_zh": "该论文提出了一种生成式视觉-语言模型(VLM)的持续学习框架，该框架使用指令引导的视觉投影器混合，以提高指令遵循能力并减轻灾难性遗忘。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product",
        "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.",
        "url": "http://arxiv.org/abs/2508.00230v1",
        "published_date": "2025-08-01T00:29:13+00:00",
        "updated_date": "2025-08-01T00:29:13+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Paul Albert",
            "Frederic Z. Zhang",
            "Hemanth Saratchandran",
            "Anton van den Hengel",
            "Ehsan Abbasnejad"
        ],
        "tldr": "This paper introduces KRAdapter, a parameter-efficient fine-tuning method using the Khatri-Rao product to address the limitations of LoRA in approximating matrices with high effective ranks, demonstrating improved performance on vision-language and large language models.",
        "tldr_zh": "该论文介绍了一种参数高效微调方法KRAdapter，它使用Khatri-Rao乘积来解决LoRA在近似具有高有效秩的矩阵时的局限性，并在视觉语言模型和大型语言模型上展示了改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI",
        "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.",
        "url": "http://arxiv.org/abs/2508.00171v1",
        "published_date": "2025-07-31T21:35:52+00:00",
        "updated_date": "2025-07-31T21:35:52+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "David Restrepo",
            "Ira Ktena",
            "Maria Vakalopoulou",
            "Stergios Christodoulidis",
            "Enzo Ferrante"
        ],
        "tldr": "The paper introduces a perturbation-based method (SMS) to quantify modality biases in VLMs for medical imaging, revealing a strong reliance on text over visual cues in existing models, even when fine-tuned for medical data.",
        "tldr_zh": "该论文介绍了一种基于扰动的方法 (SMS) 来量化医学影像中 VLM 的模态偏差，揭示了现有模型对文本的强烈依赖，即使是针对医学数据进行微调的模型也是如此。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation",
        "summary": "Fine-tuning pre-trained vision-language models has emerged as a powerful\napproach for enhancing open-vocabulary semantic segmentation (OVSS). However,\nthe substantial computational and resource demands associated with training on\nlarge datasets have prompted interest in training-free methods for OVSS.\nExisting training-free approaches primarily focus on modifying model\narchitectures and generating prototypes to improve segmentation performance.\nHowever, they often neglect the challenges posed by class redundancy, where\nmultiple categories are not present in the current test image, and\nvisual-language ambiguity, where semantic similarities among categories create\nconfusion in class activation. These issues can lead to suboptimal class\nactivation maps and affinity-refined activation maps. Motivated by these\nobservations, we propose FreeCP, a novel training-free class purification\nframework designed to address these challenges. FreeCP focuses on purifying\nsemantic categories and rectifying errors caused by redundancy and ambiguity.\nThe purified class representations are then leveraged to produce final\nsegmentation predictions. We conduct extensive experiments across eight\nbenchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,\nas a plug-and-play module, significantly boosts segmentation performance when\ncombined with other OVSS methods.",
        "url": "http://arxiv.org/abs/2508.00557v1",
        "published_date": "2025-08-01T11:55:12+00:00",
        "updated_date": "2025-08-01T11:55:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qi Chen",
            "Lingxiao Yang",
            "Yun Chen",
            "Nailong Zhao",
            "Jianhuang Lai",
            "Jie Shao",
            "Xiaohua Xie"
        ],
        "tldr": "The paper introduces FreeCP, a training-free class purification framework for open-vocabulary semantic segmentation that addresses class redundancy and visual-language ambiguity to improve segmentation performance. It acts as a plug-and-play module.",
        "tldr_zh": "本文介绍了一种名为 FreeCP 的免训练类别净化框架，用于开放词汇语义分割，通过解决类别冗余和视觉-语言歧义来提高分割性能，作为一个即插即用模块。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding",
        "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.",
        "url": "http://arxiv.org/abs/2508.00378v1",
        "published_date": "2025-08-01T07:17:12+00:00",
        "updated_date": "2025-08-01T07:17:12+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shixin Yi",
            "Lin Shang"
        ],
        "tldr": "The paper introduces CoRGI, a modular framework that enhances vision-language model reasoning by incorporating visual verification into each step of the reasoning process, leading to more grounded and factual explanations. It improves performance on VCR benchmark and works on existing VLMs without retraining.",
        "tldr_zh": "该论文介绍了CoRGI，一个通过在推理过程的每一步中加入视觉验证来增强视觉-语言模型推理的模块化框架，从而产生更具根据和事实性的解释。它在VCR基准测试中提高了性能，并且可以在现有的VLM上工作，无需重新训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios",
        "summary": "Optical Character Recognition (OCR) for mathematical formula is essential for\nthe intelligent analysis of scientific literature. However, both task-specific\nand general vision-language models often struggle to handle the structural\ndiversity, complexity, and real-world variability inherent in mathematical\ncontent. In this work, we present DocTron-Formula, a unified framework built\nupon general vision-language models, thereby eliminating the need for\nspecialized architectures. Furthermore, we introduce CSFormula, a large-scale\nand challenging dataset that encompasses multidisciplinary and structurally\ncomplex formulas at the line, paragraph, and page levels. Through\nstraightforward supervised fine-tuning, our approach achieves state-of-the-art\nperformance across a variety of styles, scientific domains, and complex\nlayouts. Experimental results demonstrate that our method not only surpasses\nspecialized models in terms of accuracy and robustness, but also establishes a\nnew paradigm for the automated understanding of complex scientific documents.",
        "url": "http://arxiv.org/abs/2508.00311v1",
        "published_date": "2025-08-01T04:34:17+00:00",
        "updated_date": "2025-08-01T04:34:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yufeng Zhong",
            "Zhixiong Zeng",
            "Lei Chen",
            "Longrong Yang",
            "Liming Zheng",
            "Jing Huang",
            "Siqi Yang",
            "Lin Ma"
        ],
        "tldr": "The paper introduces DocTron-Formula, a unified vision-language framework for mathematical formula recognition using a new challenging dataset (CSFormula), achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了 DocTron-Formula，一个用于数学公式识别的统一视觉语言框架，它使用了一个新的具有挑战性的数据集 (CSFormula)，并实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models",
        "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.",
        "url": "http://arxiv.org/abs/2508.00289v1",
        "published_date": "2025-08-01T03:26:18+00:00",
        "updated_date": "2025-08-01T03:26:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Christian Simon",
            "Masato Ishii",
            "Akio Hayakawa",
            "Zhi Zhong",
            "Shusuke Takahashi",
            "Takashi Shibuya",
            "Yuki Mitsufuji"
        ],
        "tldr": "The paper introduces TITAN-Guide, a memory-efficient, training-free guidance framework for Text-to-Video diffusion models that optimizes diffusion latents without backpropagation, achieving improved control and performance compared to existing methods.",
        "tldr_zh": "该论文介绍了TITAN-Guide，一个用于文本到视频扩散模型的内存高效、无训练的引导框架，它无需反向传播即可优化扩散潜在变量，与现有方法相比，实现了改进的控制和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters",
        "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.",
        "url": "http://arxiv.org/abs/2508.00213v1",
        "published_date": "2025-07-31T23:26:39+00:00",
        "updated_date": "2025-07-31T23:26:39+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shayan Jalilian",
            "Abdul Bais"
        ],
        "tldr": "SAM-PTx introduces a parameter-efficient method to adapt SAM for text-guided segmentation by injecting CLIP-derived text embeddings into the image encoder, demonstrating improved performance on segmentation tasks.",
        "tldr_zh": "SAM-PTx 提出了一种参数高效的方法，通过将 CLIP 衍生的文本嵌入注入到图像编码器中，来调整 SAM 以实现文本引导的分割，并在分割任务中展示了性能的提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving",
        "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.",
        "url": "http://arxiv.org/abs/2508.00589v1",
        "published_date": "2025-08-01T12:41:52+00:00",
        "updated_date": "2025-08-01T12:41:52+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.IR",
            "cs.RO",
            "68T45, 68P20, 68T10, 68T50, 68T07, 68T40",
            "I.2.10; I.4.8; I.2.9; H.3.3"
        ],
        "authors": [
            "Stefan Englmeier",
            "Max A. Büttner",
            "Katharina Winter",
            "Fabian B. Flohr"
        ],
        "tldr": "This paper introduces a context-aware motion retrieval framework for autonomous driving, using SMPL-based motion sequences and video frames encoded into a multimodal embedding space aligned with natural language, enabling text-based retrieval of human behavior and context.",
        "tldr_zh": "该论文介绍了一个用于自动驾驶的上下文感知运动检索框架，它使用基于SMPL的运动序列和视频帧编码到与自然语言对齐的多模态嵌入空间中，从而实现基于文本的人类行为和上下文检索。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Multimodal Referring Segmentation: A Survey",
        "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
        "url": "http://arxiv.org/abs/2508.00265v1",
        "published_date": "2025-08-01T02:14:00+00:00",
        "updated_date": "2025-08-01T02:14:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Henghui Ding",
            "Song Tang",
            "Shuting He",
            "Chang Liu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "tldr": "This paper surveys the field of multimodal referring segmentation across images, videos, and 3D scenes, covering methods, datasets, and applications. It also discusses generalized referring expression methods and provides performance comparisons.",
        "tldr_zh": "本文综述了跨图像、视频和 3D 场景的多模态指代表达分割领域，涵盖了方法、数据集和应用。 它还讨论了广义指代表达方法并提供了性能比较。",
        "relevance_score": 7,
        "novelty_claim_score": 4,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]