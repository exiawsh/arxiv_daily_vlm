[
    {
        "title": "Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone",
        "summary": "As Vision Language Models (VLMs) become integral to real-world applications,\nunderstanding their demographic biases is critical. We introduce GRAS, a\nbenchmark for uncovering demographic biases in VLMs across gender, race, age,\nand skin tone, offering the most diverse coverage to date. We further propose\nthe GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark\nfive state-of-the-art VLMs and reveal concerning bias levels, with the least\nbiased model attaining a GRAS Bias Score of only 2 out of 100. Our findings\nalso reveal a methodological insight: evaluating bias in VLMs with visual\nquestion answering (VQA) requires considering multiple formulations of a\nquestion. Our code, data, and evaluation results are publicly available.",
        "url": "http://arxiv.org/abs/2508.18989v1",
        "published_date": "2025-08-26T12:41:35+00:00",
        "updated_date": "2025-08-26T12:41:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaivi Malik",
            "Hasnat Md Abdullah",
            "Sriparna Saha",
            "Amit Sheth"
        ],
        "tldr": "The paper introduces GRAS, a new benchmark and bias score for evaluating demographic biases in Vision Language Models (VLMs) across gender, race, age, and skin tone, revealing significant biases in state-of-the-art models and highlighting the importance of diverse question formulations in VQA bias evaluation.",
        "tldr_zh": "该论文介绍了GRAS，一个用于评估视觉语言模型（VLM）在性别、种族、年龄和肤色方面的人口统计偏差的新基准和偏差评分，揭示了现有模型中的显著偏差，并强调了在VQA偏差评估中考虑多种问题形式的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval",
        "summary": "Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task\nthat involves retrieving videos based on queries relevant to only specific\nsegments. While existing works follow the paradigm of developing models to\nprocess unimodal features, powerful pretrained vision-language models like CLIP\nremain underexplored in this field. To bridge this gap, we propose ProPy, a\nmodel with systematic architectural adaption of CLIP specifically designed for\nPRVR. Drawing insights from the semantic relevance of multi-granularity events,\nProPy introduces two key innovations: (1) A Prompt Pyramid structure that\norganizes event prompts to capture semantics at multiple granularity levels,\nand (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that\nenables dynamic semantic interaction among events. With these designs, ProPy\nachieves SOTA performance on three public datasets, outperforming previous\nmodels by significant margins. Code is available at\nhttps://github.com/BUAAPY/ProPy.",
        "url": "http://arxiv.org/abs/2508.19024v1",
        "published_date": "2025-08-26T13:42:48+00:00",
        "updated_date": "2025-08-26T13:42:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi Pan",
            "Yujia Zhang",
            "Michael Kampffmeyer",
            "Xiaoguang Zhao"
        ],
        "tldr": "The paper proposes ProPy, a CLIP-based model for Partially Relevant Video Retrieval (PRVR) that utilizes a Prompt Pyramid and Ancestor-Descendant Interaction Mechanism to achieve state-of-the-art performance.",
        "tldr_zh": "该论文提出了ProPy，一个基于CLIP的模型，用于部分相关视频检索（PRVR）。它利用提示金字塔和祖先-后代交互机制来实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models",
        "summary": "Ensuring fairness across demographic groups in medical diagnosis is essential\nfor equitable healthcare, particularly under distribution shifts caused by\nvariations in imaging equipment and clinical practice. Vision-language models\n(VLMs) exhibit strong generalization, and text prompts encode identity\nattributes, enabling explicit identification and removal of sensitive\ndirections. However, existing debiasing approaches typically address vision and\ntext modalities independently, leaving residual cross-modal misalignment and\nfairness gaps. To address this challenge, we propose DualFairVL, a multimodal\nprompt-learning framework that jointly debiases and aligns cross-modal\nrepresentations. DualFairVL employs a parallel dual-branch architecture that\nseparates sensitive and target attributes, enabling disentangled yet aligned\nrepresentations across modalities. Approximately orthogonal text anchors are\nconstructed via linear projections, guiding cross-attention mechanisms to\nproduce fused features. A hypernetwork further disentangles attribute-related\ninformation and generates instance-aware visual prompts, which encode\ndual-modal cues for fairness and robustness. Prototype-based regularization is\napplied in the visual branch to enforce separation of sensitive features and\nstrengthen alignment with textual anchors. Extensive experiments on eight\nmedical imaging datasets across four modalities show that DualFairVL achieves\nstate-of-the-art fairness and accuracy under both in- and out-of-distribution\nsettings, outperforming full fine-tuning and parameter-efficient baselines with\nonly 3.6M trainable parameters. Code will be released upon publication.",
        "url": "http://arxiv.org/abs/2508.18886v1",
        "published_date": "2025-08-26T10:01:23+00:00",
        "updated_date": "2025-08-26T10:01:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuexuan Xia",
            "Benteng Ma",
            "Jiang He",
            "Zhiyong Wang",
            "Qi Dou",
            "Yong Xia"
        ],
        "tldr": "The paper introduces DualFairVL, a novel multimodal prompt-learning framework that enhances fairness and accuracy in medical image diagnosis by jointly debiasing and aligning vision-language models, demonstrating state-of-the-art performance in various medical imaging datasets.",
        "tldr_zh": "本文介绍了 DualFairVL，一种新颖的多模态提示学习框架，通过联合去偏置和对齐视觉-语言模型来提高医学图像诊断的公平性和准确性，并在各种医学图像数据集中展示了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods",
        "summary": "Prior human-object interaction (HOI) detection methods have integrated early\nvision-language models (VLMs) such as CLIP, but only as supporting components\nwithin their frameworks. In contrast, recent advances in large, generative VLMs\nsuggest that these models may already possess strong ability to understand\nimages involving HOI. This naturally raises an important question: can\ngeneral-purpose standalone VLMs effectively solve HOI detection, and how do\nthey compare with specialized HOI methods? Answering this requires a benchmark\nthat can accommodate both paradigms. However, existing HOI benchmarks such as\nHICO-DET were developed before the emergence of modern VLMs, and their\nevaluation protocols require exact matches to annotated HOI classes. This is\npoorly aligned with the generative nature of VLMs, which often yield multiple\nvalid interpretations in ambiguous cases. For example, a static image may\ncapture a person mid-motion with a frisbee, which can plausibly be interpreted\nas either \"throwing\" or \"catching\". When only \"catching\" is annotated, the\nother, though equally plausible for the image, is marked incorrect when exact\nmatching is used. As a result, correct predictions might be penalized,\naffecting both VLMs and HOI-specific methods. To avoid penalizing valid\npredictions, we introduce a new benchmark that reformulates HOI detection as a\nmultiple-answer multiple-choice task, where each question includes only\nground-truth positive options and a curated set of negatives that are\nconstructed to reduce ambiguity (e.g., when \"catching\" is annotated, \"throwing\"\nis not selected as a negative to avoid penalizing valid predictions). The\nproposed evaluation protocol is the first of its kind for both VLMs and HOI\nmethods, enabling direct comparison and offering new insight into the current\nstate of progress in HOI understanding.",
        "url": "http://arxiv.org/abs/2508.18753v1",
        "published_date": "2025-08-26T07:30:53+00:00",
        "updated_date": "2025-08-26T07:30:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinqian Lei",
            "Bo Wang",
            "Robby T. Tan"
        ],
        "tldr": "This paper introduces a new benchmark for HOI detection designed to address the limitations of existing benchmarks when evaluating VLMs, particularly regarding penalizing valid but unannotated interpretations. It reformulates HOI detection as a multiple-answer multiple-choice task.",
        "tldr_zh": "本文介绍了一个新的HOI检测基准，旨在解决现有基准在评估VLM时存在的局限性，尤其是在惩罚有效但未注释的解释方面。它将HOI检测重新定义为多答案多项选择任务。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering",
        "summary": "Vision-language models (VLMs) have shown significant potential for medical\ntasks; however, their general-purpose nature can limit specialized diagnostic\naccuracy, and their large size poses substantial inference costs for real-world\nclinical deployment. To address these challenges, we introduce CLARIFY, a\nSpecialist-Generalist framework for dermatological visual question answering\n(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image\nclassifier (the Specialist) that provides fast and highly accurate diagnostic\npredictions, and (ii) a powerful yet compressed conversational VLM (the\nGeneralist) that generates natural language explanations to user queries. In\nour framework, the Specialist's predictions directly guide the Generalist's\nreasoning, focusing it on the correct diagnostic path. This synergy is further\nenhanced by a knowledge graph-based retrieval module, which grounds the\nGeneralist's responses in factual dermatological knowledge, ensuring both\naccuracy and reliability. This hierarchical design not only reduces diagnostic\nerrors but also significantly improves computational efficiency. Experiments on\nour curated multimodal dermatology dataset demonstrate that CLARIFY achieves an\n18\\% improvement in diagnostic accuracy over the strongest baseline, a\nfine-tuned, uncompressed single-line VLM, while reducing the average VRAM\nrequirement and latency by at least 20\\% and 5\\%, respectively. These results\nindicate that a Specialist-Generalist system provides a practical and powerful\nparadigm for building lightweight, trustworthy, and clinically viable AI\nsystems.",
        "url": "http://arxiv.org/abs/2508.18430v1",
        "published_date": "2025-08-25T19:22:16+00:00",
        "updated_date": "2025-08-25T19:22:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aranya Saha",
            "Tanvir Ahmed Khan",
            "Ismam Nur Swapnil",
            "Mohammad Ariful Haque"
        ],
        "tldr": "The paper introduces CLARIFY, a Specialist-Generalist framework for dermatological VQA that combines a lightweight domain-trained image classifier with a compressed conversational VLM, achieving improved accuracy and efficiency compared to single-line VLMs. It provides a good example of designing a more efficient and effective VLM in a specialized domain.",
        "tldr_zh": "该论文介绍了CLARIFY，一个用于皮肤病学视觉问答的专家-通用框架，它结合了一个轻量级的领域训练图像分类器和一个压缩的对话式VLM，与单线VLM相比，实现了更高的准确性和效率。它为在特定领域设计更高效有效的VLM提供了一个很好的例子。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Articulate3D: Zero-Shot Text-Driven 3D Object Posing",
        "summary": "We propose a training-free method, Articulate3D, to pose a 3D asset through\nlanguage control. Despite advances in vision and language models, this task\nremains surprisingly challenging. To achieve this goal, we decompose the\nproblem into two steps. We modify a powerful image-generator to create target\nimages conditioned on the input image and a text instruction. We then align the\nmesh to the target images through a multi-view pose optimisation step. In\ndetail, we introduce a self-attention rewiring mechanism (RSActrl) that\ndecouples the source structure from pose within an image generative model,\nallowing it to maintain a consistent structure across varying poses. We\nobserved that differentiable rendering is an unreliable signal for articulation\noptimisation; instead, we use keypoints to establish correspondences between\ninput and target images. The effectiveness of Articulate3D is demonstrated\nacross a diverse range of 3D objects and free-form text prompts, successfully\nmanipulating poses while maintaining the original identity of the mesh.\nQuantitative evaluations and a comparative user study, in which our method was\npreferred over 85\\% of the time, confirm its superiority over existing\napproaches. Project page:https://odeb1.github.io/articulate3d_page_deb/",
        "url": "http://arxiv.org/abs/2508.19244v1",
        "published_date": "2025-08-26T17:59:17+00:00",
        "updated_date": "2025-08-26T17:59:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Oishi Deb",
            "Anjun Hu",
            "Ashkan Khakzar",
            "Philip Torr",
            "Christian Rupprecht"
        ],
        "tldr": "Articulate3D is a training-free method for posing 3D objects using language control, which utilizes a modified image generator and multi-view pose optimization based on keypoint correspondences, achieving superior performance compared to existing methods.",
        "tldr_zh": "Articulate3D 是一种无需训练的方法，通过语言控制来调整 3D 对象的姿势。该方法利用改进的图像生成器和基于关键点对应的多视角姿势优化，与现有方法相比表现更优。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation",
        "summary": "Temporal context is essential for robotic manipulation because such tasks are\ninherently non-Markovian, yet mainstream VLA models typically overlook it and\nstruggle with long-horizon, temporally dependent tasks. Cognitive science\nsuggests that humans rely on working memory to buffer short-lived\nrepresentations for immediate control, while the hippocampal system preserves\nverbatim episodic details and semantic gist of past experience for long-term\nmemory. Inspired by these mechanisms, we propose MemoryVLA, a\nCognition-Memory-Action framework for long-horizon robotic manipulation. A\npretrained VLM encodes the observation into perceptual and cognitive tokens\nthat form working memory, while a Perceptual-Cognitive Memory Bank stores\nlow-level details and high-level semantics consolidated from it. Working memory\nretrieves decision-relevant entries from the bank, adaptively fuses them with\ncurrent tokens, and updates the bank by merging redundancies. Using these\ntokens, a memory-conditioned diffusion action expert yields temporally aware\naction sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks\nacross three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it\nachieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming\nstate-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on\nBridge. On 12 real-world tasks spanning general skills and long-horizon\ntemporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon\ntasks showing a +26 improvement over state-of-the-art baseline. Project Page:\nhttps://shihao1895.github.io/MemoryVLA",
        "url": "http://arxiv.org/abs/2508.19236v1",
        "published_date": "2025-08-26T17:57:16+00:00",
        "updated_date": "2025-08-26T17:57:16+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Hao Shi",
            "Bin Xie",
            "Yingfei Liu",
            "Lin Sun",
            "Fengrong Liu",
            "Tiancai Wang",
            "Erjin Zhou",
            "Haoqiang Fan",
            "Xiangyu Zhang",
            "Gao Huang"
        ],
        "tldr": "The paper introduces MemoryVLA, a novel Vision-Language-Action model inspired by human cognitive memory processes, enhancing long-horizon robotic manipulation tasks by incorporating working memory and a perceptual-cognitive memory bank.",
        "tldr_zh": "该论文介绍了MemoryVLA，一种受人类认知记忆过程启发的新型视觉-语言-动作模型，通过结合工作记忆和感知-认知记忆库，从而提升了长时程机器人操作任务的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation",
        "summary": "Existing video avatar models can produce fluid human animations, yet they\nstruggle to move beyond mere physical likeness to capture a character's\nauthentic essence. Their motions typically synchronize with low-level cues like\naudio rhythm, lacking a deeper semantic understanding of emotion, intent, or\ncontext. To bridge this gap, \\textbf{we propose a framework designed to\ngenerate character animations that are not only physically plausible but also\nsemantically coherent and expressive.} Our model, \\textbf{OmniHuman-1.5}, is\nbuilt upon two key technical contributions. First, we leverage Multimodal Large\nLanguage Models to synthesize a structured textual representation of conditions\nthat provides high-level semantic guidance. This guidance steers our motion\ngenerator beyond simplistic rhythmic synchronization, enabling the production\nof actions that are contextually and emotionally resonant. Second, to ensure\nthe effective fusion of these multimodal inputs and mitigate inter-modality\nconflicts, we introduce a specialized Multimodal DiT architecture with a novel\nPseudo Last Frame design. The synergy of these components allows our model to\naccurately interpret the joint semantics of audio, images, and text, thereby\ngenerating motions that are deeply coherent with the character, scene, and\nlinguistic content. Extensive experiments demonstrate that our model achieves\nleading performance across a comprehensive set of metrics, including lip-sync\naccuracy, video quality, motion naturalness and semantic consistency with\ntextual prompts. Furthermore, our approach shows remarkable extensibility to\ncomplex scenarios, such as those involving multi-person and non-human subjects.\nHomepage: \\href{https://omnihuman-lab.github.io/v1_5/}",
        "url": "http://arxiv.org/abs/2508.19209v1",
        "published_date": "2025-08-26T17:15:26+00:00",
        "updated_date": "2025-08-26T17:15:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianwen Jiang",
            "Weihong Zeng",
            "Zerong Zheng",
            "Jiaqi Yang",
            "Chao Liang",
            "Wang Liao",
            "Han Liang",
            "Yuan Zhang",
            "Mingyuan Gao"
        ],
        "tldr": "The paper introduces OmniHuman-1.5, a framework for generating semantically coherent and expressive character animations by using Multimodal Large Language Models and a specialized Multimodal DiT architecture, achieving state-of-the-art performance in lip-sync accuracy, video quality, motion naturalness, and semantic consistency.",
        "tldr_zh": "该论文介绍了OmniHuman-1.5，一个通过使用多模态大型语言模型和专门的多模态DiT架构来生成语义连贯且富有表现力的人物动画的框架，在唇形同步准确性、视频质量、动作自然度和语义一致性方面实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Enhancing Document VQA Models via Retrieval-Augmented Generation",
        "summary": "Document Visual Question Answering (Document VQA) must cope with documents\nthat span dozens of pages, yet leading systems still concatenate every page or\nrely on very large vision-language models, both of which are memory-hungry.\nRetrieval-Augmented Generation (RAG) offers an attractive alternative, first\nretrieving a concise set of relevant segments before generating answers from\nthis selected evidence. In this paper, we systematically evaluate the impact of\nincorporating RAG into Document VQA through different retrieval variants -\ntext-based retrieval using OCR tokens and purely visual retrieval without OCR -\nacross multiple models and benchmarks. Evaluated on the multi-page datasets\nMP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the\n\"concatenate-all-pages\" baseline by up to +22.5 ANLS, while the visual variant\nachieves +5.0 ANLS improvement without requiring any text extraction. An\nablation confirms that retrieval and reranking components drive most of the\ngain, whereas the layout-guided chunking strategy - proposed in several recent\nworks to leverage page structure - fails to help on these datasets. Our\nexperiments demonstrate that careful evidence selection consistently boosts\naccuracy across multiple model sizes and multi-page benchmarks, underscoring\nits practical value for real-world Document VQA.",
        "url": "http://arxiv.org/abs/2508.18984v1",
        "published_date": "2025-08-26T12:32:55+00:00",
        "updated_date": "2025-08-26T12:32:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Eric López",
            "Artemis Llabrés",
            "Ernest Valveny"
        ],
        "tldr": "This paper explores Retrieval-Augmented Generation (RAG) for Document VQA, showing significant accuracy improvements by retrieving relevant document segments before answer generation, especially with text-based retrieval.",
        "tldr_zh": "本文探讨了检索增强生成（RAG）在文档视觉问答（Document VQA）中的应用，通过在生成答案之前检索相关的文档片段，显著提高了准确性，特别是基于文本的检索。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) are increasingly deployed in real-world\napplications, but their high inference cost makes them vulnerable to resource\nconsumption attacks. Prior attacks attempt to extend VLM output sequences by\noptimizing adversarial images, thereby increasing inference costs. However,\nthese extended outputs often introduce irrelevant abnormal content,\ncompromising attack stealthiness. This trade-off between effectiveness and\nstealthiness poses a major limitation for existing attacks. To address this\nchallenge, we propose \\textit{Hidden Tail}, a stealthy resource consumption\nattack that crafts prompt-agnostic adversarial images, inducing VLMs to\ngenerate maximum-length outputs by appending special tokens invisible to users.\nOur method employs a composite loss function that balances semantic\npreservation, repetitive special token induction, and suppression of the\nend-of-sequence (EOS) token, optimized via a dynamic weighting strategy.\nExtensive experiments show that \\textit{Hidden Tail} outperforms existing\nattacks, increasing output length by up to 19.2$\\times$ and reaching the\nmaximum token limit, while preserving attack stealthiness. These results\nhighlight the urgent need to improve the robustness of VLMs against\nefficiency-oriented adversarial threats. Our code is available at\nhttps://github.com/zhangrui4041/Hidden_Tail.",
        "url": "http://arxiv.org/abs/2508.18805v1",
        "published_date": "2025-08-26T08:40:22+00:00",
        "updated_date": "2025-08-26T08:40:22+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Rui Zhang",
            "Zihan Wang",
            "Tianli Yang",
            "Hongwei Li",
            "Wenbo Jiang",
            "Qingchuan Zhao",
            "Yang Liu",
            "Guowen Xu"
        ],
        "tldr": "The paper introduces 'Hidden Tail,' a novel adversarial attack against VLMs that crafts stealthy, prompt-agnostic adversarial images to induce maximum-length output sequences, thereby causing significant resource consumption without compromising stealth.",
        "tldr_zh": "该论文介绍了'Hidden Tail'，一种新型对抗攻击，通过构造隐蔽的、与提示无关的对抗图像，诱导视觉语言模型生成最大长度的输出序列，从而在不损害隐蔽性的前提下，导致显著的资源消耗。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]