[
    {
        "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
        "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
        "url": "http://arxiv.org/abs/2601.16210v1",
        "published_date": "2026-01-22T18:58:55+00:00",
        "updated_date": "2026-01-22T18:58:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Onkar Susladkar",
            "Tushar Prakash",
            "Adheesh Juvekar",
            "Kiet A. Nguyen",
            "Dong-Hwan Jang",
            "Inderjit S Dhillon",
            "Ismini Lourentzou"
        ],
        "tldr": "PyraTok introduces a language-aligned pyramidal tokenizer for video understanding and generation, using multi-scale, language-guided quantization to achieve SOTA results across several video tasks.",
        "tldr_zh": "PyraTok 引入了一种语言对齐的金字塔式分词器，用于视频理解和生成，通过多尺度、语言引导的量化，在多个视频任务上取得了 SOTA 结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
        "url": "http://arxiv.org/abs/2601.16208v1",
        "published_date": "2026-01-22T18:58:16+00:00",
        "updated_date": "2026-01-22T18:58:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengbang Tong",
            "Boyang Zheng",
            "Ziteng Wang",
            "Bingda Tang",
            "Nanye Ma",
            "Ellis Brown",
            "Jihan Yang",
            "Rob Fergus",
            "Yann LeCun",
            "Saining Xie"
        ],
        "tldr": "This paper explores scaling Representation Autoencoders (RAEs) for large-scale text-to-image generation, demonstrating improved performance and stability compared to VAE-based diffusion models, especially at larger scales.",
        "tldr_zh": "本文探索了使用表征自编码器 (RAEs) 进行大规模文本到图像生成，结果表明，与基于 VAE 的扩散模型相比，RAEs 具有更好的性能和稳定性，尤其是在更大规模下。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing",
        "summary": "Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.",
        "url": "http://arxiv.org/abs/2601.16200v1",
        "published_date": "2026-01-22T18:52:21+00:00",
        "updated_date": "2026-01-22T18:52:21+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Song Xia",
            "Meiwen Ding",
            "Chenqi Kong",
            "Wenhan Yang",
            "Xudong Jiang"
        ],
        "tldr": "This paper proposes Feature-space Smoothing (FS) with Purifier and Smoothness Mapper (PSM) to enhance the robustness of MLLMs against adversarial attacks, achieving significant reduction in attack success rates without retraining.",
        "tldr_zh": "本文提出了特征空间平滑（FS）方法，结合净化器和平滑度映射器（PSM），以增强多模态大语言模型（MLLM）对抗攻击的鲁棒性，无需重新训练即可显著降低攻击成功率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]