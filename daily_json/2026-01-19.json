[
    {
        "title": "VIRTUE: Versatile Video Retrieval Through Unified Embeddings",
        "summary": "Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.",
        "url": "http://arxiv.org/abs/2601.12193v1",
        "published_date": "2026-01-17T23:13:38+00:00",
        "updated_date": "2026-01-17T23:13:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaunak Halbe",
            "Bhagyashree Puranik",
            "Jayakrishnan Unnikrishnan",
            "Kushan Thakkar",
            "Vimal Bhat",
            "Toufiq Parag"
        ],
        "tldr": "The paper introduces VIRTUE, an MLLM-based framework for versatile video retrieval that unifies corpus and moment-level retrieval with composed multimodal queries, achieving strong zero-shot performance and competitive results with specialized models.",
        "tldr_zh": "该论文介绍了VIRTUE，一个基于MLLM的通用视频检索框架，它统一了语料库和时刻级别的检索以及组合的多模态查询，实现了强大的零样本性能，并且能够与专门的模型相媲美。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation",
        "summary": "Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.",
        "url": "http://arxiv.org/abs/2601.12493v1",
        "published_date": "2026-01-18T17:06:09+00:00",
        "updated_date": "2026-01-18T17:06:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mehrdad Noori",
            "Gustavo Adolfo Vargas Hakim",
            "David Osowiechi",
            "Fereshteh Shakeri",
            "Ali Bahri",
            "Moslem Yazdanpanah",
            "Sahar Dastani",
            "Ismail Ben Ayed",
            "Christian Desrosiers"
        ],
        "tldr": "The paper introduces Histopath-C, a benchmark for evaluating the robustness of histopathology vision-language models (VLMs) under realistic domain shifts, and proposes LATTE, a test-time adaptation method that outperforms existing approaches.",
        "tldr_zh": "该论文介绍了Histopath-C，一个用于评估组织病理学视觉-语言模型（VLM）在真实领域偏移下的鲁棒性的基准，并提出了LATTE，一种优于现有方法的测试时自适应方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeuralFur: Animal Fur Reconstruction From Multi-View Images",
        "summary": "Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.",
        "url": "http://arxiv.org/abs/2601.12481v1",
        "published_date": "2026-01-18T16:46:38+00:00",
        "updated_date": "2026-01-18T16:46:38+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Vanessa Sklyarova",
            "Berna Kabadayi",
            "Anastasios Yiannakidis",
            "Giorgio Becherini",
            "Michael J. Black",
            "Justus Thies"
        ],
        "tldr": "The paper introduces NeuralFur, a novel multi-view 3D fur reconstruction method that leverages a Vision Language Model (VLM) to guide strand-based fur generation on animals, addressing the lack of animal fur datasets.",
        "tldr_zh": "该论文介绍了 NeuralFur，一种新颖的多视角 3D 毛发重建方法，它利用视觉语言模型 (VLM) 来指导动物毛发的基于股线的生成，解决了缺乏动物毛发数据集的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models",
        "summary": "Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.",
        "url": "http://arxiv.org/abs/2601.12304v1",
        "published_date": "2026-01-18T08:05:33+00:00",
        "updated_date": "2026-01-18T08:05:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wutao Chen",
            "Huaqin Zou",
            "Chen Wan",
            "Lifeng Huang"
        ],
        "tldr": "This paper introduces a two-stage globally-diverse adversarial attack (2S-GDA) framework to improve the attack success rate against vision-language pre-training models in black-box scenarios by enhancing textual and visual perturbation diversity.",
        "tldr_zh": "本文提出了一种两阶段全局多样性对抗攻击（2S-GDA）框架，旨在通过增强文本和视觉扰动的多样性，提高黑盒场景下对视觉语言预训练模型的攻击成功率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training",
        "summary": "The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.",
        "url": "http://arxiv.org/abs/2601.12282v1",
        "published_date": "2026-01-18T06:42:24+00:00",
        "updated_date": "2026-01-18T06:42:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pralaypati Ta",
            "Sriram Venkatesaperumal",
            "Keerthi Ram",
            "Mohanasankar Sivaprakasam"
        ],
        "tldr": "CytoCLIP uses contrastive language-image pre-training (CLIP) to automatically identify brain regions in histological sections based on their cytoarchitecture, achieving high F1 scores in classification tasks.",
        "tldr_zh": "CytoCLIP利用对比语言-图像预训练（CLIP）自动识别组织切片中的大脑区域，基于其细胞结构，并在分类任务中实现了较高的F1分数。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Federated Joint Learning for Domain and Class Generalization",
        "summary": "Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \\textbf{Fed}erated Joint Learning for \\textbf{D}omain and \\textbf{C}lass \\textbf{G}eneralization, termed \\textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \\textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.",
        "url": "http://arxiv.org/abs/2601.12253v1",
        "published_date": "2026-01-18T04:24:11+00:00",
        "updated_date": "2026-01-18T04:24:11+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Haoran Xu",
            "Jiaze Li",
            "Jianzhong Ju",
            "Zhenbo Luo"
        ],
        "tldr": "The paper introduces FedDCG, a federated learning approach that jointly addresses class and domain generalization for visual-language models by using domain grouping and knowledge decoupling.",
        "tldr_zh": "该论文介绍了一种名为FedDCG的联邦学习方法，通过使用域分组和知识解耦，联合解决视觉语言模型的类泛化和域泛化问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adversarial Defense in Vision-Language Models: An Overview",
        "summary": "The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.",
        "url": "http://arxiv.org/abs/2601.12443v1",
        "published_date": "2026-01-18T14:57:51+00:00",
        "updated_date": "2026-01-18T14:57:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaowei Fu",
            "Lei Zhang"
        ],
        "tldr": "This survey paper reviews adversarial defense strategies for Vision-Language Models (VLMs), categorizing them into training-time, test-time adaptation, and training-free approaches, and discusses their strengths, limitations, and challenges.",
        "tldr_zh": "该综述论文回顾了视觉语言模型（VLM）的对抗防御策略，将其分为训练时防御、测试时自适应防御和无训练防御三种方法，并讨论了它们的优缺点和挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation",
        "summary": "Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2601.12326v1",
        "published_date": "2026-01-18T09:20:09+00:00",
        "updated_date": "2026-01-18T09:20:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jing Zhang",
            "Bingjie Fan"
        ],
        "tldr": "EmoKGEdit is a training-free image emotion editing framework that uses a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle emotional cues and preserve visual structure during emotion editing, outperforming existing methods.",
        "tldr_zh": "EmoKGEdit是一个免训练的图像情感编辑框架，它使用多模态情感关联知识图谱（MSA-KG）来分离情感线索，并在情感编辑过程中保持视觉结构，优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Less is More: Label-Guided Summarization of Procedural and Instructional Videos",
        "summary": "Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.",
        "url": "http://arxiv.org/abs/2601.12243v1",
        "published_date": "2026-01-18T03:41:48+00:00",
        "updated_date": "2026-01-18T03:41:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shreya Rajpal",
            "Michal Golovanesky",
            "Carsten Eickhoff"
        ],
        "tldr": "The paper introduces PRISM, a three-stage framework for generating semantically grounded video summaries using adaptive visual sampling, label-driven keyframe anchoring, and contextual validation with an LLM, achieving high semantic retention with minimal frames.",
        "tldr_zh": "该论文介绍了PRISM，一个三阶段框架，通过自适应视觉采样、标签驱动的关键帧锚定和使用LLM的上下文验证来生成语义明确的视频摘要，以最少的帧数实现了高的语义保留。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]