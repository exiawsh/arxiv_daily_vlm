[
    {
        "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs",
        "summary": "Multimodal Large Language Models (MLLMs) encounter significant computational\nand memory bottlenecks from the massive number of visual tokens generated by\nhigh-resolution images or multi-image inputs. Previous token compression\ntechniques are often constrained by heuristic rules that risk discarding\ncritical information. They may suffer from biases, such as attention sinks,\nthat lead to sharp performance drops under aggressive compression ratios. To\naddress these limitations, we reformulate token compression as a lightweight\nplug-and-play framework that reformulates token compression into an end-to-end\nlearnable decision process. To be specific, we propose VisionSelector, a scorer\nmodule decoupled from the MLLM backbone that incorporates a differentiable\nTop-K mechanism and a curriculum annealing strategy to bridge the\ntraining-inference gap, enabling efficient and adaptive token selection various\narbitrary compression rates. Remarkably lightweight with only 12.85M trainable\nparameters, VisionSelector demonstrates generalization across various\ncompression rates and adaptively identifying critical tokens. This leads to\nsuperior performance across all compression budgets, evidenced by preserving\n100% accuracy on MME with 30% retention budget, outperforming prior methods by\n12.14% at 10% retention budget, and doubling prefill speed. Our code is\navailable at https://github.com/JulietChoo/VisionSelector .",
        "url": "http://arxiv.org/abs/2510.16598v1",
        "published_date": "2025-10-18T17:54:18+00:00",
        "updated_date": "2025-10-18T17:54:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaying Zhu",
            "Yurui Zhu",
            "Xin Lu",
            "Wenrui Yan",
            "Dong Li",
            "Kunlin Liu",
            "Xueyang Fu",
            "Zheng-Jun Zha"
        ],
        "tldr": "The paper introduces VisionSelector, a learnable visual token compression module for MLLMs, enabling efficient and adaptive token selection with minimal parameters and superior performance at various compression rates.",
        "tldr_zh": "该论文介绍了VisionSelector，一个用于MLLM的可学习视觉令牌压缩模块，它能够以最少的参数实现高效和自适应的令牌选择，并在各种压缩率下表现出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense",
        "summary": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased.",
        "url": "http://arxiv.org/abs/2510.16596v1",
        "published_date": "2025-10-18T17:49:43+00:00",
        "updated_date": "2025-10-18T17:49:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yiyang Huang",
            "Liang Shi",
            "Yitian Zhang",
            "Yi Xu",
            "Yun Fu"
        ],
        "tldr": "This paper identifies and addresses object hallucination issues in LVLM visual encoders using a training-free framework called SHIELD, showing improved performance across benchmarks.",
        "tldr_zh": "本文提出了一个名为SHIELD的免训练框架，用于解决LVLM视觉编码器中的对象幻觉问题，并通过实验证明了其在多个基准测试中的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries",
        "summary": "Text-to-image (T2I) models, though exhibiting remarkable creativity in image\ngeneration, can be exploited to produce unsafe images. Existing safety\nmeasures, e.g., content moderation or model alignment, fail in the presence of\nwhite-box adversaries who know and can adjust model parameters, e.g., by\nfine-tuning. This paper presents a novel defensive framework, named Patronus,\nwhich equips T2I models with holistic protection to defend against white-box\nadversaries. Specifically, we design an internal moderator that decodes unsafe\ninput features into zero vectors while ensuring the decoding performance of\nbenign input features. Furthermore, we strengthen the model alignment with a\ncarefully designed non-fine-tunable learning mechanism, ensuring the T2I model\nwill not be compromised by malicious fine-tuning. We conduct extensive\nexperiments to validate the intactness of the performance on safe content\ngeneration and the effectiveness of rejecting unsafe content generation.\nResults also confirm the resilience of Patronus against various fine-tuning\nattacks by white-box adversaries.",
        "url": "http://arxiv.org/abs/2510.16581v1",
        "published_date": "2025-10-18T17:02:31+00:00",
        "updated_date": "2025-10-18T17:02:31+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Xinfeng Li",
            "Shengyuan Pang",
            "Jialin Wu",
            "Jiangyi Deng",
            "Huanlong Zhong",
            "Yanjiao Chen",
            "Jie Zhang",
            "Wenyuan Xu"
        ],
        "tldr": "The paper introduces Patronus, a novel defensive framework that protects text-to-image models from white-box adversaries by using an internal moderator and a non-fine-tunable learning mechanism to ensure safety and prevent malicious fine-tuning.",
        "tldr_zh": "该论文介绍了Patronus，一种新型防御框架，通过使用内部调节器和不可微调的学习机制来保护文本到图像模型免受白盒对抗攻击，从而确保安全并防止恶意微调。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions",
        "summary": "Despite recent advances, vision-language models trained with standard\ncontrastive objectives still struggle with compositional reasoning -- the\nability to understand structured relationships between visual and linguistic\nelements. This shortcoming is largely due to the tendency of the text encoder\nto focus on individual words rather than their relations, a limitation\nreinforced by contrastive training that primarily aligns words with visual\nobjects. In this paper, we introduce REconstruction and Alignment of text\nDescriptions (READ), a fine-tuning method designed to enhance compositional\nreasoning by adding two auxiliary objectives to the contrastive learning: (1) a\ntoken-level reconstruction objective, where a frozen pre-trained decoder\nreconstructs alternative captions based on the embedding of the original\ncaption; and (2) a sentence-level alignment objective, which explicitly aligns\nparaphrased sentences in the embedding space. We show that READ-CLIP, a model\nderived by applying the READ method to the pre-trained CLIP model, achieves the\nstate-of-the-art performance across five major compositional reasoning\nbenchmarks, outperforming the strongest conventional fine-tuning baseline by up\nto 4.1%. Furthermore, applying the READ to existing CLIP variants (including\nNegCLIP and FSC-CLIP) also improves performance on these benchmarks.\nQuantitative and qualitative analyses reveal that our proposed objectives --\nreconstruction and alignment -- offer complementary benefits: the former\nencourages the encoder to capture relationships between words within a caption,\nwhile the latter ensures consistent representations for paraphrases expressed\nwith different wording.",
        "url": "http://arxiv.org/abs/2510.16540v1",
        "published_date": "2025-10-18T15:35:36+00:00",
        "updated_date": "2025-10-18T15:35:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jihoon Kwon",
            "Kyle Min",
            "Jy-yong Sohn"
        ],
        "tldr": "The paper introduces READ, a fine-tuning method for CLIP that enhances compositional reasoning by adding token-level reconstruction and sentence-level alignment objectives, achieving state-of-the-art performance on compositional reasoning benchmarks.",
        "tldr_zh": "该论文介绍了一种名为READ的CLIP微调方法，通过添加token级别的重建和句子级别的对齐目标来增强组合推理能力，并在组合推理基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning",
        "summary": "Vision-language models (VLMs) have shown remarkable abilities by integrating\nlarge language models with visual inputs. However, they often fail to utilize\nvisual evidence adequately, either depending on linguistic priors in\nvision-centric tasks or resorting to textual shortcuts during reasoning.\nAlthough reinforcement learning (RL) can align models with desired behaviors,\nits application to VLMs has been hindered by the lack of scalable and reliable\nreward mechanisms. To overcome this challenge, we propose SSL4RL, a novel\nframework that leverages self-supervised learning (SSL) tasks as a source of\nverifiable rewards for RL-based fine-tuning. Our approach reformulates SSL\nobjectives-such as predicting image rotation or reconstructing masked\npatches-into dense, automatic reward signals, eliminating the need for human\npreference data or unreliable AI evaluators. Experiments show that SSL4RL\nsubstantially improves performance on both vision-centric and vision-language\nreasoning benchmarks. Furthermore, through systematic ablations, we identify\nkey factors-such as task difficulty, model scale, and semantic alignment with\nthe target domain-that influence the effectiveness of SSL4RL tasks, offering\nnew design principles for future work. We also demonstrate the framework's\ngenerality by applying it to graph learning, where it yields significant gains.\nSSL4RL establishes a versatile and effective paradigm for aligning multimodal\nmodels using verifiable, self-supervised objectives.",
        "url": "http://arxiv.org/abs/2510.16416v1",
        "published_date": "2025-10-18T09:22:40+00:00",
        "updated_date": "2025-10-18T09:22:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaojun Guo",
            "Runyu Zhou",
            "Yifei Wang",
            "Qi Zhang",
            "Chenheng Zhang",
            "Stefanie Jegelka",
            "Xiaohan Wang",
            "Jiajun Chai",
            "Guojun Yin",
            "Wei Lin",
            "Yisen Wang"
        ],
        "tldr": "The paper introduces SSL4RL, a framework that uses self-supervised learning tasks as intrinsic rewards for RL-based fine-tuning of Vision-Language Models, improving performance on vision-centric and vision-language reasoning benchmarks.",
        "tldr_zh": "该论文介绍了SSL4RL，一个利用自监督学习任务作为内在奖励的框架，用于对视觉语言模型进行基于强化学习的微调，从而提高模型在视觉中心和视觉语言推理基准测试上的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
        "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.",
        "url": "http://arxiv.org/abs/2510.16410v1",
        "published_date": "2025-10-18T08:53:08+00:00",
        "updated_date": "2025-10-18T08:53:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changyue Shi",
            "Minghao Chen",
            "Yiping Mao",
            "Chuxiao Yang",
            "Xinyuan Hu",
            "Jiajun Ding",
            "Zhou Yu"
        ],
        "tldr": "The paper introduces REALM, an MLLM-agent framework for open-world 3D reasoning, segmentation, and editing on Gaussian Splatting, using a global-to-local spatial grounding strategy to overcome viewpoint sensitivity and achieve robust performance on various tasks.",
        "tldr_zh": "该论文介绍了一种名为REALM的MLLM代理框架，用于在高斯溅射上进行开放世界的3D推理、分割和编辑。该框架采用全局到局部的空间定位策略，克服了视角敏感性，并在各种任务上实现了稳健的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RL makes MLLMs see better than SFT",
        "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/",
        "url": "http://arxiv.org/abs/2510.16333v1",
        "published_date": "2025-10-18T03:37:17+00:00",
        "updated_date": "2025-10-18T03:37:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Junha Song",
            "Sangdoo Yun",
            "Dongyoon Han",
            "Jaegul Choo",
            "Byeongho Heo"
        ],
        "tldr": "This paper investigates the impact of RL and SFT training on MLLM vision encoders, finding that RL leads to stronger visual representations and proposes a method (PIVOT) for efficiently training high-performing vision encoders for MLLMs.",
        "tldr_zh": "该论文研究了RL和SFT训练对MLLM视觉编码器的影响，发现RL可以产生更强的视觉表征，并提出了一种名为PIVOT的方法，用于高效训练高性能的MLLM视觉编码器。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models",
        "summary": "Video anomaly detection (VAD) has rapidly advanced by recent development of\nVision-Language Models (VLMs). While these models offer superior zero-shot\ndetection capabilities, their immense computational cost and unstable visual\ngrounding performance hinder real-time deployment. To overcome these\nchallenges, we introduce Cerberus, a two-stage cascaded system designed for\nefficient yet accurate real-time VAD. Cerberus learns normal behavioral rules\noffline, and combines lightweight filtering with fine-grained VLM reasoning\nduring online inference. The performance gains of Cerberus come from two key\ninnovations: motion mask prompting and rule-based deviation detection. The\nformer directs the VLM's attention to regions relevant to motion, while the\nlatter identifies anomalies as deviations from learned norms rather than\nenumerating possible anomalies. Extensive evaluations on four datasets show\nthat Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a\n151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art\nVLM-based VAD methods, establishing it as a practical solution for real-time\nvideo analytics.",
        "url": "http://arxiv.org/abs/2510.16290v1",
        "published_date": "2025-10-18T01:27:23+00:00",
        "updated_date": "2025-10-18T01:27:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yue Zheng",
            "Xiufang Shi",
            "Jiming Chen",
            "Yuanchao Shu"
        ],
        "tldr": "Cerberus is a two-stage cascaded system for real-time video anomaly detection that uses motion mask prompting and rule-based deviation detection to achieve high accuracy and speed.",
        "tldr_zh": "Cerberus是一个用于实时视频异常检测的两阶段级联系统，它使用运动掩模提示和基于规则的偏差检测来实现高精度和高速度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation",
        "summary": "In this work we concentrate on the task of goal-oriented Vision-and-Language\nNavigation (VLN). Existing methods often make decisions based on historical\ninformation, overlooking the future implications and long-term outcomes of the\nactions. In contrast, we aim to develop a foresighted agent. Specifically, we\ndraw upon Q-learning to train a Q-model using large-scale unlabeled trajectory\ndata, in order to learn the general knowledge regarding the layout and object\nrelations within indoor scenes. This model can generate a Q-feature, analogous\nto the Q-value in traditional Q-network, for each candidate action, which\ndescribes the potential future information that may be observed after taking\nthe specific action. Subsequently, a cross-modal future encoder integrates the\ntask-agnostic Q-feature with navigation instructions to produce a set of action\nscores reflecting future prospects. These scores, when combined with the\noriginal scores based on history, facilitate an A*-style searching strategy to\neffectively explore the regions that are more likely to lead to the\ndestination. Extensive experiments conducted on widely used goal-oriented VLN\ndatasets validate the effectiveness of the proposed method.",
        "url": "http://arxiv.org/abs/2510.16457v1",
        "published_date": "2025-10-18T11:29:33+00:00",
        "updated_date": "2025-10-18T11:29:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Peiran Xu",
            "Xicheng Gong",
            "Yadong MU"
        ],
        "tldr": "This paper introduces NavQ, a Q-learning-based approach for Vision-and-Language Navigation that leverages large-scale unlabeled trajectory data to learn a Q-model, enabling foresighted action selection by considering future implications.",
        "tldr_zh": "本文介绍了一种基于Q学习的视觉语言导航方法NavQ，该方法利用大规模无标签轨迹数据训练Q模型，通过考虑未来影响来实现有远见的动作选择。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models",
        "summary": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques.",
        "url": "http://arxiv.org/abs/2510.16295v1",
        "published_date": "2025-10-18T01:39:28+00:00",
        "updated_date": "2025-10-18T01:39:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ryoto Miyamoto",
            "Xin Fan",
            "Fuyuko Kido",
            "Tsuneo Matsumoto",
            "Hayato Yamana"
        ],
        "tldr": "This paper introduces OpenLVLM-MIA, a controlled benchmark to address biases in evaluating membership inference attacks against large vision-language models, showing that current attacks often perform at random chance under unbiased conditions.",
        "tldr_zh": "本文介绍了OpenLVLM-MIA，一个受控的基准测试，旨在解决评估针对大型视觉语言模型的成员推理攻击中的偏差问题，表明当前攻击在无偏条件下通常表现得与随机猜测无异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]