[
    {
        "title": "CogDoc: Towards Unified thinking in Documents",
        "summary": "Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution \"Fast Reading\" phase for scalable information localization,followed by a high-resolution \"Focused Thinking\" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the \"policy conflict\" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.",
        "url": "http://arxiv.org/abs/2512.12658v1",
        "published_date": "2025-12-14T12:14:17+00:00",
        "updated_date": "2025-12-14T12:14:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qixin Xu",
            "Haozhe Wang",
            "Che Liu",
            "Fangzhen Lin",
            "Wenhu Chen"
        ],
        "tldr": "The paper introduces CogDoc, a coarse-to-fine framework for document reasoning that achieves state-of-the-art performance for its model size on visually rich document benchmarks, even surpassing larger proprietary models like GPT-4o, using direct reinforcement learning.",
        "tldr_zh": "该论文介绍了CogDoc，一种用于文档推理的粗到细框架，在视觉丰富的文档基准测试上实现了其模型大小的最佳性能，甚至超过了像GPT-4o这样更大的专有模型，它使用了直接强化学习。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation",
        "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.",
        "url": "http://arxiv.org/abs/2512.12595v1",
        "published_date": "2025-12-14T08:28:50+00:00",
        "updated_date": "2025-12-14T08:28:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Karthikeya KV"
        ],
        "tldr": "This paper introduces a Vision-Enhanced LLM framework using rectified flow and bidirectional tokenization for high-resolution image synthesis and multimodal data interpretation, achieving improved image quality and reduced computational cost compared to diffusion models.",
        "tldr_zh": "本文介绍了一个视觉增强LLM框架，该框架采用修正流和双向标记化技术，用于高分辨率图像合成和多模态数据解释，与扩散模型相比，提高了图像质量并降低了计算成本。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation",
        "summary": "Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.",
        "url": "http://arxiv.org/abs/2512.12772v1",
        "published_date": "2025-12-14T17:23:21+00:00",
        "updated_date": "2025-12-14T17:23:21+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Jianghan Chao",
            "Jianzhang Gao",
            "Wenhui Tan",
            "Yuchong Sun",
            "Ruihua Song",
            "Liyun Ru"
        ],
        "tldr": "The paper introduces JointAVBench, a new benchmark for evaluating Omni-LLMs' joint audio-visual reasoning capabilities, addressing limitations in existing datasets by focusing on multi-modal dependency, diverse audio types, and varying scene spans. It reveals significant room for improvement in current Omni-LLMs.",
        "tldr_zh": "该论文介绍了 JointAVBench，这是一个新的基准，用于评估 Omni-LLM 的联合音视频推理能力。它通过关注多模态依赖、多样化的音频类型和不同的场景跨度，解决了现有数据集的局限性。结果表明，目前的 Omni-LLM 仍有很大的改进空间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning",
        "summary": "Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.",
        "url": "http://arxiv.org/abs/2512.12756v1",
        "published_date": "2025-12-14T16:41:29+00:00",
        "updated_date": "2025-12-14T16:41:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Jiang",
            "Dingkang Yang",
            "Minghao Han",
            "Jinghang Han",
            "Zizhi Chen",
            "Yizhou Liu",
            "Mingcheng Li",
            "Peng Zhai",
            "Lihua Zhang"
        ],
        "tldr": "The paper introduces FysicsWorld, a new unified full-modality benchmark for any-to-any understanding, generation, and reasoning across image, video, audio, and text, and evaluates 30+ baselines to expose performance gaps and limitations.",
        "tldr_zh": "该论文介绍了 FysicsWorld，这是一个新的统一全模态基准，用于跨图像、视频、音频和文本的任意到任意理解、生成和推理。它评估了 30 多个基线模型，以揭示性能差距和局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning",
        "summary": "Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.",
        "url": "http://arxiv.org/abs/2512.12701v1",
        "published_date": "2025-12-14T14:11:32+00:00",
        "updated_date": "2025-12-14T14:11:32+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xue Li",
            "Xiaonan Song",
            "Henry Hu"
        ],
        "tldr": "This paper introduces Adaptive Token Pruning (ATP), a method to reduce the computational cost of VLMs by dynamically retaining only the most informative tokens during inference, achieving significant speedups with minimal accuracy loss.",
        "tldr_zh": "本文介绍了自适应令牌修剪（ATP），该方法通过在推理过程中动态保留信息量最大的令牌来降低 VLM 的计算成本，从而在精度损失最小的情况下实现显著加速。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning",
        "summary": "Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing \"RL over SFT\" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.",
        "url": "http://arxiv.org/abs/2512.12690v1",
        "published_date": "2025-12-14T13:46:42+00:00",
        "updated_date": "2025-12-14T13:46:42+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yongcan Yu",
            "Lingxiao He",
            "Shuo Lu",
            "Lijun Sheng",
            "Yinuo Xu",
            "Yanbo Wang",
            "Kuangpu Guo",
            "Jianjie Cheng",
            "Meng Wang",
            "Qianlong Xie",
            "Xingxing Wang",
            "Dapeng Hu",
            "Jian Liang"
        ],
        "tldr": "This paper challenges the prevailing view that Reinforcement Learning (RL) is superior to Supervised Fine-Tuning (SFT) for Vision-Language Model (VLM) reasoning, demonstrating that SFT can be more effective, data-efficient, and generalizable in certain scenarios, particularly with weaker models.",
        "tldr_zh": "该论文挑战了目前强化学习(RL)优于监督微调(SFT)进行视觉语言模型(VLM)推理的观点，证明了SFT在某些情况下可能更有效、数据效率更高且更具泛化性，尤其是在较弱的模型上。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment",
        "summary": "CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.",
        "url": "http://arxiv.org/abs/2512.12678v1",
        "published_date": "2025-12-14T13:03:20+00:00",
        "updated_date": "2025-12-14T13:03:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fatimah Zohra",
            "Chen Zhao",
            "Hani Itani",
            "Bernard Ghanem"
        ],
        "tldr": "β-CLIP enhances vision-language alignment by introducing a multi-granular text-conditioned contrastive learning framework, achieving state-of-the-art results on dense alignment tasks without using hard negatives.",
        "tldr_zh": "β-CLIP通过引入一种多粒度文本条件对比学习框架来增强视觉-语言对齐，在密集对齐任务上实现了最先进的结果，且未使用难负例。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
        "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.",
        "url": "http://arxiv.org/abs/2512.12675v1",
        "published_date": "2025-12-14T12:58:19+00:00",
        "updated_date": "2025-12-14T12:58:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuran Wang",
            "Bohan Zeng",
            "Chengzhuo Tong",
            "Wenxuan Liu",
            "Yang Shi",
            "Xiaochen Ma",
            "Hao Liang",
            "Yuanxing Zhang",
            "Wentao Zhang"
        ],
        "tldr": "The paper introduces Scone, a unified image generation method addressing both multi-subject composition and distinction (identifying correct subjects) by using a two-stage training scheme with semantic alignment and attention-based masking, along with a new benchmark, SconeEval.",
        "tldr_zh": "该论文介绍了一种统一的图像生成方法Scone，通过两阶段训练方案，利用语义对齐和基于注意力的掩码，解决多主体组合和区分（识别正确主体）的问题，并提出了一个新的基准SconeEval。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model",
        "summary": "Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.",
        "url": "http://arxiv.org/abs/2512.12633v1",
        "published_date": "2025-12-14T10:40:27+00:00",
        "updated_date": "2025-12-14T10:40:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhou Tao",
            "Shida Wang",
            "Yongxiang Hua",
            "Haoyu Cao",
            "Linli Xu"
        ],
        "tldr": "The paper introduces DiG, a proxy task framework using differential grounding to improve fine-grained visual perception in MLLMs by learning to identify differences between similar image pairs, and demonstrates improved performance on various benchmarks.",
        "tldr_zh": "本文介绍了一种名为DiG的代理任务框架，该框架利用差异性接地来提高多模态大型语言模型中细粒度的视觉感知能力，通过学习识别相似图像对之间的差异，并在各种基准测试中展示了性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
        "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.",
        "url": "http://arxiv.org/abs/2512.12623v1",
        "published_date": "2025-12-14T10:07:45+00:00",
        "updated_date": "2025-12-14T10:07:45+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Chengzhi Liu",
            "Yuzhe Yang",
            "Yue Fan",
            "Qingyue Wei",
            "Sheng Liu",
            "Xin Eric Wang"
        ],
        "tldr": "The paper introduces DMLR, a framework for dynamic multimodal reasoning in latent space using confidence-guided optimization and dynamic visual injection, achieving improved performance and efficiency on multimodal reasoning benchmarks.",
        "tldr_zh": "该论文介绍了DMLR，一个在潜在空间中进行动态多模态推理的框架，它使用置信度引导的优化和动态视觉注入，在多模态推理基准测试中实现了更高的性能和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation",
        "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.",
        "url": "http://arxiv.org/abs/2512.12622v1",
        "published_date": "2025-12-14T09:53:15+00:00",
        "updated_date": "2025-12-14T09:53:15+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zihan Wang",
            "Seungjun Lee",
            "Guangzhao Dai",
            "Gim Hee Lee"
        ],
        "tldr": "The paper introduces D3D-VLP, a dynamic 3D vision-language-planning model that unifies planning, grounding, navigation, and question answering for embodied agents using a 3D Chain-of-Thought and synergistic learning strategy. It achieves SOTA results on multiple navigation and grounding benchmarks.",
        "tldr_zh": "该论文介绍了D3D-VLP，一个动态的3D视觉-语言-规划模型，它使用3D Chain-of-Thought和协同学习策略，统一了具身代理的规划、基础、导航和问答。该模型在多个导航和基础任务的基准测试上取得了SOTA结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models",
        "summary": "In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based \"placement plan\" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.",
        "url": "http://arxiv.org/abs/2512.12596v1",
        "published_date": "2025-12-14T08:30:15+00:00",
        "updated_date": "2025-12-14T08:30:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kei Yoshitake",
            "Kento Hosono",
            "Ken Kobayashi",
            "Kazuhide Nakata"
        ],
        "tldr": "This paper presents a two-stage, content-aware ad banner layout generation method using VLMs, which outperforms saliency-based approaches by explicitly considering image content and producing a placement plan.",
        "tldr_zh": "本文提出了一种使用视觉语言模型(VLM)的两阶段、内容感知的广告横幅布局生成方法。该方法通过显式地考虑图像内容并生成布局计划，优于基于显著性检测的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings",
        "summary": "Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.",
        "url": "http://arxiv.org/abs/2512.12492v1",
        "published_date": "2025-12-13T23:33:05+00:00",
        "updated_date": "2025-12-13T23:33:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Shengkai Xu",
            "Hsiang Lun Kao",
            "Tianxiang Xu",
            "Honghui Zhang",
            "Junqiao Wang",
            "Runmeng Ding",
            "Guanyu Liu",
            "Tianyu Shi",
            "Zhenyu Yu",
            "Guofeng Pan",
            "Ziqian Bi",
            "Yuqi Ouyang"
        ],
        "tldr": "This paper introduces AdaptiveDetector, a two-stage detector-verifier framework using YOLOv11 and a fine-tuned VLM with Group Relative Policy Optimization (GRPO) for zero-shot polyp detection in challenging real-world endoscopic conditions, significantly improving recall while maintaining precision.",
        "tldr_zh": "本文介绍了一种名为AdaptiveDetector的两阶段检测器-验证器框架，该框架使用YOLOv11和一个经过微调的VLM，通过Group Relative Policy Optimization (GRPO) 进行零样本息肉检测，解决了真实内窥镜环境中的挑战性问题，显著提高了召回率，同时保持了精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models",
        "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.",
        "url": "http://arxiv.org/abs/2512.12487v1",
        "published_date": "2025-12-13T23:06:18+00:00",
        "updated_date": "2025-12-13T23:06:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hoang Anh Just",
            "Yifei Fan",
            "Handong Zhao",
            "Jiuxiang Gu",
            "Ruiyi Zhang",
            "Simon Jenni",
            "Kushal Kafle",
            "Ruoxi Jia",
            "Jing Shi"
        ],
        "tldr": "The paper introduces PeRL-VL, a decoupled framework that enhances vision-language models by improving visual perception through a VLM-based description reward and logical reasoning through a text-only reasoning SFT stage, leading to improved accuracy on multimodal benchmarks.",
        "tldr_zh": "该论文介绍了一种解耦框架PeRL-VL，通过VLM描述奖励来改善视觉感知，并通过纯文本推理SFT阶段来提升逻辑推理能力，从而提高多模态基准测试的准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models",
        "summary": "To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.",
        "url": "http://arxiv.org/abs/2512.12571v1",
        "published_date": "2025-12-14T06:30:32+00:00",
        "updated_date": "2025-12-14T06:30:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boyeong Im",
            "Wooseok Lee",
            "Yoojin Kwon",
            "Hyung-Sin Kim"
        ],
        "tldr": "This paper introduces Multi-View Physical-prompt for Test-Time Adaptation (MVP), a novel method that leverages camera exposure settings as physical prompts to enhance the robustness of vision-language models in real-world, sensor-mediated environments, achieving significant performance gains over digital-only TTA.",
        "tldr_zh": "本文介绍了多视图物理提示的测试时自适应（MVP）方法，该方法利用相机曝光设置作为物理提示，以增强视觉语言模型在现实世界传感器环境中的鲁棒性，并在仅数字测试时自适应方面取得了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding",
        "summary": "Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.",
        "url": "http://arxiv.org/abs/2512.12560v1",
        "published_date": "2025-12-14T05:35:11+00:00",
        "updated_date": "2025-12-14T05:35:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinqi Jin",
            "Hanxun Yu",
            "Bohan Yu",
            "Kebin Liu",
            "Jian Liu",
            "Keda Tao",
            "Yixuan Pei",
            "Huan Wang",
            "Fan Dang",
            "Jiangchuan Liu",
            "Weiqiang Wang"
        ],
        "tldr": "This paper introduces StreamingAssistant, a visual token pruning method for accelerating online video understanding by reducing redundancy in both spatial and temporal dimensions, achieving improved accuracy with negligible latency.",
        "tldr_zh": "本文介绍了 StreamingAssistant，一种用于加速在线视频理解的视觉 token 剪枝方法，通过减少空间和时间维度上的冗余，以极低的延迟实现了精度的提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics",
        "summary": "Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.",
        "url": "http://arxiv.org/abs/2512.12424v1",
        "published_date": "2025-12-13T18:37:47+00:00",
        "updated_date": "2025-12-13T18:37:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tue-Thu Van-Dinh",
            "Hoang-Duy Tran",
            "Truong-Binh Duong",
            "Mai-Hanh Pham",
            "Binh-Nam Le-Nguyen",
            "Quoc-Thai Nguyen"
        ],
        "tldr": "The paper introduces ViInfographicVQA, a new benchmark for Vietnamese Infographic Visual Question Answering, featuring single and multi-image question answering to evaluate OCR, layout understanding, and cross-image reasoning in low-resource contexts.",
        "tldr_zh": "本文介绍了一个新的越南语信息图视觉问答基准 ViInfographicVQA，该基准具有单图和多图问答功能，旨在评估低资源环境中的 OCR、布局理解和跨图推理能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]