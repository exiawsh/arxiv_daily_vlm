[
    {
        "title": "MMA: Multimodal Memory Agent",
        "summary": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
        "url": "http://arxiv.org/abs/2602.16493v1",
        "published_date": "2026-02-18T14:30:35+00:00",
        "updated_date": "2026-02-18T14:30:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihao Lu",
            "Wanru Cheng",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "tldr": "The paper introduces MMA, a Multimodal Memory Agent that improves long-horizon multimodal agents by assigning dynamic reliability scores to retrieved memory items, reducing overconfident errors and improving accuracy and safety, particularly in scenarios with conflicting information. They also introduce MMA-Bench, a new benchmark for belief dynamics.",
        "tldr_zh": "该论文介绍了MMA，一种多模态记忆代理，通过为检索到的记忆项分配动态可靠性评分来改进长时多模态代理，减少过度自信的错误并提高准确性和安全性，尤其是在存在冲突信息的情况下。 他们还推出了MMA-Bench，一个新的用于信念动态的基准。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing",
        "summary": "While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.",
        "url": "http://arxiv.org/abs/2602.16455v1",
        "published_date": "2026-02-18T13:40:53+00:00",
        "updated_date": "2026-02-18T13:40:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinsong Li",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Jiaqi Wang",
            "Dahua Lin"
        ],
        "tldr": "The paper introduces Visual Self-Refine (VSR), a novel paradigm for enhancing the visual perception accuracy of LVLMs in tasks like chart parsing, by incorporating pixel-level localization and iterative visual feedback, and also provides a challenging new benchmark.",
        "tldr_zh": "该论文介绍了视觉自精炼 (VSR)，一种通过结合像素级定位和迭代视觉反馈来提高 LVLM 在图表解析等任务中视觉感知准确性的新范例，并提供了一个具有挑战性的新基准。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
        "summary": "Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.",
        "url": "http://arxiv.org/abs/2602.16422v1",
        "published_date": "2026-02-18T12:55:20+00:00",
        "updated_date": "2026-02-18T12:55:20+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ahmet Halici",
            "Ece Tugba Cebeci",
            "Musa Balci",
            "Mustafa Cini",
            "Serkan Sokmen"
        ],
        "tldr": "This paper presents a hierarchical vision-language framework leveraging a frozen pathology foundation model and Transformer decoder for automated histopathology report generation from whole slide images, incorporating multi-resolution patch selection, BioGPT tokenization, and retrieval-based verification.",
        "tldr_zh": "本文提出了一种分层视觉语言框架，利用冻结的病理基础模型和Transformer解码器，从全切片图像自动生成组织病理学报告，结合了多分辨率补丁选择、BioGPT标记化和基于检索的验证。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
        "summary": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.",
        "url": "http://arxiv.org/abs/2602.16412v1",
        "published_date": "2026-02-18T12:37:35+00:00",
        "updated_date": "2026-02-18T12:37:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daichi Yashima",
            "Shuhei Kurita",
            "Yusuke Oda",
            "Komei Sugiura"
        ],
        "tldr": "The paper introduces ReMoRa, a video MLLM that efficiently processes long videos by using motion representations instead of full RGB frames, achieving state-of-the-art results on long-video understanding benchmarks.",
        "tldr_zh": "该论文介绍了ReMoRa，一种视频多模态大语言模型，它通过使用运动表示而非完整的RGB帧来高效处理长视频，并在长视频理解基准测试中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
        "summary": "We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.",
        "url": "http://arxiv.org/abs/2602.16138v1",
        "published_date": "2026-02-18T02:06:24+00:00",
        "updated_date": "2026-02-18T02:06:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Parsa Madinei",
            "Srijita Karmakar",
            "Russell Cohen Hoffing",
            "Felix Gervitz",
            "Miguel P. Eckstein"
        ],
        "tldr": "The paper introduces IRIS, a training-free method leveraging real-time eye-tracking data to resolve ambiguity in open-ended VQA, demonstrating significant accuracy improvements on ambiguous questions with various VLMs and releasing a new benchmark dataset.",
        "tldr_zh": "该论文介绍了一种名为IRIS的免训练方法，该方法利用实时眼动追踪数据来解决开放式VQA中的歧义。实验表明，该方法显著提高了各种VLM在模糊问题上的准确性，并发布了一个新的基准数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis",
        "summary": "Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.",
        "url": "http://arxiv.org/abs/2602.16110v1",
        "published_date": "2026-02-18T00:42:41+00:00",
        "updated_date": "2026-02-18T00:42:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tianwei Lin",
            "Zhongwei Qiu",
            "Wenqiao Zhang",
            "Jiang Liu",
            "Yihan Xie",
            "Mingjian Gao",
            "Zhenxuan Fan",
            "Zhaocheng Li",
            "Sijing Li",
            "Zhongle Xie",
            "Peng LU",
            "Yueting Zhuang",
            "Yingda Xia",
            "Ling Zhang",
            "Beng Chin Ooi"
        ],
        "tldr": "The paper introduces OmniCT, a unified slice-volume LVLM for CT analysis that enhances spatial consistency and organ-level semantic understanding, achieving state-of-the-art performance on a new comprehensive CT dataset.",
        "tldr_zh": "该论文介绍了OmniCT，一种统一的切片-体积LVLM，用于CT分析，它增强了空间一致性和器官级别的语义理解，并在一个新的综合CT数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
        "summary": "Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.",
        "url": "http://arxiv.org/abs/2602.16019v1",
        "published_date": "2026-02-17T21:20:32+00:00",
        "updated_date": "2026-02-17T21:20:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ahmad Elallaf",
            "Yu Zhang",
            "Yuktha Priya Masupalli",
            "Jeong Yang",
            "Young Lee",
            "Zechun Cao",
            "Gongbo Liang"
        ],
        "tldr": "MedProbCLIP introduces a probabilistic vision-language framework for chest X-ray and radiology report retrieval, improving accuracy, calibration, and robustness compared to deterministic models like CLIP.",
        "tldr_zh": "MedProbCLIP 提出了一种用于胸部 X 光片和放射学报告检索的概率视觉-语言框架，与 CLIP 等确定性模型相比，提高了准确性、校准和鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
        "summary": "Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
        "url": "http://arxiv.org/abs/2602.15927v1",
        "published_date": "2026-02-17T18:34:59+00:00",
        "updated_date": "2026-02-17T18:34:59+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Christian Schlarmann",
            "Matthias Hein"
        ],
        "tldr": "The paper introduces a Visual Memory Injection (VMI) attack on LVLMs in multi-turn conversations, where a manipulated image triggers specific target messages from the LVLM after a triggering prompt, demonstrating vulnerability even after long conversations.",
        "tldr_zh": "该论文介绍了一种针对LVLM的多轮对话中的视觉记忆注入（VMI）攻击，其中操纵的图像会在触发提示后触发LVLM中的特定目标消息，即使经过长时间的对话也表现出漏洞。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
        "summary": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.",
        "url": "http://arxiv.org/abs/2602.15772v1",
        "published_date": "2026-02-17T18:04:13+00:00",
        "updated_date": "2026-02-17T18:04:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sen Ye",
            "Mengde Xu",
            "Shuyang Gu",
            "Di He",
            "Liwei Wang",
            "Han Hu"
        ],
        "tldr": "The paper addresses the trade-off between understanding and generation in multimodal models by proposing a Reason-Reflect-Refine (R3) framework that improves both aspects through a multi-step generate-understand-regenerate process.",
        "tldr_zh": "该论文通过提出一个Reason-Reflect-Refine（R3）框架来解决多模态模型中理解和生成之间的权衡问题，该框架通过多步生成-理解-再生成过程来改进这两个方面。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images",
        "summary": "Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.",
        "url": "http://arxiv.org/abs/2602.16502v1",
        "published_date": "2026-02-18T14:45:15+00:00",
        "updated_date": "2026-02-18T14:45:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeng Tao",
            "Ying Jiang",
            "Yunuo Chen",
            "Tianyi Xie",
            "Huamin Wang",
            "Yingnian Wu",
            "Yin Yang",
            "Abishek Sampath Kumar",
            "Kenji Tashiro",
            "Chenfanfu Jiang"
        ],
        "tldr": "DressWild proposes a feed-forward pipeline for generating physics-consistent 2D sewing patterns and 3D garments from single in-the-wild images using VLMs to normalize pose variations and transformer-based feature fusion.",
        "tldr_zh": "DressWild提出了一种前馈流程，利用视觉语言模型来规范姿势变化，并使用基于Transformer的特征融合，从单张真实图像中生成物理一致的2D缝纫图案和3D服装。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
        "summary": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",
        "url": "http://arxiv.org/abs/2602.16430v1",
        "published_date": "2026-02-18T13:03:05+00:00",
        "updated_date": "2026-02-18T13:03:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ali Faraz",
            "Raja Kolla",
            "Ashish Kulkarni",
            "Shubham Agarwal"
        ],
        "tldr": "This paper explores two training strategies for multilingual OCR systems tailored for the Indian context, achieving state-of-the-art performance with Chitrapathak-2 and developing Parichay, an OCR model for Indian government documents.",
        "tldr_zh": "本文探讨了针对印度语境的多语言OCR系统的两种训练策略，通过Chitrapathak-2实现了最先进的性能，并开发了Parichay，一个用于印度政府文件的OCR模型。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing",
        "summary": "Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias",
        "url": "http://arxiv.org/abs/2602.16149v1",
        "published_date": "2026-02-18T02:47:36+00:00",
        "updated_date": "2026-02-18T02:47:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huichan Seo",
            "Minki Hong",
            "Sieun Choi",
            "Jihie Kim",
            "Jean Oh"
        ],
        "tldr": "This paper investigates demographic biases in image-to-image editing, identifying failure modes like Soft Erasure and Stereotype Replacement, and proposes a prompt-level constraint to mitigate these issues.",
        "tldr_zh": "本文研究了图像到图像编辑中的人口统计学偏差，识别了诸如软删除和刻板印象替换等失败模式，并提出了一种提示级别的约束来缓解这些问题。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
        "summary": "We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.",
        "url": "http://arxiv.org/abs/2602.15950v1",
        "published_date": "2026-02-17T19:06:19+00:00",
        "updated_date": "2026-02-17T19:06:19+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yuval Levental"
        ],
        "tldr": "This paper reveals that VLMs perform significantly worse at localizing filled squares in binary grids compared to text symbols, suggesting that text recognition is crucial for their spatial reasoning abilities.",
        "tldr_zh": "该论文揭示了视觉语言模型在定位二元网格中的填充正方形时，性能远低于文本符号，表明文本识别对它们的空间推理能力至关重要。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]