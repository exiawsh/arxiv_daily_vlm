[
    {
        "title": "Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction",
        "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities, yet hallucinations remain a persistent challenge. This work presents a systematic analysis of the internal evolution of visual perception and token generation in LVLMs, revealing two key patterns. First, perception follows a three-stage GATE process: early layers perform a Global scan, intermediate layers Approach and Tighten on core content, and later layers Explore supplementary regions. Second, generation exhibits an SAD (Subdominant Accumulation to Dominant) pattern, where hallucinated tokens arise from the repeated accumulation of subdominant tokens lacking support from attention (visual perception) or feed-forward network (internal knowledge). Guided by these findings, we devise the VDC (Validated Dominance Correction) strategy, which detects unsupported tokens and replaces them with validated dominant ones to improve output reliability. Extensive experiments across multiple models and benchmarks confirm that VDC substantially mitigates hallucinations.",
        "url": "http://arxiv.org/abs/2512.18813v1",
        "published_date": "2025-12-21T17:05:42+00:00",
        "updated_date": "2025-12-21T17:05:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangtao Lyu",
            "Xinyi Cheng",
            "Chenghao Xu",
            "Qi Liu",
            "Muli Yang",
            "Fen Fang",
            "Huilin Chen",
            "Jiexi Yan",
            "Xu Yang",
            "Cheng Deng"
        ],
        "tldr": "This paper analyzes the internal dynamics of LVLMs, identifying perception and generation patterns that lead to hallucinations, and proposes a method (VDC) to mitigate them by replacing unsupported tokens.",
        "tldr_zh": "该论文分析了LVLM的内部动态，识别出导致幻觉的感知和生成模式，并提出了一种通过替换不支持的tokens来减轻幻觉的方法 (VDC)。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
        "summary": "Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.",
        "url": "http://arxiv.org/abs/2512.18747v1",
        "published_date": "2025-12-21T14:28:28+00:00",
        "updated_date": "2025-12-21T14:28:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuan Chen",
            "Zichen Wen",
            "Yuzhou Wu",
            "Xuyang Liu",
            "Shuang Chen",
            "Junpeng Ma",
            "Weijia Li",
            "Conghui He",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces IPCV, a training-free compression framework for MLLM visual encoders that prunes visual tokens in ViT using neighbor-guided reconstruction and attention stabilization, demonstrating improved computational efficiency without significant performance degradation.",
        "tldr_zh": "该论文介绍了一种名为IPCV的免训练MLLM视觉编码器压缩框架，通过邻域引导重建和注意力稳定化来修剪ViT中的视觉tokens，从而在不显著降低性能的情况下提高计算效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
        "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
        "url": "http://arxiv.org/abs/2512.18745v1",
        "published_date": "2025-12-21T14:23:07+00:00",
        "updated_date": "2025-12-21T14:23:07+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Kaican Li",
            "Lewei Yao",
            "Jiannan Wu",
            "Tiezheng Yu",
            "Jierun Chen",
            "Haoli Bai",
            "Lu Hou",
            "Lanqing Hong",
            "Wei Zhang",
            "Nevin L. Zhang"
        ],
        "tldr": "This paper introduces InSight-o3, a multi-agent framework that enhances multimodal models with a visual search agent, significantly improving performance on reasoning-intensive tasks, particularly those involving complex visual information.",
        "tldr_zh": "本文介绍了 InSight-o3，一个多代理框架，通过视觉搜索代理增强多模态模型，从而显著提高在推理密集型任务中的性能，尤其是在涉及复杂视觉信息的任务中。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
        "summary": "Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.",
        "url": "http://arxiv.org/abs/2512.18735v1",
        "published_date": "2025-12-21T13:50:26+00:00",
        "updated_date": "2025-12-21T13:50:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kewei Wei",
            "Bocheng Hu",
            "Jie Cao",
            "Xiaohan Chen",
            "Zhengxi Lu",
            "Wubing Xia",
            "Weili Xu",
            "Jiaao Wu",
            "Junchen He",
            "Mingyu Jia",
            "Ciyun Zhao",
            "Ye Sun",
            "Yizhi Li",
            "Zhonghan Zhao",
            "Jian Zhang",
            "Gaoang Wang"
        ],
        "tldr": "The paper introduces $M^3-Verse$, a new benchmark to evaluate LMMs' ability to understand dynamic changes in videos, revealing their limitations and proposing a baseline for improvement.",
        "tldr_zh": "该论文介绍了$M^3-Verse$，一个新的基准，用于评估LMMs理解视频中动态变化的能力，揭示了它们的局限性，并提出了一个改进的基线。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse",
        "summary": "Despite Video Large Language Models having rapidly advanced in recent years, perceptual hallucinations pose a substantial safety risk, which severely restricts their real-world applicability. While several methods for hallucination mitigation have been proposed, they often compromise the model's capacity for video understanding and reasoning. In this work, we propose SmartSight, a pioneering step to address this issue in a training-free manner by leveraging the model's own introspective capabilities. Specifically, SmartSight generates multiple candidate responses to uncover low-hallucinated outputs that are often obscured by standard greedy decoding. It assesses the hallucination of each response using the Temporal Attention Collapse score, which measures whether the model over-focuses on trivial temporal regions of the input video when generating the response. To improve efficiency, SmartSight identifies the Visual Attention Vanishing point, enabling more accurate hallucination estimation and early termination of hallucinated responses, leading to a substantial reduction in decoding cost. Experiments show that SmartSight substantially lowers hallucinations for Qwen2.5-VL-7B by 10.59% on VRIPT-HAL, while simultaneously enhancing video understanding and reasoning, boosting performance on VideoMMMU by up to 8.86%. These results highlight SmartSight's effectiveness in improving the reliability of open-source Video-LLMs.",
        "url": "http://arxiv.org/abs/2512.18671v1",
        "published_date": "2025-12-21T10:25:02+00:00",
        "updated_date": "2025-12-21T10:25:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Sun",
            "Mi Zhang",
            "Feifei Li",
            "Geng Hong",
            "Min Yang"
        ],
        "tldr": "The paper introduces SmartSight, a training-free method for mitigating hallucinations in Video-LLMs using temporal attention collapse, improving both hallucination reduction and video understanding.",
        "tldr_zh": "该论文介绍了 SmartSight，一种无需训练的方法，通过时间注意力崩溃来缓解视频语言模型中的幻觉，从而提高幻觉减少和视频理解能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenView: Empowering MLLMs with Out-of-view VQA",
        "summary": "Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.",
        "url": "http://arxiv.org/abs/2512.18563v1",
        "published_date": "2025-12-21T02:11:40+00:00",
        "updated_date": "2025-12-21T02:11:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qixiang Chen",
            "Cheng Zhang",
            "Chi-Wing Fu",
            "Jingwen Ye",
            "Jianfei Cai"
        ],
        "tldr": "The paper introduces OpenView, a pipeline, dataset, and benchmark for out-of-view visual question answering, showing that fine-tuning MLLMs on the generated data significantly improves their performance in reasoning beyond the image frame.",
        "tldr_zh": "该论文介绍了OpenView，一个用于视野外视觉问答的流程、数据集和基准，表明在生成的数据上微调MLLM可以显著提高它们在图像帧之外进行推理的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Medical Large Vision-Language Models via Alignment Distillation",
        "summary": "Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a simple, lightweight alignment distillation framework that transfers visual alignment knowledge from a domain-specific Contrastive Language-Image Pre-training (CLIP) model to Med-LVLMs. MEDALIGN introduces two distillation losses: a spatial-aware visual alignment loss based on visual token-level similarity structures, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Extensive experiments on medical report generation and medical visual question answering (VQA) benchmarks show that MEDALIGN consistently improves both performance and interpretability, yielding more visually grounded outputs.",
        "url": "http://arxiv.org/abs/2512.18554v1",
        "published_date": "2025-12-21T00:57:13+00:00",
        "updated_date": "2025-12-21T00:57:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aofei Chang",
            "Ting Wang",
            "Fenglong Ma"
        ],
        "tldr": "The paper introduces MEDALIGN, a distillation framework to improve medical VLMs by aligning visual representations and attention using a domain-specific CLIP model, leading to improved performance and interpretability in medical report generation and VQA tasks.",
        "tldr_zh": "该论文介绍了MEDALIGN，一个蒸馏框架，通过使用领域特定的CLIP模型对齐视觉表示和注意力来改进医学VLM，从而提高医学报告生成和VQA任务的性能和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GTMA: Dynamic Representation Optimization for OOD Vision-Language Models",
        "summary": "Vision-language models (VLMs) struggle in open-world applications, where out-of-distribution (OOD) concepts often trigger cross-modal alignment collapse and severely degrade zero-shot performance. We identify the root cause as modal asymmetry: while the visual encoder can extract discriminative features from unseen images, the text encoder is constrained by a fixed discrete vocabulary and cannot synthesize new semantic anchors. Existing approaches such as CoOp or LoRA provide only partial remedies, as they remain confined to the pre-trained semantic space.\n  To overcome this bottleneck, we propose dynamic representation optimization, realized through the Guided Target-Matching Adaptation (GTMA) framework. At inference time, GTMA constructs a continuous pseudo-word embedding that best aligns with an OOD image's visual anchor, effectively bypassing vocabulary limitations. The optimization is driven by an adaptive gradient-based representation policy optimization algorithm, which incorporates semantic regularization to preserve plausibility and compatibility with the model's prior knowledge.\n  Experiments on ImageNet-R and the VISTA-Beyond benchmark demonstrate that GTMA improves zero-shot and few-shot OOD accuracy by up to 15-20 percent over the base VLM while maintaining performance on in-distribution concepts. Ablation studies further confirm the necessity of pseudo-word optimization.",
        "url": "http://arxiv.org/abs/2512.18504v1",
        "published_date": "2025-12-20T20:44:07+00:00",
        "updated_date": "2025-12-20T20:44:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jensen Zhang",
            "Ningyuan Liu",
            "Keze Wang"
        ],
        "tldr": "The paper introduces GTMA, a framework that addresses the issue of vision-language models (VLMs) struggling with out-of-distribution (OOD) concepts by dynamically optimizing text representations at inference time, improving zero-shot and few-shot OOD accuracy.",
        "tldr_zh": "该论文介绍了GTMA框架，通过在推理时动态优化文本表示，解决了视觉语言模型（VLM）难以处理分布外（OOD）概念的问题，从而提高了零样本和少样本OOD准确率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models",
        "summary": "In recent years, large-scale vision-language models (VLMs) have demonstrated remarkable performance on multimodal understanding and reasoning tasks. However, handling high-dimensional visual features often incurs substantial computational and memory costs. VoCo-LLaMA alleviates this issue by compressing visual patch tokens into a few VoCo tokens, reducing computational overhead while preserving strong cross-modal alignment. Nevertheless, such approaches typically adopt a fixed compression rate, limiting their ability to adapt to varying levels of visual complexity. To address this limitation, we propose Adaptive-VoCo, a framework that augments VoCo-LLaMA with a lightweight predictor for adaptive compression. This predictor dynamically selects an optimal compression rate by quantifying an image's visual complexity using statistical cues from the vision encoder, such as patch token entropy and attention map variance. Furthermore, we introduce a joint loss function that integrates rate regularization with complexity alignment. This enables the model to balance inference efficiency with representational capacity, particularly in challenging scenarios. Experimental results show that our method consistently outperforms fixed-rate baselines across multiple multimodal tasks, highlighting the potential of adaptive visual compression for creating more efficient and robust VLMs.",
        "url": "http://arxiv.org/abs/2512.18496v1",
        "published_date": "2025-12-20T20:24:07+00:00",
        "updated_date": "2025-12-20T20:24:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyang Guo",
            "Keze Wang"
        ],
        "tldr": "The paper introduces Adaptive-VoCo, a framework that enhances VoCo-LLaMA with a complexity-aware predictor for adaptive visual token compression in VLMs, achieving better performance and efficiency compared to fixed-rate compression methods.",
        "tldr_zh": "该论文介绍了Adaptive-VoCo，一个通过复杂度感知预测器增强VoCo-LLaMA的框架，用于视觉语言模型中的自适应视觉token压缩，与固定速率压缩方法相比，实现了更好的性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer",
        "summary": "Video generation models have advanced significantly, yet they still struggle to synthesize complex human movements due to the high degrees of freedom in human articulation. This limitation stems from the intrinsic constraints of pixel-only training objectives, which inherently bias models toward appearance fidelity at the expense of learning underlying kinematic principles. To address this, we introduce EchoMotion, a framework designed to model the joint distribution of appearance and human motion, thereby improving the quality of complex human action video generation. EchoMotion extends the DiT (Diffusion Transformer) framework with a dual-branch architecture that jointly processes tokens concatenated from different modalities. Furthermore, we propose MVS-RoPE (Motion-Video Syncronized RoPE), which offers unified 3D positional encoding for both video and motion tokens. By providing a synchronized coordinate system for the dual-modal latent sequence, MVS-RoPE establishes an inductive bias that fosters temporal alignment between the two modalities. We also propose a Motion-Video Two-Stage Training Strategy. This strategy enables the model to perform both the joint generation of complex human action videos and their corresponding motion sequences, as well as versatile cross-modal conditional generation tasks. To facilitate the training of a model with these capabilities, we construct HuMoVe, a large-scale dataset of approximately 80,000 high-quality, human-centric video-motion pairs. Our findings reveal that explicitly representing human motion is complementary to appearance, significantly boosting the coherence and plausibility of human-centric video generation.",
        "url": "http://arxiv.org/abs/2512.18814v1",
        "published_date": "2025-12-21T17:08:14+00:00",
        "updated_date": "2025-12-21T17:08:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxiao Yang",
            "Hualian Sheng",
            "Sijia Cai",
            "Jing Lin",
            "Jiahao Wang",
            "Bing Deng",
            "Junzhe Lu",
            "Haoqian Wang",
            "Jieping Ye"
        ],
        "tldr": "EchoMotion introduces a dual-modality Diffusion Transformer framework with a novel positional encoding (MVS-RoPE) and a two-stage training strategy for generating high-quality human action videos with synchronized motion sequences, validated on a new large-scale dataset (HuMoVe).",
        "tldr_zh": "EchoMotion 提出了一个双模态扩散Transformer框架，通过新颖的位置编码(MVS-RoPE)和两阶段训练策略，用于生成高质量且运动序列同步的人类动作视频，并在新的大规模数据集(HuMoVe)上进行了验证。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
        "summary": "The rapid growth of short-form video platforms increases the need for privacy-preserving moderation, as cloud-based pipelines expose raw videos to privacy risks, high bandwidth costs, and inference latency. To address these challenges, we propose an on-device federated learning framework for video violence detection that integrates self-supervised VideoMAE representations, LoRA-based parameter-efficient adaptation, and defense-in-depth privacy protection. Our approach reduces the trainable parameter count to 5.5M (~3.5% of a 156M backbone) and incorporates DP-SGD with configurable privacy budgets and secure aggregation. Experiments on RWF-2000 with 40 clients achieve 77.25% accuracy without privacy protection and 65-66% under strong differential privacy, while reducing communication cost by $28.3\\times$ compared to full-model federated learning. The code is available at: {https://github.com/zyt-599/FedVideoMAE}",
        "url": "http://arxiv.org/abs/2512.18809v1",
        "published_date": "2025-12-21T17:01:44+00:00",
        "updated_date": "2025-12-21T17:01:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Ziyuan Tao",
            "Chuanzhi Xu",
            "Sandaru Jayawardana",
            "Wei Bao",
            "Kanchana Thilakarathna",
            "Teng Joon Lim"
        ],
        "tldr": "This paper introduces FedVideoMAE, a federated learning framework for privacy-preserving video violence detection using self-supervised VideoMAE, LoRA, and DP-SGD, achieving good accuracy with reduced communication costs.",
        "tldr_zh": "该论文介绍了FedVideoMAE，一个用于隐私保护的联邦学习视频暴力检测框架，它利用自监督的VideoMAE，LoRA和DP-SGD，以较低的通信成本实现了良好的准确率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference",
        "summary": "Text-to-image diffusion inference typically follows synchronized schedules, where the numerical integrator advances the latent state to the same timestep at which the denoiser is conditioned. We propose an asynchronous inference mechanism that decouples these two, allowing the denoiser to be conditioned at a different, learned timestep while keeping image update schedule unchanged. A lightweight timestep prediction module (TPM), trained with Group Relative Policy Optimization (GRPO), selects a more feasible conditioning timestep based on the current state, effectively choosing a desired noise level to control image detail and textural richness. At deployment, a scaling hyper-parameter can be used to interpolate between the original and de-synchronized timesteps, enabling conservative or aggressive adjustments. To keep the study computationally affordable, we cap the inference at 15 steps for SD3.5 and 10 steps for Flux. Evaluated on Stable Diffusion 3.5 Medium and Flux.1-dev across MS-COCO 2014 and T2I-CompBench datasets, our method optimizes a composite reward that averages Image Reward, HPSv2, CLIP Score and Pick Score, and shows consistent improvement.",
        "url": "http://arxiv.org/abs/2512.18675v1",
        "published_date": "2025-12-21T10:29:57+00:00",
        "updated_date": "2025-12-21T10:29:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Longhuan Xu",
            "Feng Yin",
            "Cunjian Chen"
        ],
        "tldr": "This paper introduces AsyncDiff, an asynchronous inference mechanism for text-to-image diffusion models that decouples the denoiser's conditioning timestep from the image update schedule, potentially improving image quality and control. They train a timestep prediction module with reinforcement learning and demonstrate improvements on SD3.5 and Flux models.",
        "tldr_zh": "本文介绍了一种用于文本到图像扩散模型的异步推理机制AsyncDiff，该机制将去噪器的条件时间步与图像更新计划分离，从而可能提高图像质量和控制。他们使用强化学习训练时间步预测模块，并在SD3.5和Flux模型上展示了改进。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback",
        "summary": "Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.",
        "url": "http://arxiv.org/abs/2512.18599v1",
        "published_date": "2025-12-21T05:12:25+00:00",
        "updated_date": "2025-12-21T05:12:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianglin Lu",
            "Yuanwei Wu",
            "Ziyi Zhao",
            "Hongcheng Wang",
            "Felix Jimenez",
            "Abrar Majeedi",
            "Yun Fu"
        ],
        "tldr": "This paper introduces a lightweight image restoration agent that leverages MLLM perceptual feedback for training in label-free environments, achieving competitive or superior performance compared to SOTA methods while significantly improving efficiency.",
        "tldr_zh": "本文介绍了一种轻量级的图像修复代理，它利用 MLLM 感知反馈在无标签环境中进行训练，与 SOTA 方法相比，实现了有竞争力的或更优越的性能，同时显著提高了效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
        "summary": "Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., \"fetch the tool\" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.",
        "url": "http://arxiv.org/abs/2512.18571v1",
        "published_date": "2025-12-21T02:45:08+00:00",
        "updated_date": "2025-12-21T02:45:08+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Weijie Zhou",
            "Xuangtang Xiong",
            "Ye Tian",
            "Lijun Yue",
            "Xinyu Wu",
            "Wei Li",
            "Chaoyang Zhao",
            "Honghui Dong",
            "Ming Tang",
            "Jinqiao Wang",
            "Zhengyou Zhang"
        ],
        "tldr": "The paper introduces ESearch-R1, a reinforcement learning framework using HC-GRPO to train cost-aware MLLM agents for interactive embodied search, balancing physical exploration and human interaction costs. It outperforms ReAct-based agents in AI2-THOR.",
        "tldr_zh": "本文介绍了ESearch-R1，一个使用HC-GRPO的强化学习框架，用于训练具有成本意识的MLLM代理进行交互式具身搜索，平衡了物理探索和人机交互的成本。在AI2-THOR中，该方法优于基于ReAct的代理。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]