[
    {
        "title": "Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models",
        "summary": "Fine-tuning open-source Vision-Language Models (VLMs) creates a critical yet\nunderexplored attack surface: vulnerabilities in the base VLM could be retained\nin fine-tuned variants, rendering them susceptible to transferable jailbreak\nattacks. To demonstrate this risk, we introduce the Simulated Ensemble Attack\n(SEA), a novel grey-box jailbreak method in which the adversary has full access\nto the base VLM but no knowledge of the fine-tuned target's weights or training\nconfiguration. To improve jailbreak transferability across fine-tuned VLMs, SEA\ncombines two key techniques: Fine-tuning Trajectory Simulation (FTS) and\nTargeted Prompt Guidance (TPG). FTS generates transferable adversarial images\nby simulating the vision encoder's parameter shifts, while TPG is a textual\nstrategy that steers the language decoder toward adversarially optimized\noutputs. Experiments on the Qwen2-VL family (2B and 7B) demonstrate that SEA\nachieves high transfer attack success rates exceeding 86.5% and toxicity rates\nnear 49.5% across diverse fine-tuned variants, even those specifically\nfine-tuned to improve safety behaviors. Notably, while direct PGD-based image\njailbreaks rarely transfer across fine-tuned VLMs, SEA reliably exploits\ninherited vulnerabilities from the base model, significantly enhancing\ntransferability. These findings highlight an urgent need to safeguard\nfine-tuned proprietary VLMs against transferable vulnerabilities inherited from\nopen-source foundations, motivating the development of holistic defenses across\nthe entire model lifecycle.",
        "url": "http://arxiv.org/abs/2508.01741v1",
        "published_date": "2025-08-03T12:51:47+00:00",
        "updated_date": "2025-08-03T12:51:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruofan Wang",
            "Xin Wang",
            "Yang Yao",
            "Xuan Tong",
            "Xingjun Ma"
        ],
        "tldr": "The paper introduces a Simulated Ensemble Attack (SEA) method to jailbreak fine-tuned Vision-Language Models (VLMs) by exploiting vulnerabilities inherited from the base model, demonstrating high transferability across diverse fine-tuned variants.",
        "tldr_zh": "该论文介绍了一种模拟集成攻击（SEA）方法，通过利用从基础模型继承的漏洞来突破微调的视觉-语言模型（VLM），从而证明了在各种微调变体中的高可迁移性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding",
        "summary": "Video Temporal Grounding (VTG) aims to precisely identify video event\nsegments in response to textual queries. The outputs of VTG tasks manifest as\nsequences of events, each defined by precise timestamps, saliency scores, and\ntextual descriptions. Despite recent advances, a fundamental limitation\npersists in existing Video Large Language Models (Video-LLMs): they process all\ntask tokens through identical and static pathways, failing to recognize that\ntemporal localization, saliency assessment, and textual generation represent\nfundamentally distinct tasks requiring specialized processing. To address this,\nwe introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that\neffectively decomposes VTG tasks by dynamically routing task-specific tokens\n(e.g., timestamps, saliency scores) to specialized experts, with increased\ncomputational efficiency. Our design choices enable precise handling of each\nsubtask, leading to improved event modeling across diverse VTG applications.\nExtensive experiments demonstrate that TimeExpert consistently achieves\nstate-of-the-art performance on various VTG tasks such as Dense Video\nCaptioning, Moment Retrieval, and Video Highlight Detection.",
        "url": "http://arxiv.org/abs/2508.01699v1",
        "published_date": "2025-08-03T10:03:58+00:00",
        "updated_date": "2025-08-03T10:03:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zuhao Yang",
            "Yingchen Yu",
            "Yunqing Zhao",
            "Shijian Lu",
            "Song Bai"
        ],
        "tldr": "The paper introduces TimeExpert, a Mixture-of-Experts Video-LLM that dynamically routes task-specific tokens to specialized experts for Video Temporal Grounding, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了 TimeExpert，一个混合专家视频大语言模型，它将特定于任务的 tokens 动态路由到专门的专家，用于视频时间定位，从而实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) often suffer from hallucination, partly due to\nchallenges in aligning multimodal information. We propose Prompt-in-Image, a\nsimple method that embeds textual instructions directly into images. This\nremoves the need for separate text inputs and forces the model to process all\ncontent through the visual channel. We evaluate this method on three popular\nopen-source VLMs: Qwen2.5-VL, LLaVA-1.5, and InstructBLIP. The results reveal\nsharp differences. Prompt-in-Image improves Qwen2.5-VL's performance,\nincreasing POPE accuracy by 4.1 percent (from 80.2 percent to 84.3 percent) and\nalso reducing hallucination rates on MS-COCO. In contrast, LLaVA-1.5 and\nInstructBLIP experience a severe performance drop, with accuracy falling from\naround 84 percent to near-random levels. Through detailed analysis, we found\nthat CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention\nbias toward embedded text regions, disrupting visual understanding. In\ncontrast, Qwen's vision encoder handles text-embedded images robustly.\nCrucially, Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal\nalignment by unifying information processing through a single modality.",
        "url": "http://arxiv.org/abs/2508.01678v1",
        "published_date": "2025-08-03T09:11:18+00:00",
        "updated_date": "2025-08-03T09:11:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhaochen Wang",
            "Yiwei Wang",
            "Yujun Cai"
        ],
        "tldr": "The paper proposes 'Prompt-in-Image,' embedding instructions in images to mitigate VLM hallucination. It shows improved performance for Qwen2.5-VL but performance degradation for LLaVA-1.5 and InstructBLIP, attributed to CLIP encoder attention biases.",
        "tldr_zh": "该论文提出了一种名为“Prompt-in-Image”的方法，通过将指令嵌入图像中来减轻视觉语言模型（VLM）的幻觉问题。该方法提升了Qwen2.5-VL的性能，但降低了LLaVA-1.5和InstructBLIP的性能，原因在于CLIP编码器的注意力偏差。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing",
        "summary": "Large Vision-Language Models (LVLMs) have achieved impressive performance in\nmultimodal tasks, but they still suffer from hallucinations, i.e., generating\ncontent that is grammatically accurate but inconsistent with visual inputs. In\nthis work, we introduce a novel map-level perspective to mitigate\nhallucinations in LVLMs, interpreting the hidden states of the model as a 2D\nsemantic map. We observe that factual information is widely distributed across\nthis map, extending beyond the localized inter- or intra-layer regions targeted\nby most existing methods (e.g., contrastive decoding and layer-wise\nconsistency). Building on this insight, we propose Map-Level Attention\nProcessing (MAP), a training-free decoding method that effectively leverages\nfactual information through attention-based map-level operations to improve\nfactual consistency. Specifically, we employ Layer-Wise Criss-Cross Attention\nto progressively refine token representations at each decoding layer by\naggregating tokens from both inter- and intra-layer dimensions. Additionally, a\nGlobal-Local Logit Fusion mechanism combines logits obtained before and after\nglobal attention to further refine predictions and improve accuracy. Our method\nconsistently improves the truthfulness and performance of LVLMs across\nbenchmarks, such as POPE, MME, and MMHal-Bench, demonstrating the potential of\nthe map-level decoding strategy.",
        "url": "http://arxiv.org/abs/2508.01653v1",
        "published_date": "2025-08-03T08:23:31+00:00",
        "updated_date": "2025-08-03T08:23:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chenxi Li",
            "Yichen Guo",
            "Benfang Qian",
            "Jinhao You",
            "Kai Tang",
            "Yaosong Du",
            "Zonghao Zhang",
            "Xiande Huang"
        ],
        "tldr": "This paper introduces Map-Level Attention Processing (MAP), a training-free decoding method for LVLMs that leverages a 2D semantic map of hidden states to mitigate hallucinations, improving factual consistency.",
        "tldr_zh": "该论文介绍了一种名为地图级别注意力处理（MAP）的免训练解码方法，用于大型视觉语言模型（LVLM），利用隐藏状态的2D语义地图来减轻幻觉，从而提高事实一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding",
        "summary": "Autoregressive models (ARMs) have long dominated the landscape of biomedical\nvision-language models (VLMs). Recently, masked diffusion models such as LLaDA\nhave emerged as promising alternatives, yet their application in the biomedical\ndomain remains largely underexplored. To bridge this gap, we introduce\n\\textbf{LLaDA-MedV}, the first large language diffusion model tailored for\nbiomedical image understanding through vision instruction tuning. LLaDA-MedV\nachieves relative performance gains of 7.855\\% over LLaVA-Med and 1.867\\% over\nLLaDA-V in the open-ended biomedical visual conversation task, and sets new\nstate-of-the-art accuracy on the closed-form subset of three VQA benchmarks:\n84.93\\% on VQA-RAD, 92.31\\% on SLAKE, and 95.15\\% on PathVQA. Furthermore, a\ndetailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of\ngenerating reasonably longer responses by explicitly controlling response\nlength, which can lead to more informative outputs. We also conduct an in-depth\nanalysis of both the training and inference stages, highlighting the critical\nroles of initialization weight selection, fine-tuning strategies, and the\ninterplay between sampling steps and response repetition. The code and model\nweight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.",
        "url": "http://arxiv.org/abs/2508.01617v1",
        "published_date": "2025-08-03T06:46:46+00:00",
        "updated_date": "2025-08-03T06:46:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuanzhao Dong",
            "Wenhui Zhu",
            "Xiwen Chen",
            "Zhipeng Wang",
            "Peijie Qiu",
            "Shao Tang",
            "Xin Li",
            "Yalin Wang"
        ],
        "tldr": "This paper introduces LLaDA-MedV, a large language diffusion model tailored for biomedical image understanding, demonstrating improved performance and response generation capabilities compared to existing models like LLaVA-Med.",
        "tldr_zh": "本文介绍了LLaDA-MedV，一个为生物医学图像理解定制的大型语言扩散模型，与现有的LLaVA-Med等模型相比，在性能和响应生成能力方面都有所提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment",
        "summary": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification.",
        "url": "http://arxiv.org/abs/2508.01602v1",
        "published_date": "2025-08-03T05:38:14+00:00",
        "updated_date": "2025-08-03T05:38:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lubin Gan",
            "Jing Zhang",
            "Linhao Qu",
            "Yijun Wang",
            "Siying Wu",
            "Xiaoyan Sun"
        ],
        "tldr": "This paper introduces FG-PAN, a novel zero-shot framework for brain tumor subtype classification that aligns refined visual patch features with fine-grained, LLM-generated text descriptions to improve class separability.",
        "tldr_zh": "本文介绍了一种名为FG-PAN的新型零样本脑肿瘤亚型分类框架，该框架将精细化的视觉patch特征与LLM生成的细粒度文本描述对齐，以提高类别可分离性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "EvoVLMA: Evolutionary Vision-Language Model Adaptation",
        "summary": "Pre-trained Vision-Language Models (VLMs) have been exploited in various\nComputer Vision tasks (e.g., few-shot recognition) via model adaptation, such\nas prompt tuning and adapters. However, existing adaptation methods are\ndesigned by human experts, requiring significant time cost and experience.\nInspired by recent advances in Large Language Models (LLMs) based code\ngeneration, we propose an Evolutionary Vision-Language Model Adaptation\n(EvoVLMA) method to automatically search training-free efficient adaptation\nalgorithms for VLMs. We recognize feature selection and logits computation as\nthe key functions in training-free VLM adaptation, and propose a two-stage\nLLM-assisted evolutionary algorithm for optimizing these parts in a sequential\nmanner, effectively addressing the challenge posed by the expansive search\nspace through a divide-and-conquer strategy. Besides, to enhance the stability\nand efficiency of searching process, we propose low-precision code conversion,\nweb based code execution and process monitoring, leading to a highly effective\nautomatic algorithm design system. Extensive experiments demonstrate that the\nalgorithms found by EvoVLMA can obtain promising results compared to previous\nmanually-designed ones. More specifically, in the 8-shot image classification\nsetting, the classical APE algorithm can be improved by 1.91 points in\nrecognition accuracy. This research opens new possibilities for automating the\noptimization of adaptation algorithms of pre-trained multimodal models. Code is\navailable at: https://github.com/kding1225/EvoVLMA",
        "url": "http://arxiv.org/abs/2508.01558v1",
        "published_date": "2025-08-03T03:11:01+00:00",
        "updated_date": "2025-08-03T03:11:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kun Ding",
            "Ying Wang",
            "Shiming Xiang"
        ],
        "tldr": "The paper introduces EvoVLMA, an LLM-assisted evolutionary algorithm for automatically discovering training-free adaptation algorithms for Vision-Language Models, achieving improved performance compared to manually designed methods.",
        "tldr_zh": "本文介绍了EvoVLMA，一种LLM辅助的进化算法，用于自动发现视觉-语言模型的免训练自适应算法，与人工设计的方法相比，实现了性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models",
        "summary": "Visual token compression is critical for Large Vision-Language Models (LVLMs)\nto efficiently process high-resolution inputs. Existing methods that typically\nadopt fixed compression ratios cannot adapt to scenes of varying complexity,\noften causing imprecise pruning that discards informative visual tokens and\nresults in degraded model performance. To address this issue, we introduce a\ndynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes\na data-driven ''glimpse'' and prunes irrelevant visual tokens in a single\nforward pass before answer generation. This approach prunes 92.6% of visual\ntokens while on average fully retaining the baseline performance on free-form\nVQA tasks. The reduced computational cost also enables more effective\nfine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline\nperformance while maintaining a similarly high pruning rate. Our work paves a\nnew way for building more powerful and efficient LVLMs.",
        "url": "http://arxiv.org/abs/2508.01548v1",
        "published_date": "2025-08-03T02:15:43+00:00",
        "updated_date": "2025-08-03T02:15:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quan-Sheng Zeng",
            "Yunheng Li",
            "Qilong Wang",
            "Peng-Tao Jiang",
            "Zuxuan Wu",
            "Ming-Ming Cheng",
            "Qibin Hou"
        ],
        "tldr": "The paper introduces GlimpsePrune, a dynamic visual token pruning framework for LVLMs that prunes irrelevant tokens based on a data-driven \"glimpse\", achieving high pruning rates while maintaining or improving performance on VQA tasks.",
        "tldr_zh": "该论文介绍了一种名为 GlimpsePrune 的动态视觉token剪枝框架，用于大型视觉语言模型（LVLM）。该框架基于数据驱动的“一瞥”来剪除不相关的token，实现了高剪枝率，同时保持或提高了在视觉问答（VQA）任务上的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "E-VRAG: Enhancing Long Video Understanding with Resource-Efficient Retrieval Augmented Generation",
        "summary": "Vision-Language Models (VLMs) have enabled substantial progress in video\nunderstanding by leveraging cross-modal reasoning capabilities. However, their\neffectiveness is limited by the restricted context window and the high\ncomputational cost required to process long videos with thousands of frames.\nRetrieval-augmented generation (RAG) addresses this challenge by selecting only\nthe most relevant frames as input, thereby reducing the computational burden.\nNevertheless, existing video RAG methods struggle to balance retrieval\nefficiency and accuracy, particularly when handling diverse and complex video\ncontent. To address these limitations, we propose E-VRAG, a novel and efficient\nvideo RAG framework for video understanding. We first apply a frame\npre-filtering method based on hierarchical query decomposition to eliminate\nirrelevant frames, reducing computational costs at the data level. We then\nemploy a lightweight VLM for frame scoring, further reducing computational\ncosts at the model level. Additionally, we propose a frame retrieval strategy\nthat leverages the global statistical distribution of inter-frame scores to\nmitigate the potential performance degradation from using a lightweight VLM.\nFinally, we introduce a multi-view question answering scheme for the retrieved\nframes, enhancing the VLM's capability to extract and comprehend information\nfrom long video contexts. Experiments on four public benchmarks show that\nE-VRAG achieves about 70% reduction in computational cost and higher accuracy\ncompared to baseline methods, all without additional training. These results\ndemonstrate the effectiveness of E-VRAG in improving both efficiency and\naccuracy for video RAG tasks.",
        "url": "http://arxiv.org/abs/2508.01546v1",
        "published_date": "2025-08-03T02:09:54+00:00",
        "updated_date": "2025-08-03T02:09:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Xu",
            "Junkang Zhang",
            "Qiang Wang",
            "Yi Liu"
        ],
        "tldr": "The paper introduces E-VRAG, a resource-efficient Retrieval Augmented Generation framework for long video understanding, achieving significant computational cost reduction and accuracy improvement by frame pre-filtering, lightweight VLM scoring, and global statistical distribution-based retrieval.",
        "tldr_zh": "本文介绍了一种资源高效的检索增强生成框架E-VRAG，用于长视频理解。该框架通过帧预过滤、轻量级VLM评分和基于全局统计分布的检索，实现了显著的计算成本降低和准确性提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable breakthroughs in\nrecent years, enabling a diverse array of applications in everyday life.\nHowever, the substantial computational and storage demands of VLMs pose\nsignificant challenges for their efficient deployment on mobile devices, which\nrepresent the most ubiquitous and accessible computing platforms today. In this\nwork, we introduce MagicVL-2B, a novel VLM meticulously optimized for flagship\nsmartphones. MagicVL-2B leverages a lightweight visual encoder with fewer than\n100M parameters and features a redesigned dynamic resolution scheme that\nadaptively generates image tokens without excessive modification of image\ndimensions. To further enhance the performance of this compact encoder within\nVLMs, we propose a multimodal curriculum learning strategy that incrementally\nincreases task difficulty and data information density throughout training.\nThis approach substantially improves the model's performance across a variety\nof sub-tasks. Extensive evaluations on standard VLM benchmarks demonstrate that\nMagicVL-2B matches the accuracy of current state-of-the-art models while\nreducing on-device power consumption by 41.1%. These results establish\nMagicVL-2B as a practical and robust solution for real-world mobile\nvision-language applications, enabling advanced multimodal intelligence to run\ndirectly on smartphones.",
        "url": "http://arxiv.org/abs/2508.01540v1",
        "published_date": "2025-08-03T01:49:08+00:00",
        "updated_date": "2025-08-03T01:49:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yi Liu",
            "Xiao Xu",
            "Zeyu Xu",
            "Meng Zhang",
            "Yibo Li",
            "Haoyu Chen",
            "Junkang Zhang",
            "Qiang Wang",
            "Jifa Sun",
            "Siling Lin",
            "Shengxun Cheng",
            "Lingshu Zhang",
            "Kang Wang"
        ],
        "tldr": "MagicVL-2B is a novel VLM optimized for mobile devices, using a lightweight visual encoder and multimodal curriculum learning to achieve state-of-the-art accuracy with significantly reduced power consumption.",
        "tldr_zh": "MagicVL-2B是一种为移动设备优化的新型VLM，它利用轻量级视觉编码器和多模态课程学习，在显著降低功耗的同时，实现了最先进的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models",
        "summary": "Image geolocalization, the task of identifying the geographic location\ndepicted in an image, is important for applications in crisis response, digital\nforensics, and location-based intelligence. While recent advances in large\nlanguage models (LLMs) offer new opportunities for visual reasoning, their\nability to perform image geolocalization remains underexplored. In this study,\nwe introduce a benchmark called IMAGEO-Bench that systematically evaluates\naccuracy, distance error, geospatial bias, and reasoning process. Our benchmark\nincludes three diverse datasets covering global street scenes, points of\ninterest (POIs) in the United States, and a private collection of unseen\nimages. Through experiments on 10 state-of-the-art LLMs, including both open-\nand closed-source models, we reveal clear performance disparities, with\nclosed-source models generally showing stronger reasoning. Importantly, we\nuncover geospatial biases as LLMs tend to perform better in high-resource\nregions (e.g., North America, Western Europe, and California) while exhibiting\ndegraded performance in underrepresented areas. Regression diagnostics\ndemonstrate that successful geolocalization is primarily dependent on\nrecognizing urban settings, outdoor environments, street-level imagery, and\nidentifiable landmarks. Overall, IMAGEO-Bench provides a rigorous lens into the\nspatial reasoning capabilities of LLMs and offers implications for building\ngeolocation-aware AI systems.",
        "url": "http://arxiv.org/abs/2508.01608v1",
        "published_date": "2025-08-03T06:04:33+00:00",
        "updated_date": "2025-08-03T06:04:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingyao Li",
            "Runlong Yu",
            "Qikai Hu",
            "Bowei Li",
            "Min Deng",
            "Yang Zhou",
            "Xiaowei Jia"
        ],
        "tldr": "This paper introduces IMAGEO-Bench, a new benchmark for evaluating image geolocalization capabilities of large language models (LLMs), revealing performance disparities and geospatial biases across various models.",
        "tldr_zh": "本文介绍了一个名为IMAGEO-Bench的新基准，用于评估大型语言模型（LLM）的图像地理定位能力，揭示了不同模型在性能和地理空间偏差方面的差异。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models",
        "summary": "In this paper, we introduce, for the first time, the concept of Set Pivot\nLearning, a paradigm shift that redefines domain generalization (DG) based on\nVision Foundation Models (VFMs). Traditional DG assumes that the target domain\nis inaccessible during training, but the emergence of VFMs, trained on vast and\ndiverse data, renders this assumption unclear and obsolete. Traditional DG\nassumes that the target domain is inaccessible during training, but the\nemergence of VFMs, which are trained on vast and diverse datasets, renders this\nassumption unclear and obsolete. To address this challenge, we propose Set\nPivot Learning (SPL), a new definition of domain migration task based on VFMs,\nwhich is more suitable for current research and application requirements.\nUnlike conventional DG methods, SPL prioritizes adaptive refinement over rigid\ndomain transfer, ensuring continuous alignment with evolving real-world\nconditions. Specifically, SPL features two key attributes: (i) Dynamic\nadaptation, transitioning from static domain alignment to flexible, task-driven\nfeature optimization, enabling models to evolve with downstream scenarios; (ii)\nVFM-centric tuning, leveraging pretrained knowledge as a pivot to hone\ntask-specific representations while preserving cross-domain robustness.\nBuilding on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combines\na Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevate\nVFM performance in targeted scenarios. Extensive experiments on benchmark\ndatasets show the effectiveness of our method, highlighting its superiority\nover state-of-the-art methods, particularly in generalized segmentation.",
        "url": "http://arxiv.org/abs/2508.01582v1",
        "published_date": "2025-08-03T04:20:35+00:00",
        "updated_date": "2025-08-03T04:20:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinhui Li",
            "Xinyu He",
            "Qiming Hu",
            "Xiaojie Guo"
        ],
        "tldr": "The paper introduces Set Pivot Learning (SPL), a novel domain generalization paradigm using Vision Foundation Models (VFMs) for generalized segmentation, and proposes a Dynamic Prompt Fine-Tuning method to improve VFM performance in target scenarios.",
        "tldr_zh": "该论文提出了集合枢轴学习（SPL），一种使用视觉基础模型（VFMs）进行广义分割的新型领域泛化范式，并提出了一种动态提示微调方法，以提高VFM在目标场景中的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Harnessing Textual Semantic Priors for Knowledge Transfer and Refinement in CLIP-Driven Continual Learning",
        "summary": "Continual learning (CL) aims to equip models with the ability to learn from a\nstream of tasks without forgetting previous knowledge. With the progress of\nvision-language models like Contrastive Language-Image Pre-training (CLIP),\ntheir promise for CL has attracted increasing attention due to their strong\ngeneralizability. However, the potential of rich textual semantic priors in\nCLIP in addressing the stability-plasticity dilemma remains underexplored.\nDuring backbone training, most approaches transfer past knowledge without\nconsidering semantic relevance, leading to interference from unrelated tasks\nthat disrupt the balance between stability and plasticity. Besides, while\ntext-based classifiers provide strong generalization, they suffer from limited\nplasticity due to the inherent modality gap in CLIP. Visual classifiers help\nbridge this gap, but their prototypes lack rich and precise semantics. To\naddress these challenges, we propose Semantic-Enriched Continual Adaptation\n(SECA), a unified framework that harnesses the anti-forgetting and structured\nnature of textual priors to guide semantic-aware knowledge transfer in the\nbackbone and reinforce the semantic structure of the visual classifier.\nSpecifically, a Semantic-Guided Adaptive Knowledge Transfer (SG-AKT) module is\nproposed to assess new images' relevance to diverse historical visual knowledge\nvia textual cues, and aggregate relevant knowledge in an instance-adaptive\nmanner as distillation signals. Moreover, a Semantic-Enhanced Visual Prototype\nRefinement (SE-VPR) module is introduced to refine visual prototypes using\ninter-class semantic relations captured in class-wise textual embeddings.\nExtensive experiments on multiple benchmarks validate the effectiveness of our\napproach.",
        "url": "http://arxiv.org/abs/2508.01579v1",
        "published_date": "2025-08-03T04:09:00+00:00",
        "updated_date": "2025-08-03T04:09:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingfeng He",
            "De Cheng",
            "Huaijie Wang",
            "Nannan Wang"
        ],
        "tldr": "This paper proposes a Semantic-Enriched Continual Adaptation (SECA) framework for CLIP-driven continual learning, using textual semantic priors to guide knowledge transfer and refine visual classifiers, addressing the stability-plasticity dilemma and modality gap.",
        "tldr_zh": "本文提出了一个语义增强的持续适应（SECA）框架，用于CLIP驱动的持续学习，利用文本语义先验来指导知识转移和改进视觉分类器，解决了稳定-可塑性困境和模态差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models",
        "summary": "Vision Foundation Models(VFMs) have achieved remarkable success in various\ncomputer vision tasks. However, their application to semantic segmentation is\nhindered by two significant challenges: (1) the disparity in data scale, as\nsegmentation datasets are typically much smaller than those used for VFM\npre-training, and (2) domain distribution shifts, where real-world segmentation\nscenarios are diverse and often underrepresented during pre-training. To\novercome these limitations, we present Rein++, an efficient VFM-based\nsegmentation framework that demonstrates superior generalization from limited\ndata and enables effective adaptation to diverse unlabeled scenarios.\nSpecifically, Rein++ comprises a domain generalization solution Rein-G and a\ndomain adaptation solution Rein-A. Rein-G introduces a set of trainable,\ninstance-aware tokens that effectively refine the VFM's features for the\nsegmentation task. This parameter-efficient approach fine-tunes less than 1% of\nthe backbone's parameters, enabling robust generalization. Building on the\nRein-G, Rein-A performs unsupervised domain adaptation at both the instance and\nlogit levels to mitigate domain shifts. In addition, it incorporates a semantic\ntransfer module that leverages the class-agnostic capabilities of the segment\nanything model to enhance boundary details in the target domain. The integrated\nRein++ pipeline first learns a generalizable model on a source domain (e.g.,\ndaytime scenes) and subsequently adapts it to diverse target domains (e.g.,\nnighttime scenes) without any target labels. Comprehensive experiments\ndemonstrate that Rein++ significantly outperforms state-of-the-art methods with\nefficient training, underscoring its roles an efficient, generalizable, and\nadaptive segmentation solution for VFMs, even for large models with billions of\nparameters. The code is available at https://github.com/wloves/Rein.",
        "url": "http://arxiv.org/abs/2508.01667v1",
        "published_date": "2025-08-03T08:53:30+00:00",
        "updated_date": "2025-08-03T08:53:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixiang Wei",
            "Xiaoxiao Ma",
            "Ruishen Yan",
            "Tao Tu",
            "Huaian Chen",
            "Jinjin Zheng",
            "Yi Jin",
            "Enhong Chen"
        ],
        "tldr": "The paper presents Rein++, a framework for semantic segmentation using Vision Foundation Models (VFMs) that addresses generalization from limited data and adaptation to diverse unlabeled scenarios, achieving state-of-the-art results with efficient training.",
        "tldr_zh": "该论文提出了 Rein++，一个使用视觉基础模型 (VFM) 的语义分割框架，解决了从有限数据中泛化和适应不同无标签场景的问题，并通过高效的训练实现了最先进的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]