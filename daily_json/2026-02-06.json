[
    {
        "title": "CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression",
        "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.",
        "url": "http://arxiv.org/abs/2602.05909v1",
        "published_date": "2026-02-05T17:25:16+00:00",
        "updated_date": "2026-02-05T17:25:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kangjie Zhang",
            "Wenxuan Huang",
            "Xin Zhou",
            "Boxiang Zhou",
            "Dejia Song",
            "Yuan Xie",
            "Baochang Zhang",
            "Lizhuang Ma",
            "Nemo Chen",
            "Xu Tang",
            "Yao Hu",
            "Shaohui Lin"
        ],
        "tldr": "The paper introduces CLIP-Map, a mapping-based CLIP compression framework using learnable matrices and Kronecker factorization to preserve information during compression, especially at high compression ratios, addressing limitations of selection-based methods.",
        "tldr_zh": "该论文介绍了一种基于映射的CLIP压缩框架CLIP-Map，它使用可学习的矩阵和Kronecker分解来在压缩过程中保留信息，尤其是在高压缩比下，解决了基于选择的方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning",
        "summary": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR",
        "url": "http://arxiv.org/abs/2602.05809v1",
        "published_date": "2026-02-05T16:02:48+00:00",
        "updated_date": "2026-02-05T16:02:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Enwei Tong",
            "Yuanchao Bai",
            "Yao Zhu",
            "Junjun Jiang",
            "Xianming Liu"
        ],
        "tldr": "The paper introduces Focus-Scan-Refine (FSR), a novel token pruning framework for vision-language models inspired by human visual perception, which improves accuracy-efficiency trade-off by focusing on key evidence, scanning for complementary context, and refining the context.",
        "tldr_zh": "本文介绍了一种新的视觉语言模型令牌修剪框架Focus-Scan-Refine (FSR)，该框架受到人类视觉感知的启发，通过关注关键证据、扫描补充上下文和优化上下文，提高了准确性和效率的平衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation",
        "summary": "With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.",
        "url": "http://arxiv.org/abs/2602.05789v1",
        "published_date": "2026-02-05T15:45:39+00:00",
        "updated_date": "2026-02-05T15:45:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hengyi Wang",
            "Ruiqiang Zhang",
            "Chang Liu",
            "Guanjie Wang",
            "Zehua Ma",
            "Han Fang",
            "Weiming Zhang"
        ],
        "tldr": "The paper introduces Allocentric Perceiver, a training-free method that improves VLMs' allocentric reasoning by explicitly transforming reconstructed 3D geometry into a query-conditioned allocentric frame, achieving performance gains on spatial reasoning benchmarks.",
        "tldr_zh": "该论文介绍了 Allocentric Perceiver，一种无需训练的方法，通过将重建的 3D 几何体显式转换为查询条件下的以自我为中心的框架，从而提高了 VLM 的以自我为中心的推理能力，并在空间推理基准测试中取得了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification",
        "summary": "Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.",
        "url": "http://arxiv.org/abs/2602.05729v1",
        "published_date": "2026-02-05T14:52:35+00:00",
        "updated_date": "2026-02-05T14:52:35+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Lexiang Hu",
            "Youze Xue",
            "Dian Li",
            "Gang Liu",
            "Zhouchen Lin"
        ],
        "tldr": "The paper introduces AGFF-Embed, a method to fuse global and fine-grained perceptual information into MLLM embeddings, enhanced by hard negative amplification, achieving state-of-the-art performance on VLM benchmarks.",
        "tldr_zh": "本文介绍了AGFF-Embed，一种将全局和细粒度感知信息融合到MLLM嵌入中的方法，通过硬负样本增强，在VLM基准测试上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation",
        "summary": "Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.",
        "url": "http://arxiv.org/abs/2602.05578v1",
        "published_date": "2026-02-05T12:03:11+00:00",
        "updated_date": "2026-02-05T12:03:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyang Chen",
            "Xiangbo Lv",
            "Zhiqiang Kou",
            "Xingdong Sheng",
            "Ning Xu",
            "Yiguo Qiao"
        ],
        "tldr": "LoGoSeg introduces a single-stage framework for open-vocabulary semantic segmentation that integrates object priors, region-aware alignment, and a dual-stream fusion mechanism to improve segmentation accuracy and generalization without relying on external data or complex architectures.",
        "tldr_zh": "LoGoSeg 提出了一个用于开放词汇语义分割的单阶段框架，集成了对象先验、区域感知对齐和双流融合机制，以提高分割精度和泛化能力，而无需依赖外部数据或复杂的架构。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator",
        "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.",
        "url": "http://arxiv.org/abs/2602.05552v1",
        "published_date": "2026-02-05T11:23:11+00:00",
        "updated_date": "2026-02-05T11:23:11+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bessie Dominguez-Dager",
            "Sergio Suescun-Ferrandiz",
            "Felix Escalona",
            "Francisco Gomez-Donoso",
            "Miguel Cazorla"
        ],
        "tldr": "The paper introduces VLN-Pilot, a framework using VLLMs to autonomously navigate indoor drones based on natural language instructions, demonstrating successful long-horizon navigation and potential for various indoor applications.",
        "tldr_zh": "该论文介绍了VLN-Pilot，一个利用VLLM通过自然语言指令自主导航室内无人机的框架，展示了成功的长距离导航，并具有在各种室内应用中的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting",
        "summary": "Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.",
        "url": "http://arxiv.org/abs/2602.05384v1",
        "published_date": "2026-02-05T07:09:57+00:00",
        "updated_date": "2026-02-05T07:09:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Feng",
            "Wei Shi",
            "Ke Zhang",
            "Xiang Fei",
            "Lei Liao",
            "Dingkang Yang",
            "Yongkun Du",
            "Xuecheng Wu",
            "Jingqun Tang",
            "Yang Liu",
            "Hong Chen",
            "Can Huang"
        ],
        "tldr": "Dolphin-v2 improves document parsing by handling both digital and photographed documents through a two-stage process, achieving significant performance gains and addressing limitations of existing systems, including code block recognition.",
        "tldr_zh": "Dolphin-v2通过两阶段流程改进了文档解析，可处理数字文档和照片文档，实现了显著的性能提升，并解决了现有系统的局限性，包括代码块识别。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs",
        "summary": "Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.",
        "url": "http://arxiv.org/abs/2602.05382v1",
        "published_date": "2026-02-05T07:07:27+00:00",
        "updated_date": "2026-02-05T07:07:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tina Khezresmaeilzadeh",
            "Jike Zhong",
            "Konstantinos Psounis"
        ],
        "tldr": "The paper introduces VRIQ, a new benchmark for evaluating visual reasoning in VLMs, finding that current VLMs struggle significantly, primarily due to perception limitations.",
        "tldr_zh": "该论文介绍了VRIQ，一个新的用于评估VLM中视觉推理能力的基准。研究发现，目前的VLM在这方面表现不佳，主要原因是感知方面的限制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Latent Reasoning via Hierarchical Visual Cues Injection",
        "summary": "The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a \"fast thinking\" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\\emph{HIVE}), a novel framework that instills deliberate, \"slow thinking\" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.",
        "url": "http://arxiv.org/abs/2602.05359v1",
        "published_date": "2026-02-05T06:31:12+00:00",
        "updated_date": "2026-02-05T06:31:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Zhang",
            "Qiangyu Yan",
            "Borui Jiang",
            "Kai Han"
        ],
        "tldr": "The paper introduces HIVE, a novel framework for multimodal latent reasoning that integrates hierarchical visual cues into the latent space of MLLMs, enabling more grounded and efficient inference without relying on textual rationales.",
        "tldr_zh": "该论文介绍了一种名为HIVE的新型多模态潜在推理框架，该框架将分层视觉线索整合到 MLLM 的潜在空间中，从而实现更扎实和高效的推理，而无需依赖文本理由。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.",
        "url": "http://arxiv.org/abs/2602.05275v1",
        "published_date": "2026-02-05T04:01:01+00:00",
        "updated_date": "2026-02-05T04:01:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qi Li",
            "Yanzhe Zhao",
            "Yongxin Zhou",
            "Yameng Wang",
            "Yandong Yang",
            "Yuanjia Zhou",
            "Jue Wang",
            "Zuojian Wang",
            "Jinxiang Liu"
        ],
        "tldr": "The paper introduces Magic-MM-Embedding, a visual-token-efficient MLLM architecture with a multi-stage training strategy, achieving state-of-the-art performance in universal multimodal embedding while reducing computational costs.",
        "tldr_zh": "该论文介绍了Magic-MM-Embedding，一种视觉token高效的MLLM架构，采用多阶段训练策略，在通用多模态嵌入方面实现了最先进的性能，同时降低了计算成本。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching",
        "summary": "Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.",
        "url": "http://arxiv.org/abs/2602.05215v1",
        "published_date": "2026-02-05T02:16:00+00:00",
        "updated_date": "2026-02-05T02:16:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Nie",
            "Wenbin An",
            "Gongjie Zhang",
            "Yicheng Xu",
            "Yap-Peng Tan",
            "Alex C. Kot",
            "Shijian Lu"
        ],
        "tldr": "The paper introduces E.M.Ground, a novel Vid-LLM for Temporal Video Grounding (TVG) that leverages holistic event perception and matching using a special <event> token, Savitzky-Golay smoothing, and multi-grained frame feature aggregation to improve accuracy and robustness.",
        "tldr_zh": "该论文介绍了一种名为E.M.Ground的新型Vid-LLM，用于时间视频定位（TVG）。它利用整体事件感知和匹配，通过特殊的<event> token、Savitzky-Golay平滑和多粒度帧特征聚合来提高准确性和鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual concept ranking uncovers medical shortcuts used by large multimodal models",
        "summary": "Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.",
        "url": "http://arxiv.org/abs/2602.05096v1",
        "published_date": "2026-02-04T22:27:34+00:00",
        "updated_date": "2026-02-04T22:27:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Joseph D. Janizek",
            "Sonnet Xu",
            "Junayd Lateef",
            "Roxana Daneshjou"
        ],
        "tldr": "This paper introduces Visual Concept Ranking (VCR) to identify important visual concepts used by large multimodal models (LMMs) in medical tasks, particularly skin lesion classification, revealing unexpected demographic performance gaps and potential model shortcuts.",
        "tldr_zh": "本文介绍了视觉概念排序(VCR)方法，用于识别大型多模态模型(LMM)在医学任务中使用的重要视觉概念，特别是皮肤病变分类，揭示了意想不到的人口统计性能差距和潜在的模型捷径。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .",
        "url": "http://arxiv.org/abs/2602.05049v1",
        "published_date": "2026-02-04T20:59:29+00:00",
        "updated_date": "2026-02-04T20:59:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Yiye Chen",
            "Yanan Jian",
            "Xiaoyi Dong",
            "Shuxin Cao",
            "Jing Wu",
            "Patricio Vela",
            "Benjamin E. Lundell",
            "Dongdong Chen"
        ],
        "tldr": "This paper proposes a training framework, VISTA, to enhance visual conditioning in Vision-Language-Action (VLA) models by aligning action prediction with visual input using preference optimization and latent-space distillation, leading to improved performance without architectural changes or new data.",
        "tldr_zh": "本文提出了一种训练框架VISTA，通过使用偏好优化和潜在空间蒸馏将动作预测与视觉输入对齐，从而增强视觉-语言-动作（VLA）模型中的视觉条件作用，在不改变架构或收集新数据的情况下提高了性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforced Attention Learning",
        "summary": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.\n  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.",
        "url": "http://arxiv.org/abs/2602.04884v1",
        "published_date": "2026-02-04T18:59:52+00:00",
        "updated_date": "2026-02-04T18:59:52+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Bangzheng Li",
            "Jianmo Ni",
            "Chen Qu",
            "Ian Miao",
            "Liu Yang",
            "Xingyu Fu",
            "Muhao Chen",
            "Derek Zhiyuan Cheng"
        ],
        "tldr": "The paper introduces Reinforced Attention Learning (RAL), a policy-gradient framework that optimizes attention distributions in Multimodal LLMs (MLLMs) instead of output tokens, leading to improved grounding and performance in image and video tasks. They also propose On-Policy Attention Distillation for cross-modal alignment.",
        "tldr_zh": "该论文介绍了强化注意力学习（RAL），这是一种策略梯度框架，它优化多模态LLM（MLLM）中的注意力分布，而不是输出tokens，从而提高了图像和视频任务中的基础和性能。他们还提出了用于跨模态对齐的On-Policy注意力蒸馏。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When LLaVA Meets Objects: Token Composition for Vision-Language-Models",
        "summary": "Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.",
        "url": "http://arxiv.org/abs/2602.04864v1",
        "published_date": "2026-02-04T18:50:46+00:00",
        "updated_date": "2026-02-04T18:50:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Soumya Jahagirdar",
            "Walid Bousselham",
            "Anna Kukleva",
            "Hilde Kuehne"
        ],
        "tldr": "The paper introduces Mask-LLaVA, a framework that uses multi-level visual features (mask-based objects, global tokens, and local patches) to create compact visual representations for autoregressive VLMs, enabling dynamic token selection at inference time to reduce compute without significant performance loss.",
        "tldr_zh": "该论文介绍了Mask-LLaVA，一个利用多层次视觉特征（基于掩码的对象、全局令牌和局部补丁）为自回归视觉语言模型创建紧凑视觉表示的框架，从而能够在推理时动态选择令牌，减少计算量而不会显著降低性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Ethology of Latent Spaces",
        "summary": "This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.\n  Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.\n  We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.",
        "url": "http://arxiv.org/abs/2602.05710v1",
        "published_date": "2026-02-05T14:37:31+00:00",
        "updated_date": "2026-02-05T14:37:31+00:00",
        "categories": [
            "cs.CY",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Philippe Boisnard"
        ],
        "tldr": "This paper analyzes how different VLMs exhibit varying sensitivities and biases in their latent spaces when interpreting art, revealing the influence of training data and model architecture on cultural and political categorizations.",
        "tldr_zh": "本文分析了不同的视觉语言模型在解释艺术时，其潜在空间如何表现出不同的敏感性和偏差，揭示了训练数据和模型架构对文化和政治分类的影响。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
        "summary": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.",
        "url": "http://arxiv.org/abs/2602.05638v1",
        "published_date": "2026-02-05T13:18:33+00:00",
        "updated_date": "2026-02-05T13:18:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinlin Wu",
            "Felix Holm",
            "Chuxi Chen",
            "An Wang",
            "Yaxin Hu",
            "Xiaofan Ye",
            "Zelin Zang",
            "Miao Xu",
            "Lihua Zhou",
            "Huai Liao",
            "Danny T. M. Chan",
            "Ming Feng",
            "Wai S. Poon",
            "Hongliang Ren",
            "Dong Yi",
            "Nassir Navab",
            "Gaofeng Meng",
            "Jiebo Luo",
            "Hongbin Liu",
            "Zhen Lei"
        ],
        "tldr": "The paper introduces UniSurg, a video-native foundation model for surgical videos trained on a large dataset using motion prediction and self-distillation techniques, achieving SOTA results on various surgical video understanding tasks.",
        "tldr_zh": "该论文介绍了UniSurg，一个视频原生的手术视频基础模型，通过在大型数据集上使用运动预测和自蒸馏技术进行训练，在各种手术视频理解任务上取得了SOTA结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation",
        "summary": "With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.",
        "url": "http://arxiv.org/abs/2602.05339v1",
        "published_date": "2026-02-05T06:05:24+00:00",
        "updated_date": "2026-02-05T06:05:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yongwoo Kim",
            "Sungmin Cha",
            "Hyunsoo Kim",
            "Jaewon Lee",
            "Donghyun Kim"
        ],
        "tldr": "This paper introduces a novel framework, PAIRed Erasing (PAIR), for selectively erasing undesirable concepts in text-to-image diffusion models while preserving semantic consistency by using unsafe-safe pairs for training and adaptation.",
        "tldr_zh": "该论文介绍了一种名为PAIRed Erasing (PAIR) 的新框架，旨在选择性地擦除文本到图像扩散模型中不需要的概念，同时通过使用不安全-安全对进行训练和适配来保持语义一致性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]