[
    {
        "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
        "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat.",
        "url": "http://arxiv.org/abs/2508.08252v1",
        "published_date": "2025-08-11T17:59:30+00:00",
        "updated_date": "2025-08-11T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuting He",
            "Guangquan Jie",
            "Changshuo Wang",
            "Yun Zhou",
            "Shuming Hu",
            "Guanbin Li",
            "Henghui Ding"
        ],
        "tldr": "The paper introduces a new task, Referring 3D Gaussian Splatting Segmentation (R3DGS), a corresponding dataset (Ref-LERF), and a novel framework called ReferSplat that achieves state-of-the-art performance on this task and 3D open-vocabulary segmentation.",
        "tldr_zh": "该论文介绍了一个新任务：Referring 3D Gaussian Splatting Segmentation (R3DGS)，以及对应的数据集 (Ref-LERF)，并提出了一个名为 ReferSplat 的新框架，该框架在该任务和3D开放词汇分割上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks",
        "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/",
        "url": "http://arxiv.org/abs/2508.08240v1",
        "published_date": "2025-08-11T17:54:31+00:00",
        "updated_date": "2025-08-11T17:54:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Kaijun Wang",
            "Liqin Lu",
            "Mingyu Liu",
            "Jianuo Jiang",
            "Zeju Li",
            "Bolin Zhang",
            "Wancai Zheng",
            "Xinyi Yu",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "tldr": "The paper introduces ODYSSEY, a framework for language-guided long-horizon mobile manipulation using quadruped robots, featuring a hierarchical vision-language model planner and a whole-body control policy, demonstrated through sim-to-real transfer.",
        "tldr_zh": "该论文介绍了 ODYSSEY，一个用于四足机器人语言引导的长期移动操作框架，具有分层视觉语言模型规划器和全身控制策略，并通过从仿真到现实的转换进行了演示。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning User Preferences for Image Generation Model",
        "summary": "User preference prediction requires a comprehensive and accurate\nunderstanding of individual tastes. This includes both surface-level\nattributes, such as color and style, and deeper content-related aspects, such\nas themes and composition. However, existing methods typically rely on general\nhuman preferences or assume static user profiles, often neglecting individual\nvariability and the dynamic, multifaceted nature of personal taste. To address\nthese limitations, we propose an approach built upon Multimodal Large Language\nModels, introducing contrastive preference loss and preference tokens to learn\npersonalized user preferences from historical interactions. The contrastive\npreference loss is designed to effectively distinguish between user ''likes''\nand ''dislikes'', while the learnable preference tokens capture shared interest\nrepresentations among existing users, enabling the model to activate\ngroup-specific preferences and enhance consistency across similar users.\nExtensive experiments demonstrate our model outperforms other methods in\npreference prediction accuracy, effectively identifying users with similar\naesthetic inclinations and providing more precise guidance for generating\nimages that align with individual tastes. The project page is\n\\texttt{https://learn-user-pref.github.io/}.",
        "url": "http://arxiv.org/abs/2508.08220v1",
        "published_date": "2025-08-11T17:39:42+00:00",
        "updated_date": "2025-08-11T17:39:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenyi Mo",
            "Ying Ba",
            "Tianyu Zhang",
            "Yalong Bai",
            "Biye Li"
        ],
        "tldr": "This paper introduces a multimodal large language model approach with contrastive preference loss and preference tokens to learn personalized user preferences for image generation, outperforming existing methods in accuracy and user alignment.",
        "tldr_zh": "本文提出了一种基于多模态大型语言模型的方法，利用对比偏好损失和偏好令牌来学习个性化的用户图像生成偏好，并在准确性和用户对齐方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model",
        "summary": "Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks.",
        "url": "http://arxiv.org/abs/2508.08199v1",
        "published_date": "2025-08-11T17:17:20+00:00",
        "updated_date": "2025-08-11T17:17:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiqi He",
            "Zhenhao Zhang",
            "Yixiang Zhang",
            "Xiongjun Zhao",
            "Shaoliang Peng"
        ],
        "tldr": "The paper introduces Spatial-ORMLLM, a novel vision-language model for 3D spatial reasoning in operating rooms using only RGB modality, achieving state-of-the-art performance on clinical datasets.",
        "tldr_zh": "该论文介绍了Spatial-ORMLLM，一种新型的视觉-语言模型，仅使用RGB模态即可在手术室中进行3D空间推理，并在临床数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
        "summary": "Accurately grounding regions of interest (ROIs) is critical for diagnosis and\ntreatment planning in medical imaging. While multimodal large language models\n(MLLMs) combine visual perception with natural language, current\nmedical-grounding pipelines still rely on supervised fine-tuning with explicit\nspatial hints, making them ill-equipped to handle the implicit queries common\nin clinical practice. This work makes three core contributions. We first define\nUnified Medical Reasoning Grounding (UMRG), a novel vision-language task that\ndemands clinical reasoning and pixel-level grounding. Second, we release\nU-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside\nimplicit clinical queries and reasoning traces, spanning 10 modalities, 15\nsuper-categories, and 108 specific categories. Finally, we introduce\nMedReasoner, a modular framework that distinctly separates reasoning from\nsegmentation: an MLLM reasoner is optimized with reinforcement learning, while\na frozen segmentation expert converts spatial prompts into masks, with\nalignment achieved through format and accuracy rewards. MedReasoner achieves\nstate-of-the-art performance on U-MRG-14K and demonstrates strong\ngeneralization to unseen clinical queries, underscoring the significant promise\nof reinforcement learning for interpretable medical grounding.",
        "url": "http://arxiv.org/abs/2508.08177v1",
        "published_date": "2025-08-11T16:59:06+00:00",
        "updated_date": "2025-08-11T16:59:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhonghao Yan",
            "Muxi Diao",
            "Yuxuan Yang",
            "Jiayuan Xu",
            "Kaizhou Zhang",
            "Ruoyan Jing",
            "Lele Yang",
            "Yanxi Liu",
            "Kongming Liang",
            "Zhanyu Ma"
        ],
        "tldr": "MedReasoner introduces a reinforcement learning-based framework for medical image grounding with implicit clinical queries, along with a new dataset, U-MRG-14K, achieving state-of-the-art performance.",
        "tldr_zh": "MedReasoner 提出了一个基于强化学习的医学图像基准框架，用于处理隐式临床查询，并发布了一个新的数据集 U-MRG-14K，实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning",
        "summary": "This paper introduces TBAC-UniImage, a novel unified model for multimodal\nunderstanding and generation. We achieve this by deeply integrating a\npre-trained Diffusion Model, acting as a generative ladder, with a Multimodal\nLarge Language Model (MLLM). Previous diffusion-based unified models face two\nprimary limitations. One approach uses only the MLLM's final hidden state as\nthe generative condition. This creates a shallow connection, as the generator\nis isolated from the rich, hierarchical representations within the MLLM's\nintermediate layers. The other approach, pretraining a unified generative\narchitecture from scratch, is computationally expensive and prohibitive for\nmany researchers. To overcome these issues, our work explores a new paradigm.\nInstead of relying on a single output, we use representations from multiple,\ndiverse layers of the MLLM as generative conditions for the diffusion model.\nThis method treats the pre-trained generator as a ladder, receiving guidance\nfrom various depths of the MLLM's understanding process. Consequently,\nTBAC-UniImage achieves a much deeper and more fine-grained unification of\nunderstanding and generation.",
        "url": "http://arxiv.org/abs/2508.08098v1",
        "published_date": "2025-08-11T15:37:22+00:00",
        "updated_date": "2025-08-11T15:37:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junzhe Xu",
            "Yuyang Yin",
            "Xi Chen"
        ],
        "tldr": "TBAC-UniImage unifies multimodal understanding and generation by using a pre-trained diffusion model as a generative ladder, guided by diverse layers of a Multimodal Large Language Model (MLLM), achieving a deeper integration compared to previous methods.",
        "tldr_zh": "TBAC-UniImage通过利用预训练扩散模型作为生成梯，并由多模态大型语言模型（MLLM）的不同层引导，统一了多模态理解和生成，实现了比以往方法更深入的整合。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering",
        "summary": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field.",
        "url": "http://arxiv.org/abs/2508.07918v1",
        "published_date": "2025-08-11T12:32:48+00:00",
        "updated_date": "2025-08-11T12:32:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xing Zi",
            "Jinghao Xiao",
            "Yunxiao Shi",
            "Xian Tao",
            "Jun Li",
            "Ali Braytee",
            "Mukesh Prasad"
        ],
        "tldr": "The paper introduces RSVLM-QA, a new large-scale remote sensing VQA dataset with diverse question types and rich annotations generated using LLMs, and benchmarks several VLMs on it, suggesting it can effectively evaluate VLM reasoning abilities in the remote sensing domain.",
        "tldr_zh": "该论文介绍了RSVLM-QA，这是一个新的大规模遥感VQA数据集，具有多样的问题类型和使用LLM生成的丰富注释，并在其上对多个VLM进行了基准测试，表明它可以有效地评估遥感领域中VLM的推理能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning",
        "summary": "Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios.",
        "url": "http://arxiv.org/abs/2508.07871v1",
        "published_date": "2025-08-11T11:41:51+00:00",
        "updated_date": "2025-08-11T11:41:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanshu Li",
            "Jianjiang Yang",
            "Zhennan Shen",
            "Ligong Han",
            "Haoyan Xu",
            "Ruixiang Tang"
        ],
        "tldr": "This paper introduces CATP, a training-free token pruning method that improves the efficiency and performance of multimodal in-context learning by selectively removing redundant image tokens, achieving a performance gain and latency reduction across multiple LVLMs and benchmarks.",
        "tldr_zh": "本文介绍了一种名为CATP的免训练令牌剪枝方法，通过选择性地删除冗余图像令牌，提高了多模态上下文学习的效率和性能，并在多个LVLM和基准测试中实现了性能提升和延迟降低。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model",
        "summary": "Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5.",
        "url": "http://arxiv.org/abs/2508.07863v1",
        "published_date": "2025-08-11T11:26:10+00:00",
        "updated_date": "2025-08-11T11:26:10+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Bin Cao",
            "Sipeng Zheng",
            "Ye Wang",
            "Lujie Xia",
            "Qianshan Wei",
            "Qin Jin",
            "Jing Liu",
            "Zongqing Lu"
        ],
        "tldr": "The paper introduces Being-M0.5, a real-time controllable Vision-Language-Motion Model (VLMM) addressing controllability limitations in existing models through a novel part-aware tokenization technique and a large-scale human motion dataset, HuMo100M.",
        "tldr_zh": "该论文介绍了Being-M0.5，一个实时的、可控的视觉-语言-动作模型（VLMM），通过一种新颖的部件感知标记化技术和一个大规模的人类动作数据集HuMo100M，解决了现有模型在可控性方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Effortless Vision-Language Model Specialization in Histopathology without Annotation",
        "summary": "Recent advances in Vision-Language Models (VLMs) in histopathology, such as\nCONCH and QuiltNet, have demonstrated impressive zero-shot classification\ncapabilities across various tasks. However, their general-purpose design may\nlead to suboptimal performance in specific downstream applications. While\nsupervised fine-tuning methods address this issue, they require manually\nlabeled samples for adaptation. This paper investigates annotation-free\nadaptation of VLMs through continued pretraining on domain- and task-relevant\nimage-caption pairs extracted from existing databases. Our experiments on two\nVLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs\nsubstantially enhance both zero-shot and few-shot performance. Notably, with\nlarger training sizes, continued pretraining matches the performance of\nfew-shot methods while eliminating manual labeling. Its effectiveness,\ntask-agnostic design, and annotation-free workflow make it a promising pathway\nfor adapting VLMs to new histopathology tasks. Code is available at\nhttps://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.",
        "url": "http://arxiv.org/abs/2508.07835v1",
        "published_date": "2025-08-11T10:39:27+00:00",
        "updated_date": "2025-08-11T10:39:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingna Qiu",
            "Nishanth Jain",
            "Jonas Ammeling",
            "Marc Aubreville",
            "Katharina Breininger"
        ],
        "tldr": "This paper presents an annotation-free method for specializing vision-language models (VLMs) in histopathology by continued pretraining on image-caption pairs extracted from existing databases, demonstrating improved zero-shot and few-shot performance on downstream tasks.",
        "tldr_zh": "本文提出了一种无需标注的方法，通过在从现有数据库中提取的图像-文本对上进行持续预训练，来专门化组织病理学中的视觉-语言模型（VLM），并在下游任务上展示了改进的零样本和小样本性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization",
        "summary": "Vision Language Models (VLMs) encode multimodal inputs over large, complex,\nand difficult-to-interpret architectures, which limit transparency and trust.\nWe propose a Multimodal Inversion for Model Interpretation and\nConceptualization (MIMIC) framework to visualize the internal representations\nof VLMs by synthesizing visual concepts corresponding to internal encodings.\nMIMIC uses a joint VLM-based inversion and a feature alignment objective to\naccount for VLM's autoregressive processing. It additionally includes a triplet\nof regularizers for spatial alignment, natural image smoothness, and semantic\nrealism. We quantitatively and qualitatively evaluate MIMIC by inverting visual\nconcepts over a range of varying-length free-form VLM output texts. Reported\nresults include both standard visual quality metrics as well as semantic\ntext-based metrics. To the best of our knowledge, this is the first model\ninversion approach addressing visual interpretations of VLM concepts.",
        "url": "http://arxiv.org/abs/2508.07833v1",
        "published_date": "2025-08-11T10:36:58+00:00",
        "updated_date": "2025-08-11T10:36:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Animesh Jain",
            "Alexandros Stergiou"
        ],
        "tldr": "The paper introduces MIMIC, a novel framework for inverting VLMs to visualize and interpret internal representations as visual concepts, aiming to improve transparency and trust in these models.",
        "tldr_zh": "本文介绍了一种新的框架MIMIC，用于反演VLM并将内部表示可视化为视觉概念，旨在提高这些模型的透明度和可信度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP",
        "summary": "Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap\nwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of\nlocal inductive biases for dense prediction and their reliance on inflexible\nfeature fusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method integrates a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks.",
        "url": "http://arxiv.org/abs/2508.07819v1",
        "published_date": "2025-08-11T10:03:45+00:00",
        "updated_date": "2025-08-11T10:03:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ke Ma",
            "Jun Long",
            "Hongxiao Fei",
            "Liujie Hua",
            "Yueyi Luo"
        ],
        "tldr": "This paper introduces an architectural co-design framework, Conv-LoRA and Dynamic Fusion Gateway (DFG), to adapt pre-trained Vision-Language Models (VLMs) for Zero-Shot Anomaly Detection, improving accuracy and robustness.",
        "tldr_zh": "该论文介绍了一个架构协同设计框架，Conv-LoRA和动态融合网关（DFG），用于调整预训练的视觉语言模型（VLM）以进行零样本异常检测，从而提高准确性和鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning",
        "summary": "Generating 3D human poses from multimodal inputs such as images or text\nrequires models to capture both rich spatial and semantic correspondences.\nWhile pose-specific multimodal large language models (MLLMs) have shown promise\nin this task, they are typically trained with supervised objectives such as\nSMPL parameter regression or token-level prediction, which struggle to model\nthe inherent ambiguity and achieve task-specific alignment required for\naccurate 3D pose generation. To address these limitations, we propose Pose-RFT,\na reinforcement fine-tuning framework tailored for 3D human pose generation in\nMLLMs. We formulate the task as a hybrid action reinforcement learning problem\nthat jointly optimizes discrete language prediction and continuous pose\ngeneration. To this end, we introduce HyGRPO, a hybrid reinforcement learning\nalgorithm that performs group-wise reward normalization over sampled responses\nto guide joint optimization of discrete and continuous actions. Pose-RFT\nfurther incorporates task-specific reward functions to guide optimization\ntowards spatial alignment in image-to-pose generation and semantic consistency\nin text-to-pose generation. Extensive experiments on multiple pose generation\nbenchmarks demonstrate that Pose-RFT significantly improves performance over\nexisting pose-specific MLLMs, validating the effectiveness of hybrid action\nreinforcement fine-tuning for 3D pose generation.",
        "url": "http://arxiv.org/abs/2508.07804v1",
        "published_date": "2025-08-11T09:44:58+00:00",
        "updated_date": "2025-08-11T09:44:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bao Li",
            "Xiaomei Zhang",
            "Miao Xu",
            "Zhaoxin Fan",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "tldr": "The paper introduces Pose-RFT, a reinforcement fine-tuning framework for 3D human pose generation in MLLMs, using a hybrid reinforcement learning algorithm (HyGRPO) to optimize discrete language prediction and continuous pose generation.",
        "tldr_zh": "该论文介绍了 Pose-RFT，一个用于 MLLM 中 3D 人体姿势生成的强化微调框架，它使用混合强化学习算法 (HyGRPO) 来优化离散语言预测和连续姿势生成。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models",
        "summary": "Unlike bitmap images, scalable vector graphics (SVG) maintain quality when\nscaled, frequently employed in computer vision and artistic design in the\nrepresentation of SVG code. In this era of proliferating AI-powered systems,\nenabling AI to understand and generate SVG has become increasingly urgent.\nHowever, AI-driven SVG understanding and generation (U&G) remain significant\nchallenges. SVG code, equivalent to a set of curves and lines controlled by\nfloating-point parameters, demands high precision in SVG U&G. Besides, SVG\ngeneration operates under diverse conditional constraints, including textual\nprompts and visual references, which requires powerful multi-modal processing\nfor condition-to-SVG transformation. Recently, the rapid growth of Multi-modal\nLarge Language Models (MLLMs) have demonstrated capabilities to process\nmulti-modal inputs and generate complex vector controlling parameters,\nsuggesting the potential to address SVG U&G tasks within a unified model. To\nunlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset\ncalled UniSVG, comprising 525k data items, tailored for MLLM training and\nevaluation. To our best knowledge, it is the first comprehensive dataset\ndesigned for unified SVG generation (from textual prompts and images) and SVG\nunderstanding (color, category, usage, etc.). As expected, learning on the\nproposed dataset boosts open-source MLLMs' performance on various SVG U&G\ntasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,\nbenchmark, weights, codes and experiment details on\nhttps://ryanlijinke.github.io/.",
        "url": "http://arxiv.org/abs/2508.07766v1",
        "published_date": "2025-08-11T08:50:14+00:00",
        "updated_date": "2025-08-11T08:50:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinke Li",
            "Jiarui Yu",
            "Chenxing Wei",
            "Hande Dong",
            "Qiang Lin",
            "Liangjing Yang",
            "Zhicai Wang",
            "Yanbin Hao"
        ],
        "tldr": "The paper introduces UniSVG, a large-scale dataset for training and evaluating Multimodal Large Language Models (MLLMs) on SVG understanding and generation tasks, claiming superior performance compared to GPT-4V after training on the dataset.",
        "tldr_zh": "该论文介绍了一个名为UniSVG的大规模数据集，用于训练和评估多模态大型语言模型（MLLM）在SVG理解和生成任务上的表现。论文声称，经过在该数据集上训练后，模型性能优于GPT-4V。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents",
        "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.",
        "url": "http://arxiv.org/abs/2508.07642v1",
        "published_date": "2025-08-11T05:50:30+00:00",
        "updated_date": "2025-08-11T05:50:30+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Tianyi Ma",
            "Yue Zhang",
            "Zehao Wang",
            "Parisa Kordjamshidi"
        ],
        "tldr": "The paper introduces SkillNav, a modular VLN framework using skill-based reasoning and a VLM-based router to achieve state-of-the-art performance on R2R and strong generalization on GSA-R2R benchmarks.",
        "tldr_zh": "该论文介绍了SkillNav，一个模块化的VLN框架，它使用基于技能的推理和一个基于VLM的路由器，在R2R上实现了最先进的性能，并在GSA-R2R基准测试上实现了强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information",
        "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.",
        "url": "http://arxiv.org/abs/2508.07630v1",
        "published_date": "2025-08-11T05:19:23+00:00",
        "updated_date": "2025-08-11T05:19:23+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "I.2.7; I.2.10; I.4.10; I.7.5"
        ],
        "authors": [
            "Anirudh Iyengar Kaniyar Narayana Iyengar",
            "Srija Mukhopadhyay",
            "Adnan Qidwai",
            "Shubhankar Singh",
            "Dan Roth",
            "Vivek Gupta"
        ],
        "tldr": "The paper introduces InterChart, a new benchmark for evaluating VLMs' ability to reason across multiple charts, revealing their limitations in complex, multi-visual environments.",
        "tldr_zh": "该论文介绍了InterChart，这是一个新的基准，用于评估视觉语言模型在多个图表上进行推理的能力，揭示了它们在复杂、多视觉环境中的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models",
        "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
        "url": "http://arxiv.org/abs/2508.07570v1",
        "published_date": "2025-08-11T03:03:34+00:00",
        "updated_date": "2025-08-11T03:03:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Khanh-Binh Nguyen",
            "Phuoc-Nguyen Bui",
            "Hyunseung Choo",
            "Duc Thanh Nguyen"
        ],
        "tldr": "The paper introduces ACE, a novel test-time adaptation framework for VLMs that enhances caching by using class-specific thresholds for storing high-confidence embeddings, leading to improved robustness and generalization in out-of-distribution scenarios.",
        "tldr_zh": "该论文介绍了一种名为ACE的新型VLMs测试时自适应框架，该框架通过使用特定于类别的阈值来存储高置信度嵌入，从而增强缓存，从而在分布外场景中提高了鲁棒性和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FormCoach: Lift Smarter, Not Harder",
        "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI.",
        "url": "http://arxiv.org/abs/2508.07501v1",
        "published_date": "2025-08-10T22:33:43+00:00",
        "updated_date": "2025-08-10T22:33:43+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Xiaoye Zuo",
            "Nikos Athanasiou",
            "Ginger Delmas",
            "Yiming Huang",
            "Xingyu Fu",
            "Lingjie Liu"
        ],
        "tldr": "FormCoach introduces an AI-powered personal training system utilizing vision-language models for real-time form correction during exercises, along with a benchmark dataset and evaluation pipeline to advance research in AI-driven coaching.",
        "tldr_zh": "FormCoach 提出了一个基于视觉语言模型的人工智能个人训练系统，用于运动时实时纠正姿势，并提供了一个基准数据集和评估流程，以推进人工智能驱动的教练研究。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
        "summary": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
        "url": "http://arxiv.org/abs/2508.08066v1",
        "published_date": "2025-08-11T15:10:52+00:00",
        "updated_date": "2025-08-11T15:10:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Weitai Kang",
            "Weiming Zhuang",
            "Zhizhong Li",
            "Yan Yan",
            "Lingjuan Lyu"
        ],
        "tldr": "This paper investigates design choices for visual grounding in Multimodal Large Language Models (MLLMs) using LLaVA-1.5, identifying effective designs and optimizing fine-tuning data, achieving performance improvements on RefCOCO datasets.",
        "tldr_zh": "该论文使用LLaVA-1.5研究了多模态大型语言模型（MLLM）中视觉基础的设计选择，确定了有效的设计并优化了微调数据，在RefCOCO数据集上实现了性能改进。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation",
        "summary": "Depth estimation, essential for autonomous driving, seeks to interpret the 3D\nenvironment surrounding vehicles. The development of radar sensors, known for\ntheir cost-efficiency and robustness, has spurred interest in radar-camera\nfusion-based solutions. However, existing algorithms fuse features from these\nmodalities without accounting for weather conditions, despite radars being\nknown to be more robust than cameras under adverse weather. Additionally, while\nVision-Language models have seen rapid advancement, utilizing language\ndescriptions alongside other modalities for depth estimation remains an open\nchallenge. This paper first introduces a text-generation strategy along with\nfeature extraction and fusion techniques that can assist monocular depth\nestimation pipelines, leading to improved accuracy across different algorithms\non the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion\nalgorithm that enhances text feature extraction by incorporating radar point\ninformation. To address the impact of weather on sensor performance, we\nintroduce a weather-aware fusion block that adaptively adjusts radar weighting\nbased on current weather conditions. Our method, benchmarked on the nuScenes\ndataset, demonstrates performance gains over the state-of-the-art, achieving a\n12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:\nhttps://github.com/harborsarah/TRIDE",
        "url": "http://arxiv.org/abs/2508.08038v1",
        "published_date": "2025-08-11T14:39:41+00:00",
        "updated_date": "2025-08-11T14:39:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huawei Sun",
            "Zixu Wang",
            "Hao Feng",
            "Julius Ott",
            "Lorenzo Servadei",
            "Robert Wille"
        ],
        "tldr": "The paper introduces TRIDE, a radar-camera fusion network enhanced with text features and a weather-aware fusion block for improved depth estimation, showing significant improvements on nuScenes dataset.",
        "tldr_zh": "该论文介绍了TRIDE，一个雷达-相机融合网络，通过文本特征和天气感知融合模块增强，从而改进深度估计，并在nuScenes数据集上显示出显著的改进。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding",
        "summary": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG",
        "url": "http://arxiv.org/abs/2508.07925v1",
        "published_date": "2025-08-11T12:38:46+00:00",
        "updated_date": "2025-08-11T12:38:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jin-Seop Lee",
            "SungJoon Lee",
            "Jaehan Ahn",
            "YunSeok Choi",
            "Jee-Hyong Lee"
        ],
        "tldr": "The paper introduces TAG, a zero-shot video temporal grounding approach that addresses semantic fragmentation and skewed similarity distributions without relying on LLMs, achieving state-of-the-art results on benchmark datasets.",
        "tldr_zh": "该论文介绍了一种名为TAG的零样本视频时序定位方法，该方法解决了语义碎片化和倾斜的相似度分布问题，且不依赖于大型语言模型，并在基准数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild",
        "summary": "Large vision models like the Segment Anything Model (SAM) exhibit significant\nlimitations when applied to downstream tasks in the wild. Consequently,\nreference segmentation, which leverages reference images and their\ncorresponding masks to impart novel knowledge to the model, emerges as a\npromising new direction for adapting vision models. However, existing reference\nsegmentation approaches predominantly rely on meta-learning, which still\nnecessitates an extensive meta-training process and brings massive data and\ncomputational cost. In this study, we propose a novel approach by representing\nthe inherent correspondence between reference-target image pairs as a pseudo\nvideo. This perspective allows the latest version of SAM, known as SAM2, which\nis equipped with interactive video object segmentation (iVOS) capabilities, to\nbe adapted to downstream tasks in a lightweight manner. We term this approach\nCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:\nthe Diffusion-Based Semantic Transition (DBST) module employs a diffusion model\nto construct a semantic transformation sequence, while the Test-Time Geometric\nAlignment (TTGA) module aligns the geometric changes within this sequence\nthrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,\nachieving segmentation performance improvements exceeding 5% over SOTA methods.\nImplementation is provided in the supplementary materials.",
        "url": "http://arxiv.org/abs/2508.07759v1",
        "published_date": "2025-08-11T08:42:49+00:00",
        "updated_date": "2025-08-11T08:42:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Wang",
            "Zekun Li",
            "Jian Zhang",
            "Lei Qi",
            "Yinghuan Shi"
        ],
        "tldr": "The paper proposes CAV-SAM, a novel approach for adapting SAM2 to reference segmentation tasks by representing reference-target image correspondence as a pseudo video and using test-time fine-tuning, achieving over 5% improvement over SOTA methods.",
        "tldr_zh": "该论文提出了一种名为CAV-SAM的新方法，通过将参考图像与目标图像的对应关系表示为伪视频，并采用测试时微调，使SAM2能够适应参考分割任务，性能比现有方法提高了5%以上。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding",
        "summary": "Temporal Video Grounding (TVG) aims to precisely localize video segments\ncorresponding to natural language queries, which is a critical capability for\nlong-form video understanding. Although existing reinforcement learning\napproaches encourage models to generate reasoning chains before predictions,\nthey fail to explicitly constrain the reasoning process to ensure the quality\nof the final temporal predictions. To address this limitation, we propose\nTimestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),\na novel framework that introduces timestamp anchors within the reasoning\nprocess to enforce explicit supervision to the thought content. These anchors\nserve as intermediate verification points. More importantly, we require each\nreasoning step to produce increasingly accurate temporal estimations, thereby\nensuring that the reasoning process contributes meaningfully to the final\nprediction. To address the challenge of low-probability anchor generation in\nmodels (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation\ntraining strategy: (1) initial GRPO training to collect 30K high-quality\nreasoning traces containing multiple timestamp anchors, (2) supervised\nfine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the\nSFT-enhanced model. This three-stage training strategy enables robust anchor\ngeneration while maintaining reasoning quality. Experiments show that our model\nachieves state-of-the-art performance while producing interpretable, verifiable\nreasoning chains with progressively refined temporal estimations.",
        "url": "http://arxiv.org/abs/2508.07683v1",
        "published_date": "2025-08-11T06:59:32+00:00",
        "updated_date": "2025-08-11T06:59:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chaohong Guo",
            "Xun Mo",
            "Yongwei Nie",
            "Xuemiao Xu",
            "Chao Xu",
            "Fei Yu",
            "Chengjiang Long"
        ],
        "tldr": "The paper introduces TAR-TVG, a framework that enhances VLMs for temporal video grounding by using timestamp anchors within the reasoning process and a three-stage training strategy to improve anchor generation and reasoning quality, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了TAR-TVG，一个通过在推理过程中使用时间戳锚点来增强VLM用于时间视频定位的框架，并采用三阶段训练策略来提高锚点生成和推理质量，从而达到最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning",
        "summary": "Visual Robot Manipulation (VRM) aims to enable a robot to follow natural\nlanguage instructions based on robot states and visual observations, and\ntherefore requires costly multi-modal data. To compensate for the deficiency of\nrobot data, existing approaches have employed vision-language pretraining with\nlarge-scale data. However, they either utilize web data that differs from\nrobotic tasks, or train the model in an implicit way (e.g., predicting future\nframes at the pixel level), thus showing limited generalization ability under\ninsufficient robot data. In this paper, we propose to learn from large-scale\nhuman action video datasets in an explicit way (i.e., imitating human actions\nfrom hand keypoints), introducing Visual Robot Manipulation with Analogical\nReasoning (AR-VRM). To acquire action knowledge explicitly from human action\nvideos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,\nenabling the VLM to learn human action knowledge and directly predict human\nhand keypoints. During fine-tuning on robot data, to facilitate the robotic arm\nin imitating the action patterns of human motions, we first retrieve human\naction videos that perform similar manipulation tasks and have similar\nhistorical observations , and then learn the Analogical Reasoning (AR) map\nbetween human hand keypoints and robot components. Taking advantage of focusing\non action keypoints instead of irrelevant visual cues, our method achieves\nleading performance on the CALVIN benchmark {and real-world experiments}. In\nfew-shot scenarios, our AR-VRM outperforms previous methods by large margins ,\nunderscoring the effectiveness of explicitly imitating human actions under data\nscarcity.",
        "url": "http://arxiv.org/abs/2508.07626v1",
        "published_date": "2025-08-11T05:09:58+00:00",
        "updated_date": "2025-08-11T05:09:58+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Dejie Yang",
            "Zijing Zhao",
            "Yang Liu"
        ],
        "tldr": "The paper introduces AR-VRM, a novel approach to visual robot manipulation that leverages human action video datasets and analogical reasoning to improve performance, especially in data-scarce scenarios, by explicitly imitating human actions from hand keypoints.",
        "tldr_zh": "该论文介绍了AR-VRM，一种新颖的视觉机器人操作方法，利用人类动作视频数据集和类比推理来提高性能，尤其是在数据稀缺的情况下，通过显式模仿人类手部关键点的动作。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning",
        "summary": "Existing open-source datasets for arbitrary-instruction image editing remain\nsuboptimal, while a plug-and-play editing module compatible with\ncommunity-prevalent generative models is notably absent. In this paper, we\nfirst introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse\nediting tasks, including subject-driven generation. We utilize the\nindustry-leading unified image generation models and expert models to construct\nthe data. Meanwhile, we design reasonable editing instructions with the VLM and\nimplement various scoring mechanisms to filter the data. As a result, we\nconstruct 3.7 million high-quality data with balanced categories. Second, to\nbetter integrate seamlessly with community image generation models, we design\ntask-aware MoE-LoRA training based on FLUX.1, with only 8\\% of the parameters\nof the full model. To further improve the final performance, we utilize the\ninternal representations of the diffusion model and define positive/negative\nsamples based on image editing types to introduce contrastive learning.\nExtensive experiments demonstrate that the model's editing performance is\ncompetitive among many excellent models. Additionally, the constructed dataset\nexhibits substantial advantages over existing open-source datasets. The\nopen-source code, checkpoints, and datasets for X2Edit can be found at the\nfollowing link: https://github.com/OPPO-Mente-Lab/X2Edit.",
        "url": "http://arxiv.org/abs/2508.07607v1",
        "published_date": "2025-08-11T04:22:49+00:00",
        "updated_date": "2025-08-11T04:22:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Ma",
            "Xujie Zhu",
            "Zihao Pan",
            "Qirong Peng",
            "Xu Guo",
            "Chen Chen",
            "Haonan Lu"
        ],
        "tldr": "This paper introduces X2Edit, a large-scale dataset for arbitrary-instruction image editing, and a task-aware MoE-LoRA training method integrated with contrastive learning to improve editing performance with community image generation models.",
        "tldr_zh": "本文介绍了X2Edit，一个用于任意指令图像编辑的大规模数据集，以及一种任务感知的MoE-LoRA训练方法，该方法与对比学习相结合，以提高社区图像生成模型在图像编辑方面的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing",
        "summary": "Transformer-based diffusion models have recently superseded traditional U-Net\narchitectures, with multimodal diffusion transformers (MM-DiT) emerging as the\ndominant approach in state-of-the-art models like Stable Diffusion 3 and\nFlux.1. Previous approaches have relied on unidirectional cross-attention\nmechanisms, with information flowing from text embeddings to image latents. In\ncontrast, MMDiT introduces a unified attention mechanism that concatenates\ninput projections from both modalities and performs a single full attention\noperation, allowing bidirectional information flow between text and image\nbranches. This architectural shift presents significant challenges for existing\nediting techniques. In this paper, we systematically analyze MM-DiT's attention\nmechanism by decomposing attention matrices into four distinct blocks,\nrevealing their inherent characteristics. Through these analyses, we propose a\nrobust, prompt-based image editing method for MM-DiT that supports global to\nlocal edits across various MM-DiT variants, including few-step models. We\nbelieve our findings bridge the gap between existing U-Net-based methods and\nemerging architectures, offering deeper insights into MMDiT's behavioral\npatterns.",
        "url": "http://arxiv.org/abs/2508.07519v1",
        "published_date": "2025-08-11T00:40:12+00:00",
        "updated_date": "2025-08-11T00:40:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Joonghyuk Shin",
            "Alchan Hwang",
            "Yujin Kim",
            "Daneul Kim",
            "Jaesik Park"
        ],
        "tldr": "This paper analyzes the attention mechanism in Multimodal Diffusion Transformers (MM-DiT) and proposes a prompt-based image editing method applicable to various MM-DiT models, bridging the gap between U-Net and transformer-based editing techniques.",
        "tldr_zh": "本文分析了多模态扩散Transformer (MM-DiT) 中的注意力机制，并提出了一种基于提示的图像编辑方法，该方法适用于各种MM-DiT模型，弥合了基于U-Net和基于Transformer的编辑技术之间的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding",
        "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval.",
        "url": "http://arxiv.org/abs/2508.07493v1",
        "published_date": "2025-08-10T21:44:43+00:00",
        "updated_date": "2025-08-10T21:44:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Chen",
            "Ming Li",
            "Jihyung Kil",
            "Chenguang Wang",
            "Tong Yu",
            "Ryan Rossi",
            "Tianyi Zhou",
            "Changyou Chen",
            "Ruiyi Zhang"
        ],
        "tldr": "The paper introduces VisR-Bench, a new multilingual benchmark for visual retrieval-augmented generation in long documents, highlighting challenges for MLLMs in structured tables and low-resource languages.",
        "tldr_zh": "该论文介绍了VisR-Bench，这是一个新的多语言基准，用于长文档中的视觉检索增强生成，突出了MLLM在结构化表格和低资源语言方面的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models",
        "summary": "Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released.",
        "url": "http://arxiv.org/abs/2508.07996v1",
        "published_date": "2025-08-11T13:59:22+00:00",
        "updated_date": "2025-08-11T13:59:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thinesh Thiyakesan Ponbagavathi",
            "Chengzheng Yang",
            "Alina Roitberg"
        ],
        "tldr": "The paper introduces ProGraD, a prompt-driven method to enhance Vision Foundation Models (VFMs) for Group Activity Detection (GAD) by incorporating learnable group prompts and a GroupContext Transformer, achieving state-of-the-art results on GAD benchmarks with improved interpretability.",
        "tldr_zh": "本文介绍了一种名为ProGraD的提示驱动方法，通过结合可学习的群体提示和一个GroupContext Transformer来增强视觉基础模型（VFMs）在群体活动检测（GAD）方面的性能，并在GAD基准测试中取得了最先进的结果，并提高了可解释性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models",
        "summary": "No-reference image quality assessment (NR-IQA) aims to simulate the process\nof perceiving image quality aligned with subjective human perception. However,\nexisting NR-IQA methods either focus on global representations that leads to\nlimited insights into the semantically salient regions or employ a uniform\nweighting for region features that weakens the sensitivity to local quality\nvariations. In this paper, we propose a fine-grained image quality assessment\nmodel, named RSFIQA, which integrates region-level distortion information to\nperceive multi-dimensional quality discrepancies. To enhance regional quality\nawareness, we first utilize the Segment Anything Model (SAM) to dynamically\npartition the input image into non-overlapping semantic regions. For each\nregion, we teach a powerful Multi-modal Large Language Model (MLLM) to extract\ndescriptive content and perceive multi-dimensional distortions, enabling a\ncomprehensive understanding of both local semantics and quality degradations.\nTo effectively leverage this information, we introduce Region-Aware Semantic\nAttention (RSA) mechanism, which generates a global attention map by\naggregating fine-grained representations from local regions. In addition,\nRSFIQA is backbone-agnostic and can be seamlessly integrated into various deep\nneural network architectures. Extensive experiments demonstrate the robustness\nand effectiveness of the proposed method, which achieves competitive quality\nprediction performance across multiple benchmark datasets.",
        "url": "http://arxiv.org/abs/2508.07818v1",
        "published_date": "2025-08-11T10:03:00+00:00",
        "updated_date": "2025-08-11T10:03:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenyue Song",
            "Chen Hui",
            "Haiqi Zhu",
            "Feng Jiang",
            "Yachun Mi",
            "Wei Zhang",
            "Shaohui Liu"
        ],
        "tldr": "The paper introduces RSFIQA, a no-reference image quality assessment model leveraging SAM and MLLMs with a Region-Aware Semantic Attention mechanism for fine-grained quality perception, achieving competitive performance on benchmark datasets.",
        "tldr_zh": "本文介绍了一种名为RSFIQA的无参考图像质量评估模型，该模型利用SAM和MLLM，并结合区域感知语义注意力机制进行细粒度质量感知，在基准数据集上实现了具有竞争力的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]