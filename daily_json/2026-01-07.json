[
    {
        "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
        "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
        "url": "http://arxiv.org/abs/2601.03193v1",
        "published_date": "2026-01-06T17:15:50+00:00",
        "updated_date": "2026-01-06T17:15:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruiyan Han",
            "Zhen Fang",
            "XinYu Sun",
            "Yuchen Ma",
            "Ziheng Wang",
            "Yu Zeng",
            "Zehui Chen",
            "Lin Chen",
            "Wenxuan Huang",
            "Wei-Jie Xu",
            "Yi Cao",
            "Feng Zhao"
        ],
        "tldr": "The paper introduces UniCorn, a self-improvement framework for Unified Multimodal Models (UMMs) that enhances generative capabilities via self-play and cognitive pattern reconstruction, achieving state-of-the-art results on several image generation benchmarks.",
        "tldr_zh": "该论文介绍了UniCorn，一种统一多模态模型（UMM）的自我改进框架，通过自我博弈和认知模式重构来增强生成能力，并在多个图像生成基准测试中实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors",
        "summary": "Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.",
        "url": "http://arxiv.org/abs/2601.02908v1",
        "published_date": "2026-01-06T10:45:53+00:00",
        "updated_date": "2026-01-06T10:45:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Wei-Yuan Cheng",
            "Kai-Po Chang",
            "Chi-Pin Huang",
            "Fu-En Yang",
            "Yu-Chiang Frank Wang"
        ],
        "tldr": "The paper introduces TA-Prompting, a method that enhances VideoLLMs for dense video captioning using temporal anchors to precisely localize events and an event coherent sampling strategy for caption selection, achieving superior performance on benchmark datasets.",
        "tldr_zh": "该论文介绍了TA-Prompting，一种通过使用时间锚点精确定位事件并使用事件连贯抽样策略进行字幕选择来增强VideoLLM的方法，从而在基准数据集上实现了卓越的性能，用于密集视频字幕。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs",
        "summary": "Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to \"imagine\" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.",
        "url": "http://arxiv.org/abs/2601.02771v1",
        "published_date": "2026-01-06T07:05:35+00:00",
        "updated_date": "2026-01-06T07:05:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boyu Chang",
            "Qi Wang",
            "Xi Guo",
            "Zhixiong Nan",
            "Yazhou Yao",
            "Tianfei Zhou"
        ],
        "tldr": "The paper introduces AbductiveMLLM, a novel framework that enhances visual abductive reasoning in MLLMs by mimicking human cognition through a REASONER (verbal domain) and an IMAGINER (pictorial domain), achieving state-of-the-art performance on VAR benchmarks.",
        "tldr_zh": "该论文介绍了一种名为AbductiveMLLM的新框架，通过模仿人类认知，利用推理器（语言域）和想象器（图像域）增强多模态大语言模型中的视觉溯因推理能力，并在视觉溯因推理基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation",
        "summary": "Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix",
        "url": "http://arxiv.org/abs/2601.03191v1",
        "published_date": "2026-01-06T17:13:23+00:00",
        "updated_date": "2026-01-06T17:13:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Anees Ur Rehman Hashmi",
            "Numan Saeed",
            "Christoph Lippert"
        ],
        "tldr": "AnatomiX is a new multimodal LLM designed for anatomically grounded chest X-ray interpretation, achieving significant improvements in anatomy and phrase grounding compared to existing approaches.",
        "tldr_zh": "AnatomiX是一种新型多模态LLM，专为解剖学胸部X光片解释而设计，与现有方法相比，在解剖学和短语基础方面取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs",
        "summary": "Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise \"experts\" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.",
        "url": "http://arxiv.org/abs/2601.03100v1",
        "published_date": "2026-01-06T15:31:19+00:00",
        "updated_date": "2026-01-06T15:31:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chenchen Lin",
            "Sanbao Su",
            "Rachel Luo",
            "Yuxiao Chen",
            "Yan Wang",
            "Marco Pavone",
            "Fei Miao"
        ],
        "tldr": "This paper introduces TGIF, a text-guided inter-layer fusion module for multimodal LLMs that dynamically fuses vision encoder layers based on the text prompt, mitigating hallucination and improving performance across several benchmarks.",
        "tldr_zh": "该论文介绍了TGIF，一个用于多模态LLM的文本引导层间融合模块，它根据文本提示动态融合视觉编码器层，从而减轻幻觉并提高在多个基准测试中的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios",
        "summary": "Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.",
        "url": "http://arxiv.org/abs/2601.03011v1",
        "published_date": "2026-01-06T13:36:43+00:00",
        "updated_date": "2026-01-06T13:36:43+00:00",
        "categories": [
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Yihan Wei",
            "Shenghai Yuan",
            "Tianchen Deng",
            "Boyang Lou",
            "Enwen Hu"
        ],
        "tldr": "The paper presents ReCCur, a low-compute framework for curating corner-case vision-language data at scale using a multi-agent recursive pipeline with minimal human supervision, improving purity and separability.",
        "tldr_zh": "该论文提出了ReCCur，一种低计算框架，用于大规模管理角落案例的视觉语言数据。该框架使用多智能体递归管道，只需最少的人工监督，从而提高纯度和可分离性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning",
        "summary": "Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.",
        "url": "http://arxiv.org/abs/2601.02918v1",
        "published_date": "2026-01-06T11:00:17+00:00",
        "updated_date": "2026-01-06T11:00:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guoqiang Liang",
            "Jianyi Wang",
            "Zhonghua Wu",
            "Shangchen Zhou"
        ],
        "tldr": "The paper introduces Zoom-IQA, a VLM-based Image Quality Assessment model with region-aware reasoning, trained using a two-stage pipeline involving supervised fine-tuning and reinforcement learning to improve robustness, explainability, and generalization.",
        "tldr_zh": "该论文介绍了 Zoom-IQA，一种基于 VLM 的图像质量评估模型，具有区域感知推理能力，采用两阶段训练流程，包括监督微调和强化学习，以提高鲁棒性、可解释性和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models",
        "summary": "Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.",
        "url": "http://arxiv.org/abs/2601.02825v1",
        "published_date": "2026-01-06T08:55:23+00:00",
        "updated_date": "2026-01-06T08:55:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruiyang Zhang",
            "Dongzhan Zhou",
            "Zhedong Zheng"
        ],
        "tldr": "The paper introduces SketchThinker-R1, a method for training large multimodal models to perform sketch-style reasoning, reducing computational cost without sacrificing accuracy by focusing on salient information.",
        "tldr_zh": "本文介绍了SketchThinker-R1，一种训练大型多模态模型执行草图式推理的方法，通过关注显著信息，在不牺牲准确性的前提下降低计算成本。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench",
        "summary": "While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.",
        "url": "http://arxiv.org/abs/2601.02737v1",
        "published_date": "2026-01-06T05:58:50+00:00",
        "updated_date": "2026-01-06T05:58:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zanting Ye",
            "Xiaolong Niu",
            "Xuanbin Wu",
            "Xu Han",
            "Shengyuan Liu",
            "Jing Hao",
            "Zhihao Peng",
            "Hao Sun",
            "Jieqin Lv",
            "Fanghu Wang",
            "Yanchao Huang",
            "Hubing Wu",
            "Yixuan Yuan",
            "Habib Zaidi",
            "Arman Rahmim",
            "Yefeng Zheng",
            "Lijun Lu"
        ],
        "tldr": "This paper introduces PET-Bench, a large-scale functional imaging benchmark for evaluating MLLMs, and proposes Atomic Visual Alignment (AVA) to address the 'CoT hallucination trap' in diagnostic reasoning, demonstrating improved accuracy in PET imaging analysis.",
        "tldr_zh": "本文介绍了PET-Bench，一个大规模功能成像基准，用于评估MLLM，并提出了原子视觉对齐(AVA)来解决诊断推理中的“CoT幻觉陷阱”，从而提高了PET成像分析的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
        "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
        "url": "http://arxiv.org/abs/2601.02358v1",
        "published_date": "2026-01-05T18:56:34+00:00",
        "updated_date": "2026-01-05T18:56:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyi Chen",
            "Tong He",
            "Zhoujie Fu",
            "Pengfei Wan",
            "Kun Gai",
            "Weicai Ye"
        ],
        "tldr": "VINO is a unified visual generator using a shared diffusion backbone conditioned on text, images, and videos for various generation and editing tasks, showcasing strong visual quality and controllable multi-identity edits.",
        "tldr_zh": "VINO是一个统一的视觉生成器，它使用共享的扩散骨干网络，以文本、图像和视频为条件，用于各种生成和编辑任务，展现出强大的视觉质量和可控的多身份编辑。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Faithful Reasoning in Comics for Small MLLMs",
        "summary": "Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\\mathbf{12.1\\%}$ across different MLLMs.",
        "url": "http://arxiv.org/abs/2601.02991v1",
        "published_date": "2026-01-06T13:00:21+00:00",
        "updated_date": "2026-01-06T13:00:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chengcheng Feng",
            "Haojie Yin",
            "Yucheng Jin",
            "Kaizhu Huang"
        ],
        "tldr": "This paper introduces a novel comic reasoning framework using modular CoT generation, GRPO-based reinforcement fine-tuning, and a structured reward to improve the reasoning capabilities of small MLLMs on CVQA and other abstract visual reasoning tasks, outperforming SOTA methods.",
        "tldr_zh": "该论文介绍了一种新的漫画推理框架，该框架使用模块化CoT生成、基于GRPO的强化微调和结构化奖励，以提高小型MLLM在CVQA和其他抽象视觉推理任务上的推理能力，优于SOTA方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding",
        "summary": "Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.",
        "url": "http://arxiv.org/abs/2601.02927v1",
        "published_date": "2026-01-06T11:11:06+00:00",
        "updated_date": "2026-01-06T11:11:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Iñaki Erregue",
            "Kamal Nasrollahi",
            "Sergio Escalera"
        ],
        "tldr": "PrismVAU is a lightweight system for real-time video anomaly understanding that uses prompt optimization and a single off-the-shelf MLLM, achieving competitive performance without fine-tuning or external modules.",
        "tldr_zh": "PrismVAU是一个轻量级的实时视频异常理解系统，它使用提示优化和单个现成的MLLM，在没有微调或外部模块的情况下实现了竞争性的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework",
        "summary": "Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.",
        "url": "http://arxiv.org/abs/2601.02783v1",
        "published_date": "2026-01-06T07:41:44+00:00",
        "updated_date": "2026-01-06T07:41:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junjue Wang",
            "Yanfei Zhong",
            "Zihang Chen",
            "Zhuo Zheng",
            "Ailong Ma",
            "Liangpei Zhang"
        ],
        "tldr": "The paper introduces EarthVL, a framework with a dataset and network for Earth vision-language understanding and generation, focusing on city planning applications and using a progressive, object-centric approach.",
        "tldr_zh": "该论文介绍了EarthVL，一个用于地球视觉-语言理解和生成的框架，包含数据集和网络，重点关注城市规划应用，并采用渐进式的、以对象为中心的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding",
        "summary": "Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/",
        "url": "http://arxiv.org/abs/2601.02457v1",
        "published_date": "2026-01-05T18:55:45+00:00",
        "updated_date": "2026-01-05T18:55:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Souhail Hadgi",
            "Bingchen Gong",
            "Ramana Sundararaman",
            "Emery Pierson",
            "Lei Li",
            "Peter Wonka",
            "Maks Ovsjanikov"
        ],
        "tldr": "The paper introduces PatchAlign3D, a 3D encoder that generates language-aligned patch-level features from point clouds for improved part-level reasoning, outperforming existing methods in zero-shot 3D part segmentation without multi-view rendering.",
        "tldr_zh": "该论文介绍了PatchAlign3D，一个从点云生成语言对齐的patch级别特征的3D编码器，以改善局部部件级别的推理，在零样本3D部件分割中优于现有方法，且无需多视图渲染。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]