[
    {
        "title": "Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis",
        "summary": "Marine mammal vocalization analysis depends on interpreting bioacoustic\nspectrograms. Vision Language Models (VLMs) are not trained on these\ndomain-specific visualizations. We investigate whether VLMs can extract\nmeaningful patterns from spectrograms visually. Our framework integrates VLM\ninterpretation with LLM-based validation to build domain knowledge. This\nenables adaptation to acoustic data without manual annotation or model\nretraining.",
        "url": "http://arxiv.org/abs/2509.05703v1",
        "published_date": "2025-09-06T12:36:59+00:00",
        "updated_date": "2025-09-06T12:36:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR"
        ],
        "authors": [
            "Ragib Amin Nihal",
            "Benjamin Yen",
            "Takeshi Ashizawa",
            "Kazuhiro Nakadai"
        ],
        "tldr": "This paper explores using Vision Language Models augmented with domain knowledge for analyzing underwater bioacoustic spectrograms, enabling adaptation to acoustic data without manual annotation or model retraining.",
        "tldr_zh": "本文探讨了使用领域知识增强的视觉语言模型来分析水下生物声学频谱图，从而无需手动注释或模型重新训练即可适应声学数据。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization",
        "summary": "Human action recognition often struggles with deep semantic understanding,\ncomplex contextual information, and fine-grained distinction, limitations that\ntraditional methods frequently encounter when dealing with diverse video data.\nInspired by the remarkable capabilities of large language models, this paper\nintroduces LVLM-VAR, a novel framework that pioneers the application of\npre-trained Vision-Language Large Models (LVLMs) to video action recognition,\nemphasizing enhanced accuracy and interpretability. Our method features a\nVideo-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video\nsequences into discrete, semantically and temporally consistent \"semantic\naction tokens,\" effectively crafting an \"action narrative\" that is\ncomprehensible to an LVLM. These tokens, combined with natural language\ninstructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)\nfor robust action classification and semantic reasoning. LVLM-VAR not only\nachieves state-of-the-art or highly competitive performance on challenging\nbenchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant\nimprovements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),\nbut also substantially boosts model interpretability by generating natural\nlanguage explanations for its predictions.",
        "url": "http://arxiv.org/abs/2509.05695v1",
        "published_date": "2025-09-06T12:11:43+00:00",
        "updated_date": "2025-09-06T12:11:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingwei Peng",
            "Zhixuan Qiu",
            "Boyu Jin",
            "Surasakdi Siripong"
        ],
        "tldr": "The paper introduces LVLM-VAR, a novel framework leveraging Vision-Language Large Models for video action recognition by converting videos into semantic tokens and using a fine-tuned LVLM for classification and explanation, achieving state-of-the-art results and improved interpretability.",
        "tldr_zh": "该论文介绍了LVLM-VAR，一种利用视觉-语言大模型进行视频动作识别的新框架，该框架通过将视频转换为语义tokens，并使用微调后的LVLM进行分类和解释，从而实现最先进的结果并提高可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance",
        "summary": "Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)\nexcel in single-turn tasks but face significant challenges in multi-turn\ninteractions requiring deep contextual understanding and complex visual\nreasoning, often leading to fragmented reasoning, context loss, and\nhallucinations. To address these limitations, we propose Context-Aware\nMulti-Turn Visual Reasoning (CAMVR), a novel framework designed to empower\nLVLMs with robust and coherent multi-turn visual-textual inference\ncapabilities. CAMVR introduces two key innovations: a Visual-Textual Context\nMemory Unit (VCMU), a dynamic read-write memory network that stores and manages\ncritical visual features, textual semantic representations, and their\ncross-modal correspondences from each interaction turn; and an Adaptive Visual\nFocus Guidance (AVFG) mechanism, which leverages the VCMU's context to\ndynamically adjust the visual encoder's attention to contextually relevant\nimage regions. Our multi-level reasoning integration strategy ensures that\nresponse generation is deeply coherent with both current inputs and accumulated\nhistorical context. Extensive experiments on challenging datasets, including\nVisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following\n(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art\nperformance.",
        "url": "http://arxiv.org/abs/2509.05669v1",
        "published_date": "2025-09-06T10:14:49+00:00",
        "updated_date": "2025-09-06T10:14:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijie Shen",
            "Xinrui Wang",
            "Yuanqi Nie",
            "Apiradee Boonmee"
        ],
        "tldr": "The paper introduces CAMVR, a framework to improve multi-turn visual-textual reasoning in LVLMs using a dynamic memory unit and adaptive visual guidance, achieving state-of-the-art results on several datasets.",
        "tldr_zh": "该论文提出了 CAMVR 框架，通过动态记忆单元和自适应视觉引导来提升 LVLM 中的多轮视觉-文本推理能力，并在多个数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters",
        "summary": "Deciphering oracle bone characters (OBCs), the oldest attested form of\nwritten Chinese, has remained the ultimate, unwavering goal of scholars,\noffering an irreplaceable key to understanding humanity's early modes of\nproduction. Current decipherment methodologies of OBC are primarily constrained\nby the sporadic nature of archaeological excavations and the limited corpus of\ninscriptions. With the powerful visual perception capability of large\nmultimodal models (LMMs), the potential of using LMMs for visually deciphering\nOBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed\nto evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It\nincludes 20k meticulously collected OBC and real object images, forming over\n15k multi-choice questions. We also conduct subjective annotations to\ninvestigate the consistency of the reference point between humans and LMMs in\nvisual reasoning. Experiments indicate that general LMMs possess preliminary\nvisual decipherment skills, and LMMs are not effectively using visual\ninformation, while most of the time they are limited by language priors. We\nhope that our dataset can facilitate the evaluation and optimization of visual\nattention in future OBC-oriented LMMs. The code and dataset will be available\nat https://github.com/OBI-Future/PictOBI-20k.",
        "url": "http://arxiv.org/abs/2509.05773v1",
        "published_date": "2025-09-06T16:55:52+00:00",
        "updated_date": "2025-09-06T16:55:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijian Chen",
            "Wenjie Hua",
            "Jinhao Li",
            "Lirong Deng",
            "Fan Du",
            "Tingzhu Chen",
            "Guangtao Zhai"
        ],
        "tldr": "The paper introduces PictOBI-20k, a dataset for evaluating Large Multimodal Models (LMMs) on visual decipherment tasks of pictographic oracle bone characters (OBCs). Experiments reveal LMMs possess preliminary skills but are limited by language priors.",
        "tldr_zh": "该论文介绍了 PictOBI-20k，一个用于评估大型多模态模型 (LMM) 在象形甲骨文字符视觉解读任务上的数据集。实验表明 LMM 具有初步技能，但受到语言先验的限制。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning",
        "summary": "Pruning accelerates compute-bound models by reducing computation. Recently\napplied to Vision-Language-Action (VLA) models, existing methods prune tokens\nusing only local info from current action, ignoring global context from prior\nactions, causing >20% success rate drop and limited speedup. We observe high\nsimilarity across consecutive actions and propose leveraging both local\n(current) and global (past) info for smarter token selection. We introduce\nSpecPrune-VLA, a training-free method with two-level pruning and heuristic\ncontrol: (1) Static pruning at action level: uses global history and local\ncontext to reduce visual tokens per action; (2) Dynamic pruning at layer level:\nprunes tokens per layer based on layer-specific importance; (3) Lightweight\naction-aware controller: classifies actions as coarse/fine-grained (by speed),\nadjusting pruning aggressiveness since fine-grained actions are\npruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times\nspeedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.\nOpenVLA-OFT, with negligible success rate loss.",
        "url": "http://arxiv.org/abs/2509.05614v1",
        "published_date": "2025-09-06T06:22:19+00:00",
        "updated_date": "2025-09-06T06:22:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Hanzhen Wang",
            "Jiaming Xu",
            "Jiayi Pan",
            "Yongkang Zhou",
            "Guohao Dai"
        ],
        "tldr": "This paper introduces SpecPrune-VLA, a training-free pruning method for Vision-Language-Action models that leverages both local and global context to achieve significant speedups with minimal performance loss. It uses static action-level pruning, dynamic layer-level pruning, and an action-aware controller.",
        "tldr_zh": "本文介绍了一种名为SpecPrune-VLA的免训练剪枝方法，用于视觉-语言-动作模型，该方法利用局部和全局上下文信息，在性能损失最小的情况下实现显著的加速效果。它采用了静态动作级剪枝、动态层级剪枝和一个动作感知的控制器。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization",
        "summary": "Video summarization aims to select keyframes that are visually diverse and\ncan represent the whole story of a given video. Previous approaches have\nfocused on global interlinkability between frames in a video by temporal\nmodeling. However, fine-grained visual entities, such as objects, are also\nhighly related to the main content of the video. Moreover, language-guided\nvideo summarization, which has recently been studied, requires a comprehensive\nlinguistic understanding of complex real-world videos. To consider how all the\nobjects are semantically related to each other, this paper regards video\nsummarization as a language-guided spatiotemporal graph modeling problem. We\npresent recursive spatiotemporal graph networks, called VideoGraph, which\nformulate the objects and frames as nodes of the spatial and temporal graphs,\nrespectively. The nodes in each graph are connected and aggregated with graph\nedges, representing the semantic relationships between the nodes. To prevent\nthe edges from being configured with visual similarity, we incorporate language\nqueries derived from the video into the graph node representations, enabling\nthem to contain semantic knowledge. In addition, we adopt a recursive strategy\nto refine initial graphs and correctly classify each frame node as a keyframe.\nIn our experiments, VideoGraph achieves state-of-the-art performance on several\nbenchmarks for generic and query-focused video summarization in both supervised\nand unsupervised manners. The code is available at\nhttps://github.com/park-jungin/videograph.",
        "url": "http://arxiv.org/abs/2509.05604v1",
        "published_date": "2025-09-06T05:37:31+00:00",
        "updated_date": "2025-09-06T05:37:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jungin Park",
            "Jiyoung Lee",
            "Kwanghoon Sohn"
        ],
        "tldr": "This paper introduces VideoGraph, a language-guided recursive spatiotemporal graph network for video summarization, achieving state-of-the-art performance by incorporating language queries into graph nodes to represent semantic relationships between objects and frames.",
        "tldr_zh": "本文介绍了一种名为 VideoGraph 的语言引导的递归时空图网络，用于视频摘要。通过将语言查询融入图节点，表示对象和帧之间的语义关系，从而实现最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting",
        "summary": "Recently, distilling open-vocabulary language features from 2D images into 3D\nGaussians has attracted significant attention. Although existing methods\nachieve impressive language-based interactions of 3D scenes, we observe two\nfundamental issues: background Gaussians contributing negligibly to a rendered\npixel get the same feature as the dominant foreground ones, and multi-view\ninconsistencies due to view-specific noise in language embeddings. We introduce\nVisibility-Aware Language Aggregation (VALA), a lightweight yet effective\nmethod that computes marginal contributions for each ray and applies a\nvisibility-aware gate to retain only visible Gaussians. Moreover, we propose a\nstreaming weighted geometric median in cosine space to merge noisy multi-view\nfeatures. Our method yields a robust, view-consistent language feature\nembedding in a fast and memory-efficient manner. VALA improves open-vocabulary\nlocalization and segmentation across reference datasets, consistently\nsurpassing existing works.",
        "url": "http://arxiv.org/abs/2509.05515v1",
        "published_date": "2025-09-05T21:56:11+00:00",
        "updated_date": "2025-09-05T21:56:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sen Wang",
            "Kunyi Li",
            "Siyun Liang",
            "Elena Alegret",
            "Jing Ma",
            "Nassir Navab",
            "Stefano Gasperini"
        ],
        "tldr": "The paper introduces Visibility-Aware Language Aggregation (VALA) to improve open-vocabulary segmentation in 3D Gaussian Splatting by addressing issues of background noise and multi-view inconsistencies in language embeddings.",
        "tldr_zh": "该论文介绍了可见性感知语言聚合（VALA），通过解决语言嵌入中的背景噪声和多视角不一致性问题，从而改进3D高斯溅射中的开放词汇分割。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]