[
    {
        "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer",
        "summary": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.",
        "url": "http://arxiv.org/abs/2509.16197v1",
        "published_date": "2025-09-19T17:58:00+00:00",
        "updated_date": "2025-09-19T17:58:00+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Yanghao Li",
            "Rui Qian",
            "Bowen Pan",
            "Haotian Zhang",
            "Haoshuo Huang",
            "Bowen Zhang",
            "Jialing Tong",
            "Haoxuan You",
            "Xianzhi Du",
            "Zhe Gan",
            "Hyunjik Kim",
            "Chao Jia",
            "Zhenbang Wang",
            "Yinfei Yang",
            "Mingfei Gao",
            "Zi-Yi Dou",
            "Wenze Hu",
            "Chang Gao",
            "Dongxu Li",
            "Philipp Dufter",
            "Zirui Wang",
            "Guoli Yin",
            "Zhengdong Zhang",
            "Chen Chen",
            "Yang Zhao",
            "Ruoming Pang",
            "Zhifeng Chen"
        ],
        "tldr": "The paper introduces Manzano, a unified multimodal LLM framework that balances image understanding and generation using a hybrid image tokenizer and unified training recipe, achieving state-of-the-art results among unified models.",
        "tldr_zh": "该论文介绍了Manzano，一个统一的多模态LLM框架，它通过使用混合图像分词器和统一的训练方法来平衡图像理解和生成，并在统一模型中实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
        "summary": "Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs.",
        "url": "http://arxiv.org/abs/2509.16163v1",
        "published_date": "2025-09-19T17:16:32+00:00",
        "updated_date": "2025-09-19T17:16:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Het Patel",
            "Muzammil Allie",
            "Qian Zhang",
            "Jia Chen",
            "Evangelos E. Papalexakis"
        ],
        "tldr": "This paper introduces a lightweight, plug-and-play defense against adversarial attacks on VLMs using tensor decomposition, showing improved robustness on COCO and Flickr30K without retraining.",
        "tldr_zh": "该论文提出了一种轻量级的即插即用方法，通过张量分解防御视觉语言模型上的对抗攻击，在COCO和Flickr30K数据集上表现出更高的鲁棒性，且无需重新训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model",
        "summary": "We introduce SEE&TREK, the first training-free prompting framework tailored\nto enhance the spatial understanding of Multimodal Large Language Models\n(MLLMS) under vision-only constraints. While prior efforts have incorporated\nmodalities like depth or point clouds to improve spatial reasoning, purely\nvisualspatial understanding remains underexplored. SEE&TREK addresses this gap\nby focusing on two core principles: increasing visual diversity and motion\nreconstruction. For visual diversity, we conduct Maximum Semantic Richness\nSampling, which employs an off-the-shell perception model to extract\nsemantically rich keyframes that capture scene structure. For motion\nreconstruction, we simulate visual trajectories and encode relative spatial\npositions into keyframes to preserve both spatial relations and temporal\ncoherence. Our method is training&GPU-free, requiring only a single forward\npass, and can be seamlessly integrated into existing MLLM'S. Extensive\nexperiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently\nboosts various MLLM S performance across diverse spatial reasoning tasks with\nthe most +3.5% improvement, offering a promising path toward stronger spatial\nintelligence.",
        "url": "http://arxiv.org/abs/2509.16087v1",
        "published_date": "2025-09-19T15:30:26+00:00",
        "updated_date": "2025-09-19T15:30:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pengteng Li",
            "Pinhao Song",
            "Wuyang Li",
            "Weiyu Guo",
            "Huizai Yao",
            "Yijie Xu",
            "Dugang Liu",
            "Hui Xiong"
        ],
        "tldr": "The paper introduces See&Trek, a training-free prompting framework that enhances spatial understanding in MLLMs using visual diversity and motion reconstruction, achieving performance improvements on spatial reasoning tasks.",
        "tldr_zh": "该论文介绍了See&Trek，一种无需训练的提示框架，通过视觉多样性和运动重建增强MLLM中的空间理解，并在空间推理任务上实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model",
        "summary": "Group activity detection (GAD) aims to simultaneously identify group members\nand categorize their collective activities within video sequences. Existing\ndeep learning-based methods develop specialized architectures (e.g.,\ntransformer networks) to model the dynamics of individual roles and semantic\ndependencies between individuals and groups. However, they rely solely on\nimplicit pattern recognition from visual features and struggle with contextual\nreasoning and explainability. In this work, we propose LIR-GAD, a novel\nframework of language-instructed reasoning for GAD via Multimodal Large\nLanguage Model (MLLM). Our approach expand the original vocabulary of MLLM by\nintroducing an activity-level <ACT> token and multiple cluster-specific <GROUP>\ntokens. We process video frames alongside two specially designed tokens and\nlanguage instructions, which are then integrated into the MLLM. The pretrained\ncommonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP>\ntokens to effectively capture the semantic information of collective activities\nand learn distinct representational features of different groups, respectively.\nAlso, we introduce a multi-label classification loss to further enhance the\n<ACT> token's ability to learn discriminative semantic representations. Then,\nwe design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates\nMLLM's hidden embeddings corresponding to the designed tokens with visual\nfeatures, significantly enhancing the performance of GAD. Both quantitative and\nqualitative experiments demonstrate the superior performance of our proposed\nmethod in GAD taks.",
        "url": "http://arxiv.org/abs/2509.16054v1",
        "published_date": "2025-09-19T15:05:45+00:00",
        "updated_date": "2025-09-19T15:05:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jihua Peng",
            "Qianxiong Xu",
            "Yichen Liu",
            "Chenxi Liu",
            "Cheng Long",
            "Rui Zhao",
            "Ziyue Li"
        ],
        "tldr": "This paper introduces LIR-GAD, a novel framework for group activity detection using multimodal large language models (MLLMs) with language-instructed reasoning, achieving superior performance by expanding the MLLM vocabulary and introducing a multimodal dual-alignment fusion module.",
        "tldr_zh": "本文介绍了LIR-GAD，一个新颖的群体活动检测框架，它使用多模态大型语言模型（MLLM）和语言指导的推理，通过扩展MLLM词汇表并引入多模态双重对齐融合模块，从而实现了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation",
        "summary": "Score Distillation Sampling (SDS) enables high-quality text-to-3D generation\nby supervising 3D models through the denoising of multi-view 2D renderings,\nusing a pretrained text-to-image diffusion model to align with the input prompt\nand ensure 3D consistency. However, existing SDS-based methods face two\nfundamental limitations: (1) their reliance on CLIP-style text encoders leads\nto coarse semantic alignment and struggles with fine-grained prompts; and (2)\n2D diffusion priors lack explicit 3D spatial constraints, resulting in\ngeometric inconsistencies and inaccurate object relationships in multi-object\nscenes. To address these challenges, we propose VLM3D, a novel text-to-3D\ngeneration framework that integrates large vision-language models (VLMs) into\nthe SDS pipeline as differentiable semantic and spatial priors. Unlike standard\ntext-to-image diffusion priors, VLMs leverage rich language-grounded\nsupervision that enables fine-grained prompt alignment. Moreover, their\ninherent vision language modeling provides strong spatial understanding, which\nsignificantly enhances 3D consistency for single-object generation and improves\nrelational reasoning in multi-object scenes. We instantiate VLM3D based on the\nopen-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.\nExperiments across diverse objects and complex scenes show that VLM3D\nsignificantly outperforms prior SDS-based methods in semantic fidelity,\ngeometric coherence, and spatial correctness.",
        "url": "http://arxiv.org/abs/2509.15772v1",
        "published_date": "2025-09-19T08:54:52+00:00",
        "updated_date": "2025-09-19T08:54:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weimin Bai",
            "Yubo Li",
            "Weijian Luo",
            "Wenzheng Chen",
            "He Sun"
        ],
        "tldr": "This paper introduces VLM3D, a text-to-3D generation framework that uses VLMs to improve semantic fidelity and geometric consistency in 3D models generated from text prompts, outperforming existing Score Distillation Sampling (SDS) methods.",
        "tldr_zh": "本文介绍了一种名为VLM3D的文本到3D生成框架，该框架利用视觉语言模型（VLM）来提高从文本提示生成的3D模型的语义保真度和几何一致性，优于现有的分数蒸馏采样（SDS）方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions",
        "summary": "The whole is greater than the sum of its parts-even in 3D-text contrastive\nlearning. We introduce SceneForge, a novel framework that enhances contrastive\nalignment between 3D point clouds and text through structured multi-object\nscene compositions. SceneForge leverages individual 3D shapes to construct\nmulti-object scenes with explicit spatial relations, pairing them with coherent\nmulti-object descriptions refined by a large language model. By augmenting\ncontrastive training with these structured, compositional samples, SceneForge\neffectively addresses the scarcity of large-scale 3D-text datasets,\nsignificantly enriching data complexity and diversity. We systematically\ninvestigate critical design elements, such as the optimal number of objects per\nscene, the proportion of compositional samples in training batches, and scene\nconstruction strategies. Extensive experiments demonstrate that SceneForge\ndelivers substantial performance gains across multiple tasks, including\nzero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,\nas well as few-shot part segmentation on ShapeNetPart. SceneForge's\ncompositional augmentations are model-agnostic, consistently improving\nperformance across multiple encoder architectures. Moreover, SceneForge\nimproves 3D visual question answering on ScanQA, generalizes robustly to\nretrieval scenarios with increasing scene complexity, and showcases spatial\nreasoning capabilities by adapting spatial configurations to align precisely\nwith textual instructions.",
        "url": "http://arxiv.org/abs/2509.15693v1",
        "published_date": "2025-09-19T07:13:45+00:00",
        "updated_date": "2025-09-19T07:13:45+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Cristian Sbrolli",
            "Matteo Matteucci"
        ],
        "tldr": "SceneForge enhances 3D-text alignment by generating structured multi-object scenes with corresponding text descriptions for contrastive learning, improving performance on various 3D understanding tasks.",
        "tldr_zh": "SceneForge通过生成具有相应文本描述的结构化多对象场景来进行对比学习，从而增强了3D文本对齐，并提高了在各种3D理解任务上的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?",
        "summary": "Multimodal large language models (MLLMs) excel at general video understanding\nbut struggle with fast, high-frequency sports like tennis, where rally clips\nare short yet information-dense. To systematically evaluate MLLMs in this\nchallenging domain, we present TennisTV, the first and most comprehensive\nbenchmark for tennis video understanding. TennisTV models each rally as a\ntemporal-ordered sequence of consecutive stroke events, using automated\npipelines for filtering and question generation. It covers 8 tasks at rally and\nstroke levels and includes 2,500 human-verified questions. Evaluating 16\nrepresentative MLLMs, we provide the first systematic assessment of tennis\nvideo understanding. Results reveal substantial shortcomings and yield two key\ninsights: (i) frame-sampling density should be tailored and balanced across\ntasks, and (ii) improving temporal grounding is essential for stronger\nreasoning.",
        "url": "http://arxiv.org/abs/2509.15602v1",
        "published_date": "2025-09-19T05:08:05+00:00",
        "updated_date": "2025-09-19T05:08:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongyuan Bao",
            "Lejun Zhang"
        ],
        "tldr": "The paper introduces TennisTV, a new benchmark for evaluating Multimodal Large Language Models (MLLMs) on tennis video understanding, highlighting their shortcomings and suggesting improvements in temporal grounding and frame-sampling strategies.",
        "tldr_zh": "该论文介绍了 TennisTV，这是一个新的基准，用于评估多模态大型语言模型 (MLLM) 在网球视频理解方面的能力，强调了它们的不足，并提出了改进时间定位和帧采样策略的建议。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
        "summary": "In the field of AI-driven human-GUI interaction automation, while rapid\nadvances in multimodal large language models and reinforcement fine-tuning\ntechniques have yielded remarkable progress, a fundamental challenge persists:\ntheir interaction logic significantly deviates from natural human-GUI\ncommunication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL),\na brain-inspired framework for human-GUI interaction that mimics the human\ncognitive process between users and graphical interfaces. The system decomposes\ninteractions into three biologically plausible phases: (1) Blink - rapid\ndetection and attention to relevant screen areas, analogous to saccadic eye\nmovements; (2) Think - higher-level reasoning and decision-making, mirroring\ncognitive planning; and (3) Link - generation of executable commands for\nprecise motor control, emulating human action selection mechanisms.\nAdditionally, we introduce two key technical innovations for the BTL framework:\n(1) Blink Data Generation - an automated annotation pipeline specifically\noptimized for blink data, and (2) BTL Reward -- the first rule-based reward\nmechanism that enables reinforcement learning driven by both process and\noutcome. Building upon this framework, we develop a GUI agent model named\nBTL-UI, which demonstrates consistent state-of-the-art performance across both\nstatic GUI understanding and dynamic interaction tasks in comprehensive\nbenchmarks. These results provide conclusive empirical validation of the\nframework's efficacy in developing advanced GUI Agents.",
        "url": "http://arxiv.org/abs/2509.15566v1",
        "published_date": "2025-09-19T04:03:44+00:00",
        "updated_date": "2025-09-19T04:03:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shaojie Zhang",
            "Ruoceng Zhang",
            "Pei Fu",
            "Shaokang Wang",
            "Jiahui Yang",
            "Xin Du",
            "Shiqi Cui",
            "Bin Qin",
            "Ying Huang",
            "Zhenbo Luo",
            "Jian Luan"
        ],
        "tldr": "The paper introduces BTL-UI, a novel GUI agent model based on a brain-inspired \"Blink-Think-Link\" framework, demonstrating state-of-the-art performance in GUI understanding and interaction tasks.",
        "tldr_zh": "该论文介绍了一种名为 BTL-UI 的新型 GUI 代理模型，该模型基于受大脑启发的“Blink-Think-Link”框架，并在 GUI 理解和交互任务中表现出最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models",
        "summary": "Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities\nbut remain vulnerable to hallucinations from intrinsic errors and adversarial\nattacks from external exploitations, limiting their reliability in real-world\napplications. We present ORCA, an agentic reasoning framework that improves the\nfactual accuracy and adversarial robustness of pretrained LVLMs through\ntest-time structured inference reasoning with a suite of small vision models\n(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act\nloop, querying multiple visual tools with evidential questions, validating\ncross-model inconsistencies, and refining predictions iteratively without\naccess to model internals or retraining. ORCA also stores intermediate\nreasoning traces, which supports auditable decision-making. Though designed\nprimarily to mitigate object-level hallucinations, ORCA also exhibits emergent\nadversarial robustness without requiring adversarial training or defense\nmechanisms. We evaluate ORCA across three settings: (1) clean images on\nhallucination benchmarks, (2) adversarially perturbed images without defense,\nand (3) adversarially perturbed images with defense applied. On the POPE\nhallucination benchmark, ORCA improves standalone LVLM performance by +3.64\\%\nto +40.67\\% across different subsets. Under adversarial perturbations on POPE,\nORCA achieves an average accuracy gain of +20.11\\% across LVLMs. When combined\nwith defense techniques on adversarially perturbed AMBER images, ORCA further\nimproves standalone LVLM performance, with gains ranging from +1.20\\% to\n+48.00\\% across evaluation metrics. These results demonstrate that ORCA offers\na promising path toward building more reliable and robust multimodal systems.",
        "url": "http://arxiv.org/abs/2509.15435v1",
        "published_date": "2025-09-18T21:17:23+00:00",
        "updated_date": "2025-09-18T21:17:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Chung-En Johnny Yu",
            "Hsuan-Chih",
            "Chen",
            "Brian Jalaian",
            "Nathaniel D. Bastian"
        ],
        "tldr": "The paper introduces ORCA, an agentic reasoning framework that enhances the factual accuracy and adversarial robustness of LVLMs by iteratively validating cross-model inconsistencies and refining predictions using multiple small vision models.",
        "tldr_zh": "该论文介绍了ORCA，一个代理推理框架，通过迭代验证跨模型不一致性并使用多个小型视觉模型优化预测，从而提高LVLM的事实准确性和对抗鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation",
        "summary": "Point cloud segmentation is central to autonomous driving and 3D scene\nunderstanding. While voxel- and point-based methods dominate recent research\ndue to their compatibility with deep architectures and ability to capture\nfine-grained geometry, they often incur high computational cost, irregular\nmemory access, and limited real-time efficiency. In contrast, range-view\nmethods, though relatively underexplored - can leverage mature 2D semantic\nsegmentation techniques for fast and accurate predictions. Motivated by the\nrapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot\nrecognition, and multimodal tasks, we investigate whether SAM2, the current\nstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone for\nLiDAR point cloud segmentation in the range view. We present , to our\nknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,\ncoupling efficient 2D feature extraction with standard\nprojection/back-projection to operate on point clouds. To optimize SAM2 for\nrange-view representations, we implement several architectural modifications to\nthe encoder: (1) a novel module that emphasizes horizontal spatial dependencies\ninherent in LiDAR range images, (2) a customized configuration of tailored to\nthe geometric properties of spherical projections, and (3) an adapted mechanism\nin the encoder backbone specifically designed to capture the unique spatial\npatterns and discontinuities present in range-view pseudo-images. Our approach\nachieves competitive performance on SemanticKITTI while benefiting from the\nspeed, scalability, and deployment simplicity of 2D-centric pipelines. This\nwork highlights the viability of VFMs as general-purpose backbones for 3D\nperception and opens a path toward unified, foundation-model-driven LiDAR\nsegmentation. Results lets us conclude that range-view segmentation methods\nusing VFMs leads to promising results.",
        "url": "http://arxiv.org/abs/2509.15886v1",
        "published_date": "2025-09-19T11:33:10+00:00",
        "updated_date": "2025-09-19T11:33:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Paul Julius Kühn",
            "Duc Anh Nguyen",
            "Arjan Kuijper",
            "Holger Graf",
            "Dieter Fellner",
            "Saptarshi Neil Sinha"
        ]
    },
    {
        "title": "Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance",
        "summary": "Large Vision-Language Models (LVLMs) have significantly advanced multimodal\nunderstanding but still struggle with efficiently processing high-resolution\nimages. Recent approaches partition high-resolution images into multiple\nsub-images, dramatically increasing the number of visual tokens and causing\nexponential computational overhead during inference. To address these\nlimitations, we propose a training-free token pruning strategy, Pyramid Token\nPruning (PTP), that integrates bottom-up visual saliency at both region and\ntoken levels with top-down instruction-guided importance. Inspired by human\nvisual attention mechanisms, PTP selectively retains more tokens from visually\nsalient regions and further leverages textual instructions to pinpoint tokens\nmost relevant to specific multimodal tasks. Extensive experiments across 13\ndiverse benchmarks demonstrate that our method substantially reduces\ncomputational overhead and inference latency with minimal performance loss.",
        "url": "http://arxiv.org/abs/2509.15704v1",
        "published_date": "2025-09-19T07:28:17+00:00",
        "updated_date": "2025-09-19T07:28:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxuan Liang",
            "Xu Li",
            "Xiaolei Chen",
            "Yi Zheng",
            "Haotian Chen",
            "Bin Li",
            "Xiangyang Xue"
        ]
    },
    {
        "title": "SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters",
        "summary": "Recent advances in vision-language models (VLMs) have enabled powerful\nmultimodal reasoning, but state-of-the-art approaches typically rely on\nextremely large models with prohibitive computational and memory requirements.\nThis makes their deployment challenging in resource-constrained environments\nsuch as warehouses, robotics, and industrial applications, where both\nefficiency and robust spatial understanding are critical. In this work, we\npresent SmolRGPT, a compact vision-language architecture that explicitly\nincorporates region-level spatial reasoning by integrating both RGB and depth\ncues. SmolRGPT employs a three-stage curriculum that progressively align visual\nand language features, enables spatial relationship understanding, and adapts\nto task-specific datasets. We demonstrate that with only 600M parameters,\nSmolRGPT achieves competitive results on challenging warehouse spatial\nreasoning benchmarks, matching or exceeding the performance of much larger\nalternatives. These findings highlight the potential for efficient, deployable\nmultimodal intelligence in real-world settings without sacrificing core spatial\nreasoning capabilities. The code of the experimentation will be available at:\nhttps://github.com/abtraore/SmolRGPT",
        "url": "http://arxiv.org/abs/2509.15490v1",
        "published_date": "2025-09-18T23:55:51+00:00",
        "updated_date": "2025-09-18T23:55:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Abdarahmane Traore",
            "Éric Hervet",
            "Andy Couturier"
        ]
    },
    {
        "title": "CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization",
        "summary": "Recent advances in pre-training vision-language models (VLMs), e.g.,\ncontrastive language-image pre-training (CLIP) methods, have shown great\npotential in learning out-of-distribution (OOD) representations. Despite\nshowing competitive performance, the prompt-based CLIP methods still suffer\nfrom: i) inaccurate text descriptions, which leads to degraded accuracy and\nrobustness, and poses a challenge for zero-shot CLIP methods. ii) limited\nvision-language embedding alignment, which significantly affects the\ngeneralization performance. To tackle the above issues, this paper proposes a\nnovel Conditional Domain prompt Learning (CoDoL) method, which utilizes\nreadily-available domain information to form prompts and improves the\nvision-language embedding alignment for improving OOD generalization. To\ncapture both instance-specific and domain-specific information, we further\npropose a lightweight Domain Meta Network (DMN) to generate input-conditional\ntokens for images in each domain. Extensive experiments on four OOD benchmarks\n(PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed\nCoDoL in terms of improving the vision-language embedding alignment as well as\nthe out-of-distribution generalization performance.",
        "url": "http://arxiv.org/abs/2509.15330v1",
        "published_date": "2025-09-18T18:23:59+00:00",
        "updated_date": "2025-09-18T18:23:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Min Zhang",
            "Bo Jiang",
            "Jie Zhou",
            "Yimeng Liu",
            "Xin Lin"
        ]
    }
]