[
    {
        "title": "Automating construction safety inspections using a multi-modal vision-language RAG framework",
        "summary": "Conventional construction safety inspection methods are often inefficient as\nthey require navigating through large volume of information. Recent advances in\nlarge vision-language models (LVLMs) provide opportunities to automate safety\ninspections through enhanced visual and linguistic understanding. However,\nexisting applications face limitations including irrelevant or unspecific\nresponses, restricted modal inputs and hallucinations. Utilisation of Large\nLanguage Models (LLMs) for this purpose is constrained by availability of\ntraining data and frequently lack real-time adaptability. This study introduces\nSiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)\nframework for automating construction safety inspection reports by integrating\nvisual and audio inputs. Using real-world data, SiteShield outperformed\nunimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,\nprecision of 0.76, and recall of 0.96. The findings indicate that SiteShield\noffers a novel pathway to enhance information retrieval and efficiency in\ngenerating safety reports.",
        "url": "http://arxiv.org/abs/2510.04145v1",
        "published_date": "2025-10-05T10:48:54+00:00",
        "updated_date": "2025-10-05T10:48:54+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.IR"
        ],
        "authors": [
            "Chenxin Wang",
            "Elyas Asadi Shamsabadi",
            "Zhaohui Chen",
            "Luming Shen",
            "Alireza Ahmadian Fard Fini",
            "Daniel Dias-da-Costa"
        ],
        "tldr": "The paper introduces SiteShield, a multi-modal vision-language RAG framework for automating construction safety inspection reports, outperforming unimodal LLMs with RAG on real-world data.",
        "tldr_zh": "该论文介绍了SiteShield，一个用于自动化建筑安全检查报告的多模态视觉语言RAG框架，在真实世界数据上优于带有RAG的单模态LLM。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding",
        "summary": "Multimodal large language models (MLLMs) have markedly expanded the\ncompetence of graphical user-interface (GUI) systems, propelling them beyond\ncontrolled simulations into complex, real-world environments across diverse\nplatforms. However, practical usefulness is still bounded by the reliability of\nvisual grounding, i.e., mapping textual references to exact on-screen elements.\nThis limitation prevents the system from accurately performing pointer-level\nactions such as clicking or dragging. To address it, we introduce GUI-Spotlight\n-- a model trained for image-grounded reasoning that dynamically invokes\nmultiple specialized tools to iteratively narrow its focus to the relevant\nregion of the screen, thereby substantially improving visual grounding\naccuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only\n18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with\n9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).",
        "url": "http://arxiv.org/abs/2510.04039v1",
        "published_date": "2025-10-05T05:15:45+00:00",
        "updated_date": "2025-10-05T05:15:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bin Lei",
            "Nuo Xu",
            "Ali Payani",
            "Mingyi Hong",
            "Chunhua Liao",
            "Yu Cao",
            "Caiwen Ding"
        ],
        "tldr": "The paper introduces GUI-Spotlight, a model for improved visual grounding in GUIs, achieving state-of-the-art results on the ScreenSpot-Pro benchmark with significantly fewer training samples.",
        "tldr_zh": "该论文介绍了GUI-Spotlight，一种用于改进GUI中视觉定位的模型，在ScreenSpot-Pro基准测试中取得了最先进的结果，且使用的训练样本明显更少。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models",
        "summary": "Embedding vision-language models (VLMs) are typically pretrained with short\ntext windows (<77 tokens), which forces the truncation of long-format captions.\nYet, the distribution of biomedical captions from large-scale open source\nliterature reveals that a huge portion of captions far exceed 77 tokens. To\nthis end, we investigate the impact of pretraining on long-format biomedical\ncaptions by extending the context length of text encoders in VLMs. We find that\nlonger context (thus, enabling additional supervision provided in long-format\ncaptions) correlates with better retrieval and classification performance.\nGiven this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M\nimage-caption pairs enriched with context-aware descriptions from full-text\narticles, providing longer and additional textual supervision. Using\nBIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a\ntext encoder supporting windows of up to 512 tokens. Our model extends context\ncapacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption\nretrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in\nRecall@1 and +2% average improvements in classification, while also converging\nfaster than short-context. Our results demonstrate that long-context modeling\nis a promising direction for advancing biomedical VLMs.",
        "url": "http://arxiv.org/abs/2510.03978v1",
        "published_date": "2025-10-04T23:38:18+00:00",
        "updated_date": "2025-10-04T23:38:18+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Min Woo Sun",
            "Alejandro Lozano",
            "Javier Gamazo Tejero",
            "Vishwesh Nath",
            "Xiao Xiao Sun",
            "James Burgess",
            "Yuhui Zhang",
            "Kun Yuan",
            "Robert Tibshirani",
            "Sean Huver",
            "Serena Yeung-Levy"
        ],
        "tldr": "The paper introduces a long-context biomedical VLM, BMC-LongCLIP, trained on a new dataset BIOMEDICA-LongCAP with captions up to 512 tokens, achieving significant improvements in retrieval and classification compared to models trained with shorter contexts.",
        "tldr_zh": "该论文介绍了一种长文本生物医学视觉语言模型BMC-LongCLIP，它使用新的数据集BIOMEDICA-LongCAP进行训练，该数据集的文本长度高达512个token。与使用较短文本训练的模型相比，该模型在检索和分类方面取得了显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs",
        "summary": "While Video Large Language Models (Video-LLMs) have demonstrated remarkable\nperformance across general video understanding benchmarks-particularly in video\ncaptioning and descriptive tasks-they consistently underperform on tasks that\nrequire fine-grained temporal understanding. This limitation arises due to the\nlack of visual complexity and temporal nuance in current fine-tuning datasets,\nleading these models to rely heavily on language-based reasoning rather than\ntruly understanding video dynamics. In this work, we propose TimeWarp, a\nsystematic method to create a targeted synthetic temporal dataset to fine-tune\nthe model's responses to encourage it to focus on the given input video. We\nintroduce a large-scale preference dataset, created using TimeWarp, that\ncaptures intricate temporal dynamics often overlooked, grounding the model's\nresponses to visual and temporal information. We demonstrate that when our\nmethod is applied to existing models, it significantly improves performance on\ntemporal understanding benchmarks, highlighting the effectiveness of our\nproposed datasets in advancing temporal understanding in Video-LLMs, resulting\nin an absolute improvement in performance across seven benchmarks. Code is\navailable at https://github.com/sameepv21/timewarp.",
        "url": "http://arxiv.org/abs/2510.03955v1",
        "published_date": "2025-10-04T21:48:40+00:00",
        "updated_date": "2025-10-04T21:48:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sameep Vani",
            "Shreyas Jena",
            "Maitreya Patel",
            "Chitta Baral",
            "Somak Aditya",
            "Yezhou Yang"
        ],
        "tldr": "The paper introduces TimeWarp, a method for creating synthetic temporal datasets to improve Video-LLMs' fine-grained temporal understanding, demonstrating improved performance on temporal understanding benchmarks.",
        "tldr_zh": "该论文介绍了TimeWarp，一种用于创建合成时间数据集的方法，以提高视频大语言模型（Video-LLM）的细粒度时间理解能力，并在时间理解基准测试中表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models",
        "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance\non vision-language reasoning tasks. However, their potential for zero-shot\nfine-grained image classification, a challenging task requiring precise\ndifferentiation between visually similar categories, remains underexplored. We\npresent a novel method that transforms zero-shot fine-grained image\nclassification into a visual question-answering framework, leveraging LVLMs'\ncomprehensive understanding capabilities rather than relying on direct class\nname generation. We enhance model performance through a novel attention\nintervention technique. We also address a key limitation in existing datasets\nby developing more comprehensive and precise class description benchmarks. We\nvalidate the effectiveness of our method through extensive experimentation\nacross multiple fine-grained image classification benchmarks. Our proposed\nmethod consistently outperforms the current state-of-the-art (SOTA) approach,\ndemonstrating both the effectiveness of our method and the broader potential of\nLVLMs for zero-shot fine-grained classification tasks. Code and Datasets:\nhttps://github.com/Atabuzzaman/Fine-grained-classification",
        "url": "http://arxiv.org/abs/2510.03903v1",
        "published_date": "2025-10-04T18:56:41+00:00",
        "updated_date": "2025-10-04T18:56:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md. Atabuzzaman",
            "Andrew Zhang",
            "Chris Thomas"
        ],
        "tldr": "This paper introduces a novel method for zero-shot fine-grained image classification using LVLMs by transforming it into a visual question-answering framework and employing attention intervention, achieving SOTA results and enhanced datasets.",
        "tldr_zh": "本文提出了一种新的零样本细粒度图像分类方法，该方法利用大型视觉语言模型（LVLM），通过将其转化为视觉问答框架并采用注意力干预，实现了最先进的性能，并改进了数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert",
        "summary": "Although Vision-Language Models (VLM) have demonstrated impressive planning\nand reasoning capabilities, translating these abilities into the physical world\nintroduces significant challenges. Conventional Vision-Language-Action (VLA)\nmodels, which integrate reasoning and action into a monolithic architecture,\ngeneralize poorly because they are constrained by scarce, narrow-domain data.\nWhile recent dual-system approaches attempt to decouple \"thinking\" from\n\"acting\", they are often constrained by semantic ambiguities within the action\nmodule. This ambiguity makes large-scale, cross-task training infeasible.\nConsequently, these systems typically necessitate fine-tuning on newly\ncollected data when deployed to novel environments, and the cooperation\nmechanism between the two systems remains ill-defined. To address these\nlimitations, we introduce, for the first time, a framework centered around a\ngeneralizable action expert. Our approach utilizes sparse 3D trajectories as an\nintermediate representation, effectively bridging the high-level planning\ncapabilities of the VLM with the low-level physical action module. During the\nplanning phase, the VLM is only required to generate coarse 3D waypoints. These\nwaypoints are then processed by our generalizable action expert, which refines\nthem into dense, executable action sequences by sampling real-time point cloud\nobservations of the environment. To promote training efficiency and robust\ngeneralization, we introduce a novel \"Action Pre-training, Pointcloud\nFine-tuning\" paradigm. Our method combines the broad generalization\ncapabilities of VLMs in visual understanding and planning with the\nfine-grained, action-level generalization of action expert.",
        "url": "http://arxiv.org/abs/2510.03896v1",
        "published_date": "2025-10-04T18:33:27+00:00",
        "updated_date": "2025-10-04T18:33:27+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Mingyu Liu",
            "Zheng Huang",
            "Xiaoyi Lin",
            "Muzhi Zhu",
            "Canyu Zhao",
            "Zongze Du",
            "Yating Wang",
            "Haoyi Zhu",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "tldr": "This paper introduces a framework that uses a generalizable action expert with sparse 3D trajectories to bridge the gap between VLMs' planning abilities and physical action, employing a novel \"Action Pre-training, Pointcloud Fine-tuning\" paradigm.",
        "tldr_zh": "本文介绍了一个框架，该框架使用具有稀疏3D轨迹的通用动作专家来弥合VLM的规划能力与物理动作之间的差距，并采用了一种新颖的“动作预训练，点云微调”范例。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge",
        "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nhere\\footnote{https://github.com/mhson-kyle/World-To-Image}.",
        "url": "http://arxiv.org/abs/2510.04201v1",
        "published_date": "2025-10-05T13:35:30+00:00",
        "updated_date": "2025-10-05T13:35:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Moo Hyun Son",
            "Jintaek Oh",
            "Sun Bin Mun",
            "Jaechul Roh",
            "Sehyun Choi"
        ],
        "tldr": "The paper introduces World-To-Image, a framework that enhances text-to-image generation for novel entities by using an agent to search the web for relevant images and optimize prompts, demonstrably improving semantic fidelity and visual aesthetics.",
        "tldr_zh": "该论文介绍了一个名为World-To-Image的框架，它通过使用代理在网络上搜索相关图像并优化提示来增强文本到图像生成对新实体的处理能力，从而显著提高了语义保真度和视觉美感。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs",
        "summary": "This paper identifies a critical yet underexplored challenge in distilling\nfrom multimodal large language models (MLLMs): the reasoning trajectories\ngenerated by multiple drifting teachers exhibit concept drift, whereby their\nreasoning distributions evolve unpredictably and transmit biases to the student\nmodel, ultimately compromising its performance. To tackle this issue, we\npioneer a theoretical connection between concept drift and knowledge\ndistillation, casting the non-stationary reasoning dynamics from multiple MLLM\nteachers as next-token prediction of multi-stream reasoning trajectories.Guided\nby concept drift, we introduce the \"learn, compare, critique\" paradigm,\nculminating in autonomous preference optimization (APO). Under the active\nguidance of the teachers, the student model first learns and self-distils\npreferred thinking by comparing multiple teachers. It then engages in critical\nreflection over the drifting inference from teachers, performing concept\nalignment through APO, ultimately yielding a robust, consistent, and\ngeneralizable model.Extensive experiments demonstrate our superior performance\nof consistency, robustness and generalization within knowledge distillation.\nBesides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers\nAlignment X-rays), comprising 170,982 distilled reasoning trajectories derived\nfrom publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public\nat: https://anonymous.4open.science/r/Autonomous-Distillation/.",
        "url": "http://arxiv.org/abs/2510.04142v1",
        "published_date": "2025-10-05T10:42:21+00:00",
        "updated_date": "2025-10-05T10:42:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xiaoyu Yang",
            "Jie Lu",
            "En Yu"
        ],
        "tldr": "The paper introduces a novel approach, \"learn, compare, critique\", for knowledge distillation from multiple drifting MLLMs, using concept alignment to address biases and improve consistency, robustness, and generalization. They also contribute a large-scale dataset, CXR-MAX.",
        "tldr_zh": "该论文提出了一种新颖的方法 \"learn, compare, critique\"，用于从多个漂移的 MLLM 中进行知识蒸馏，使用概念对齐来解决偏差并提高一致性、鲁棒性和泛化能力。他们还贡献了一个大型数据集 CXR-MAX。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning",
        "summary": "We present \\emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA\nframework that preserves a fixed token budget by first \\emph{localizing}\nquestion-relevant interval(s) with a low-fps skim and then \\emph{answering} via\nspan-aware reallocation of visual tokens at higher effective frame rate,\nemitting an interleaved output with both spans and the final option for direct\nattribution. We also introduce \\dataname{}, which converts description based\nevent graphs into \\emph{span-grounded} multiple-choice QA by pairing each\nquestion with \\emph{ground-truth} time span(s) and related reasoning. ViTL is\ntrained end-to-end with an interleaved group-relative objective that couples\ntemporal IoU for localization with answer correctness, allowing credit to flow\nfrom answers back to spans without increasing compute. Under fixed token\nbudgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and\ntemporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations\nshow that span-aware token reallocation consistently surpasses uniform\nsampling. Together, \\dataname{} and ViTL provide an interpretable,\ncompute-efficient recipe for scalable long-video QA.",
        "url": "http://arxiv.org/abs/2510.04022v1",
        "published_date": "2025-10-05T04:03:31+00:00",
        "updated_date": "2025-10-05T04:03:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chendong Wang",
            "Donglin Bai",
            "Yifan Yang",
            "Xiao Jin",
            "Anlan Zhang",
            "Rui Wang",
            "Shiqi Jiang",
            "Yuqing Yang",
            "Hao Wu",
            "Qi Dai",
            "Chong Luo",
            "Ting Cao",
            "Lili Qiu",
            "Suman Banerjee"
        ],
        "tldr": "The paper introduces Video-in-the-Loop (ViTL), a framework for efficient long-video QA using span-grounded reasoning and interleaved outputs, achieving better performance with less frame input. It also introduces a new dataset to facilitate this task.",
        "tldr_zh": "该论文介绍了 Video-in-the-Loop (ViTL)，一个用于高效长视频 QA 的框架，它使用 span-grounded 推理和交错输出，以更少的帧输入实现了更好的性能。 该论文还介绍了一个新的数据集来促进这项任务。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Visual Lifelog Retrieval through Captioning-Enhanced Interpretation",
        "summary": "People often struggle to remember specific details of past experiences, which\ncan lead to the need to revisit these memories. Consequently, lifelog retrieval\nhas emerged as a crucial application. Various studies have explored methods to\nfacilitate rapid access to personal lifelogs for memory recall assistance. In\nthis paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval\nSystem for extracting specific images from a user's visual lifelog based on\ntextual queries. Unlike traditional embedding-based methods, our system first\ngenerates captions for visual lifelogs and then utilizes a text embedding model\nto project both the captions and user queries into a shared vector space.\nVisual lifelogs, captured through wearable cameras, provide a first-person\nviewpoint, necessitating the interpretation of the activities of the individual\nbehind the camera rather than merely describing the scene. To address this, we\nintroduce three distinct approaches: the single caption method, the collective\ncaption method, and the merged caption method, each designed to interpret the\nlife experiences of lifeloggers. Experimental results show that our method\neffectively describes first-person visual images, enhancing the outcomes of\nlifelog retrieval. Furthermore, we construct a textual dataset that converts\nvisual lifelogs into captions, thereby reconstructing personal life\nexperiences.",
        "url": "http://arxiv.org/abs/2510.04010v1",
        "published_date": "2025-10-05T03:00:58+00:00",
        "updated_date": "2025-10-05T03:00:58+00:00",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yu-Fei Shih",
            "An-Zi Yen",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "tldr": "This paper introduces a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval System that uses captions generated from visual lifelogs to improve text-based retrieval of specific images, addressing the challenge of interpreting first-person visual data.",
        "tldr_zh": "本文介绍了一种基于字幕的视觉生活日志（CIVIL）检索系统，该系统使用从视觉生活日志生成的字幕来改进基于文本的特定图像检索，从而解决了第一人称视觉数据解释的挑战。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance",
        "summary": "Autonomous driving perception systems are particularly vulnerable in foggy\nconditions, where light scattering reduces contrast and obscures fine details\ncritical for safe operation. While numerous defogging methods exist-from\nhandcrafted filters to learned restoration models-improvements in image\nfidelity do not consistently translate into better downstream detection and\nsegmentation. Moreover, prior evaluations often rely on synthetic data, leaving\nquestions about real-world transferability. We present a structured empirical\nstudy that benchmarks a comprehensive set of pipelines, including (i) classical\nfilters, (ii) modern defogging networks, (iii) chained variants\n(filter$\\rightarrow$model, model$\\rightarrow$filter), and (iv) prompt-driven\nvisual--language image editing models (VLM) applied directly to foggy images.\nUsing Foggy Cityscapes, we assess both image quality and downstream performance\non object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals\nwhen defogging helps, when chaining yields synergy or degradation, and how\nVLM-based editors compare to dedicated approaches. In addition, we evaluate\nqualitative rubric-based scores from a VLM judge and quantify their alignment\nwith task metrics, showing strong correlations with mAP. Together, these\nresults establish a transparent, task-oriented benchmark for defogging methods\nand highlight the conditions under which preprocessing genuinely improves\nautonomous perception in adverse weather.",
        "url": "http://arxiv.org/abs/2510.03906v1",
        "published_date": "2025-10-04T19:05:04+00:00",
        "updated_date": "2025-10-04T19:05:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ardalan Aryashad",
            "Parsa Razmara",
            "Amin Mahjoub",
            "Seyedarmin Azizi",
            "Mahdi Salmani",
            "Arad Firouzkouhi"
        ],
        "tldr": "This paper benchmarks various defogging methods, including VLMs, for autonomous driving perception using object detection and segmentation tasks on Foggy Cityscapes, highlighting their impact on downstream performance.",
        "tldr_zh": "该论文通过在Foggy Cityscapes数据集上进行目标检测和分割任务，对各种去雾方法（包括视觉语言模型）在自动驾驶感知中的性能进行了基准测试，强调了它们对下游任务的影响。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]