[
    {
        "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation",
        "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
        "url": "http://arxiv.org/abs/2509.03498v1",
        "published_date": "2025-09-03T17:29:50+00:00",
        "updated_date": "2025-09-03T17:29:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Han Li",
            "Xinyu Peng",
            "Yaoming Wang",
            "Zelin Peng",
            "Xin Chen",
            "Rongxiang Weng",
            "Jingang Wang",
            "Xunliang Cai",
            "Wenrui Dai",
            "Hongkai Xiong"
        ],
        "tldr": "OneCAT is a decoder-only autoregressive multimodal model that unifies understanding, generation, and editing with improved efficiency and performance compared to existing open-source models by eliminating external vision components during inference.",
        "tldr_zh": "OneCAT是一个纯解码器自回归多模态模型，通过消除推理过程中外部视觉组件，统一理解、生成和编辑，与现有开源模型相比，提高了效率和性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data",
        "summary": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs.",
        "url": "http://arxiv.org/abs/2509.03501v1",
        "published_date": "2025-09-03T17:33:20+00:00",
        "updated_date": "2025-09-03T17:33:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Honglu Zhou",
            "Xiangyu Peng",
            "Shrikant Kendre",
            "Michael S. Ryoo",
            "Silvio Savarese",
            "Caiming Xiong",
            "Juan Carlos Niebles"
        ],
        "tldr": "The paper introduces Strefer, a synthetic data generation framework to improve Video LLMs' spatiotemporal reasoning via instruction tuning, outperforming baselines without human annotation or proprietary models.",
        "tldr_zh": "该论文介绍了Strefer，一个通过合成数据生成框架，利用指令微调来提升视频大语言模型在时空推理方面的能力，且无需人工标注或专有模型，性能优于基线模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection",
        "summary": "Hallucinations in multimodal large language model are caused by the\ntext-visual bias and the co-occurrence bias. The former reflects an\nover-reliance on text information in the decision-making process, while the\nlatter arises from the statistical object-pairing patterns abstracted from the\ntraining data. Existing mitigation methods heuristically address these biases\nwithout understanding the fluctuating bias level across the instances. We first\npropose estimating the influence of respective token types (visual, prompt, and\nprevious outputs) using a gradient-based self-reflection method. The estimated\ntoken influence further enables the detection of object-related visual tokens\nand their integration into an influence-aware contrastive decoding framework to\nmitigate both types of biases simultaneously. Our method operates without the\nneed for additional resources, such as costly fine-tuning, extra models, or\ndata statistics. Extensive experiments show it effectively reduces\nhallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.",
        "url": "http://arxiv.org/abs/2509.03113v1",
        "published_date": "2025-09-03T08:13:52+00:00",
        "updated_date": "2025-09-03T08:13:52+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Shan Wang",
            "Maying Shen",
            "Nadine Chang",
            "Chuong Nguyen",
            "Hongdong Li",
            "Jose M. Alvarez"
        ],
        "tldr": "This paper introduces a gradient-based self-reflection method to mitigate hallucinations in multimodal large language models by addressing text-visual and co-occurrence biases, achieving significant accuracy improvements without additional resources.",
        "tldr_zh": "本文提出了一种基于梯度的自反思方法，通过解决文本-视觉和共现偏差来减轻多模态大型语言模型中的幻觉，无需额外资源即可显著提高准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens",
        "summary": "Large Vision-Language Models (LVLMs) generate contextually relevant responses\nby jointly interpreting visual and textual inputs. However, our finding reveals\nthey often mistakenly perceive text inputs lacking visual evidence as being\npart of the image, leading to erroneous responses. In light of this finding, we\nprobe whether LVLMs possess an internal capability to determine if textual\nconcepts are grounded in the image, and discover a specific subset of\nFeed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons,\nthat consistently signal the visual absence through a distinctive activation\npattern. Leveraging these patterns, we develop a detection module that\nsystematically classifies whether an input token is visually grounded. Guided\nby its prediction, we propose a method to refine the outputs by reinterpreting\nquestion prompts or replacing the detected absent tokens during generation.\nExtensive experiments show that our method effectively mitigates the models'\ntendency to falsely presume the visual presence of text input and its\ngenerality across various LVLMs.",
        "url": "http://arxiv.org/abs/2509.03025v1",
        "published_date": "2025-09-03T05:17:25+00:00",
        "updated_date": "2025-09-03T05:17:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sohee Kim",
            "Soohyun Ryu",
            "Joonhyung Park",
            "Eunho Yang"
        ],
        "tldr": "This paper identifies a flaw in LVLMs where they mistakenly assume text inputs lacking visual evidence are present in the image, and proposes a method to mitigate this issue by detecting and addressing visually absent tokens.",
        "tldr_zh": "该论文发现大型视觉语言模型（LVLMs）存在一个缺陷，即它们错误地假设缺乏视觉证据的文本输入出现在图像中，并提出了一种通过检测和处理视觉上缺失的token来缓解此问题的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models",
        "summary": "Accurate short-horizon trajectory prediction is pivotal for safe and reliable\nautonomous driving, yet existing vision-language models (VLMs) often fail to\neffectively ground their reasoning in scene dynamics and domain knowledge. To\naddress this challenge, this paper introduces KEPT, a knowledge-enhanced VLM\nframework that predicts ego trajectories directly from consecutive front-view\ndriving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video\nencoder, trained via self-supervised learning with hard-negative mining, with a\nscalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.\nRetrieved priors are embedded into chain-of-thought (CoT) prompts with explicit\nplanning constraints, while a triple-stage fine-tuning schedule incrementally\naligns the language head to metric spatial cues, physically feasible motion,\nand temporally conditioned front-view planning. Evaluated on nuScenes dataset,\nKEPT achieves state-of-the-art performance across open-loop protocols: under\nNoAvg, it achieves 0.70m average L2 with a 0.21\\% collision rate; under TemAvg\nwith lightweight ego status, it attains 0.31m average L2 and a 0.07\\% collision\nrate. Ablation studies show that all three fine-tuning stages contribute\ncomplementary benefits, and that using Top-2 retrieved exemplars yields the\nbest accuracy-safety trade-off. The k-means-clustered HNSW index delivers\nsub-millisecond retrieval latency, supporting practical deployment. These\nresults indicate that retrieval-augmented, CoT-guided VLMs offer a promising,\ndata-efficient pathway toward interpretable and trustworthy autonomous driving.",
        "url": "http://arxiv.org/abs/2509.02966v1",
        "published_date": "2025-09-03T03:10:42+00:00",
        "updated_date": "2025-09-03T03:10:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yujin Wang",
            "Tianyi Wang",
            "Quanfeng Liu",
            "Wenxian Fan",
            "Junfeng Jiao",
            "Christian Claudel",
            "Yunbing Yan",
            "Bingzhao Gao",
            "Jianqiang Wang",
            "Hong Chen"
        ],
        "tldr": "The paper introduces KEPT, a knowledge-enhanced vision-language model framework for predicting ego-trajectories from driving frames, utilizing retrieval-augmented, chain-of-thought prompting and a novel fine-tuning schedule, achieving state-of-the-art results on the nuScenes dataset.",
        "tldr_zh": "该论文介绍了一种名为KEPT 的知识增强型视觉语言模型框架，用于预测驾驶帧中的自我轨迹。它利用检索增强的链式思维提示和一种新的微调方案，在 nuScenes 数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly",
        "summary": "Assistants on assembly tasks have a large potential to benefit humans from\neveryday tasks to industrial settings. However, no testbeds support\napplication-oriented system evaluation in a practical setting, especially in\nassembly. To foster the development, we propose a new multimodal QA dataset on\nassembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs\nthat require the multimodal understanding of human-activity recordings and\ntheir instruction manuals in an online-style manner. In the development, we\nadopt a semi-automated QA annotation approach, where LLMs generate candidates\nand humans verify them, as a cost-effective method, and further improve it by\nintegrating fine-grained action labels to diversify question types.\nFurthermore, we create instruction task graphs for the target tasks of\nassembling toy vehicles. These newly created task graphs are used in our\nbenchmarking experiment, as well as to facilitate the human verification\nprocess in the QA annotation. Utilizing our dataset, we benchmark models,\nincluding competitive proprietary multimodal models. Our results suggest great\nroom for improvement for the current models. We believe our new evaluation\ndataset can contribute to the further development of procedural-activity\nassistants.",
        "url": "http://arxiv.org/abs/2509.02949v1",
        "published_date": "2025-09-03T02:26:48+00:00",
        "updated_date": "2025-09-03T02:26:48+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Kimihiro Hasegawa",
            "Wiradee Imrattanatrai",
            "Masaki Asada",
            "Susan Holm",
            "Yuran Wang",
            "Vincent Zhou",
            "Ken Fukuda",
            "Teruko Mitamura"
        ],
        "tldr": "The paper introduces ProMQA-Assembly, a new multimodal QA dataset for assembly tasks, designed to evaluate and improve procedural-activity assistants, utilizing a semi-automated annotation approach and instruction task graphs.",
        "tldr_zh": "该论文介绍了一个新的多模态问答数据集ProMQA-Assembly，用于装配任务，旨在评估和改进程序性活动助手，采用半自动化标注方法和指令任务图。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA",
        "summary": "In this paper, we propose a novel parameter-efficient adaptation method for\nNo- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized\nin pixel-space. Unlike full fine-tuning of Multimodal Large Language Models\n(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base\nmodel), while keeping the underlying model fully frozen. During inference,\nthese visual prompts are combined with images via addition and processed by\nmPLUG-Owl2 with the textual query \"Rate the technical quality of the image.\"\nEvaluations across distortion types (synthetic, realistic, AI-generated) on\nKADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against\nfull finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on\nKADID-10k. To our knowledge, this is the first work to leverage pixel-space\nvisual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level\nvision tasks. The source code is publicly available at https: // github. com/\nyahya-ben/ mplug2-vp-for-nriqa .",
        "url": "http://arxiv.org/abs/2509.03494v1",
        "published_date": "2025-09-03T17:23:24+00:00",
        "updated_date": "2025-09-03T17:23:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yahya Benmahane",
            "Mohammed El Hassouni"
        ],
        "tldr": "This paper introduces a parameter-efficient method for No-Reference Image Quality Assessment (NR-IQA) by adapting mPLUG-Owl2 using pixel-level visual prompts, achieving competitive performance with minimal trainable parameters.",
        "tldr_zh": "本文提出了一种参数高效的免参考图像质量评估（NR-IQA）方法，通过使用像素级视觉提示来调整mPLUG-Owl2，以最少的训练参数实现了具有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Empowering Lightweight MLLMs with Reasoning via Long CoT SFT",
        "summary": "While Reinforcement Learning with Verifiable Rewards has enhanced the\nreasoning of large-scale language models (LLMs), its efficacy for lightweight\nmultimodal language models (MLLMs) with fewer than seven billion parameters\nremains underexplored. This paper investigates the role of long\nChain-of-Thought (long CoT) data in enhancing the reasoning abilities of such\nMLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT\ndata significantly improves MLLM reasoning. Furthermore, we observe that after\nthis initial SFT phase, MLLMs can achieve additional performance gains through\na subsequent RL stage. We conclude that a SFT stage with long CoT data is a\ncritical prerequisite for developing the reasoning capabilities of lightweight\nMLLMs.",
        "url": "http://arxiv.org/abs/2509.03321v1",
        "published_date": "2025-09-03T13:53:29+00:00",
        "updated_date": "2025-09-03T13:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linyu Ou"
        ],
        "tldr": "This paper explores using long Chain-of-Thought (CoT) Supervised Fine-Tuning (SFT) to improve the reasoning abilities of lightweight Multimodal Language Models (MLLMs), finding it to be a crucial prerequisite for developing reasoning capabilities.",
        "tldr_zh": "本文探讨了使用长链思维（CoT）监督微调（SFT）来提高轻量级多模态语言模型（MLLMs）的推理能力，发现这是发展推理能力的关键先决条件。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?",
        "summary": "Multi-modal large language models (MLLMs) have shown impressive\ngeneralization across tasks using images and text modalities. While their\nextension to video has enabled tasks such as video question answering and video\ncaptioning, their pixel-level visual grounding abilities are less studied. In\nthis work, we raise the pertinent question of whether motion is used in\npixel-level visual grounding and whether video MLLMs can segment objects based\non natural language expressions describing their motion patterns. We identify\nthe shortcomings in the current benchmarks, where we show that a single frame\ncan often suffice for capturing the motion referring expression without any\ntemporal reasoning. To address this, we introduce four motion-centric probing\ntechniques, particularly designed for the visual grounding task, to study video\nMLLMs' ability to identify true motion from a fake one and their ability to\ngrasp the motion order. Consequently, we provide a motion-centric benchmark,\nMoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging\nthe interaction between motion and language rather than being dominated by\nstatic appearance cues emphasized in existing visual grounding datasets. We\nfurther establish strong single-image baselines that are on par with or\noutperform prior methods. Finally, we explore simple motion-centric adaptation\ntechniques that provide state-of-the-art performance on our MoCentric-Bench.\nOur motion-centric benchmark, evaluation and findings challenge future models\nto improve dense spatiotemporal grounding and pixel-level understanding within\nvideos. Code and datasets will be made publicly available at\nhttps://github.com/MSiam/PixFoundation-2.0.git.",
        "url": "http://arxiv.org/abs/2509.02807v1",
        "published_date": "2025-09-02T20:21:11+00:00",
        "updated_date": "2025-09-02T20:21:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mennatullah Siam"
        ],
        "tldr": "The paper investigates whether video MLLMs actually utilize motion in visual grounding tasks, finds that they often rely on static cues, and introduces a new motion-centric benchmark (MoCentric-Bench) to address this issue, along with motion-centric adaptation techniques that show improved performance.",
        "tldr_zh": "该论文研究了视频多模态大语言模型在视觉定位任务中是否真正利用了运动信息，发现它们通常依赖于静态线索，并引入了一个新的以运动为中心的基准测试（MoCentric-Bench）来解决这个问题，以及展示了改进性能的以运动为中心的适应技术。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]