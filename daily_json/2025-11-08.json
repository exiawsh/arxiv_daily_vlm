[
    {
        "title": "InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation",
        "summary": "We introduce InfinityStar, a unified spacetime autoregressive framework for\nhigh-resolution image and dynamic video synthesis. Building on the recent\nsuccess of autoregressive modeling in both vision and language, our purely\ndiscrete approach jointly captures spatial and temporal dependencies within a\nsingle architecture. This unified design naturally supports a variety of\ngeneration tasks such as text-to-image, text-to-video, image-to-video, and long\ninteractive video synthesis via straightforward temporal autoregression.\nExtensive experiments demonstrate that InfinityStar scores 83.74 on VBench,\noutperforming all autoregressive models by large margins, even surpassing some\ndiffusion competitors like HunyuanVideo. Without extra optimizations, our model\ngenerates a 5s, 720p video approximately 10x faster than leading\ndiffusion-based methods. To our knowledge, InfinityStar is the first discrete\nautoregressive video generator capable of producing industrial level 720p\nvideos. We release all code and models to foster further research in efficient,\nhigh-quality video generation.",
        "url": "http://arxiv.org/abs/2511.04675v1",
        "published_date": "2025-11-06T18:58:03+00:00",
        "updated_date": "2025-11-06T18:58:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinlai Liu",
            "Jian Han",
            "Bin Yan",
            "Hui Wu",
            "Fengda Zhu",
            "Xing Wang",
            "Yi Jiang",
            "Bingyue Peng",
            "Zehuan Yuan"
        ],
        "tldr": "InfinityStar is a unified spacetime autoregressive framework for high-resolution image and video synthesis, outperforming existing autoregressive models and even some diffusion models in speed and quality, particularly for 720p video generation.",
        "tldr_zh": "InfinityStar是一个统一的时空自回归框架，用于高分辨率图像和视频合成。在速度和质量上，它优于现有的自回归模型，甚至一些扩散模型，尤其是在生成720p视频方面。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
        "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.",
        "url": "http://arxiv.org/abs/2511.04668v1",
        "published_date": "2025-11-06T18:53:31+00:00",
        "updated_date": "2025-11-06T18:53:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ellis Brown",
            "Arijit Ray",
            "Ranjay Krishna",
            "Ross Girshick",
            "Rob Fergus",
            "Saining Xie"
        ],
        "tldr": "The paper introduces SIMS-V, a framework for generating spatially-rich video training data using 3D simulators, which significantly improves the spatial reasoning capabilities of video LLMs with a relatively small amount of simulated data.",
        "tldr_zh": "该论文介绍了SIMS-V，一个利用3D模拟器生成富含空间信息的视频训练数据的框架，通过相对较少的模拟数据，显著提升了视频LLM的空间推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts",
        "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via $k$-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score $s(x)$. We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.",
        "url": "http://arxiv.org/abs/2511.04655v1",
        "published_date": "2025-11-06T18:43:21+00:00",
        "updated_date": "2025-11-06T18:43:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ellis Brown",
            "Jihan Yang",
            "Shusheng Yang",
            "Rob Fergus",
            "Saining Xie"
        ],
        "tldr": "The paper proposes a framework for diagnosing and mitigating non-visual biases in multimodal benchmarks by \"training on the test set\" to identify and remove exploitable patterns, creating more robust evaluation metrics for MLLMs.",
        "tldr_zh": "该论文提出了一种框架，通过“在测试集上训练”来诊断和缓解多模态基准测试中的非视觉偏差，从而识别和消除可利用的模式，为MLLM创建更强大的评估指标。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    }
]