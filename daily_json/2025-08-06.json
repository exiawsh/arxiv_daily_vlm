[
    {
        "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?",
        "summary": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis.",
        "url": "http://arxiv.org/abs/2508.03644v1",
        "published_date": "2025-08-05T16:55:02+00:00",
        "updated_date": "2025-08-05T16:55:02+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Wenxuan Shen",
            "Mingjia Wang",
            "Yaochen Wang",
            "Dongping Chen",
            "Junjie Yang",
            "Yao Wan",
            "Weiwei Lin"
        ],
        "tldr": "The paper introduces Double-Bench, a large-scale, multilingual, and multimodal benchmark for evaluating document RAG systems, highlighting current limitations and future research directions.",
        "tldr_zh": "该论文介绍了Double-Bench，一个大规模、多语言和多模态的基准，用于评估文档RAG系统，强调了当前的局限性和未来的研究方向。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models",
        "summary": "Personalized generation in T2I diffusion models aims to naturally incorporate\nindividual user preferences into the generation process with minimal user\nintervention. However, existing studies primarily rely on prompt-level modeling\nwith large-scale models, often leading to inaccurate personalization due to the\nlimited input token capacity of T2I diffusion models. To address these\nlimitations, we propose DrUM, a novel method that integrates user profiling\nwith a transformer-based adapter to enable personalized generation through\ncondition-level modeling in the latent space. DrUM demonstrates strong\nperformance on large-scale datasets and seamlessly integrates with open-source\ntext encoders, making it compatible with widely used foundation T2I models\nwithout requiring additional fine-tuning.",
        "url": "http://arxiv.org/abs/2508.03481v1",
        "published_date": "2025-08-05T14:14:55+00:00",
        "updated_date": "2025-08-05T14:14:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Hyungjin Kim",
            "Seokho Ahn",
            "Young-Duk Seo"
        ],
        "tldr": "The paper introduces DrUM, a novel method for personalized text-to-image generation that uses condition-level modeling within diffusion models to overcome the limitations of prompt-level modeling, achieving strong performance without requiring fine-tuning of foundation models.",
        "tldr_zh": "该论文介绍了DrUM，一种新颖的个性化文本到图像生成方法，它使用扩散模型中的条件级别建模来克服提示级别建模的局限性，在无需对基础模型进行微调的情况下实现了强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models",
        "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nsignificant progress across multiple domains. However, these models still face\nthe inherent challenge of integrating vision and language for collaborative\ninference, which often leads to \"hallucinations\", outputs that are not grounded\nin the corresponding images. Many efforts have been made to address these\nissues, but each comes with its own limitations, such as high computational\ncost or expensive dataset annotation. Recent research shows that LVLMs exhibit\na long-term bias where hallucinations increase as the sequence length grows,\nyet the underlying cause remains poorly understood. Building on extensive\nresearch into attention mechanisms in LVLMs, we analyze the relationship\nbetween this long-term bias and visual attention. In our research, we identify\na consistent phenomenon in current LVLMs: the model's attention to visual input\ndiminishes as the generated sequence grows, which we hypothesize to be a key\nfactor contributing to observed increasing hallucinations. Based on these\ninsights, we propose Image attention-guided Key-value merging cOllaborative\nDecoding (IKOD), a collaborative decoding strategy generating more\nimage-focused sequences. This method derives logits from shorter sequences with\nhigher image attention through key-value merging and combines them with those\nfrom the original decoding, effectively mitigating attention degradation and\nsuppressing hallucinations while not incurring too much inference cost.\nExtensive experiments on both hallucination and comprehensive benchmarks\ndemonstrate IKOD's superior effectiveness in mitigating hallucinations and\nimproving comprehensive capacities for LVLMs. Importantly, IKOD requires no\nadditional training or external tools, making it a lightweight and efficient\nframework applicable to various models.",
        "url": "http://arxiv.org/abs/2508.03469v1",
        "published_date": "2025-08-05T14:05:15+00:00",
        "updated_date": "2025-08-05T14:05:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiabing Yang",
            "Chenhang Cui",
            "Yiyang Zhou",
            "Yixiang Chen",
            "Peng Xia",
            "Ying Wei",
            "Tao Yu",
            "Yan Huang",
            "Liang Wang"
        ],
        "tldr": "The paper identifies visual attention degradation as a cause of hallucinations in LVLMs and proposes IKOD, a lightweight decoding strategy, to mitigate this issue without additional training or external tools.",
        "tldr_zh": "该论文指出视觉注意力退化是LVLMs中幻觉现象的原因，并提出了IKOD，一种轻量级的解码策略，可以在不需要额外训练或外部工具的情况下缓解此问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling",
        "summary": "Existing vision-language models (VLMs), whether generalists or specialists,\nremain constrained by their parameter scale, lack robust self-correction\ncapabilities, and underperform in tasks involving long visual contexts and\ncomplex reasoning, resulting in suboptimal performance on document-based tasks.\nTo address this, we propose MACT, a Multi-Agent Collaboration framework with\nTest-Time scaling, tailored for visual document understanding and visual\nquestion answering (VQA). It comprises four distinct small-scale agents, i.e.,\nplanning, execution, judgment, and answer agents, with clearly defined roles\nand effective collaboration. Notably, the judgment agent exclusively verifies\ncorrectness and redirects to prior agents for revisions, outperforming\nconventional correction strategies. To further expand the capability boundaries\nof the framework, we propose mixed reward modeling that balances agent-specific\nabilities and global collaboration, as well as agent-wise hybrid test-time\nscaling, which customizes different scaling strategies for each agent based on\ntheir functions. Evaluated on benchmarks spanning both document-based and\nnon-document-based settings, our MACT shows superior performance with a smaller\nparameter scale without sacrificing the ability of general and mathematical\ntasks. Especially, it stands out in benchmarks involving long visual contexts\nand complicated reasoning. The three variants of MACT consistently hold the top\nthree positions in average scores, leading in 13 of the 15 benchmarks. Code\nwill be available at: https://github.com/YU-deep/MACT.git.",
        "url": "http://arxiv.org/abs/2508.03404v1",
        "published_date": "2025-08-05T12:52:09+00:00",
        "updated_date": "2025-08-05T12:52:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinlei Yu",
            "Zhangquan Chen",
            "Yudong Zhang",
            "Shilin Lu",
            "Ruolin Shen",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Yanwei Fu",
            "Shuicheng Yan"
        ],
        "tldr": "The paper introduces MACT, a multi-agent collaboration framework with test-time scaling for visual document understanding and VQA, which uses small-scale agents with defined roles and outperforms existing VLMs, especially in tasks requiring long visual contexts and complex reasoning.",
        "tldr_zh": "本文提出了MACT，一个用于视觉文档理解和VQA的多智能体协作框架，具有测试时缩放功能。它使用具有明确角色的小规模智能体，并且优于现有的VLM，尤其是在需要长视觉上下文和复杂推理的任务中。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models",
        "summary": "Federated Learning (FL) is an established paradigm for training deep learning\nmodels on decentralized data. However, as the size of the models grows,\nconventional FL approaches often require significant computational resources on\nclient devices, which may not be feasible. We introduce FedPromo, a novel\nframework that enables efficient adaptation of large-scale foundation models\nstored on a central server to new domains encountered only by remote clients.\nInstead of directly training the large model on client devices, FedPromo\noptimizes lightweight proxy models via FL, significantly reducing computational\noverhead while maintaining privacy. Our method follows a two-stage process:\nfirst, server-side knowledge distillation aligns the representations of a\nlarge-scale foundation model (e.g., a transformer) with those of a compact\ncounterpart (e.g., a CNN). Then, the compact model encoder is deployed to\nclient devices, where trainable classifiers are learned locally. These\nclassifiers are subsequently aggregated and seamlessly transferred back to the\nfoundation model, facilitating personalized adaptation without requiring direct\naccess to user data. Through novel regularization strategies, our framework\nenables decentralized multi-domain learning, balancing performance, privacy,\nand resource efficiency. Extensive experiments on five image classification\nbenchmarks demonstrate that FedPromo outperforms existing methods while\nassuming limited-resource clients.",
        "url": "http://arxiv.org/abs/2508.03356v1",
        "published_date": "2025-08-05T12:00:49+00:00",
        "updated_date": "2025-08-05T12:00:49+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Matteo Caligiuri",
            "Francesco Barbato",
            "Donald Shenaj",
            "Umberto Michieli",
            "Pietro Zanuttigh"
        ],
        "tldr": "FedPromo introduces a federated learning framework that adapts large foundation models to new domains on resource-constrained edge devices by training lightweight proxy models and transferring learned classifiers back to the central server.",
        "tldr_zh": "FedPromo 提出了一种联邦学习框架，通过训练轻量级代理模型并将学习到的分类器传输回中央服务器，使大型基础模型能够适应资源受限的边缘设备上的新领域。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation",
        "summary": "Post-training quantization (PTQ) has emerged as an effective approach for\ncompressing large models and accelerating their inference without retraining.\nWhile PTQ has been extensively studied in the context of large language models\n(LLMs), its applicability to vision-language models (VLMs) remains\nunderexplored. In this paper, we identify a modality discrepancy (\\emph{i.e.},\nlimited text tokens \\emph{vs.} excessive and redundant vision tokens) of VLMs.\nHowever, existing Hessian-based LLM PTQ methods treat all tokens equally during\nquantization, resulting in severe performance drops when applied to VLMs.\nMotivated by this observation, we propose a novel importance-aware PTQ\nframework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token\nredundancy, VLMQ 1) optimizes an importance-aware objective that yields an\nenhanced Hessian with token-level importance factors, while retaining\ncompatibility with parallelized weight updates, and 2) ensures efficiency and\neffectiveness by computing these factors via a single lightweight block-wise\nbackward pass, guided by a theoretical connection to token-level perturbations.\nExtensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the\nstate-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit\nsettings. For example, it achieves a substantial \\textbf{16.45\\%} improvement\non MME-RealWorld under 2-bit quantization.",
        "url": "http://arxiv.org/abs/2508.03351v1",
        "published_date": "2025-08-05T11:57:03+00:00",
        "updated_date": "2025-08-05T11:57:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yufei Xue",
            "Yushi Huang",
            "Jiawei Shao",
            "Jun Zhang"
        ],
        "tldr": "The paper introduces VLMQ, a novel post-training quantization framework for vision-language models that addresses modality discrepancy and achieves state-of-the-art performance, especially in low-bit settings.",
        "tldr_zh": "该论文介绍了VLMQ，一种新颖的视觉语言模型后训练量化框架，解决了模态差异问题，并实现了最先进的性能，尤其是在低比特设置下。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration",
        "summary": "The practical application of Multimodal Large Language Models (MLLMs) to\nVideo Question Answering (Video-QA) is severely hindered by the high token cost\nof processing numerous video frames. While increasing the number of sampled\nframes is a common strategy, we observe a \"less is more\" phenomenon where\nexcessive frames can paradoxically degrade performance due to context dilution.\nConcurrently, state-of-the-art keyframe selection methods, while effective,\nstill yield significant temporal redundancy, which we term 'visual echoes'. To\naddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel\npost-processing method that intelligently prunes the selected keyframes. AFP\nemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 and\nCLIP feature space to identify and merge these echoes into single\nrepresentatives. To compensate for information loss, we then introduce a\nlightweight, text-based semantic graph that provides critical context with\nminimal token overhead. Conducting extensive experiments on the LongVideoBench\nand VideoMME benchmarks across multiple leading MLLMs, our full approach\ndemonstrates a drastic reduction in required frames by up to 86.9% and total\ninput tokens by up to 83.2%. Crucially, by providing a concise, high-quality\nset of frames, our method not only enhances efficiency but often improves\naccuracy over baselines that use more frames. The code will be released upon\npublication.",
        "url": "http://arxiv.org/abs/2508.03337v1",
        "published_date": "2025-08-05T11:31:55+00:00",
        "updated_date": "2025-08-05T11:31:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaoguang Wang",
            "Jianxiang He",
            "Yijie Xu",
            "Ziyang Chen",
            "Weiyu Guo",
            "Hui Xiong"
        ],
        "tldr": "This paper introduces Adaptive Frame-Pruning (AFP) to reduce token cost and improve accuracy in Video-QA tasks by intelligently pruning redundant frames and integrating a text-based semantic graph, achieving significant frame and token reduction while maintaining or improving accuracy.",
        "tldr_zh": "本文介绍了一种自适应帧修剪（AFP）方法，通过智能修剪冗余帧并整合基于文本的语义图，来降低视频问答（Video-QA）任务中的token成本并提高准确性，在保持或提高准确性的同时，显著减少帧数和token数量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation",
        "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
        "url": "http://arxiv.org/abs/2508.03320v1",
        "published_date": "2025-08-05T10:59:01+00:00",
        "updated_date": "2025-08-05T10:59:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiyu Wang",
            "Yi Peng",
            "Yimeng Gan",
            "Liang Hu",
            "Tianyidan Xie",
            "Xiaokun Wang",
            "Yichen Wei",
            "Chuanxin Tang",
            "Bo Zhu",
            "Changshi Li",
            "Hongyang Wei",
            "Eric Li",
            "Xuchen Song",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "tldr": "Skywork UniPic is a 1.5B parameter autoregressive model unifying image understanding, text-to-image generation, and image editing, achieving SOTA performance on commodity hardware with curated datasets and a resolution-aware training schedule.",
        "tldr_zh": "Skywork UniPic 是一个 1.5B 参数的自回归模型，统一了图像理解、文本到图像生成和图像编辑。它在商业硬件上实现了 SOTA 性能，并采用精心策划的数据集和分辨率感知训练计划。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations",
        "summary": "Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable\nability to infer users' locations from public shared images, posing a\nsubstantial risk to geoprivacy. Although adversarial perturbations offer a\npotential defense, current methods are ill-suited for this scenario: they often\nperform poorly on high-resolution images and low perturbation budgets, and may\nintroduce irrelevant semantic content. To address these limitations, we propose\nGeoShield, a novel adversarial framework designed for robust geoprivacy\nprotection in real-world scenarios. GeoShield comprises three key modules: a\nfeature disentanglement module that separates geographical and non-geographical\ninformation, an exposure element identification module that pinpoints\ngeo-revealing regions within an image, and a scale-adaptive enhancement module\nthat jointly optimizes perturbations at both global and local levels to ensure\neffectiveness across resolutions. Extensive experiments on challenging\nbenchmarks show that GeoShield consistently surpasses prior methods in\nblack-box settings, achieving strong privacy protection with minimal impact on\nvisual or semantic quality. To our knowledge, this work is the first to explore\nadversarial perturbations for defending against geolocation inference by\nadvanced VLMs, providing a practical and effective solution to escalating\nprivacy concerns.",
        "url": "http://arxiv.org/abs/2508.03209v1",
        "published_date": "2025-08-05T08:37:06+00:00",
        "updated_date": "2025-08-05T08:37:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinwei Liu",
            "Xiaojun Jia",
            "Yuan Xun",
            "Simeng Qin",
            "Xiaochun Cao"
        ],
        "tldr": "The paper introduces GeoShield, a novel adversarial framework that protects geolocation privacy against VLMs by using adversarial perturbations designed to minimize visual and semantic impact while effectively obfuscating geo-revealing information in images.",
        "tldr_zh": "该论文介绍了一种名为GeoShield的新型对抗框架，通过使用对抗性扰动来保护地理位置隐私，以最大限度地减少视觉和语义影响，同时有效地模糊图像中泄露地理信息的区域，从而防御视觉语言模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration",
        "summary": "Open Vocabulary Human-Object Interaction (HOI) detection aims to detect\ninteractions between humans and objects while generalizing to novel interaction\nclasses beyond the training set. Current methods often rely on Vision and\nLanguage Models (VLMs) but face challenges due to suboptimal image encoders, as\nimage-level pre-training does not align well with the fine-grained region-level\ninteraction detection required for HOI. Additionally, effectively encoding\ntextual descriptions of visual appearances remains difficult, limiting the\nmodel's ability to capture detailed HOI relationships. To address these issues,\nwe propose INteraction-aware Prompting with Concept Calibration (INP-CC), an\nend-to-end open-vocabulary HOI detector that integrates interaction-aware\nprompts and concept calibration. Specifically, we propose an interaction-aware\nprompt generator that dynamically generates a compact set of prompts based on\nthe input scene, enabling selective sharing among similar interactions. This\napproach directs the model's attention to key interaction patterns rather than\ngeneric image-level semantics, enhancing HOI detection. Furthermore, we refine\nHOI concept representations through language model-guided calibration, which\nhelps distinguish diverse HOI concepts by investigating visual similarities\nacross categories. A negative sampling strategy is also employed to improve\ninter-modal similarity modeling, enabling the model to better differentiate\nvisually similar but semantically distinct actions. Extensive experimental\nresults demonstrate that INP-CC significantly outperforms state-of-the-art\nmodels on the SWIG-HOI and HICO-DET datasets. Code is available at\nhttps://github.com/ltttpku/INP-CC.",
        "url": "http://arxiv.org/abs/2508.03207v1",
        "published_date": "2025-08-05T08:33:58+00:00",
        "updated_date": "2025-08-05T08:33:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ting Lei",
            "Shaofeng Yin",
            "Qingchao Chen",
            "Yuxin Peng",
            "Yang Liu"
        ],
        "tldr": "This paper introduces INP-CC, a novel open-vocabulary HOI detection method using interaction-aware prompting and concept calibration to improve performance on SWIG-HOI and HICO-DET datasets.",
        "tldr_zh": "本文介绍了INP-CC，一种新颖的开放词汇HOI检测方法，它使用交互感知提示和概念校准来提高在SWIG-HOI和HICO-DET数据集上的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision",
        "summary": "Large Vision-Language Models (LVLMs) recently achieve significant\nbreakthroughs in understanding complex visual-textual contexts. However,\nhallucination issues still limit their real-world applicability. Although\nprevious mitigation methods effectively reduce hallucinations in photographic\nimages, they largely overlook the potential risks posed by stylized images,\nwhich play crucial roles in critical scenarios such as game scene\nunderstanding, art education, and medical analysis. In this work, we first\nconstruct a dataset comprising photographic images and their corresponding\nstylized versions with carefully annotated caption labels. We then conduct\nhead-to-head comparisons on both discriminative and generative tasks by\nbenchmarking 13 advanced LVLMs on the collected datasets. Our findings reveal\nthat stylized images tend to induce significantly more hallucinations than\ntheir photographic counterparts. To address this issue, we propose Style-Aware\nVisual Early Revision SAVER, a novel mechanism that dynamically adjusts LVLMs'\nfinal outputs based on the token-level visual attention patterns, leveraging\nearly-layer feedback to mitigate hallucinations caused by stylized images.\nExtensive experiments demonstrate that SAVER achieves state-of-the-art\nperformance in hallucination mitigation across various models, datasets, and\ntasks.",
        "url": "http://arxiv.org/abs/2508.03177v1",
        "published_date": "2025-08-05T07:41:25+00:00",
        "updated_date": "2025-08-05T07:41:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoxu Li",
            "Chenqi Kong",
            "Yi Yu",
            "Qiangqiang Wu",
            "Xinghao Jiang",
            "Ngai-Man Cheung",
            "Bihan Wen",
            "Alex Kot",
            "Xudong Jiang"
        ],
        "tldr": "The paper introduces SAVER, a novel mechanism to mitigate hallucinations in Large Vision-Language Models (LVLMs), especially when dealing with stylized images, by dynamically adjusting outputs based on token-level visual attention.",
        "tldr_zh": "该论文介绍了一种名为SAVER的新机制，通过基于token级别的视觉注意力动态调整输出，以减轻大型视觉语言模型(LVLMs)中的幻觉问题，尤其是在处理风格化图像时。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying",
        "summary": "In recent years, unified vision-language models (VLMs) have rapidly advanced,\neffectively tackling both visual understanding and generation tasks within a\nsingle design. While many unified VLMs have explored various design choices,\nthe recent hypothesis from OpenAI's GPT-4o suggests a promising generation\npipeline: Understanding VLM->Visual Feature->Projector->Diffusion Model->Image.\nThe understanding VLM is frozen, and only the generation-related modules are\ntrained. This pipeline maintains the strong capability of understanding VLM\nwhile enabling the image generation ability of the unified VLM. Although this\npipeline has shown very promising potential for the future development of\nunified VLM, how to easily enable image editing capability is still unexplored.\nIn this paper, we introduce a novel training-free framework named UniEdit-I to\nenable the unified VLM with image editing capability via three iterative steps:\nunderstanding, editing, and verifying. 1. The understanding step analyzes the\nsource image to create a source prompt through structured semantic analysis and\nmakes minimal word replacements to form the target prompt based on the editing\ninstruction. 2. The editing step introduces a time-adaptive offset, allowing\nfor coherent editing from coarse to fine throughout the denoising process. 3.\nThe verification step checks the alignment between the target prompt and the\nintermediate edited image, provides automatic consistency scores and corrective\nfeedback, and determines whether to stop early or continue the editing loop.\nThis understanding, editing, and verifying loop iterates until convergence,\ndelivering high-fidelity editing in a training-free manner. We implemented our\nmethod based on the latest BLIP3-o and achieved state-of-the-art (SOTA)\nperformance on the GEdit-Bench benchmark.",
        "url": "http://arxiv.org/abs/2508.03142v1",
        "published_date": "2025-08-05T06:42:09+00:00",
        "updated_date": "2025-08-05T06:42:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengyu Bai",
            "Jintao Chen",
            "Xiang Bai",
            "Yilong Chen",
            "Qi She",
            "Ming Lu",
            "Shanghang Zhang"
        ],
        "tldr": "The paper introduces UniEdit-I, a training-free image editing framework for unified VLMs that utilizes an iterative understanding, editing, and verifying process, achieving SOTA performance on GEdit-Bench based on BLIP3-o.",
        "tldr_zh": "该论文介绍了UniEdit-I，一个用于统一视觉语言模型的免训练图像编辑框架，它利用迭代理解、编辑和验证过程，并在基于BLIP3-o的GEdit-Bench上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery",
        "summary": "Vision language models (VLMs) that enable natural language interaction with\nsatellite imagery can democratize Earth observation by accelerating expert\nworkflows, making data accessible to non-specialists, and enabling planet-scale\nautomation. However, existing datasets focus mainly on short-term,\nhigh-resolution imagery from a limited number of satellites, overlooking\nlow-resolution, multi-satellite, long-term archives, such as Landsat, that are\nessential for affordable and bias-robust global monitoring. We address this gap\nwith Landsat30-AU, a large-scale vision-language dataset built from 30-meter\nresolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over\nAustralia, spanning more than 36 years. The dataset includes two components:\nLandsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA,\ncomprising 17,725 human-verified visual question answering (VQA) samples across\neight remote sensing domains. Both datasets are curated through a bootstrapped\npipeline that leverages generic VLMs with iterative refinement and human\nverification to ensure quality. Our evaluation of eight VLMs on our benchmark\nreveals that off-the-shelf models struggle to understand satellite imagery. The\nopen-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in\ncaptioning and a VQA accuracy of 0.48, highlighting the limitations of current\napproaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on\nLandsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and\nboosts VQA accuracy from \\textbf{0.74} to 0.87. Code and data are available at\nhttps://github.com/papersubmit1/landsat30-au.",
        "url": "http://arxiv.org/abs/2508.03127v1",
        "published_date": "2025-08-05T06:16:46+00:00",
        "updated_date": "2025-08-05T06:16:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sai Ma",
            "Zhuang Li",
            "John A Taylor"
        ],
        "tldr": "The paper introduces Landsat30-AU, a large-scale vision-language dataset built from Australian Landsat imagery for training and evaluating VLMs, revealing current models' limitations and demonstrating performance improvements through fine-tuning.",
        "tldr_zh": "该论文介绍了 Landsat30-AU，这是一个基于澳大利亚 Landsat 图像构建的大规模视觉语言数据集，用于训练和评估 VLMs，揭示了当前模型的局限性，并通过微调展示了性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning",
        "summary": "Few-shot learning (FSL) often requires effective adaptation of models using\nlimited labeled data. However, most existing FSL methods rely on entangled\nrepresentations, requiring the model to implicitly recover the unmixing process\nto obtain disentangled representations using only limited supervision, which\nhinders effective adaptation. Recent theoretical studies show that multimodal\ncontrastive learning methods, such as CLIP, can disentangle latent\nrepresentations up to linear transformations. In light of this, we propose the\nCausal CLIP Adapter (CCA), a novel framework that explicitly disentangles\nvisual features extracted from CLIP using unsupervised Independent Component\nAnalysis (ICA). This removes the need to learn the unmixing process from the\nlabeled data, thereby reducing the number of trainable parameters and\nmitigating overfitting. Taking a step further, while ICA can obtain visual\ndisentangled representations, it may also disrupt CLIP's intra- and inter-modal\nalignment. To counteract this, CCA further leverages CLIP's inherent\ncross-modal alignment by enhancing it in two ways: unidirectionally, through\nfine-tuning a CLIP-based text classifier, and bidirectionally, via a\ncross-attention mechanism that enriches visual and textual representations\nthrough mutual interaction. Both unimodal and cross-modal classification\noutputs can be effectively combined linearly to improve classification\naccuracy. Extensive experiments on 11 benchmark datasets demonstrate that our\nmethod consistently outperforms state-of-the-art approaches in terms of\nfew-shot performance and robustness to distributional shifts, while maintaining\ncomputational efficiency. Code will be available at\nhttps://github.com/tianjiao-j/CCA.",
        "url": "http://arxiv.org/abs/2508.03102v1",
        "published_date": "2025-08-05T05:30:42+00:00",
        "updated_date": "2025-08-05T05:30:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianjiao Jiang",
            "Zhen Zhang",
            "Yuhang Liu",
            "Javen Qinfeng Shi"
        ],
        "tldr": "The paper introduces Causal CLIP Adapter (CCA), a novel few-shot learning framework that disentangles visual features using ICA and enhances cross-modal alignment in CLIP for improved performance and robustness. It achieves state-of-the-art results on multiple datasets.",
        "tldr_zh": "本文提出了一种名为Causal CLIP Adapter (CCA) 的新型少样本学习框架，该框架使用ICA解耦视觉特征，并增强CLIP中的跨模态对齐，从而提高性能和鲁棒性。它在多个数据集上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "T2UE: Generating Unlearnable Examples from Text Descriptions",
        "summary": "Large-scale pre-training frameworks like CLIP have revolutionized multimodal\nlearning, but their reliance on web-scraped datasets, frequently containing\nprivate user data, raises serious concerns about misuse. Unlearnable Examples\n(UEs) have emerged as a promising countermeasure against unauthorized model\ntraining, employing carefully crafted unlearnable noise to disrupt the learning\nof meaningful representations from protected data. Current approaches typically\ngenerate UEs by jointly optimizing unlearnable noise for both images and their\nassociated text descriptions (or labels). However, this optimization process is\noften computationally prohibitive for on-device execution, forcing reliance on\nexternal third-party services. This creates a fundamental privacy paradox:\nusers must initially expose their data to these very services to achieve\nprotection, thereby compromising privacy in the process. Such a contradiction\nhas severely hindered the development of practical, scalable data protection\nsolutions. To resolve this paradox, we introduce \\textbf{Text-to-Unlearnable\nExample (T2UE)}, a novel framework that enables users to generate UEs using\nonly text descriptions. T2UE circumvents the need for original image data by\nemploying a text-to-image (T2I) model to map text descriptions into the image\n(noise) space, combined with an error-minimization framework to produce\neffective unlearnable noise. Extensive experiments show that T2UE-protected\ndata substantially degrades performance in downstream tasks (e.g., cross-modal\nretrieval) for state-of-the-art models. Notably, the protective effect\ngeneralizes across diverse architectures and even to supervised learning\nsettings. Our work demonstrates the feasibility of \"zero-contact data\nprotection\", where personal data can be safeguarded based solely on their\ntextual descriptions, eliminating the need for direct data exposure.",
        "url": "http://arxiv.org/abs/2508.03091v1",
        "published_date": "2025-08-05T05:10:14+00:00",
        "updated_date": "2025-08-05T05:10:14+00:00",
        "categories": [
            "cs.AI",
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Xingjun Ma",
            "Hanxun Huang",
            "Tianwei Song",
            "Ye Sun",
            "Yifeng Gao",
            "Yu-Gang Jiang"
        ],
        "tldr": "The paper introduces T2UE, a novel framework that generates unlearnable examples from text descriptions, addressing the privacy concerns associated with current methods that require exposing data to third-party services.",
        "tldr_zh": "该论文介绍了一种名为T2UE的新框架，该框架可以通过文本描述生成不可学习的样本，从而解决了当前需要将数据暴露给第三方服务的方法所带来的隐私问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction",
        "summary": "This paper introduces VisuCraft, a novel framework designed to significantly\nenhance the capabilities of Large Vision-Language Models (LVLMs) in complex\nvisual-guided creative content generation. Existing LVLMs often exhibit\nlimitations in maintaining high visual fidelity, genuine creativity, and\nprecise adherence to nuanced user instructions when generating long-form texts.\nVisuCraft addresses these challenges by integrating a multimodal structured\ninformation extractor (E) and a dynamic prompt generation module (G). The\nextractor distills fine-grained visual attributes from input images into a\nrich, structured representation, which the dynamic prompt module then combines\nwith user instructions to create highly optimized prompts for underlying LVLMs\n(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed\nImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,\nand Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs\nacross tasks like story generation and poetry composition. Our results\ndemonstrate remarkable improvements, particularly in creativity and instruction\nadherence, validating VisuCraft's effectiveness in producing imaginative,\nvisually grounded, and user-aligned long-form creative text. This work unlocks\nnew potential for LVLMs in sophisticated creative AI applications.",
        "url": "http://arxiv.org/abs/2508.02890v1",
        "published_date": "2025-08-04T20:36:55+00:00",
        "updated_date": "2025-08-04T20:36:55+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Rongxin Jiang",
            "Robert Long",
            "Chenghao Gu",
            "Mingrui Yan"
        ],
        "tldr": "VisuCraft enhances LVLMs for creative content generation by extracting structured visual information and using it to create optimized prompts, improving creativity and instruction adherence. It is evaluated on a new dataset ImageStoryGen-500K with new metric VisuGen Metrics.",
        "tldr_zh": "VisuCraft通过提取结构化的视觉信息并用它来创建优化的提示，从而增强LVLM的创造性内容生成能力，从而提高创造力和指令遵循度。它在新的数据集ImageStoryGen-500K上使用新的度量标准VisuGen Metrics进行评估。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection",
        "summary": "Despite substantial progress in anomaly synthesis methods, existing\ndiffusion-based and coarse inpainting pipelines commonly suffer from structural\ndeficiencies such as micro-structural discontinuities, limited semantic\ncontrollability, and inefficient generation. To overcome these limitations, we\nintroduce ARAS, a language-conditioned, auto-regressive anomaly synthesis\napproach that precisely injects local, text-specified defects into normal\nimages via token-anchored latent editing. Leveraging a hard-gated\nauto-regressive operator and a training-free, context-preserving masked\nsampling kernel, ARAS significantly enhances defect realism, preserves\nfine-grained material textures, and provides continuous semantic control over\nsynthesized anomalies. Integrated within our Quality-Aware Re-weighted Anomaly\nDetection (QARAD) framework, we further propose a dynamic weighting strategy\nthat emphasizes high-quality synthetic samples by computing an image-text\nsimilarity score with a dual-encoder model. Extensive experiments across three\nbenchmark datasets-MVTec AD, VisA, and BTAD, demonstrate that our QARAD\noutperforms SOTA methods in both image- and pixel-level anomaly detection\ntasks, achieving improved accuracy, robustness, and a 5 times synthesis speedup\ncompared to diffusion-based alternatives. Our complete code and synthesized\ndataset will be publicly available.",
        "url": "http://arxiv.org/abs/2508.03539v1",
        "published_date": "2025-08-05T15:07:32+00:00",
        "updated_date": "2025-08-05T15:07:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Long Qian",
            "Bingke Zhu",
            "Yingying Chen",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "tldr": "The paper introduces ARAS and QARAD, a language-conditioned anomaly synthesis and detection framework using auto-regressive anomaly insertion and quality-aware re-weighting, achieving SOTA performance with faster synthesis speeds.",
        "tldr_zh": "该论文介绍了ARAS和QARAD，一个语言条件下的异常合成与检测框架，使用自回归异常插入和质量感知重加权，实现了SOTA性能并加快了合成速度。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EditGarment: An Instruction-Based Garment Editing Dataset Constructed with Automated MLLM Synthesis and Semantic-Aware Evaluation",
        "summary": "Instruction-based garment editing enables precise image modifications via\nnatural language, with broad applications in fashion design and customization.\nUnlike general editing tasks, it requires understanding garment-specific\nsemantics and attribute dependencies. However, progress is limited by the\nscarcity of high-quality instruction-image pairs, as manual annotation is\ncostly and hard to scale. While MLLMs have shown promise in automated data\nsynthesis, their application to garment editing is constrained by imprecise\ninstruction modeling and a lack of fashion-specific supervisory signals. To\naddress these challenges, we present an automated pipeline for constructing a\ngarment editing dataset. We first define six editing instruction categories\naligned with real-world fashion workflows to guide the generation of balanced\nand diverse instruction-image triplets. Second, we introduce Fashion Edit\nScore, a semantic-aware evaluation metric that captures semantic dependencies\nbetween garment attributes and provides reliable supervision during\nconstruction. Using this pipeline, we construct a total of 52,257 candidate\ntriplets and retain 20,596 high-quality triplets to build EditGarment, the\nfirst instruction-based dataset tailored to standalone garment editing. The\nproject page is https://yindq99.github.io/EditGarment-project/.",
        "url": "http://arxiv.org/abs/2508.03497v1",
        "published_date": "2025-08-05T14:28:45+00:00",
        "updated_date": "2025-08-05T14:28:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Deqiang Yin",
            "Junyi Guo",
            "Huanda Lu",
            "Fangyu Wu",
            "Dongming Lu"
        ],
        "tldr": "The paper introduces EditGarment, a new instruction-based dataset for garment editing, constructed using an automated MLLM synthesis pipeline and a novel semantic-aware evaluation metric called Fashion Edit Score.",
        "tldr_zh": "本文介绍了一个新的基于指令的服装编辑数据集EditGarment。该数据集使用自动MLLM合成流程和一个名为Fashion Edit Score的语义感知评估指标构建。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection",
        "summary": "Recently, large pre-trained vision-language models have shown remarkable\nperformance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single\nauxiliary dataset, the model enables cross-category anomaly detection on\ndiverse datasets covering industrial defects and medical lesions. Compared to\nmanually designed prompts, prompt learning eliminates the need for expert\nknowledge and trial-and-error. However, it still faces the following\nchallenges: (i) static learnable tokens struggle to capture the continuous and\ndiverse patterns of normal and anomalous states, limiting generalization to\nunseen categories; (ii) fixed textual labels provide overly sparse category\ninformation, making the model prone to overfitting to a specific semantic\nsubspace. To address these issues, we propose Conditional Prompt Synthesis\n(CoPS), a novel framework that synthesizes dynamic prompts conditioned on\nvisual features to enhance ZSAD performance. Specifically, we extract\nrepresentative normal and anomaly prototypes from fine-grained patch features\nand explicitly inject them into prompts, enabling adaptive state modeling.\nGiven the sparsity of class labels, we leverage a variational autoencoder to\nmodel semantic image features and implicitly fuse varied class tokens into\nprompts. Additionally, integrated with our spatially-aware alignment mechanism,\nextensive experiments demonstrate that CoPS surpasses state-of-the-art methods\nby 2.5% AUROC in both classification and segmentation across 13 industrial and\nmedical datasets. Code will be available at https://github.com/cqylunlun/CoPS.",
        "url": "http://arxiv.org/abs/2508.03447v1",
        "published_date": "2025-08-05T13:47:45+00:00",
        "updated_date": "2025-08-05T13:47:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyu Chen",
            "Zhen Qu",
            "Wei Luo",
            "Haiming Yao",
            "Yunkang Cao",
            "Yuxin Jiang",
            "Yinan Duan",
            "Huiyuan Luo",
            "Chengkan Lv",
            "Zhengtao Zhang"
        ],
        "tldr": "The paper introduces CoPS, a conditional prompt synthesis framework for zero-shot anomaly detection that uses visual feature-conditioned prompts to improve performance, achieving state-of-the-art results on industrial and medical datasets.",
        "tldr_zh": "本文介绍了一种条件提示合成框架CoPS，用于零样本异常检测，该框架使用视觉特征调节的提示来提高性能，并在工业和医疗数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models",
        "summary": "With growing interest in deploying text-to-video (T2V) models in\nresource-constrained environments, reducing their high computational cost has\nbecome crucial, leading to extensive research on pruning and knowledge\ndistillation methods while maintaining performance. However, existing\ndistillation methods primarily rely on supervised fine-tuning (SFT), which\noften leads to mode collapse as pruned models with reduced capacity fail to\ndirectly match the teacher's outputs, ultimately resulting in degraded quality.\nTo address this challenge, we propose an effective distillation method, ReDPO,\nthat integrates DPO and SFT. Our approach leverages DPO to guide the student\nmodel to focus on recovering only the targeted properties, rather than\npassively imitating the teacher, while also utilizing SFT to enhance overall\nperformance. We additionally propose V.I.P., a novel framework for filtering\nand curating high-quality pair datasets, along with a step-by-step online\napproach for calibrated training. We validate our method on two leading T2V\nmodels, VideoCrafter2 and AnimateDiff, achieving parameter reduction of 36.2%\nand 67.5% each, while maintaining or even surpassing the performance of full\nmodels. Further experiments demonstrate the effectiveness of both ReDPO and\nV.I.P. framework in enabling efficient and high-quality video generation. Our\ncode and videos are available at https://jiiiisoo.github.io/VIP.github.io/.",
        "url": "http://arxiv.org/abs/2508.03254v1",
        "published_date": "2025-08-05T09:31:54+00:00",
        "updated_date": "2025-08-05T09:31:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jisoo Kim",
            "Wooseok Seo",
            "Junwan Kim",
            "Seungho Park",
            "Sooyeon Park",
            "Youngjae Yu"
        ],
        "tldr": "This paper introduces V.I.P., a novel framework with ReDPO for efficiently distilling text-to-video diffusion models by combining DPO and SFT, achieving significant parameter reduction while maintaining or improving performance.",
        "tldr_zh": "本文介绍了一种名为V.I.P.的新框架，该框架结合了DPO和SFT，通过ReDPO有效地精馏文本到视频扩散模型，在保持或提高性能的同时，显著减少了参数量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models",
        "summary": "The rapid expansion of applications using Large Vision-Language Models\n(LVLMs), such as GPT-4o, has raised significant concerns about their fairness.\nWhile existing studies primarily focus on demographic attributes such as race\nand gender, fairness across a broader range of attributes remains largely\nunexplored. In this study, we construct an open-set knowledge base of bias\nattributes leveraging Large Language Models (LLMs) and evaluate the fairness of\nLVLMs across finer-grained attributes. Our experimental results reveal that\nLVLMs exhibit biased outputs across a diverse set of attributes and further\ndemonstrate that cultural, environmental, and behavioral factors have a more\npronounced impact on LVLM decision-making than traditional demographic\nattributes.",
        "url": "http://arxiv.org/abs/2508.03079v1",
        "published_date": "2025-08-05T04:52:32+00:00",
        "updated_date": "2025-08-05T04:52:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zaiying Zhao",
            "Toshihiko Yamasaki"
        ],
        "tldr": "This paper investigates fairness in Large Vision-Language Models (LVLMs) across a diverse set of fine-grained attributes beyond traditional demographics, revealing significant biases related to cultural, environmental, and behavioral factors.",
        "tldr_zh": "本文研究了大型视觉语言模型（LVLMs）在传统人口统计学之外的各种细粒度属性上的公平性，揭示了与文化、环境和行为因素相关的重大偏见。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces",
        "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task.",
        "url": "http://arxiv.org/abs/2508.02917v1",
        "published_date": "2025-08-04T21:45:21+00:00",
        "updated_date": "2025-08-04T21:45:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.RO"
        ],
        "authors": [
            "Vebjørn Haug Kåsene",
            "Pierre Lison"
        ],
        "tldr": "This paper explores the effectiveness of fine-tuning off-the-shelf LVLMs for Vision-and-Language Navigation (VLN) using both low-level and panoramic action spaces, finding that while they can perform VLN, they underperform models specifically designed for the task.",
        "tldr_zh": "本文探讨了微调现成的LVLM在视觉-语言导航(VLN)中使用低级和全景动作空间的有效性，发现它们可以执行VLN，但性能低于专门为此任务设计的模型。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework",
        "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. \\textbf{In the second stage}, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/",
        "url": "http://arxiv.org/abs/2508.02807v1",
        "published_date": "2025-08-04T18:27:55+00:00",
        "updated_date": "2025-08-04T18:27:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tongchun Zuo",
            "Zaiyu Huang",
            "Shuliang Ning",
            "Ente Lin",
            "Chao Liang",
            "Zerong Zheng",
            "Jianwen Jiang",
            "Yuan Zhang",
            "Mingyuan Gao",
            "Xin Dong"
        ],
        "tldr": "DreamVVT, a two-stage Diffusion Transformer framework, addresses challenges in video virtual try-on by leveraging unpaired data and pretrained models to generate realistic and temporally coherent results in unconstrained scenarios.",
        "tldr_zh": "DreamVVT是一个两阶段的Diffusion Transformer框架，通过利用非配对数据和预训练模型，解决了视频虚拟试穿中的挑战，从而在不受约束的场景中生成逼真且时间连贯的结果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Can Large Vision-Language Models Understand Multimodal Sarcasm?",
        "summary": "Sarcasm is a complex linguistic phenomenon that involves a disparity between\nliteral and intended meanings, making it challenging for sentiment analysis and\nother emotion-sensitive tasks. While traditional sarcasm detection methods\nprimarily focus on text, recent approaches have incorporated multimodal\ninformation. However, the application of Large Visual Language Models (LVLMs)\nin Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we\nevaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm\nDetection and Multimodal Sarcasm Explanation. Through comprehensive\nexperiments, we identify key limitations, such as insufficient visual\nunderstanding and a lack of conceptual knowledge. To address these issues, we\npropose a training-free framework that integrates in-depth object extraction\nand external conceptual knowledge to improve the model's ability to interpret\nand explain sarcasm in multimodal contexts. The experimental results on\nmultiple models show the effectiveness of our proposed framework. The code is\navailable at https://github.com/cp-cp/LVLM-MSA.",
        "url": "http://arxiv.org/abs/2508.03654v1",
        "published_date": "2025-08-05T17:05:11+00:00",
        "updated_date": "2025-08-05T17:05:11+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Xinyu Wang",
            "Yue Zhang",
            "Liqiang Jing"
        ],
        "tldr": "This paper explores the performance of Large Vision-Language Models (LVLMs) in multimodal sarcasm analysis, identifies limitations, and proposes a training-free framework incorporating object extraction and external knowledge to improve sarcasm understanding and explanation.",
        "tldr_zh": "本文探讨了大型视觉语言模型（LVLMs）在多模态讽刺分析中的表现，指出了其局限性，并提出了一个无需训练的框架，该框架结合了对象提取和外部知识，以提高讽刺理解和解释能力。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]