[
    {
        "title": "S-Chain: Structured Visual Chain-of-Thought For Medicine",
        "summary": "Faithful reasoning in medical vision-language models (VLMs) requires not only\naccurate predictions but also transparent alignment between textual rationales\nand visual evidence. While Chain-of-Thought (CoT) prompting has shown promise\nin medical visual question answering (VQA), no large-scale expert-level dataset\nhas captured stepwise reasoning with precise visual grounding. We introduce\nS-Chain, the first large-scale dataset of 12,000 expert-annotated medical\nimages with bounding boxes and structured visual CoT (SV-CoT), explicitly\nlinking visual regions to reasoning steps. The dataset further supports 16\nlanguages, totaling over 700k VQA pairs for broad multilingual applicability.\nUsing S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,\nLLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that\nSV-CoT supervision significantly improves interpretability, grounding fidelity,\nand robustness. Beyond benchmarking, we study its synergy with\nretrieval-augmented generation, revealing how domain knowledge and visual\ngrounding interact during autoregressive reasoning. Finally, we propose a new\nmechanism that strengthens the alignment between visual evidence and reasoning,\nimproving both reliability and efficiency. S-Chain establishes a new benchmark\nfor grounded medical reasoning and paves the way toward more trustworthy and\nexplainable medical VLMs.",
        "url": "http://arxiv.org/abs/2510.22728v1",
        "published_date": "2025-10-26T15:57:14+00:00",
        "updated_date": "2025-10-26T15:57:14+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Khai Le-Duc",
            "Duy M. H. Nguyen",
            "Phuong T. H. Trinh",
            "Tien-Phat Nguyen",
            "Nghiem T. Diep",
            "An Ngo",
            "Tung Vu",
            "Trinh Vuong",
            "Anh-Tien Nguyen",
            "Mau Nguyen",
            "Van Trung Hoang",
            "Khai-Nguyen Nguyen",
            "Hy Nguyen",
            "Chris Ngo",
            "Anji Liu",
            "Nhat Ho",
            "Anne-Christin Hauschild",
            "Khanh Xuan Nguyen",
            "Thanh Nguyen-Tang",
            "Pengtao Xie",
            "Daniel Sonntag",
            "James Zou",
            "Mathias Niepert",
            "Anh Totti Nguyen"
        ],
        "tldr": "This paper introduces S-Chain, a large-scale dataset of expert-annotated medical images with structured visual Chain-of-Thought annotations, and demonstrates its effectiveness in improving the interpretability, grounding fidelity, and robustness of medical VLMs.",
        "tldr_zh": "本文介绍了S-Chain，一个大规模专家标注的医学图像数据集，带有结构化的视觉思维链注释，并证明了其在提高医学视觉语言模型的解释性、基础保真度和鲁棒性方面的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Top-Down Semantic Refinement for Image Captioning",
        "summary": "Large Vision-Language Models (VLMs) face an inherent contradiction in image\ncaptioning: their powerful single-step generation capabilities often lead to a\nmyopic decision-making process. This makes it difficult to maintain global\nnarrative coherence while capturing rich details, a limitation that is\nparticularly pronounced in tasks that require multi-step and complex scene\ndescription. To overcome this fundamental challenge, we redefine image\ncaptioning as a goal-oriented hierarchical refinement planning problem, and\nfurther propose a novel framework, named Top-Down Semantic Refinement (TDSR),\nwhich models the generation process as a Markov Decision Process (MDP).\nHowever, planning within the vast state space of a VLM presents a significant\ncomputational hurdle. Our core contribution, therefore, is the design of a\nhighly efficient Monte Carlo Tree Search (MCTS) algorithm tailored for VLMs. By\nincorporating a visual-guided parallel expansion and a lightweight value\nnetwork, our TDSR reduces the call frequency to the expensive VLM by an order\nof magnitude without sacrificing planning quality. Furthermore, an adaptive\nearly stopping mechanism dynamically matches computational overhead to the\nimage's complexity. Extensive experiments on multiple benchmarks, including\nDetailCaps, COMPOSITIONCAP, and POPE, demonstrate that our TDSR, as a\nplug-and-play module, can significantly enhance the performance of existing\nVLMs (e.g., LLaVA-1.5, Qwen2.5-VL) by achieving state-of-the-art or highly\ncompetitive results in fine-grained description, compositional generalization,\nand hallucination suppression.",
        "url": "http://arxiv.org/abs/2510.22391v1",
        "published_date": "2025-10-25T18:27:00+00:00",
        "updated_date": "2025-10-25T18:27:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jusheng Zhang",
            "Kaitong Cai",
            "Jing Yang",
            "Jian Wang",
            "Chengpei Tang",
            "Keze Wang"
        ],
        "tldr": "The paper introduces Top-Down Semantic Refinement (TDSR), a framework that uses Monte Carlo Tree Search to improve the performance of VLMs in image captioning by addressing issues with global narrative coherence and detail capture. It achieves state-of-the-art results on several benchmarks.",
        "tldr_zh": "该论文介绍了一种名为Top-Down Semantic Refinement (TDSR)的框架，该框架使用蒙特卡洛树搜索来提升VLMs在图像描述方面的性能，通过解决全局叙事连贯性和细节捕捉问题。该方法在多个基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring",
        "summary": "We introduce the {\\em Atlas Urban Index} (AUI), a metric for measuring urban\ndevelopment computed using Sentinel-2 \\citep{spoto2012sentinel2} satellite\nimagery. Existing approaches, such as the {\\em Normalized Difference Built-up\nIndex} (NDBI), often struggle to accurately capture urban development due to\nfactors like atmospheric noise, seasonal variation, and cloud cover. These\nlimitations hinder large-scale monitoring of human development and\nurbanization. To address these challenges, we propose an approach that\nleverages {\\em Vision-Language Models }(VLMs) to provide a development score\nfor regions. Specifically, we collect a time series of Sentinel-2 images for\neach region. Then, we further process the images within fixed time windows to\nget an image with minimal cloud cover, which serves as the representative image\nfor that time window. To ensure consistent scoring, we adopt two strategies:\n(i) providing the VLM with a curated set of reference images representing\ndifferent levels of urbanization, and (ii) supplying the most recent past image\nto both anchor temporal consistency and mitigate cloud-related noise in the\ncurrent image. Together, these components enable AUI to overcome the challenges\nof traditional urbanization indices and produce more reliable and stable\ndevelopment scores. Our qualitative experiments on Bangalore suggest that AUI\noutperforms standard indices such as NDBI.",
        "url": "http://arxiv.org/abs/2510.22702v1",
        "published_date": "2025-10-26T14:53:36+00:00",
        "updated_date": "2025-10-26T14:53:36+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.ET",
            "eess.IV"
        ],
        "authors": [
            "Mithul Chander",
            "Sai Pragnya Ranga",
            "Prathamesh Mayekar"
        ],
        "tldr": "The paper introduces Atlas Urban Index (AUI), a VLM-based metric using Sentinel-2 imagery to monitor urban development, addressing limitations of traditional indices like NDBI by incorporating temporal consistency and reference images for improved accuracy and stability.",
        "tldr_zh": "该论文介绍了Atlas Urban Index (AUI)，一种基于VLM的指标，利用Sentinel-2卫星图像监测城市发展。它通过结合时间一致性和参考图像来解决传统指标（如NDBI）的局限性，从而提高准确性和稳定性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation",
        "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising\nmethod to generate factual and up-to-date responses of Multimodal Large\nLanguage Models (MLLMs) by incorporating non-parametric knowledge from external\nknowledge bases. However, existing MRAG approaches suffer from static retrieval\nstrategies, inflexible modality selection, and suboptimal utilization of\nretrieved information, leading to three critical challenges: determining when\nto retrieve, what modality to incorporate, and how to utilize retrieved\ninformation effectively. To address these challenges, we introduce Windsock, a\nquery-dependent module making decisions on retrieval necessity and modality\nselection, effectively reducing computational overhead and improving response\nquality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction\nTuning, an adaptive training strategy that enhances MLLMs' ability to utilize\nretrieved information while maintaining robustness against noise. Moreover, we\nadopt a self-assessment approach leveraging knowledge within MLLMs to convert\nquestion-answering datasets to MRAG training datasets. Extensive experiments\ndemonstrate that our proposed method significantly improves the generation\nquality by 17.07% while reducing 8.95% retrieval times.",
        "url": "http://arxiv.org/abs/2510.22694v1",
        "published_date": "2025-10-26T14:36:16+00:00",
        "updated_date": "2025-10-26T14:36:16+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.IR"
        ],
        "authors": [
            "Shu Zhao",
            "Tianyi Shen",
            "Nilesh Ahuja",
            "Omesh Tickoo",
            "Vijaykrishnan Narayanan"
        ],
        "tldr": "This paper introduces Windsock, an adaptive MRAG approach with Dynamic Noise-Resistance (DANCE) training and self-assessment for improved MLLM generation quality and retrieval efficiency.",
        "tldr_zh": "本文介绍了Windsock，一种自适应的MRAG方法，结合动态噪声抵抗（DANCE）训练和自我评估，以提高MLLM的生成质量和检索效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STATUS Bench: A Rigorous Benchmark for Evaluating Object State Understanding in Vision-Language Models",
        "summary": "Object state recognition aims to identify the specific condition of objects,\nsuch as their positional states (e.g., open or closed) and functional states\n(e.g., on or off). While recent Vision-Language Models (VLMs) are capable of\nperforming a variety of multimodal tasks, it remains unclear how precisely they\ncan identify object states. To alleviate this issue, we introduce the STAte and\nTransition UnderStanding Benchmark (STATUS Bench), the first benchmark for\nrigorously evaluating the ability of VLMs to understand subtle variations in\nobject states in diverse situations. Specifically, STATUS Bench introduces a\nnovel evaluation scheme that requires VLMs to perform three tasks\nsimultaneously: object state identification (OSI), image retrieval (IR), and\nstate change identification (SCI). These tasks are defined over our fully\nhand-crafted dataset involving image pairs, their corresponding object state\ndescriptions and state change descriptions. Furthermore, we introduce a\nlarge-scale training dataset, namely STATUS Train, which consists of 13 million\nsemi-automatically created descriptions. This dataset serves as the largest\nresource to facilitate further research in this area. In our experiments, we\ndemonstrate that STATUS Bench enables rigorous consistency evaluation and\nreveal that current state-of-the-art VLMs still significantly struggle to\ncapture subtle object state distinctions. Surprisingly, under the proposed\nrigorous evaluation scheme, most open-weight VLMs exhibited chance-level\nzero-shot performance. After fine-tuning on STATUS Train, Qwen2.5-VL achieved\nperformance comparable to Gemini 2.0 Flash. These findings underscore the\nnecessity of STATUS Bench and Train for advancing object state recognition in\nVLM research.",
        "url": "http://arxiv.org/abs/2510.22571v1",
        "published_date": "2025-10-26T08:04:28+00:00",
        "updated_date": "2025-10-26T08:04:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Mahiro Ukai",
            "Shuhei Kurita",
            "Nakamasa Inoue"
        ],
        "tldr": "The paper introduces STATUS Bench, a new benchmark and training dataset for rigorously evaluating and improving object state understanding in Vision-Language Models, revealing limitations in current VLMs.",
        "tldr_zh": "该论文介绍了STATUS Bench，一个新的基准测试和训练数据集，用于严格评估和提升视觉语言模型中的对象状态理解能力，揭示了当前VLM的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Open Multimodal Retrieval-Augmented Factual Image Generation",
        "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.",
        "url": "http://arxiv.org/abs/2510.22521v1",
        "published_date": "2025-10-26T04:13:31+00:00",
        "updated_date": "2025-10-26T04:13:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Yang Tian",
            "Fan Liu",
            "Jingyuan Zhang",
            "Wei Bi",
            "Yupeng Hu",
            "Liqiang Nie"
        ],
        "tldr": "The paper introduces ORIG, an agentic open multimodal retrieval-augmented framework for factual image generation, which iteratively retrieves and filters web-based multimodal evidence to improve factual consistency in generated images. They also introduce a benchmark, FIG-Eval, for systematic evaluation.",
        "tldr_zh": "该论文介绍了一种名为ORIG的代理式开放多模态检索增强框架，用于生成事实性图像。该框架迭代地检索和过滤来自网络的证据，以提高生成图像的事实一致性。作者还引入了一个名为FIG-Eval的基准，用于系统评估。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents",
        "summary": "There has been a surge of interest in assistive wearable agents: agents\nembodied in wearable form factors (e.g., smart glasses) who take assistive\nactions toward a user's goal/query (e.g. \"Where did I leave my keys?\"). In this\nwork, we consider the important complementary problem of inferring that goal\nfrom multi-modal contextual observations. Solving this \"goal inference\" problem\nholds the promise of eliminating the effort needed to interact with such an\nagent. This work focuses on creating WAGIBench, a strong benchmark to measure\nprogress in solving this problem using vision-language models (VLMs). Given the\nlimited prior work in this area, we collected a novel dataset comprising 29\nhours of multimodal data from 348 participants across 3,477 recordings,\nfeaturing ground-truth goals alongside accompanying visual, audio, digital, and\nlongitudinal contextual observations. We validate that human performance\nexceeds model performance, achieving 93% multiple-choice accuracy compared with\n84% for the best-performing VLM. Generative benchmark results that evaluate\nseveral families of modern vision-language models show that larger models\nperform significantly better on the task, yet remain far from practical\nusefulness, as they produce relevant goals only 55% of the time. Through a\nmodality ablation, we show that models benefit from extra information in\nrelevant modalities with minimal performance degradation from irrelevant\nmodalities.",
        "url": "http://arxiv.org/abs/2510.22443v1",
        "published_date": "2025-10-25T21:54:01+00:00",
        "updated_date": "2025-10-25T21:54:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Vijay Veerabadran",
            "Fanyi Xiao",
            "Nitin Kamra",
            "Pedro Matias",
            "Joy Chen",
            "Caley Drooff",
            "Brett D Roads",
            "Riley Williams",
            "Ethan Henderson",
            "Xuanyi Zhao",
            "Kevin Carlberg",
            "Joseph Tighe",
            "Karl Ridgeway"
        ],
        "tldr": "This paper introduces WAGIBench, a new multimodal egocentric dataset and benchmark for goal inference in assistive wearable agents, evaluating the performance of various Vision-Language Models and finding them to lag behind human performance.",
        "tldr_zh": "本文介绍了WAGIBench，一个新的多模态以自我为中心的benchmark数据集，用于辅助可穿戴代理中的目标推断，评估了各种视觉语言模型的性能，并发现它们落后于人类的表现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance",
        "summary": "Scalable Vector Graphics (SVGs) are fundamental to digital design and robot\ncontrol, encoding not only visual structure but also motion paths in\ninteractive drawings. In this work, we introduce RoboSVG, a unified multimodal\nframework for generating interactive SVGs guided by textual, visual, and\nnumerical signals. Given an input query, the RoboSVG model first produces\nmultimodal guidance, then synthesizes candidate SVGs through dedicated\ngeneration modules, and finally refines them under numerical guidance to yield\nhigh-quality outputs. To support this framework, we construct RoboDraw, a\nlarge-scale dataset of one million examples, each pairing an SVG generation\ncondition (e.g., text, image, and partial SVG) with its corresponding\nground-truth SVG code. RoboDraw dataset enables systematic study of four tasks,\nincluding basic generation (Text-to-SVG, Image-to-SVG) and interactive\ngeneration (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments\ndemonstrate that RoboSVG achieves superior query compliance and visual fidelity\nacross tasks, establishing a new state of the art in versatile SVG generation.\nThe dataset and source code of this project will be publicly available soon.",
        "url": "http://arxiv.org/abs/2510.22684v1",
        "published_date": "2025-10-26T13:57:08+00:00",
        "updated_date": "2025-10-26T13:57:08+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jiuniu Wang",
            "Gongjie Zhang",
            "Quanhao Qian",
            "Junlong Gao",
            "Deli Zhao",
            "Ran Xu"
        ],
        "tldr": "The paper introduces RoboSVG, a multimodal framework for generating interactive SVGs from text, images, and partial SVGs, supported by a large-scale dataset called RoboDraw, achieving state-of-the-art results in SVG generation tasks.",
        "tldr_zh": "该论文介绍了RoboSVG，一个多模态框架，用于从文本、图像和部分SVG生成交互式SVG，并由一个名为RoboDraw的大规模数据集支持，在SVG生成任务中实现了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SARCLIP: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery",
        "summary": "Synthetic Aperture Radar (SAR) has emerged as a crucial imaging modality due\nto its all-weather capabilities. While recent advancements in self-supervised\nlearning and Masked Image Modeling (MIM) have paved the way for SAR foundation\nmodels, these approaches primarily focus on low-level visual features, often\noverlooking multimodal alignment and zero-shot target recognition within SAR\nimagery. To address this limitation, we construct SARCLIP-1M, a large-scale\nvision language dataset comprising over one million text-image pairs aggregated\nfrom existing datasets. We further introduce SARCLIP, the first vision language\nfoundation model tailored for the SAR domain. Our SARCLIP model is trained\nusing a contrastive vision language learning approach by domain transferring\nstrategy, enabling it to bridge the gap between SAR imagery and textual\ndescriptions. Extensive experiments on image-text retrieval and zero-shot\nclassification tasks demonstrate the superior performance of SARCLIP in feature\nextraction and interpretation, significantly outperforming state-of-the-art\nfoundation models and advancing the semantic understanding of SAR imagery. The\ncode and datasets will be released soon.",
        "url": "http://arxiv.org/abs/2510.22665v1",
        "published_date": "2025-10-26T13:04:50+00:00",
        "updated_date": "2025-10-26T13:04:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiwei Ma",
            "Zhiyu Wang",
            "Wang Liu",
            "Xukun Lu",
            "Bin Deng",
            "Puhong Duan",
            "Xudong Kang",
            "Shutao Li"
        ],
        "tldr": "SARCLIP introduces a vision-language foundation model for SAR imagery by creating a large-scale dataset (SARCLIP-1M) and training a contrastive model, demonstrating improved performance on image-text retrieval and zero-shot classification.",
        "tldr_zh": "SARCLIP通过构建大规模数据集（SARCLIP-1M）并训练对比模型，为SAR图像引入了一个视觉-语言基础模型，并在图像-文本检索和零样本分类方面表现出优异的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Privacy-Aware Federated nnU-Net for ECG Page Digitization",
        "summary": "Deep neural networks can convert ECG page images into analyzable waveforms,\nyet centralized training often conflicts with cross-institutional privacy and\ndeployment constraints. A cross-silo federated digitization framework is\npresented that trains a full-model nnU-Net segmentation backbone without\nsharing images and aggregates updates across sites under realistic non-IID\nheterogeneity (layout, grid style, scanner profile, noise).\n  The protocol integrates three standard server-side aggregators--FedAvg,\nFedProx, and FedAdam--and couples secure aggregation with central, user-level\ndifferential privacy to align utility with formal guarantees. Key features\ninclude: (i) end-to-end full-model training and synchronization across clients;\n(ii) secure aggregation so the server only observes a clipped, weighted sum\nonce a participation threshold is met; (iii) central Gaussian DP with Renyi\naccounting applied post-aggregation for auditable user-level privacy; and (iv)\na calibration-aware digitization pipeline comprising page normalization, trace\nsegmentation, grid-leakage suppression, and vectorization to twelve-lead\nsignals.\n  Experiments on ECG pages rendered from PTB-XL show consistently faster\nconvergence and higher late-round plateaus with adaptive server updates\n(FedAdam) relative to FedAvg and FedProx, while approaching centralized\nperformance. The privacy mechanism maintains competitive accuracy while\npreventing exposure of raw images or per-client updates, yielding deployable,\nauditable guarantees suitable for multi-institution settings.",
        "url": "http://arxiv.org/abs/2510.22387v1",
        "published_date": "2025-10-25T18:10:05+00:00",
        "updated_date": "2025-10-25T18:10:05+00:00",
        "categories": [
            "cs.CR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nader Nemati"
        ],
        "tldr": "This paper presents a privacy-aware federated learning approach using nnU-Net for ECG page digitization, addressing privacy concerns in multi-institutional settings with differential privacy and secure aggregation.",
        "tldr_zh": "本文提出了一种使用nnU-Net的隐私感知联邦学习方法，用于ECG页面数字化，通过差分隐私和安全聚合解决多机构环境中的隐私问题。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation",
        "summary": "Pose estimation serves as a cornerstone of computer vision for understanding\nanimal posture, behavior, and welfare. Yet, agricultural applications remain\nconstrained by the scarcity of large, annotated datasets for livestock,\nespecially dairy cattle. This study evaluates the potential and limitations of\ncross-species transfer learning by adapting ZebraPose - a vision\ntransformer-based model trained on synthetic zebra imagery - for 27-keypoint\ndetection in dairy cows under real barn conditions. Using three configurations\n- a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), a\nsubset of the APT-36K benchmark dataset, and their combination, we\nsystematically assessed model accuracy and generalization across environments.\nWhile the combined model achieved promising performance (AP = 0.86, AR = 0.87,\nPCK 0.5 = 0.869) on in-distribution data, substantial generalization failures\noccurred when applied to unseen barns and cow populations. These findings\nexpose the synthetic-to-real domain gap as a major obstacle to agricultural AI\ndeployment and emphasize that morphological similarity between species is\ninsufficient for cross-domain transfer. The study provides practical insights\ninto dataset diversity, environmental variability, and computational\nconstraints that influence real-world deployment of livestock monitoring\nsystems. We conclude with a call for agriculture-first AI design, prioritizing\nfarm-level realism, cross-environment robustness, and open benchmark datasets\nto advance trustworthy and scalable animal-centric technologies.",
        "url": "http://arxiv.org/abs/2510.22618v1",
        "published_date": "2025-10-26T10:31:22+00:00",
        "updated_date": "2025-10-26T10:31:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mackenzie Tapp",
            "Sibi Chakravarthy Parivendan",
            "Kashfia Sailunaz",
            "Suresh Neethirajan"
        ],
        "tldr": "This paper explores cross-species transfer learning, specifically adapting a zebra pose estimation model to dairy cattle, revealing limitations due to the synthetic-to-real domain gap and advocating for agriculture-first AI design.",
        "tldr_zh": "本文探讨了跨物种迁移学习，特别是将斑马姿态估计模型适配到奶牛，揭示了由于合成到真实的领域差距所带来的局限性，并提倡农业优先的AI设计。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    }
]