[
    {
        "title": "Simple o3: Towards Interleaved Vision-Language Reasoning",
        "summary": "Multimodal Large Language Models (MLLMs) have shown impressive performance on\nvision-language tasks, but their long Chain-of-Thought (CoT) capabilities in\nmultimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which\nemulates human-like ''thinking with image'' through iterative visual\ntransformations and linguistic reasoning, we propose Simple o3, an end-to-end\nframework that integrates dynamic tool interactions (e.g., cropping, zooming,\nand reusing) into interleaved vision-language reasoning via supervised\nfine-tuning (SFT). Our approach features a scalable data synthesis pipeline\nthat generates high-quality interleaved vision-language reasoning chains via an\n''observe-reason-act'' cycle, complete with executable visual operations and\nrigorous verification, yielding the open-source TWI-Tools-146K dataset.\nExperimental results demonstrate Simple o3's superior performance on diverse\nbenchmarks, outperforming existing approaches. By combining enhanced reasoning\ncapabilities, Simple o3 establishes a powerful yet computationally affordable\nparadigm for advancing multimodal reasoning. Remarkably, we provide the first\nin-depth analysis of different interleaved reasoning strategies, offering\ninsights into their impact on model performance. We found that by introducing\nadditional visual tokens for interleaved vision-language reasoning, reusing and\nmagnifying the original image significantly improves the model's visual\nreasoning and fine-grained perception, while image cropping based on precise\nvisual grounding allows the model to effectively focus on key entities or\nregions, further enhancing its capabilities.",
        "url": "http://arxiv.org/abs/2508.12109v1",
        "published_date": "2025-08-16T17:15:39+00:00",
        "updated_date": "2025-08-16T17:15:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ye Wang",
            "Qianglong Chen",
            "Zejun Li",
            "Siyuan Wang",
            "Shijie Guo",
            "Zhirui Zhang",
            "Zhongyu Wei"
        ],
        "tldr": "The paper introduces Simple o3, a novel end-to-end framework that integrates dynamic tool interactions into interleaved vision-language reasoning via supervised fine-tuning, achieving superior performance on diverse benchmarks by effectively reusing, magnifying, and cropping images.",
        "tldr_zh": "该论文介绍了Simple o3，一种新颖的端到端框架，通过监督微调将动态工具交互集成到交错的视觉-语言推理中，通过有效地重用、放大和裁剪图像，在各种基准测试中实现了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine",
        "summary": "Vision-and-language models (VLMs) have been increasingly explored in the\nmedical domain, particularly following the success of CLIP in general domain.\nHowever, unlike the relatively straightforward pairing of 2D images and text,\ncurating large-scale paired data in the medical field for volumetric modalities\nsuch as CT scans remains a challenging and time-intensive process. This\ndifficulty often limits the performance on downstream tasks. To address these\nchallenges, we propose a novel vision-language pre-training (VLP) framework,\ntermed as \\textbf{VELVET-Med}, specifically designed for limited volumetric\ndata such as 3D CT and associated radiology reports. Instead of relying on\nlarge-scale data collection, our method focuses on the development of effective\npre-training objectives and model architectures. The key contributions are: 1)\nWe incorporate uni-modal self-supervised learning into VLP framework, which are\noften underexplored in the existing literature. 2) We propose a novel language\nencoder, termed as \\textbf{TriBERT}, for learning multi-level textual\nsemantics. 3) We devise the hierarchical contrastive learning to capture\nmulti-level vision-language correspondence. Using only 38,875 scan-report\npairs, our approach seeks to uncover rich spatial and semantic relationships\nembedded in volumetric medical images and corresponding clinical narratives,\nthereby enhancing the generalization ability of the learned encoders. The\nresulting encoders exhibit strong transferability, achieving state-of-the-art\nperformance across a wide range of downstream tasks, including 3D segmentation,\ncross-modal retrieval, visual question answering, and report generation.",
        "url": "http://arxiv.org/abs/2508.12108v1",
        "published_date": "2025-08-16T17:08:43+00:00",
        "updated_date": "2025-08-16T17:08:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyang Zhang",
            "Yang Yu",
            "Xulei Yang",
            "Si Yong Yeo"
        ],
        "tldr": "VELVET-Med is a novel vision-language pre-training framework for volumetric medical images, using a small dataset and focusing on effective pre-training objectives, achieving SOTA performance on various downstream tasks.",
        "tldr_zh": "VELVET-Med是一个新颖的视觉-语言预训练框架，专为体积医学图像设计。它使用小规模数据集，侧重于有效的预训练目标，并在各种下游任务上实现了SOTA性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems",
        "summary": "Bongard Problems (BPs) provide a challenging testbed for abstract visual\nreasoning (AVR), requiring models to identify visual concepts fromjust a few\nexamples and describe them in natural language. Early BP benchmarks featured\nsynthetic black-and-white drawings, which might not fully capture the\ncomplexity of real-world scenes. Subsequent BP datasets employed real-world\nimages, albeit the represented concepts are identifiable from high-level image\nfeatures, reducing the task complexity. Differently, the recently released\nBongard-RWR dataset aimed at representing abstract concepts formulated in the\noriginal BPs using fine-grained real-world images. Its manual construction,\nhowever, limited the dataset size to just $60$ instances, constraining\nevaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset\ncomposed of $5\\,400$ instances that represent original BP abstract concepts\nusing real-world-like images generated via a vision language model (VLM)\npipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually\ncurated images and generate new descriptions aligned with the underlying\nconcepts, use Flux.1-dev to synthesize images from these descriptions, and\nmanually verify that the generated images faithfully reflect the intended\nconcepts. We evaluate state-of-the-art VLMs across diverse BP formulations,\nincluding binary and multiclass classification, as well as textual answer\ngeneration. Our findings reveal that while VLMs can recognize coarse-grained\nvisual concepts, they consistently struggle with discerning fine-grained\nconcepts, highlighting limitations in their reasoning capabilities.",
        "url": "http://arxiv.org/abs/2508.12026v1",
        "published_date": "2025-08-16T12:26:44+00:00",
        "updated_date": "2025-08-16T12:26:44+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Szymon Pawlonka",
            "Mikołaj Małkiński",
            "Jacek Mańdziuk"
        ],
        "tldr": "The paper introduces Bongard-RWR+, a large-scale dataset of real-world-like images generated via VLM for evaluating fine-grained visual concept reasoning in VLMs, and finds that current VLMs still struggle with this task.",
        "tldr_zh": "本文介绍了Bongard-RWR+，一个大规模的真实世界图像数据集，该数据集通过VLM生成，用于评估VLM中细粒度视觉概念推理能力。研究发现，目前的VLM在此任务中仍然存在困难。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding",
        "summary": "With the rapid advancement of e-commerce, exploring general representations\nrather than task-specific ones has attracted increasing research attention. For\nproduct understanding, although existing discriminative dual-flow architectures\ndrive progress in this field, they inherently struggle to model the many-to-one\nalignment between multiple images and texts of products. Therefore, we argue\nthat generative Multimodal Large Language Models (MLLMs) hold significant\npotential for improving product representation learning. Nevertheless,\nachieving this goal still remains non-trivial due to several key challenges:\nthe lack of multimodal and aspect-aware modeling modules in typical LLMs; the\ncommon presence of background noise in product images; and the absence of a\nstandard benchmark for evaluation. To address these issues, we propose the\nfirst generative MLLM-based model named MOON for product representation\nlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for\ntargeted modeling of multimodal and aspect-specific product content; (2)\neffectively detects core semantic regions in product images to mitigate the\ndistraction and interference caused by background noise; and (3) introduces the\nspecialized negative sampling strategy to increase the difficulty and diversity\nof negative samples. In addition, we release a large-scale multimodal benchmark\nMBE for various product understanding tasks. Experimentally, our model\ndemonstrates competitive zero-shot performance on both our benchmark and the\npublic dataset, showcasing strong generalization across various downstream\ntasks, including cross-modal retrieval, product classification, and attribute\nprediction. Furthermore, the case study and visualization illustrate the\neffectiveness of MOON for product understanding.",
        "url": "http://arxiv.org/abs/2508.11999v1",
        "published_date": "2025-08-16T09:59:25+00:00",
        "updated_date": "2025-08-16T09:59:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Daoze Zhang",
            "Zhanheng Nie",
            "Jianyu Liu",
            "Chenghan Fu",
            "Wanxian Guan",
            "Yuan Gao",
            "Jun Song",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "tldr": "The paper introduces MOON, a generative MLLM-based model for product representation learning in e-commerce, addressing limitations of existing discriminative models and providing a new benchmark dataset, MBE.",
        "tldr_zh": "该论文介绍了MOON，一个基于生成式MLLM的电商产品表征学习模型，解决了现有判别式模型的局限性，并提供了一个新的基准数据集MBE。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models",
        "summary": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in\nimages or videos based on natural language instructions. While recent\nmultimodal large language models (MLLMs) have achieved strong performance on\nIVS, their inference cost remains a major bottleneck, particularly in video. We\nempirically analyze visual token sampling in MLLMs and observe a strong\ncorrelation between subset token coverage and segmentation performance. This\nmotivates our design of a simple and effective token pruning method that\nselects a compact yet spatially representative subset of tokens to accelerate\ninference. In this paper, we introduce a novel visual token pruning method for\nIVS, called EVTP-IV, which builds upon the k-center by integrating spatial\ninformation to ensure better coverage. We further provide an\ninformation-theoretic analysis to support our design. Experiments on standard\nIVS benchmarks show that our method achieves up to 5X speed-up on video tasks\nand 3.5X on image tasks, while maintaining comparable accuracy using only 20%\nof the tokens. Our method also consistently outperforms state-of-the-art\npruning baselines under varying pruning ratios.",
        "url": "http://arxiv.org/abs/2508.11886v1",
        "published_date": "2025-08-16T03:16:33+00:00",
        "updated_date": "2025-08-16T03:16:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Wenhui Zhu",
            "Xiwen Chen",
            "Zhipeng Wang",
            "Shao Tang",
            "Sayan Ghosh",
            "Xuanzhao Dong",
            "Rajat Koner",
            "Yalin Wang"
        ],
        "tldr": "The paper introduces EVTP-IVS, a novel visual token pruning method for accelerating inference in Instructed Visual Segmentation (IVS) tasks using MLLMs, achieving significant speed-ups while maintaining accuracy.",
        "tldr_zh": "本文介绍了一种名为 EVTP-IVS 的新型视觉 token 剪枝方法，用于加速 MLLM 在指令式视觉分割 (IVS) 任务中的推理，在保持精度的同时实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition",
        "summary": "Adapter-based fine-tuning has gained remarkable attention in adapting large\npre-trained vision language models (VLMs) for a wide range of downstream tasks\nefficiently. In this paradigm, only the inserted adapters are fine-tuned,\nwithout the need for training the original VLM backbone. Existing works scale\nadapters by integrating them into every layer of VLMs to increase the capacity\nof adapters. However, these methods face two primary limitations: 1) limited\ncompression rate due to ignoring cross-layer redundancy, and 2) limited\nrepresentational capacity across homogeneous adapters. In this paper, we\npropose a novel vision-language fine-tuning framework based on cross-layer\ntensor ring decomposition (TRD) with the integration and collaboration of\ndiverse adapters, called AdaRing, achieving ultra-light parameter-efficient\nadaptation of VLMs on various tasks. To remove the high redundancy that exists\namong adapters across layers, we exploit the tensor-level low-rankness to\nformulate adapters as layer-shared tensor cores and layer-specific slices.\nMoreover, guided by generalization-aware fine-tuning, diverse rank-driven\nadapters cooperate to handle tasks that require different representations. Our\nexperiments show that the proposed AdaRing achieves the state-of-the-art\nperformance while reducing average training parameters by 90%.",
        "url": "http://arxiv.org/abs/2508.11870v1",
        "published_date": "2025-08-16T01:56:27+00:00",
        "updated_date": "2025-08-16T01:56:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ying Huang",
            "Yuanbin Man",
            "Wenqi Jia",
            "Zhengzhong Tu",
            "Junzhou Huang",
            "Miao Yin"
        ],
        "tldr": "The paper introduces AdaRing, a novel vision-language fine-tuning framework using cross-layer tensor ring decomposition to achieve ultra-light parameter-efficient adaptation of VLMs, reducing training parameters by 90% while maintaining state-of-the-art performance.",
        "tldr_zh": "该论文介绍了 AdaRing，一种新颖的视觉语言微调框架，它使用跨层张量环分解来实现 VLM 的超轻量参数高效适配，在保持最先进性能的同时，将训练参数减少了 90%。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection",
        "summary": "The modern web is saturated with multimodal content, intensifying the\nchallenge of detecting hateful memes, where harmful intent is often conveyed\nthrough subtle interactions between text and image under the guise of humor or\nsatire. While recent advances in Vision-Language Models (VLMs) show promise,\nthese models lack support for fine-grained supervision and remain susceptible\nto implicit hate speech. In this paper, we present a dual-pronged approach to\nimprove multimodal hate detection. First, we propose a prompt optimization\nframework that systematically varies prompt structure, supervision granularity,\nand training modality. We show that prompt design and label scaling both\ninfluence performance, with structured prompts improving robustness even in\nsmall models, and InternVL2 achieving the best F1-scores across binary and\nscaled settings. Second, we introduce a multimodal data augmentation pipeline\nthat generates 2,479 counterfactually neutral memes by isolating and rewriting\nthe hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,\nsuccessfully reduces spurious correlations and improves classifier\ngeneralization. Our approaches inspire new directions for building synthetic\ndata to train robust and fair vision-language models. Our findings demonstrate\nthat prompt structure and data composition are as critical as model size, and\nthat targeted augmentation can support more trustworthy and context-sensitive\nhate detection.",
        "url": "http://arxiv.org/abs/2508.11808v1",
        "published_date": "2025-08-15T21:31:00+00:00",
        "updated_date": "2025-08-15T21:31:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.MM",
            "I.2.7; I.2.10"
        ],
        "authors": [
            "Sahajpreet Singh",
            "Rongxin Ouyang",
            "Subhayan Mukerjee",
            "Kokil Jaidka"
        ],
        "tldr": "This paper introduces a dual-pronged approach for improving multimodal hate detection using prompt optimization and a novel multimodal data augmentation pipeline powered by LLM-VLM agents to generate counterfactually neutral memes.",
        "tldr_zh": "本文提出了一种双管齐下的方法，通过提示优化和由LLM-VLM代理驱动的新型多模态数据增强管道来生成反事实中性表情包，以改进多模态仇恨检测。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models",
        "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input.",
        "url": "http://arxiv.org/abs/2508.12081v1",
        "published_date": "2025-08-16T15:31:14+00:00",
        "updated_date": "2025-08-16T15:31:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haidong Xu",
            "Guangwei Xu",
            "Zhedong Zheng",
            "Xiatian Zhu",
            "Wei Ji",
            "Xiangtai Li",
            "Ruijie Guo",
            "Meishan Zhang",
            "Min zhang",
            "Hao Fei"
        ],
        "tldr": "The paper introduces VimoRAG, a retrieval-augmented framework leveraging in-the-wild videos to enhance 3D motion generation for motion LLMs, addressing out-of-domain issues and error propagation through specialized retriever and trainer mechanisms.",
        "tldr_zh": "该论文介绍了VimoRAG，一个检索增强框架，利用实际视频来增强运动LLM的3D运动生成，通过专门的检索器和训练器机制解决了领域外问题和错误传播。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering",
        "summary": "Solving tough clinical questions that require both image and text\nunderstanding is still a major challenge in healthcare AI. In this work, we\npropose Q-FSRU, a new model that combines Frequency Spectrum Representation and\nFusion (FSRU) with a method called Quantum Retrieval-Augmented Generation\n(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in\nfeatures from medical images and related text, then shifts them into the\nfrequency domain using Fast Fourier Transform (FFT). This helps it focus on\nmore meaningful data and filter out noise or less useful information. To\nimprove accuracy and ensure that answers are based on real knowledge, we add a\nquantum-inspired retrieval system. It fetches useful medical facts from\nexternal sources using quantum-based similarity techniques. These details are\nthen merged with the frequency-based features for stronger reasoning. We\nevaluated our model using the VQA-RAD dataset, which includes real radiology\nimages and questions. The results showed that Q-FSRU outperforms earlier\nmodels, especially on complex cases needing image-text reasoning. The mix of\nfrequency and quantum information improves both performance and explainability.\nOverall, this approach offers a promising way to build smart, clear, and\nhelpful AI tools for doctors.",
        "url": "http://arxiv.org/abs/2508.12036v1",
        "published_date": "2025-08-16T13:21:49+00:00",
        "updated_date": "2025-08-16T13:21:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rakesh Thakur",
            "Yusra Tariq"
        ],
        "tldr": "The paper introduces Q-FSRU, a novel medical VQA model combining frequency spectrum analysis with quantum-inspired retrieval to improve accuracy and explainability, showing promising results on the VQA-RAD dataset.",
        "tldr_zh": "该论文介绍了 Q-FSRU，一种新颖的医疗 VQA 模型，它结合了频谱分析和量子启发的检索，以提高准确性和可解释性，并在 VQA-RAD 数据集上显示出有希望的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series",
        "summary": "Vision-language models have shown significant promise in remote sensing\napplications, particularly for land-use and land-cover (LULC) via zero-shot\nclassification and retrieval. However, current approaches face two key\nchallenges: reliance on large spatial tiles that increase computational cost,\nand dependence on text-based supervision, which is often not readily available.\nIn this work, we present TimeSenCLIP, a lightweight framework that reevaluate\nthe role of spatial context by evaluating the effectiveness of a single pixel\nby leveraging its temporal and spectral dimensions, for classifying LULC and\necosystem types. By leveraging spectral and temporal information from\nSentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,\nwe minimises the need for caption-based training while preserving semantic\nalignment between overhead (satellite) and ground perspectives. Our approach is\ngrounded in the LUCAS and Sen4Map datasets, and evaluated on classification\ntasks including LULC, crop type, and ecosystem type. We demonstrate that single\npixel inputs, when combined with temporal and spectral cues, are sufficient for\nthematic mapping, offering a scalable and efficient alternative for large-scale\nremote sensing applications. Code is available at\nhttps://github.com/pallavijain-pj/TimeSenCLIP",
        "url": "http://arxiv.org/abs/2508.11919v1",
        "published_date": "2025-08-16T05:44:33+00:00",
        "updated_date": "2025-08-16T05:44:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pallavi Jain",
            "Diego Marcos",
            "Dino Ienco",
            "Roberto Interdonato",
            "Tristan Berchoux"
        ],
        "tldr": "TimeSenCLIP is a lightweight vision-language model for remote sensing that uses single-pixel time series data from Sentinel-2 imagery, paired with geo-tagged ground-level photos, for LULC classification, achieving efficiency by minimizing reliance on large spatial tiles and text-based supervision.",
        "tldr_zh": "TimeSenCLIP是一个轻量级的遥感视觉-语言模型，它使用来自 Sentinel-2 影像的单像素时间序列数据，并结合地理标记的地面照片，用于土地利用和土地覆盖 (LULC) 分类。该模型通过最小化对大型空间瓦片和基于文本的监督的依赖，实现了高效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models",
        "summary": "Attribute Value Extraction (AVE) is important for structuring product\ninformation in e-commerce. However, existing AVE datasets are primarily limited\nto text-to-text or image-to-text settings, lacking support for product videos,\ndiverse attribute coverage, and public availability. To address these gaps, we\nintroduce VideoAVE, the first publicly available video-to-text e-commerce AVE\ndataset across 14 different domains and covering 172 unique attributes. To\nensure data quality, we propose a post-hoc CLIP-based Mixture of Experts\nfiltering system (CLIP-MoE) to remove the mismatched video-product pairs,\nresulting in a refined dataset of 224k training data and 25k evaluation data.\nIn order to evaluate the usability of the dataset, we further establish a\ncomprehensive benchmark by evaluating several state-of-the-art video vision\nlanguage models (VLMs) under both attribute-conditioned value prediction and\nopen attribute-value pair extraction tasks. Our results analysis reveals that\nvideo-to-text AVE remains a challenging problem, particularly in open settings,\nand there is still room for developing more advanced VLMs capable of leveraging\neffective temporal information. The dataset and benchmark code for VideoAVE are\navailable at: https://github.com/gjiaying/VideoAVE",
        "url": "http://arxiv.org/abs/2508.11801v1",
        "published_date": "2025-08-15T20:58:47+00:00",
        "updated_date": "2025-08-15T20:58:47+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Ming Cheng",
            "Tong Wu",
            "Jiazhen Hu",
            "Jiaying Gong",
            "Hoda Eldardiry"
        ],
        "tldr": "The paper introduces VideoAVE, a new publicly available video-to-text dataset for e-commerce attribute value extraction, along with a CLIP-based filtering system and benchmark evaluations of state-of-the-art VLMs.",
        "tldr_zh": "该论文介绍了 VideoAVE，这是一个新的公开视频到文本数据集，用于电商属性值提取，以及一个基于 CLIP 的过滤系统和对最先进 VLM 的基准评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]