[
    {
        "title": "Visual Spatial Tuning",
        "summary": "Capturing spatial relationships from visual inputs is a cornerstone of\nhuman-like general intelligence. Several previous studies have tried to enhance\nthe spatial awareness of Vision-Language Models (VLMs) by adding extra expert\nencoders, which brings extra overhead and usually harms general capabilities.\nTo enhance the spatial ability in general architectures, we introduce Visual\nSpatial Tuning (VST), a comprehensive framework to cultivate VLMs with\nhuman-like visuospatial abilities, from spatial perception to reasoning. We\nfirst attempt to enhance spatial perception in VLMs by constructing a\nlarge-scale dataset termed VST-P, which comprises 4.1 million samples spanning\n19 skills across single views, multiple images, and videos. Then, we present\nVST-R, a curated dataset with 135K samples that instruct models to reason in\nspace. In particular, we adopt a progressive training pipeline: supervised\nfine-tuning to build foundational spatial knowledge, followed by reinforcement\nlearning to further improve spatial reasoning abilities. Without the\nside-effect to general capabilities, the proposed VST consistently achieves\nstate-of-the-art results on several spatial benchmarks, including $34.8\\%$ on\nMMSI-Bench and $61.2\\%$ on VSIBench. It turns out that the\nVision-Language-Action models can be significantly enhanced with the proposed\nspatial tuning paradigm, paving the way for more physically grounded AI.",
        "url": "http://arxiv.org/abs/2511.05491v1",
        "published_date": "2025-11-07T18:59:16+00:00",
        "updated_date": "2025-11-07T18:59:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Yang",
            "Ziyu Zhu",
            "Yanwei Li",
            "Jingjia Huang",
            "Shen Yan",
            "Siyuan Zhou",
            "Zhe Liu",
            "Xiangtai Li",
            "Shuangye Li",
            "Wenqian Wang",
            "Yi Lin",
            "Hengshuang Zhao"
        ],
        "tldr": "This paper introduces Visual Spatial Tuning (VST), a framework to enhance spatial abilities in VLMs using two new datasets (VST-P and VST-R) and a progressive training pipeline, achieving SOTA results on spatial benchmarks without harming general capabilities.",
        "tldr_zh": "本文介绍了视觉空间调整（VST），一种利用两个新的数据集（VST-P和VST-R）和渐进式训练流程来增强VLMs空间能力的方法，在空间基准测试上取得了SOTA结果，且不损害通用能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
        "summary": "Temporal search aims to identify a minimal set of relevant frames from tens\nof thousands based on a given query, serving as a foundation for accurate\nlong-form video understanding. Existing works attempt to progressively narrow\nthe search space. However, these approaches typically rely on a hand-crafted\nsearch process, lacking end-to-end optimization for learning optimal search\nstrategies. In this paper, we propose TimeSearch-R, which reformulates temporal\nsearch as interleaved text-video thinking, seamlessly integrating searching\nvideo clips into the reasoning process through reinforcement learning (RL).\nHowever, applying RL training methods, such as Group Relative Policy\nOptimization (GRPO), to video reasoning can result in unsupervised intermediate\nsearch decisions. This leads to insufficient exploration of the video content\nand inconsistent logical reasoning. To address these issues, we introduce GRPO\nwith Completeness Self-Verification (GRPO-CSV), which gathers searched video\nframes from the interleaved reasoning process and utilizes the same policy\nmodel to verify the adequacy of searched frames, thereby improving the\ncompleteness of video reasoning. Additionally, we construct datasets\nspecifically designed for the SFT cold-start and RL training of GRPO-CSV,\nfiltering out samples with weak temporal dependencies to enhance task\ndifficulty and improve temporal search capabilities. Extensive experiments\ndemonstrate that TimeSearch-R achieves significant improvements on temporal\nsearch benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as\nlong-form video understanding benchmarks like VideoMME and MLVU. Notably,\nTimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%\nimprovement over the base model Qwen2.5-VL and 2.0% over the advanced video\nreasoning model Video-R1. Our code is available at\nhttps://github.com/Time-Search/TimeSearch-R.",
        "url": "http://arxiv.org/abs/2511.05489v1",
        "published_date": "2025-11-07T18:58:25+00:00",
        "updated_date": "2025-11-07T18:58:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junwen Pan",
            "Qizhe Zhang",
            "Rui Zhang",
            "Ming Lu",
            "Xin Wan",
            "Yuan Zhang",
            "Chang Liu",
            "Qi She"
        ],
        "tldr": "The paper introduces TimeSearch-R, a reinforcement learning based temporal search method for long-form video understanding, achieving state-of-the-art results on several benchmarks by introducing a self-verification mechanism for improved search completeness.",
        "tldr_zh": "该论文介绍了TimeSearch-R，一种基于强化学习的时间搜索方法，用于长视频理解。通过引入自验证机制来提高搜索完整性，并在多个基准测试中实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]