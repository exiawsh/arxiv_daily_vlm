[
    {
        "title": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation",
        "summary": "Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.",
        "url": "http://arxiv.org/abs/2512.03992v1",
        "published_date": "2025-12-03T17:22:29+00:00",
        "updated_date": "2025-12-03T17:22:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zexin Lin",
            "Hawen Wan",
            "Yebin Zhong",
            "Xiaoqiang"
        ],
        "tldr": "The paper introduces DIQ-H, a new benchmark to evaluate VLM robustness under dynamic visual degradation, revealing significant performance gaps in state-of-the-art VLMs in handling temporal inconsistencies and visual corruptions.",
        "tldr_zh": "该论文介绍了DIQ-H，一个新的基准，用于评估VLM在动态视觉退化下的鲁棒性，揭示了最先进的VLM在处理时间不一致性和视觉损坏方面的显著性能差距。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning",
        "summary": "Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.",
        "url": "http://arxiv.org/abs/2512.03963v1",
        "published_date": "2025-12-03T16:57:00+00:00",
        "updated_date": "2025-12-03T16:57:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Wu",
            "Li Yang",
            "Gen Zhan",
            "Yiting Liao",
            "Junlin Li",
            "Deliang Fu",
            "Li Zhang",
            "Limin Wang"
        ],
        "tldr": "The paper introduces TempR1, a temporal-aware multi-task reinforcement learning framework, to improve the temporal understanding of MLLMs across diverse tasks. It achieves state-of-the-art performance by leveraging a multi-task corpus and a tailored RL approach.",
        "tldr_zh": "本文介绍了一个名为TempR1的时间感知多任务强化学习框架，旨在提高多模态大型语言模型（MLLMs）在各种任务中的时间理解能力。通过利用多任务语料库和定制的强化学习方法，该框架实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.",
        "url": "http://arxiv.org/abs/2512.03794v1",
        "published_date": "2025-12-03T13:43:30+00:00",
        "updated_date": "2025-12-03T13:43:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Zichuan Lin",
            "Yicheng Liu",
            "Yang Yang",
            "Lvfang Tao",
            "Deheng Ye"
        ],
        "tldr": "AdaptVision introduces an efficient VLM approach using adaptive visual token acquisition via a coarse-to-fine method, trained with reinforcement learning and decoupled turn policy optimization, achieving improved performance with fewer tokens.",
        "tldr_zh": "AdaptVision 提出了一种高效的视觉语言模型方法，通过粗到细的方法实现自适应视觉令牌获取，使用强化学习和解耦的 turn policy 优化进行训练，从而以更少的令牌实现了更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
        "summary": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.",
        "url": "http://arxiv.org/abs/2512.03746v1",
        "published_date": "2025-12-03T12:44:15+00:00",
        "updated_date": "2025-12-03T12:44:15+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zirun Guo",
            "Minjie Hong",
            "Feng Zhang",
            "Kai Jia",
            "Tao Jin"
        ],
        "tldr": "The paper introduces CodeVision, a code-as-tool framework to improve the robustness and scalability of MLLMs in handling image operations through code generation and reinforcement learning, addressing brittleness issues in existing methods.",
        "tldr_zh": "该论文介绍了CodeVision，一个代码即工具框架，通过代码生成和强化学习来提高MLLM在处理图像操作方面的鲁棒性和可扩展性，解决了现有方法中的脆弱性问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
        "summary": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.",
        "url": "http://arxiv.org/abs/2512.03667v1",
        "published_date": "2025-12-03T10:55:07+00:00",
        "updated_date": "2025-12-03T10:55:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ge-Peng Ji",
            "Jingyi Liu",
            "Deng-Ping Fan",
            "Nick Barnes"
        ],
        "tldr": "The paper introduces Colon-X, an open initiative with a large multimodal colonoscopy dataset (ColonVQA) and a reasoning-centric model (ColonR1) to improve clinical reasoning in colonoscopy, addressing the limitations of existing MLLMs in this domain.",
        "tldr_zh": "该论文介绍了Colon-X，一个开放倡议，包含大型多模态结肠镜检查数据集(ColonVQA)和一个以推理为中心的模型(ColonR1)，旨在提高结肠镜检查中的临床推理能力，解决现有MLLM在该领域的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding",
        "summary": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git",
        "url": "http://arxiv.org/abs/2512.03558v1",
        "published_date": "2025-12-03T08:25:22+00:00",
        "updated_date": "2025-12-03T08:25:22+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Huy Quang Ung",
            "Guillaume Habault",
            "Yasutaka Nishimura",
            "Hao Niu",
            "Roberto Legaspi",
            "Tomoki Oya",
            "Ryoichi Kojima",
            "Masato Taya",
            "Chihiro Ono",
            "Atsunori Minamikawa",
            "Yan Liu"
        ],
        "tldr": "The paper introduces CartoMapQA, a new benchmark dataset for evaluating Vision-Language Models (LVLMs) on their understanding of cartographic maps, highlighting their current limitations in map-specific semantics and geospatial reasoning.",
        "tldr_zh": "本文介绍 CartoMapQA，一个新的基准数据集，用于评估视觉-语言模型（LVLMs）对制图地图的理解能力，突出了它们在地图特定语义和地理空间推理方面的当前局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention",
        "summary": "Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on \"how to intervene\" but overlooking the prerequisite \"when to intervene\", which leads to the \"over-intervention\" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.",
        "url": "http://arxiv.org/abs/2512.03542v1",
        "published_date": "2025-12-03T08:03:54+00:00",
        "updated_date": "2025-12-03T08:03:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nan Sun",
            "Zhenyu Zhang",
            "Xixun Lin",
            "Kun Wang",
            "Yanmin Shang",
            "Naibin Gu",
            "Shuohuan Wang",
            "Yu Sun",
            "Hua Wu",
            "Haifeng Wang",
            "Yanan Cao"
        ],
        "tldr": "This paper introduces V-ITI, a method to mitigate hallucinations in MLLMs by detecting and addressing visual neglect during inference, improving reliability in vision-language tasks.",
        "tldr_zh": "本文介绍了V-ITI，一种通过在推理过程中检测和解决视觉忽视来减轻多模态大语言模型（MLLM）中的幻觉的方法，从而提高视觉语言任务的可靠性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
        "summary": "Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.",
        "url": "http://arxiv.org/abs/2512.03514v1",
        "published_date": "2025-12-03T07:17:59+00:00",
        "updated_date": "2025-12-03T07:17:59+00:00",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Adithya S Kolavi",
            "Vyoman Jain"
        ],
        "tldr": "The paper introduces M3DR, a framework for multilingual multimodal document retrieval that leverages synthetic data and contrastive training to achieve state-of-the-art cross-lingual retrieval performance across 22 languages, using both dense vector and ColBERT-style architectures.",
        "tldr_zh": "该论文介绍了 M3DR，一个用于多语言多模态文档检索的框架，它利用合成数据和对比学习，在使用密集向量和 ColBERT 风格架构的 22 种语言上实现了最先进的跨语言检索性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation",
        "summary": "Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.",
        "url": "http://arxiv.org/abs/2512.03508v1",
        "published_date": "2025-12-03T06:58:38+00:00",
        "updated_date": "2025-12-03T06:58:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seogkyu Jeon",
            "Kibeom Hong",
            "Hyeran Byun"
        ],
        "tldr": "This paper introduces DPMFormer, a domain generalization framework for semantic segmentation that uses domain-aware prompt learning and contrastive learning with texture perturbation to improve robustness against domain shifts when using vision-language models.",
        "tldr_zh": "本文介绍了一种用于语义分割的领域泛化框架DPMFormer，它利用领域感知的提示学习和带有纹理扰动的对比学习，在使用视觉语言模型时提高对领域转移的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "EEA: Exploration-Exploitation Agent for Long Video Understanding",
        "summary": "Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.",
        "url": "http://arxiv.org/abs/2512.03500v1",
        "published_date": "2025-12-03T06:48:36+00:00",
        "updated_date": "2025-12-03T06:48:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Te Yang",
            "Xiangyu Zhu",
            "Bo Wang",
            "Quan Chen",
            "Peng Jiang",
            "Zhen Lei"
        ],
        "tldr": "The paper introduces EEA, a video agent framework for efficient long-form video understanding that balances exploration and exploitation using semantic guidance and hierarchical tree search, achieving superior performance and computational efficiency.",
        "tldr_zh": "该论文介绍了一种名为EEA的视频代理框架，用于高效的长视频理解。该框架通过语义引导和分层树搜索平衡了探索和利用，从而实现了卓越的性能和计算效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis",
        "summary": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.",
        "url": "http://arxiv.org/abs/2512.03477v1",
        "published_date": "2025-12-03T06:09:14+00:00",
        "updated_date": "2025-12-03T06:09:14+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zijian Gu",
            "Yuxi Liu",
            "Zhenhao Zhang",
            "Song Wang"
        ],
        "tldr": "The paper introduces fairness-aware Low-Rank Adaptation methods for Vision-Language Models to reduce diagnostic accuracy disparities in glaucoma diagnosis across demographic groups, achieving significant disparity reduction with minimal impact on overall accuracy and using very few trainable parameters.",
        "tldr_zh": "该论文介绍了用于视觉语言模型的公平感知低秩自适应方法，旨在减少青光眼诊断中不同人群的诊断准确性差异。该方法在对整体准确性影响最小的情况下，显著减少了差异，并且仅使用了非常少的训练参数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation",
        "summary": "Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.",
        "url": "http://arxiv.org/abs/2512.03445v1",
        "published_date": "2025-12-03T04:55:54+00:00",
        "updated_date": "2025-12-03T04:55:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xieji Li",
            "Siyuan Yan",
            "Yingsheng Liu",
            "H. Peter Soyer",
            "Monika Janda",
            "Victoria Mar",
            "Zongyuan Ge"
        ],
        "tldr": "This paper introduces a novel Vision-Language Pretraining (VLP) framework, O-MAKE, for medical image analysis, using a Multi-Agent data GENeration (MAGEN) system to enhance data quality and incorporating ontology-based knowledge to improve learning from complex medical texts. It achieves state-of-the-art results in dermatology.",
        "tldr_zh": "本文提出了一种新颖的视觉-语言预训练（VLP）框架O-MAKE，用于医学图像分析。该框架使用多智能体数据生成（MAGEN）系统来提高数据质量，并结合基于本体的知识来改善从复杂医学文本中的学习。在皮肤病学领域取得了当前最优的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding",
        "summary": "Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.",
        "url": "http://arxiv.org/abs/2512.03284v1",
        "published_date": "2025-12-02T22:49:01+00:00",
        "updated_date": "2025-12-02T22:49:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongpei Zheng",
            "Shijie Li",
            "Yanran Li",
            "Hujun Yin"
        ],
        "tldr": "The paper introduces a new 3D visual question answering dataset (H$^2$U3D) for house-scale scene understanding and an active perception framework (SpatialReasoner) that efficiently explores 3D scenes to answer questions, outperforming GPT-4o and Gemini-2.5-Pro.",
        "tldr_zh": "该论文介绍了一个新的3D视觉问答数据集(H$^2$U3D)，用于房屋尺度场景理解，并提出了一个主动感知框架(SpatialReasoner)，可以有效地探索3D场景以回答问题，性能优于GPT-4o和Gemini-2.5-Pro。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Object Counting with GPT-4o and GPT-5: A Comparative Study",
        "summary": "Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.",
        "url": "http://arxiv.org/abs/2512.03233v1",
        "published_date": "2025-12-02T21:07:13+00:00",
        "updated_date": "2025-12-02T21:07:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Richard Füzesséry",
            "Kaziwa Saleh",
            "Sándor Szénási",
            "Zoltán Vámossy"
        ],
        "tldr": "This paper explores the zero-shot object counting capabilities of GPT-4o and GPT-5 using textual prompts, achieving comparable or superior performance to existing zero-shot methods on the FSC-147 dataset.",
        "tldr_zh": "本文研究了GPT-4o和GPT-5在零样本条件下的目标计数能力，仅使用文本提示，并在FSC-147数据集上实现了与现有零样本方法相当甚至更优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping",
        "summary": "Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.",
        "url": "http://arxiv.org/abs/2512.03173v1",
        "published_date": "2025-12-02T19:16:39+00:00",
        "updated_date": "2025-12-02T19:16:39+00:00",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Joan Nwatu",
            "Longju Bai",
            "Oana Ignat",
            "Rada Mihalcea"
        ],
        "tldr": "The paper introduces the Culture Affordance Atlas, a function-centric re-annotation of the Dollar Street dataset, to mitigate cultural biases in Vision-Language models and improve their performance across diverse socioeconomic contexts.",
        "tldr_zh": "该论文介绍了文化可供性图谱，一个以功能为中心的 Dollar Street 数据集的重新标注版本，旨在减轻视觉-语言模型中的文化偏差，并提高其在不同社会经济环境下的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
        "summary": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
        "url": "http://arxiv.org/abs/2512.03043v2",
        "published_date": "2025-12-02T18:59:52+00:00",
        "updated_date": "2025-12-03T08:46:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaituo Feng",
            "Manyuan Zhang",
            "Hongyu Li",
            "Kaixuan Fan",
            "Shuang Chen",
            "Yilei Jiang",
            "Dian Zheng",
            "Peiwen Sun",
            "Yiyuan Zhang",
            "Haoze Sun",
            "Yan Feng",
            "Peng Pei",
            "Xunliang Cai",
            "Xiangyu Yue"
        ],
        "tldr": "The paper introduces OneThinker, an all-in-one reasoning model for image and video understanding across diverse visual tasks, trained using a large corpus and a novel RL optimization method, demonstrating strong performance and knowledge transfer.",
        "tldr_zh": "该论文介绍了OneThinker，一个用于图像和视频理解的多合一推理模型，可以处理各种视觉任务。该模型使用大型语料库和新的强化学习优化方法进行训练，并展示了强大的性能和知识迁移能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PPTArena: A Benchmark for Agentic PowerPoint Editing",
        "summary": "We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.",
        "url": "http://arxiv.org/abs/2512.03042v1",
        "published_date": "2025-12-02T18:59:50+00:00",
        "updated_date": "2025-12-02T18:59:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Michael Ofengenden",
            "Yunze Man",
            "Ziqi Pang",
            "Yu-Xiong Wang"
        ],
        "tldr": "The paper introduces PPTArena, a benchmark for PowerPoint editing, and PPTPilot, a slide-editing agent that outperforms existing systems, highlighting challenges in reliable document-scale PPT editing.",
        "tldr_zh": "该论文介绍了PPTArena，一个用于PowerPoint编辑的基准，以及PPTPilot，一个优于现有系统的幻灯片编辑代理，强调了可靠的文档级PPT编辑方面的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
        "summary": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
        "url": "http://arxiv.org/abs/2512.04000v1",
        "published_date": "2025-12-03T17:36:06+00:00",
        "updated_date": "2025-12-03T17:36:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jialuo Li",
            "Bin Li",
            "Jiahao Li",
            "Yan Lu"
        ],
        "tldr": "This paper introduces DIG, a training-free frame selection framework for long-form video understanding that adapts its frame selection strategy based on query type (global vs. localized), improving LMM performance while maintaining efficiency.",
        "tldr_zh": "该论文提出了DIG，一种用于长视频理解的免训练帧选择框架，该框架基于查询类型（全局 vs. 局部）调整其帧选择策略，从而在保持效率的同时提高LMM性能。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation",
        "summary": "Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \\href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.",
        "url": "http://arxiv.org/abs/2512.03996v1",
        "published_date": "2025-12-03T17:27:53+00:00",
        "updated_date": "2025-12-03T17:27:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hang Xu",
            "Linjiang Huang",
            "Feng Zhao"
        ],
        "tldr": "This paper introduces a test-time scaling method for text-to-image diffusion models that leverages text embedding perturbation, guided by frequency analysis, to enhance generative diversity and quality with minimal computational overhead.",
        "tldr_zh": "本文提出了一种文本到图像扩散模型的测试时缩放方法，该方法利用文本嵌入扰动，并以频率分析为指导，以最小的计算开销来提高生成的多样性和质量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation",
        "summary": "Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the \"intrinsic core distribution\" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA",
        "url": "http://arxiv.org/abs/2512.03844v1",
        "published_date": "2025-12-03T14:45:57+00:00",
        "updated_date": "2025-12-03T14:45:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Letian Zhou",
            "Songhua Liu",
            "Xinchao Wang"
        ],
        "tldr": "The paper introduces CoDA, a training-free dataset distillation method that leverages off-the-shelf text-to-image models and a core distribution alignment strategy to generate highly representative distilled datasets, achieving state-of-the-art performance on ImageNet-1K.",
        "tldr_zh": "该论文介绍了CoDA，一种无需训练的数据集蒸馏方法，利用现成的文本到图像模型和核心分布对齐策略来生成具有高度代表性的蒸馏数据集，在ImageNet-1K上实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models",
        "summary": "Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.",
        "url": "http://arxiv.org/abs/2512.03749v1",
        "published_date": "2025-12-03T12:46:42+00:00",
        "updated_date": "2025-12-03T12:46:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Korada Sri Vardhana",
            "Shrikrishna Lolla",
            "Soma Biswas"
        ],
        "tldr": "This paper introduces SelfDebias, a fully unsupervised test-time debiasing method for text-to-image diffusion models, which identifies semantic clusters to guide the diffusion process without requiring human annotations.",
        "tldr_zh": "本文介绍了一种名为SelfDebias的完全无监督的文本到图像扩散模型的测试时去偏方法，该方法通过识别语义聚类来引导扩散过程，无需人工标注。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
        "summary": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.",
        "url": "http://arxiv.org/abs/2512.03534v1",
        "published_date": "2025-12-03T07:54:05+00:00",
        "updated_date": "2025-12-03T07:54:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Subin Kim",
            "Sangwoo Mo",
            "Mamshad Nayeem Rizve",
            "Yiran Xu",
            "Difan Liu",
            "Jinwoo Shin",
            "Tobias Hinz"
        ],
        "tldr": "The paper introduces PRIS, a framework that adaptively revises the text prompt during inference based on the generated visuals, coupled with a new element-level factual correction verifier, to improve text-to-visual generation, especially when scaling inference resources.",
        "tldr_zh": "本文介绍了一个名为PRIS的框架，该框架在推理过程中根据生成的视觉效果自适应地修改文本提示，并结合一种新的元素级事实校正验证器，以改进文本到视觉的生成，尤其是在扩展推理资源时。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation",
        "summary": "The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.",
        "url": "http://arxiv.org/abs/2512.03499v1",
        "published_date": "2025-12-03T06:47:56+00:00",
        "updated_date": "2025-12-03T06:47:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Renqi Chen",
            "Haoyang Su",
            "Shixiang Tang"
        ],
        "tldr": "The paper introduces NAS-LoRA, a parameter-efficient fine-tuning method for adapting visual foundation models like SAM to specific domains by integrating a neural architecture search block within LoRA and employing a stage-wise optimization strategy, resulting in improved performance and reduced training costs.",
        "tldr_zh": "该论文介绍了NAS-LoRA，一种参数高效的微调方法，通过在LoRA中集成神经架构搜索块和采用分阶段优化策略，使视觉基础模型（如SAM）适应特定领域，从而提高性能并降低训练成本。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models",
        "summary": "Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.",
        "url": "http://arxiv.org/abs/2512.03463v1",
        "published_date": "2025-12-03T05:36:46+00:00",
        "updated_date": "2025-12-03T05:36:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Shojiro Yamabe",
            "Futa Waseda",
            "Daiki Shiono",
            "Tsubasa Takahashi"
        ],
        "tldr": "The paper introduces Text-Printed Images (TPI), a method for generating synthetic images from text descriptions by directly rendering the text on a canvas, enabling effective text-centric training for large vision-language models at a lower cost than using real or diffusion-generated images.",
        "tldr_zh": "该论文介绍了文本打印图像（TPI），这是一种通过将文本直接渲染到画布上，从文本描述生成合成图像的方法，从而能够以比使用真实或扩散生成的图像更低的成本，为大型视觉-语言模型进行有效的以文本为中心的训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ViDiC: Video Difference Captioning",
        "summary": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
        "url": "http://arxiv.org/abs/2512.03405v1",
        "published_date": "2025-12-03T03:23:24+00:00",
        "updated_date": "2025-12-03T03:23:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangtao Wu",
            "Shihao Li",
            "Zhaozhou Bian",
            "Yuanxing Zhang",
            "Jialu Chen",
            "Runzhe Wen",
            "An Ping",
            "Yiwen He",
            "Jiakai Wang",
            "Jiaheng Liu"
        ],
        "tldr": "The paper introduces ViDiC, a new task and dataset (ViDiC-1K) for Video Difference Captioning, aimed at evaluating the comparative understanding abilities of Multimodal Large Language Models (MLLMs) on video pairs, revealing a performance gap in existing models.",
        "tldr_zh": "该论文介绍了 ViDiC，一个新的视频差异描述任务和数据集 (ViDiC-1K)，旨在评估多模态大型语言模型 (MLLM) 在视频对上的比较理解能力，并揭示了现有模型的性能差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
        "summary": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
        "url": "http://arxiv.org/abs/2512.03000v2",
        "published_date": "2025-12-02T18:24:27+00:00",
        "updated_date": "2025-12-03T18:51:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kairun Wen",
            "Yuzhi Huang",
            "Runyu Chen",
            "Hui Zheng",
            "Yunlong Lin",
            "Panwang Pan",
            "Chenxin Li",
            "Wenyan Cong",
            "Jian Zhang",
            "Junbin Lu",
            "Chenguo Lin",
            "Dilin Wang",
            "Zhicheng Yan",
            "Hongyu Xu",
            "Justin Theiss",
            "Yue Huang",
            "Xinghao Ding",
            "Rakesh Ranjan",
            "Zhiwen Fan"
        ],
        "tldr": "The paper introduces DynamicVerse, a framework for creating a large-scale, physically-aware, multimodal 4D dataset from internet videos, claiming superior performance in video depth, camera pose, and intrinsics estimation compared to existing methods.",
        "tldr_zh": "该论文介绍了DynamicVerse，一个从互联网视频创建大规模、物理感知、多模态4D数据集的框架，声称在视频深度、相机姿态和内在参数估计方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers",
        "summary": "Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.\n  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\\times$ and $2.37\\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).",
        "url": "http://arxiv.org/abs/2512.03451v1",
        "published_date": "2025-12-03T05:08:18+00:00",
        "updated_date": "2025-12-03T05:08:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zhiye Song",
            "Steve Dai",
            "Ben Keller",
            "Brucek Khailany"
        ],
        "tldr": "GalaxyDiT introduces a training-free method for accelerating video generation in Diffusion Transformers by using guidance alignment and adaptive proxy selection, achieving significant speedups with minimal performance drops.",
        "tldr_zh": "GalaxyDiT 提出了一种无需训练的方法，通过在扩散 Transformer 中使用引导对齐和自适应代理选择来加速视频生成，实现了显著的加速，同时性能下降极小。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]