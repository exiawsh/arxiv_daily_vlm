[
    {
        "title": "SAIL-VL2 Technical Report",
        "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
        "url": "http://arxiv.org/abs/2509.14033v1",
        "published_date": "2025-09-17T14:34:02+00:00",
        "updated_date": "2025-09-17T14:34:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijie Yin",
            "Yongjie Ye",
            "Fangxun Shu",
            "Yue Liao",
            "Zijian Kang",
            "Hongyuan Dong",
            "Haiyang Yu",
            "Dingkang Yang",
            "Jiacong Wang",
            "Han Wang",
            "Wenzhuo Liu",
            "Xiao Liang",
            "Shuicheng Yan",
            "Chao Feng"
        ],
        "tldr": "SAIL-VL2 is an open-suite vision-language foundation model with state-of-the-art performance on various benchmarks, achieved through improved data curation, a progressive training framework, and architectural innovations like sparse MoE designs. It outperforms other open-source models under 4B parameters on the OpenCompass leaderboard.",
        "tldr_zh": "SAIL-VL2 是一个开放套件的视觉语言基础模型，在各种基准测试中表现出色，这得益于改进的数据整理、渐进式训练框架以及稀疏 MoE 设计等架构创新。在 OpenCompass 排行榜上，它优于其他 4B 参数以下的开源模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
        "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.",
        "url": "http://arxiv.org/abs/2509.14232v1",
        "published_date": "2025-09-17T17:59:14+00:00",
        "updated_date": "2025-09-17T17:59:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaokai Wang",
            "Penghao Yin",
            "Xiangyu Zhao",
            "Changyao Tian",
            "Yu Qiao",
            "Wenhai Wang",
            "Jifeng Dai",
            "Gen Luo"
        ],
        "tldr": "The paper introduces GenExam, a novel benchmark for multidisciplinary text-to-image exams, highlighting the limitations of current state-of-the-art models in integrating knowledge, reasoning, and generation.",
        "tldr_zh": "该论文介绍了 GenExam，这是一个用于多学科文本到图像考试的新基准，强调了当前最先进模型在集成知识、推理和生成方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dense Video Understanding with Gated Residual Tokenization",
        "summary": "High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.14199v1",
        "published_date": "2025-09-17T17:34:40+00:00",
        "updated_date": "2025-09-17T17:34:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Haichao Zhang",
            "Wenhao Chai",
            "Shwai He",
            "Ang Li",
            "Yun Fu"
        ],
        "tldr": "This paper introduces Dense Video Understanding (DVU) with Gated Residual Tokenization (GRT) and a new benchmark (DIVE) for high-frame-rate video comprehension, addressing the limitations of existing VLLMs that rely on low-frame-rate sampling.",
        "tldr_zh": "本文提出了密集视频理解 (DVU) 和门控残差令牌化 (GRT)，以及一个新的基准测试 (DIVE)，用于高帧率视频理解，解决了现有 VLLM 依赖于低帧率采样的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook",
        "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.",
        "url": "http://arxiv.org/abs/2509.14142v1",
        "published_date": "2025-09-17T16:21:34+00:00",
        "updated_date": "2025-09-17T16:21:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peng Xu",
            "Shengwu Xiong",
            "Jiajun Zhang",
            "Yaxiong Chen",
            "Bowen Zhou",
            "Chen Change Loy",
            "David A. Clifton",
            "Kyoung Mu Lee",
            "Luc Van Gool",
            "Ruiming He",
            "Ruilin Yao",
            "Xinwei Long",
            "Jirui Huang",
            "Kai Tian",
            "Sa Yang",
            "Yihua Shao",
            "Jin Feng",
            "Yue Zhong",
            "Jiakai Zhou",
            "Cheng Tang",
            "Tianyu Zou",
            "Yifang Zhang",
            "Junming Liang",
            "Guoyou Li",
            "Zhaoxiang Wang",
            "Qiang Zhou",
            "Yichen Zhao",
            "Shili Xiong",
            "Hyeongjin Nam",
            "Jaerin Lee",
            "Jaeyoung Chung",
            "JoonKyu Park",
            "Junghun Oh",
            "Kanggeon Lee",
            "Wooseok Lee",
            "Juneyoung Ro",
            "Turghun Osman",
            "Can Hu",
            "Chaoyang Liao",
            "Cheng Chen",
            "Chengcheng Han",
            "Chenhao Qiu",
            "Chong Peng",
            "Cong Xu",
            "Dailin Li",
            "Feiyu Wang",
            "Feng Gao",
            "Guibo Zhu",
            "Guopeng Tang",
            "Haibo Lu",
            "Han Fang",
            "Han Qi",
            "Hanxiao Wu",
            "Haobo Cheng",
            "Hongbo Sun",
            "Hongyao Chen",
            "Huayong Hu",
            "Hui Li",
            "Jiaheng Ma",
            "Jiang Yu",
            "Jianing Wang",
            "Jie Yang",
            "Jing He",
            "Jinglin Zhou",
            "Jingxuan Li",
            "Josef Kittler",
            "Lihao Zheng",
            "Linnan Zhao",
            "Mengxi Jia",
            "Muyang Yan",
            "Nguyen Thanh Thien",
            "Pu Luo",
            "Qi Li",
            "Shien Song",
            "Shijie Dong",
            "Shuai Shao",
            "Shutao Li",
            "Taofeng Xue",
            "Tianyang Xu",
            "Tianyi Gao",
            "Tingting Li",
            "Wei Zhang",
            "Weiyang Su",
            "Xiaodong Dong",
            "Xiao-Jun Wu",
            "Xiaopeng Zhou",
            "Xin Chen",
            "Xin Wei",
            "Xinyi You",
            "Xudong Kang",
            "Xujie Zhou",
            "Xusheng Liu",
            "Yanan Wang",
            "Yanbin Huang",
            "Yang Liu",
            "Yang Yang",
            "Yanglin Deng",
            "Yashu Kang",
            "Ye Yuan",
            "Yi Wen",
            "Yicen Tian",
            "Yilin Tao",
            "Yin Tang",
            "Yipeng Lin",
            "Yiqing Wang",
            "Yiting Xi",
            "Yongkang Yu",
            "Yumei Li",
            "Yuxin Qin",
            "Yuying Chen",
            "Yuzhe Cen",
            "Zhaofan Zou",
            "Zhaohong Liu",
            "Zhehao Shen",
            "Zhenglin Du",
            "Zhengyang Li",
            "Zhenni Huang",
            "Zhenwei Shao",
            "Zhilong Song",
            "Zhiyong Feng",
            "Zhiyu Wang",
            "Zhou Yu",
            "Ziang Li",
            "Zihan Zhai",
            "Zijian Zhang",
            "Ziyang Peng",
            "Ziyun Xiao",
            "Zongshu Li"
        ],
        "tldr": "The paper introduces the MARS2 2025 Challenge on Multimodal Reasoning, presenting two new datasets (Lens and AdsQA) and evaluating various MLLMs and task-specific models across three competition tracks, with publicly available resources.",
        "tldr_zh": "本文介绍了 MARS2 2025 多模态推理挑战赛，提出了两个新的数据集（Lens 和 AdsQA），并在三个竞赛项目中评估了各种 MLLM 和特定任务模型，所有资源均公开。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement",
        "summary": "Current multi-object tracking (MOT) algorithms typically overlook issues\ninherent in low-quality videos, leading to significant degradation in tracking\nperformance when confronted with real-world image deterioration. Therefore,\nadvancing the application of MOT algorithms in real-world low-quality video\nscenarios represents a critical and meaningful endeavor. To address the\nchallenges posed by low-quality scenarios, inspired by vision-language models,\nthis paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking\nframework (VSE-MOT). Specifically, we first design a tri-branch architecture\nthat leverages a vision-language model to extract global visual semantic\ninformation from images and fuse it with query vectors. Subsequently, to\nfurther enhance the utilization of visual semantic information, we introduce\nthe Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion\nModule (VSFM). The MOT-Adapter adapts the extracted global visual semantic\ninformation to suit multi-object tracking tasks, while the VSFM improves the\nefficacy of feature fusion. Through extensive experiments, we validate the\neffectiveness and superiority of the proposed method in real-world low-quality\nvideo scenarios. Its tracking performance metrics outperform those of existing\nmethods by approximately 8% to 20%, while maintaining robust performance in\nconventional scenarios.",
        "url": "http://arxiv.org/abs/2509.14060v1",
        "published_date": "2025-09-17T15:04:45+00:00",
        "updated_date": "2025-09-17T15:04:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jun Du",
            "Weiwei Xing",
            "Ming Li",
            "Fei Richard Yu"
        ],
        "tldr": "The paper introduces VSE-MOT, a novel multi-object tracking framework that leverages visual semantic enhancement via a vision-language model to improve tracking performance in low-quality video scenes. It outperforms existing methods by 8-20%.",
        "tldr_zh": "该论文提出了 VSE-MOT，一种新颖的多目标跟踪框架，利用视觉语义增强并通过视觉语言模型来提高低质量视频场景中的跟踪性能。 其性能优于现有方法 8-20%。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics",
        "summary": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.",
        "url": "http://arxiv.org/abs/2509.13858v1",
        "published_date": "2025-09-17T09:48:39+00:00",
        "updated_date": "2025-09-17T09:48:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qianxin Xia",
            "Jiawei Du",
            "Guoming Lu",
            "Zhiyong Shu",
            "Jielei Wang"
        ],
        "tldr": "The paper introduces EDITS, a novel dataset distillation framework that leverages implicit textual semantics from VLMs and LLMs to generate a compact, high-quality synthetic dataset.",
        "tldr_zh": "该论文介绍了 EDITS，一种新颖的数据集精馏框架，它利用来自 VLM 和 LLM 的隐式文本语义来生成紧凑、高质量的合成数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models",
        "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance.",
        "url": "http://arxiv.org/abs/2509.13836v1",
        "published_date": "2025-09-17T09:08:05+00:00",
        "updated_date": "2025-09-17T09:08:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Weihang Wang",
            "Xinhao Li",
            "Ziyue Wang",
            "Yan Pang",
            "Jielei Zhang",
            "Peiyi Li",
            "Qiang Zhang",
            "Longwen Gao"
        ],
        "tldr": "This paper introduces VHBench-10, a new benchmark for evaluating object hallucination in LVLMs, and proposes VisionWeaver, a context-aware routing network to mitigate these hallucinations by dynamically aggregating features from multiple visual encoders.",
        "tldr_zh": "本文介绍了一个新的基准测试 VHBench-10，用于评估 LVLM 中的对象幻觉，并提出了 VisionWeaver，一种上下文感知路由网络，通过动态聚合来自多个视觉编码器的特征来减轻这些幻觉。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation",
        "summary": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.",
        "url": "http://arxiv.org/abs/2509.13760v1",
        "published_date": "2025-09-17T07:16:06+00:00",
        "updated_date": "2025-09-17T07:16:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinwoo Jeon",
            "JunHyeok Oh",
            "Hayeong Lee",
            "Byung-Jun Lee"
        ],
        "tldr": "This paper proposes an iterative prompt refinement algorithm for safer text-to-image generation using VLMs to analyze both prompts and generated images, improving safety and user intent alignment. They also introduce a new dataset with textual and visual safety signals.",
        "tldr_zh": "该论文提出了一种迭代提示细化算法，使用视觉语言模型分析提示和生成的图像，以实现更安全的文本到图像生成，从而提高安全性和用户意图对齐。他们还引入了一个新的数据集，其中包含文本和视觉安全信号。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation",
        "summary": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.",
        "url": "http://arxiv.org/abs/2509.13676v1",
        "published_date": "2025-09-17T04:04:08+00:00",
        "updated_date": "2025-09-17T04:04:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaobo Yang",
            "Xiaojin Gong"
        ],
        "tldr": "This paper introduces a novel semantic visual projector that leverages SAM to compress visual tokens for MLLM-based Referring Image Segmentation, achieving significant speedups without performance loss.",
        "tldr_zh": "该论文介绍了一种新颖的语义视觉投影器，它利用SAM压缩MLLM的Referring Image Segmentation的视觉token，在不损失性能的情况下实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators",
        "summary": "We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that\nreframes interleaved image-text generation as a tool-use problem. LLM-I is\ndesigned to overcome the \"one-tool\" bottleneck of current unified models, which\nare limited to synthetic imagery and struggle with tasks requiring factual\ngrounding or programmatic precision. Our framework empowers a central LLM or\nMLLM agent to intelligently orchestrate a diverse toolkit of specialized visual\ntools, including online image search, diffusion-based generation, code\nexecution, and image editing. The agent is trained to select and apply these\ntools proficiently via a Reinforcement Learning (RL) framework that features a\nhybrid reward system combining rule-based logic with judgments from LLM and\nMLLM evaluators. Trained on a diverse new dataset using four different model\nbackbones, LLM-I demonstrates state-of-the-art performance, outperforming\nexisting methods by a large margin across four benchmarks. We also introduce a\nnovel test-time scaling strategy that provides further performance gains.\nProject Page: https://github.com/ByteDance-BandAI/LLM-I.",
        "url": "http://arxiv.org/abs/2509.13642v1",
        "published_date": "2025-09-17T02:33:29+00:00",
        "updated_date": "2025-09-17T02:33:29+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Zirun Guo",
            "Feng Zhang",
            "Kai Jia",
            "Tao Jin"
        ],
        "tldr": "The paper introduces LLM-I, a framework that uses an LLM/MLLM agent to orchestrate various visual tools for interleaved image-text generation via Reinforcement Learning, achieving state-of-the-art results on multiple benchmarks.",
        "tldr_zh": "该论文介绍了LLM-I，一个通过LLM/MLLM智能体协调各种视觉工具进行交错图像-文本生成的框架，并通过强化学习实现，在多个基准测试中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation",
        "summary": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.",
        "url": "http://arxiv.org/abs/2509.13590v1",
        "published_date": "2025-09-16T23:15:44+00:00",
        "updated_date": "2025-09-16T23:15:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Samer Al-Hamadani"
        ],
        "tldr": "This paper presents a VLM-based framework using Google Gemini 2.5 Flash for automated medical image analysis and clinical report generation across multiple imaging modalities, demonstrating high performance in anomaly detection and workflow efficiency.",
        "tldr_zh": "本文提出了一个基于视觉-语言模型（VLM）的框架，利用谷歌 Gemini 2.5 Flash，用于跨多种成像模式的自动化医学图像分析和临床报告生成，并在异常检测和工作流程效率方面表现出高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes",
        "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.",
        "url": "http://arxiv.org/abs/2509.13484v1",
        "published_date": "2025-09-16T19:31:40+00:00",
        "updated_date": "2025-09-16T19:31:40+00:00",
        "categories": [
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Liu Liu",
            "Alexandra Kudaeva",
            "Marco Cipriano",
            "Fatimeh Al Ghannam",
            "Freya Tan",
            "Gerard de Melo",
            "Andres Sevtsuk"
        ],
        "tldr": "The paper introduces a new task of social group region detection in urban scenes using VLMs, along with a new dataset to support the task. They propose a modular pipeline, MINGLE, that leverages VLMs for reasoning about social affiliation.",
        "tldr_zh": "该论文介绍了一个新的城市场景中社会群体区域检测任务，并提出了一个新的数据集来支持该任务。他们提出了一个模块化流水线 MINGLE，利用 VLMs 来推理社会关系。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark",
        "summary": "While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.",
        "url": "http://arxiv.org/abs/2509.14227v1",
        "published_date": "2025-09-17T17:58:06+00:00",
        "updated_date": "2025-09-17T17:58:06+00:00",
        "categories": [
            "cs.CV",
            "I.2.10; I.2.7"
        ],
        "authors": [
            "Nisarg A. Shah",
            "Amir Ziai",
            "Chaitanya Ekanadham",
            "Vishal M. Patel"
        ],
        "tldr": "The paper introduces Cinéaste, a new benchmark for evaluating fine-grained contextual reasoning in long-form movie understanding, highlighting the limitations of existing MLLMs in long-range temporal reasoning.",
        "tldr_zh": "该论文介绍了Cinéaste，一个新的用于评估长篇电影理解中细粒度上下文推理的基准，强调了现有MLLM在长程时间推理方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration",
        "summary": "Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,\nverifying its superiority as a general zero-shot anomaly detection framework.",
        "url": "http://arxiv.org/abs/2509.14084v1",
        "published_date": "2025-09-17T15:29:25+00:00",
        "updated_date": "2025-09-17T15:29:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyi Yuan",
            "Jianxiong Ye",
            "Wenkang Chen",
            "Chenqiang Gao"
        ],
        "tldr": "The paper introduces AD-DINOv3, a novel vision-language framework that adapts DINOv3 for zero-shot anomaly detection by using anomaly-aware calibration to address domain bias and improve anomaly discrimination.",
        "tldr_zh": "该论文介绍了AD-DINOv3，一种新型的视觉-语言框架，通过使用异常感知校准来解决域偏差并提高异常区分能力，从而将DINOv3用于零样本异常检测。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation",
        "summary": "Visual counting is a fundamental yet challenging task, especially when users\nneed to count objects of a specific type in complex scenes. While recent\nmodels, including class-agnostic counting models and large vision-language\nmodels (VLMs), show promise in counting tasks, their ability to perform\nfine-grained, intent-driven counting remains unclear. In this paper, we\nintroduce PairTally, a benchmark dataset specifically designed to evaluate\nfine-grained visual counting. Each of the 681 high-resolution images in\nPairTally contains two object categories, requiring models to distinguish and\ncount based on subtle differences in shape, size, color, or semantics. The\ndataset includes both inter-category (distinct categories) and intra-category\n(closely related subcategories) settings, making it suitable for rigorous\nevaluation of selective counting capabilities. We benchmark a variety of\nstate-of-the-art models, including exemplar-based methods, language-prompted\nmodels, and large VLMs. Our results show that despite recent advances, current\nmodels struggle to reliably count what users intend, especially in fine-grained\nand visually ambiguous cases. PairTally provides a new foundation for\ndiagnosing and improving fine-grained visual counting systems.",
        "url": "http://arxiv.org/abs/2509.13939v1",
        "published_date": "2025-09-17T13:06:58+00:00",
        "updated_date": "2025-09-17T13:06:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gia Khanh Nguyen",
            "Yifeng Huang",
            "Minh Hoai"
        ],
        "tldr": "The paper introduces PairTally, a new benchmark dataset for evaluating fine-grained visual counting, and demonstrates that current AI models struggle with intent-driven counting in visually ambiguous scenarios.",
        "tldr_zh": "该论文介绍了一个名为PairTally的新基准数据集，用于评估细粒度的视觉计数，并表明当前的AI模型在视觉模糊的情况下难以进行意图驱动的计数。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration",
        "summary": "Large Vision-Language Models (LVLMs) have manifested strong visual question\nanswering capability. However, they still struggle with aligning the rationale\nand the generated answer, leading to inconsistent reasoning and incorrect\nresponses. To this end, this paper introduces the Self-Rationale Calibration\n(SRC) framework to iteratively calibrate the alignment between rationales and\nanswers. SRC begins by employing a lightweight \"rationale fine-tuning\"\napproach, which modifies the model's response format to require a rationale\nbefore deriving an answer without explicit prompts. Next, SRC searches for a\ndiverse set of candidate responses from the fine-tuned LVLMs for each sample,\nfollowed by a proposed pairwise scoring strategy using a tailored scoring\nmodel, R-Scorer, to evaluate both rationale quality and factual consistency of\ncandidates. Based on a confidence-weighted preference curation process, SRC\ndecouples the alignment calibration into a preference fine-tuning manner,\nleading to significant improvements of LVLMs in perception, reasoning, and\ngeneralization across multiple benchmarks. Our results emphasize the\nrationale-oriented alignment in exploring the potential of LVLMs.",
        "url": "http://arxiv.org/abs/2509.13919v1",
        "published_date": "2025-09-17T11:27:33+00:00",
        "updated_date": "2025-09-17T11:27:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanchen Wu",
            "Ke Yan",
            "Shouhong Ding",
            "Ziyin Zhou",
            "Xiaoqiang Li"
        ],
        "tldr": "This paper introduces Self-Rationale Calibration (SRC), a framework to improve the alignment between rationales and answers in LVLMs, leading to better reasoning and performance on visual question answering tasks.",
        "tldr_zh": "本文介绍了自我推理校准（SRC）框架，旨在提升大型视觉语言模型中推理与答案之间的一致性，从而改善视觉问答任务中的推理和性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving",
        "summary": "While reasoning technology like Chain of Thought (CoT) has been widely\nadopted in Vision Language Action (VLA) models, it demonstrates promising\ncapabilities in end to end autonomous driving. However, recent efforts to\nintegrate CoT reasoning often fall short in simple scenarios, introducing\nunnecessary computational overhead without improving decision quality. To\naddress this, we propose AdaThinkDrive, a novel VLA framework with a dual mode\nreasoning mechanism inspired by fast and slow thinking. First, our framework is\npretrained on large scale autonomous driving (AD) scenarios using both question\nanswering (QA) and trajectory datasets to acquire world knowledge and driving\ncommonsense. During supervised fine tuning (SFT), we introduce a two mode\ndataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the\nmodel to distinguish between scenarios that require reasoning. Furthermore, an\nAdaptive Think Reward strategy is proposed in conjunction with the Group\nRelative Policy Optimization (GRPO), which rewards the model for selectively\napplying CoT by comparing trajectory quality across different reasoning modes.\nExtensive experiments on the Navsim benchmark show that AdaThinkDrive achieves\na PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.\nMoreover, ablations show that AdaThinkDrive surpasses both the never Think and\nalways Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also\nreduces inference time by 14% compared to the always Think baseline,\ndemonstrating its ability to balance accuracy and efficiency through adaptive\nreasoning.",
        "url": "http://arxiv.org/abs/2509.13769v1",
        "published_date": "2025-09-17T07:35:39+00:00",
        "updated_date": "2025-09-17T07:35:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuechen Luo",
            "Fang Li",
            "Shaoqing Xu",
            "Zhiyi Lai",
            "Lei Yang",
            "Qimao Chen",
            "Ziang Luo",
            "Zixun Xie",
            "Shengyin Jiang",
            "Jiaxin Liu",
            "Long Chen",
            "Bing Wang",
            "Zhi-xin Yang"
        ],
        "tldr": "AdaThinkDrive is a novel VLA framework that adaptively applies Chain of Thought reasoning in autonomous driving, improving performance and reducing inference time compared to fixed reasoning approaches.",
        "tldr_zh": "AdaThinkDrive 是一种新颖的 VLA 框架，它在自动驾驶中自适应地应用思维链推理，与固定推理方法相比，提高了性能并减少了推理时间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]