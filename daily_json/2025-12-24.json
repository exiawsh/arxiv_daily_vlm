[
    {
        "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
        "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
        "url": "http://arxiv.org/abs/2512.20557v1",
        "published_date": "2025-12-23T17:56:36+00:00",
        "updated_date": "2025-12-23T17:56:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengchao Zhou",
            "Yuxin Chen",
            "Yuying Ge",
            "Wei Huang",
            "Jiehong Lin",
            "Ying Shan",
            "Xiaojuan Qi"
        ],
        "tldr": "The paper introduces DSR Suite, a framework including a data generation pipeline and a Geometry Selection Module (GSM), to improve dynamic spatial reasoning in VLMs by leveraging 4D geometric priors extracted from in-the-wild videos. Experiments show improved performance on dynamic spatial reasoning tasks without sacrificing general video understanding accuracy.",
        "tldr_zh": "该论文介绍了DSR Suite，包括一个数据生成流程和一个几何选择模块（GSM），旨在通过利用从真实视频中提取的4D几何先验来提高VLMs的动态空间推理能力。实验表明，在不牺牲通用视频理解准确性的前提下，动态空间推理任务的性能得到了提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
        "summary": "We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.",
        "url": "http://arxiv.org/abs/2512.20387v1",
        "published_date": "2025-12-23T14:22:26+00:00",
        "updated_date": "2025-12-23T14:22:26+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "YuChe Hsu",
            "AnJui Wang",
            "TsaiChing Ni",
            "YuanFu Yang"
        ],
        "tldr": "The paper introduces a Vision-Language Simulation Model (VLSM) for generating executable industrial system code from layout sketches and natural language prompts, supported by a new large-scale dataset and evaluation metrics.",
        "tldr_zh": "该论文介绍了一种视觉-语言模拟模型 (VLSM)，用于从布局草图和自然语言提示生成可执行的工业系统代码，并由一个新的大规模数据集和评估指标提供支持。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
        "summary": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
        "url": "http://arxiv.org/abs/2512.20362v1",
        "published_date": "2025-12-23T13:44:41+00:00",
        "updated_date": "2025-12-23T13:44:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "V. Kovalev",
            "A. Kuvshinov",
            "A. Buzovkin",
            "D. Pokidov",
            "D. Timonin"
        ],
        "tldr": "The paper introduces CRAFT, a training-free framework for improving text-to-image generation by using explicit, structured reasoning and agentic feedback tuning based on verifying generated images against dependency-structured visual questions derived from the input prompt, leading to improved compositional accuracy and text rendering with minimal overhead.",
        "tldr_zh": "该论文介绍CRAFT，一个无需训练的框架，通过显式、结构化的推理和基于代理的反馈调整，来改进文本到图像的生成。CRAFT通过根据从输入提示导出的依赖结构化视觉问题来验证生成的图像，从而在最小的开销下提高构图准确性和文本渲染。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
        "summary": "Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.",
        "url": "http://arxiv.org/abs/2512.20174v1",
        "published_date": "2025-12-23T09:14:16+00:00",
        "updated_date": "2025-12-23T09:14:16+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.IR"
        ],
        "authors": [
            "Hao Guo",
            "Xugong Qin",
            "Jun Jie Ou Yang",
            "Peng Zhang",
            "Gangyan Zeng",
            "Yubo Li",
            "Hailun Lin"
        ],
        "tldr": "This paper introduces a new dataset (NL-DIR) for Natural Language-based Document Image Retrieval, aiming to address the limitations of existing image-based methods by using natural language queries with fine-grained semantics. They provide a benchmark, evaluation metrics, and initial experiments using VLM models.",
        "tldr_zh": "本文介绍了一个新的数据集 (NL-DIR)，用于基于自然语言的文档图像检索。该数据集旨在通过使用具有细粒度语义的自然语言查询，来解决现有基于图像的方法的局限性。他们提供了一个基准、评估指标，并使用 VLM 模型进行了初步实验。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
        "summary": "Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding",
        "url": "http://arxiv.org/abs/2512.20042v1",
        "published_date": "2025-12-23T04:21:15+00:00",
        "updated_date": "2025-12-23T04:21:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nguyen Lam Phu Quy",
            "Pham Phu Hoa",
            "Tran Chi Nguyen",
            "Dao Sy Duy Minh",
            "Nguyen Hoang Minh Ngoc",
            "Huynh Trung Kiet"
        ],
        "tldr": "This paper presents a multimodal pipeline that enriches image captions with external textual knowledge retrieved from related images and articles, significantly improving caption informativeness, particularly for event-based content.",
        "tldr_zh": "该论文提出了一个多模态流程，通过从相关图像和文章中检索到的外部文本知识来丰富图像字幕，从而显著提高字幕的信息量，尤其是在基于事件的内容方面。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
        "summary": "Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.",
        "url": "http://arxiv.org/abs/2512.20013v1",
        "published_date": "2025-12-23T03:10:17+00:00",
        "updated_date": "2025-12-23T03:10:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zepeng Xin",
            "Kaiyu Li",
            "Luodi Chen",
            "Wanchen Li",
            "Yuchen Xiao",
            "Hui Qiao",
            "Weizhan Zhang",
            "Deyu Meng",
            "Xiangyong Cao"
        ],
        "tldr": "The paper introduces LaSeRS, a large-scale dataset for language-guided remote sensing image segmentation, and SegEarth-R2, an MLLM architecture designed for this task, demonstrating strong performance on the new dataset and existing benchmarks.",
        "tldr_zh": "该论文介绍了一个名为LaSeRS的大规模数据集，用于语言引导的遥感图像分割，并提出了SegEarth-R2，一个为该任务设计的MLLM架构，在新的数据集和现有基准测试中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SE360: Semantic Edit in 360$^\\circ$ Panoramas via Hierarchical Data Construction",
        "summary": "While instruction-based image editing is emerging, extending it to 360$^\\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.",
        "url": "http://arxiv.org/abs/2512.19943v1",
        "published_date": "2025-12-23T00:24:46+00:00",
        "updated_date": "2025-12-23T00:24:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyi Zhong",
            "Fang-Lue Zhang",
            "Andrew Chalmers",
            "Taehyun Rhee"
        ],
        "tldr": "The paper introduces SE360, a framework for instruction-based object editing in 360° panoramas that addresses limitations of existing methods by using a novel coarse-to-fine data generation pipeline and a Transformer-based diffusion model.",
        "tldr_zh": "该论文介绍了SE360，一个用于360°全景图中基于指令的对象编辑框架。该框架通过使用一种新颖的由粗到精的数据生成流程和一个基于Transformer的扩散模型，解决了现有方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
        "summary": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",
        "url": "http://arxiv.org/abs/2512.19686v1",
        "published_date": "2025-12-22T18:59:03+00:00",
        "updated_date": "2025-12-22T18:59:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixuan Ye",
            "Quande Liu",
            "Cong Wei",
            "Yuanxing Zhang",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Wenhan Luo"
        ],
        "tldr": "This paper introduces a Visual-Aware Chain-of-Thought (Visual-Aware CoT) approach to improve visual consistency in multi-modal generation by incorporating adaptive visual planning and iterative visual correction, achieving superior performance compared to existing methods.",
        "tldr_zh": "本文提出了一种视觉感知链式思考（Visual-Aware CoT）方法，通过结合自适应视觉规划和迭代视觉校正，以提高多模态生成中的视觉一致性，并实现了优于现有方法的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
        "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
        "url": "http://arxiv.org/abs/2512.19683v1",
        "published_date": "2025-12-22T18:58:12+00:00",
        "updated_date": "2025-12-22T18:58:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingrui Wu",
            "Zhaozhi Wang",
            "Fangjinhua Wang",
            "Jiaolong Yang",
            "Marc Pollefeys",
            "Tong Zhang"
        ],
        "tldr": "This paper introduces a new large-scale benchmark for evaluating spatial reasoning in MLLMs using outdoor, pedestrian-perspective video data with precise 3D information, revealing limitations of current MLLMs in open-world settings.",
        "tldr_zh": "该论文介绍了一个新的大规模基准，用于评估MLLM在开放世界中的空间推理能力，使用带有精确3D信息的室外行人视角视频数据，揭示了当前MLLM的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
        "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
        "url": "http://arxiv.org/abs/2512.19663v1",
        "published_date": "2025-12-22T18:41:45+00:00",
        "updated_date": "2025-12-22T18:41:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Argha Kamal Samanta",
            "Harshika Goyal",
            "Vasudha Joshi",
            "Tushar Mungle",
            "Pabitra Mitra"
        ],
        "tldr": "This paper proposes a knowledge-enhanced multimodal transformer framework for diabetic retinopathy diagnosis, integrating retinal images, clinical text, and patient data, achieving significant improvements in cross-modal retrieval and diagnostic performance compared to CLIP and other baselines.",
        "tldr_zh": "该论文提出了一种用于糖尿病视网膜病变诊断的知识增强型多模态Transformer框架，集成了视网膜图像、临床文本和患者数据，与CLIP和其他基线相比，在跨模态检索和诊断性能方面取得了显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
        "summary": "Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.",
        "url": "http://arxiv.org/abs/2512.20417v1",
        "published_date": "2025-12-23T15:01:05+00:00",
        "updated_date": "2025-12-23T15:01:05+00:00",
        "categories": [
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Pedro Domingos",
            "João Pereira",
            "Vasco Lopes",
            "João Neves",
            "David Semedo"
        ],
        "tldr": "The paper introduces Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that enhances Large Vision-Language Models for video anomaly detection by introducing inductive criminal bias, improving F1-score and anomaly classification performance.",
        "tldr_zh": "该论文介绍了 Chain-of-Anomaly-Thoughts (CoAT)，一种多代理推理框架，通过引入归纳犯罪偏见来增强大型视觉语言模型在视频异常检测方面的能力，从而提高 F1 分数和异常分类性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation",
        "summary": "With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.",
        "url": "http://arxiv.org/abs/2512.20257v1",
        "published_date": "2025-12-23T11:14:58+00:00",
        "updated_date": "2025-12-23T11:14:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniele Cardullo",
            "Simone Teglia",
            "Irene Amerini"
        ],
        "tldr": "LADLE-MM is a resource-efficient multimodal misinformation detector that utilizes learned ensembles and a fixed reference space (BLIP) to achieve competitive or superior performance compared to SOTA methods while using significantly fewer parameters and limited annotations.",
        "tldr_zh": "LADLE-MM是一个资源高效的多模态错误信息检测器，它利用学习的集成方法和固定的参考空间(BLIP)，在参数显著减少和标注数据有限的情况下，实现了与最先进方法相比具有竞争力或更优越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
        "summary": "The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.",
        "url": "http://arxiv.org/abs/2512.20145v1",
        "published_date": "2025-12-23T08:15:34+00:00",
        "updated_date": "2025-12-23T08:15:34+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Xiang Chen",
            "Yixin Ou",
            "Quan Feng",
            "Lei Li",
            "Piji Li",
            "Haibo Ye",
            "Sheng-Jun Huang",
            "Shuofei Qiao",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "tldr": "The paper introduces RetroPrompt, a retrieval-augmented prompt learning method for pre-trained foundation models that uses a knowledge base to enhance generalization and reduce rote memorization, improving performance in zero-shot and few-shot learning.",
        "tldr_zh": "该论文介绍了一种名为RetroPrompt的检索增强提示学习方法，用于预训练的基础模型。该方法利用知识库来增强泛化能力并减少死记硬背，从而提高零样本和少样本学习的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
        "summary": "Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.",
        "url": "http://arxiv.org/abs/2512.19934v1",
        "published_date": "2025-12-22T23:42:45+00:00",
        "updated_date": "2025-12-22T23:42:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Wentao Wu",
            "Xiao Wang",
            "Chenglong Li",
            "Jin Tang",
            "Bin Luo"
        ],
        "tldr": "The paper introduces VehicleMAE-V2, a novel vehicle-centric pre-trained large model that utilizes multimodal structured priors (symmetry, contour, and semantics) to enhance vehicle perception representation learning, and introduces a new large-scale dataset Autobot4M.",
        "tldr_zh": "该论文介绍了VehicleMAE-V2，一种新的以车辆为中心的预训练大模型，利用多模态结构先验（对称性、轮廓和语义）来增强车辆感知表征学习，并介绍了一个新的大型数据集Autobot4M。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
        "summary": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",
        "url": "http://arxiv.org/abs/2512.19675v1",
        "published_date": "2025-12-22T18:53:03+00:00",
        "updated_date": "2025-12-22T18:53:03+00:00",
        "categories": [
            "econ.GN",
            "cs.CV",
            "cs.DL"
        ],
        "authors": [
            "Niclas Griesshaber",
            "Jochen Streb"
        ],
        "tldr": "The paper presents a multimodal LLM pipeline using Gemini models to construct a dataset of German patents from archival image scans, claiming superior speed, cost-effectiveness, and quality compared to human research assistants, and open-sources the dataset and pipeline.",
        "tldr_zh": "该论文介绍了一个使用 Gemini 模型的多模态 LLM 流程，用于从档案图像扫描构建德国专利数据集，声称与人工研究助理相比，在速度、成本效益和质量方面都更胜一筹，并开源了数据集和流程。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]