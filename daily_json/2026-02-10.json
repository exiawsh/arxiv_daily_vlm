[
    {
        "title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models",
        "summary": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.",
        "url": "http://arxiv.org/abs/2602.08861v1",
        "published_date": "2026-02-09T16:24:53+00:00",
        "updated_date": "2026-02-09T16:24:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangtian Zheng",
            "Zishuo Wang",
            "Yuxin Peng"
        ],
        "tldr": "The paper introduces TiFRe, a text-guided video frame reduction framework for Video MLLMs that uses a text-guided sampling strategy and a frame matching and merging mechanism to reduce computational costs while improving performance on video-language tasks.",
        "tldr_zh": "该论文介绍了TiFRe，一种用于视频多模态大语言模型的文本引导视频帧减少框架，它使用文本引导采样策略和帧匹配与合并机制，以降低计算成本，同时提高视频语言任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning",
        "summary": "The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.",
        "url": "http://arxiv.org/abs/2602.08828v1",
        "published_date": "2026-02-09T16:00:01+00:00",
        "updated_date": "2026-02-09T16:00:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Tan",
            "Jun Lan",
            "Senyuan Shi",
            "Zichang Tan",
            "Zijian Yu",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Jun Wan",
            "Zhen Lei"
        ],
        "tldr": "VideoVeritas detects AI-generated videos by enhancing the perception capabilities of MLLMs through perception pretext reinforcement learning and introduces a new dataset, MintVid, for robust evaluation.",
        "tldr_zh": "VideoVeritas通过感知预训练强化学习增强MLLM的感知能力，从而检测AI生成的视频，并引入了一个新的数据集MintVid，用于鲁棒性评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing",
        "summary": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.",
        "url": "http://arxiv.org/abs/2602.08820v1",
        "published_date": "2026-02-09T15:56:05+00:00",
        "updated_date": "2026-02-09T15:56:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Yang",
            "Zhiyu Tan",
            "Jia Gong",
            "Luozheng Qin",
            "Hesen Chen",
            "Xiaomeng Yang",
            "Yuqing Sun",
            "Yuetan Lin",
            "Mengping Yang",
            "Hao Li"
        ],
        "tldr": "Omni-Video 2 connects MLLMs with video diffusion models using a lightweight adapter to achieve high-quality video generation and editing, showing superior performance on complex editing tasks and competitive generation quality.",
        "tldr_zh": "Omni-Video 2通过轻量级适配器将多模态大语言模型与视频扩散模型连接起来，实现了高质量的视频生成和编辑。在复杂的编辑任务中表现出卓越的性能，并在视频生成任务中实现了具有竞争力的质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models",
        "summary": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.",
        "url": "http://arxiv.org/abs/2602.08735v1",
        "published_date": "2026-02-09T14:39:43+00:00",
        "updated_date": "2026-02-09T14:39:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masanari Oi",
            "Koki Maeda",
            "Ryuto Koike",
            "Daisuke Oba",
            "Nakamasa Inoue",
            "Naoaki Okazaki"
        ],
        "tldr": "The paper introduces HATCH, a training framework for MLLMs that improves multi-image spatial reasoning by explicitly modeling cross-view correspondence and viewpoint transformation, achieving state-of-the-art or competitive results on multiple benchmarks.",
        "tldr_zh": "该论文介绍了HATCH，一种用于多模态大型语言模型的训练框架，通过显式建模跨视图对应关系和视点变换来提高多图像空间推理能力，并在多个基准测试中实现了最先进或有竞争力的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
        "summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
        "url": "http://arxiv.org/abs/2602.08683v1",
        "published_date": "2026-02-09T14:06:17+00:00",
        "updated_date": "2026-02-09T14:06:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feilong Tang",
            "Xiang An",
            "Yunyao Yan",
            "Yin Xie",
            "Bin Qin",
            "Kaicheng Yang",
            "Yifei Shen",
            "Yuanhan Zhang",
            "Chunyuan Li",
            "Shikun Feng",
            "Changrui Chen",
            "Huajie Tan",
            "Ming Hu",
            "Manyuan Zhang",
            "Bo Li",
            "Ziyong Feng",
            "Ziwei Liu",
            "Zongyuan Ge",
            "Jiankang Deng"
        ],
        "tldr": "The paper introduces OneVision-Encoder (OV-Encoder), a codec-aligned sparse video encoder that focuses computation on information-rich regions, achieving state-of-the-art performance on multimodal tasks while using fewer resources compared to existing vision backbones.",
        "tldr_zh": "该论文介绍了OneVision-Encoder (OV-Encoder)，一种编解码器对齐的稀疏视频编码器，它将计算集中在信息丰富的区域，在多模态任务上实现了最先进的性能，同时与现有的视觉骨干网络相比使用了更少的资源。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
        "summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.",
        "url": "http://arxiv.org/abs/2602.08503v1",
        "published_date": "2026-02-09T10:55:13+00:00",
        "updated_date": "2026-02-09T10:55:13+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Yi Ding",
            "Ziliang Qiu",
            "Bolian Li",
            "Ruqi Zhang"
        ],
        "tldr": "This paper introduces 'Octopus,' a reinforcement learning framework for improving self-correction in vision-language models by synthesizing self-correction examples from existing rollouts, achieving state-of-the-art performance among open-source VLMs.",
        "tldr_zh": "该论文介绍了'Octopus'，一个强化学习框架，通过从现有rollout中合成自我纠正的例子，来改进视觉语言模型中的自我纠正，并在开源视觉语言模型中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries",
        "summary": "Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.",
        "url": "http://arxiv.org/abs/2602.08448v1",
        "published_date": "2026-02-09T10:00:22+00:00",
        "updated_date": "2026-02-09T10:00:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haocheng Lu",
            "Nan Zhang",
            "Wei Tao",
            "Xiaoyang Qu",
            "Guokuan Li",
            "Jiguang Wan",
            "Jianzong Wang"
        ],
        "tldr": "The paper introduces Vista, a scene-aware framework for streaming video question answering that dynamically segments, compresses, and recalls video scenes to improve efficiency and scalability in long-form, real-time scenarios.",
        "tldr_zh": "该论文介绍了Vista，一个场景感知的流视频问答框架，它动态地分割、压缩和召回视频场景，以提高在长时、实时场景中的效率和可扩展性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning",
        "summary": "The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.",
        "url": "http://arxiv.org/abs/2602.08346v1",
        "published_date": "2026-02-09T07:31:14+00:00",
        "updated_date": "2026-02-09T07:31:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujin Zhou",
            "Pengcheng Wen",
            "Jiale Chen",
            "Boqin Yin",
            "Han Zhu",
            "Jiaming Ji",
            "Juntao Dai",
            "Chi-Min Chan",
            "Sirui Han"
        ],
        "tldr": "This paper introduces a new benchmark for evaluating Process Reward Models (PRMs) specifically designed for the \"thinking with images\" paradigm, highlighting the shortcomings of current LVLMs as PRMs and defining fine-grained error types in visual reasoning.",
        "tldr_zh": "本文介绍了一个新的基准，用于评估专门为“图像思维”范例设计的流程奖励模型 (PRM)，强调了当前 LVLM 作为 PRM 的缺点，并定义了视觉推理中的细粒度错误类型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science",
        "summary": "Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.",
        "url": "http://arxiv.org/abs/2602.08342v1",
        "published_date": "2026-02-09T07:28:49+00:00",
        "updated_date": "2026-02-09T07:28:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jie Zhang",
            "Xingtong Yu",
            "Yuan Fang",
            "Rudi Stouffs",
            "Zdravko Trivic"
        ],
        "tldr": "This paper introduces a spatially grounded dataset (UGData) and a two-stage training strategy (UGE) for learning transferable multimodal embeddings in urban environments, demonstrating significant improvements in urban understanding tasks using various VLM backbones.",
        "tldr_zh": "本文介绍了一个空间接地的城市环境多模态嵌入数据集（UGData）和一个两阶段训练策略（UGE），利用多个VLM骨干网络，在城市理解任务中取得了显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT",
        "summary": "Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.",
        "url": "http://arxiv.org/abs/2602.08339v1",
        "published_date": "2026-02-09T07:26:40+00:00",
        "updated_date": "2026-02-09T07:26:40+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chengyi Du",
            "Yazhe Niu",
            "Dazhong Shen",
            "Luxin Xu"
        ],
        "tldr": "The paper introduces CoTZero, an annotation-free method for improving VLMs' visual reasoning by synthesizing hierarchical chain-of-thought data and using cognitively coherent rewards during fine-tuning, leading to improved performance on a multi-level semantic inconsistency benchmark.",
        "tldr_zh": "该论文介绍了CoTZero，一种无需标注的方法，通过合成分层思维链数据并在微调期间使用认知连贯奖励来改善视觉语言模型的视觉推理能力，从而在多层次语义不一致性基准测试中提高了性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models",
        "summary": "To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.",
        "url": "http://arxiv.org/abs/2602.08336v1",
        "published_date": "2026-02-09T07:17:57+00:00",
        "updated_date": "2026-02-09T07:17:57+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Cheng Yang",
            "Chufan Shi",
            "Bo Shui",
            "Yaokang Wu",
            "Muzi Tao",
            "Huijuan Wang",
            "Ivan Yee Lee",
            "Yong Liu",
            "Xuezhe Ma",
            "Taylor Berg-Kirkpatrick"
        ],
        "tldr": "The paper introduces UReason, a benchmark to evaluate the effectiveness of reasoning in multimodal image generation models. It reveals a 'Reasoning Paradox' where reasoning traces improve performance, but their direct integration hinders visual synthesis due to contextual interference.",
        "tldr_zh": "该论文介绍了UReason，一个用于评估多模态图像生成模型中推理有效性的基准。研究揭示了一个“推理悖论”，即推理轨迹可以提高性能，但由于上下文干扰，它们的直接集成会阻碍视觉合成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
        "summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.",
        "url": "http://arxiv.org/abs/2602.08241v1",
        "published_date": "2026-02-09T03:33:23+00:00",
        "updated_date": "2026-02-09T03:33:23+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Siqu Ou",
            "Tianrui Wan",
            "Zhiyuan Zhao",
            "Junyu Gao",
            "Xuelong Li"
        ],
        "tldr": "The paper introduces SAYO, a reinforcement learning-trained visual reasoning model for MLLMs that improves visual attention and performance on multimodal tasks by providing region-level visual attention-based rewards during training.",
        "tldr_zh": "该论文介绍了一种名为SAYO的视觉推理模型，它通过强化学习训练，利用区域级别的视觉注意力奖励，改善了多模态LLM的视觉注意力和在多模态任务上的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
        "summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
        "url": "http://arxiv.org/abs/2602.08236v1",
        "published_date": "2026-02-09T03:21:48+00:00",
        "updated_date": "2026-02-09T03:21:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Shoubin Yu",
            "Yue Zhang",
            "Zun Wang",
            "Jaehong Yoon",
            "Huaxiu Yao",
            "Mingyu Ding",
            "Mohit Bansal"
        ],
        "tldr": "This paper introduces AVIC, an adaptive framework that selectively invokes world models for visual imagination in spatial reasoning, demonstrating improved efficiency and accuracy by reasoning about the sufficiency of static visual evidence before engaging imagination.",
        "tldr_zh": "本文介绍了一个自适应框架AVIC，该框架选择性地调用世界模型进行空间推理中的视觉想象，通过在进行想象之前推理静态视觉证据的充分性，从而提高了效率和准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension",
        "summary": "Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.",
        "url": "http://arxiv.org/abs/2602.08211v1",
        "published_date": "2026-02-09T02:22:39+00:00",
        "updated_date": "2026-02-09T02:22:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yik Lung Pang",
            "Changjae Oh"
        ],
        "tldr": "The paper introduces a training-free \"Chain-of-Caption\" framework to enhance the Referring Expression Comprehension (REC) performance of Multimodal Large Language Models (MLLMs) by incorporating additional visual and textual context. Experiments demonstrate significant performance gains on standard REC datasets.",
        "tldr_zh": "本文提出了一种名为“Chain-of-Caption”的免训练框架，通过整合额外的视觉和文本信息来提升多模态大型语言模型（MLLM）在指代表达理解（REC）任务中的表现。实验表明，该方法在标准REC数据集上取得了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Robustness of Vision Language Models Against Split-Image Harmful Input Attacks",
        "summary": "Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.",
        "url": "http://arxiv.org/abs/2602.08136v1",
        "published_date": "2026-02-08T21:52:42+00:00",
        "updated_date": "2026-02-08T21:52:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Md Rafi Ur Rashid",
            "MD Sadik Hossain Shanto",
            "Vishnu Asutosh Dasu",
            "Shagufta Mehnaz"
        ],
        "tldr": "This paper introduces a novel split-image visual jailbreak attack (SIVA) against VLMs, exploiting vulnerabilities in safety alignment, and proposes methods to address this issue. The proposed attack demonstrates superior transferability compared to existing methods.",
        "tldr_zh": "本文提出了一种新型的分割图像视觉越狱攻击（SIVA），旨在利用视觉语言模型（VLM）在安全性对齐方面的漏洞。该攻击在不同模型间的迁移性优于现有方法，并提出解决此问题的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval",
        "summary": "Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.",
        "url": "http://arxiv.org/abs/2602.08099v1",
        "published_date": "2026-02-08T19:39:32+00:00",
        "updated_date": "2026-02-08T19:39:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Issar Tzachor",
            "Dvir Samuel",
            "Rami Ben-Ari"
        ],
        "tldr": "The paper proposes a method, VidVec, that leverages intermediate layers of MLLMs for video-text embedding and retrieval, achieving state-of-the-art performance without visual fine-tuning by using a text-based alignment strategy.",
        "tldr_zh": "该论文提出了一种名为VidVec的方法，利用MLLM的中间层进行视频-文本嵌入和检索，通过使用基于文本的对齐策略，无需视觉微调即可实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing",
        "summary": "Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \\href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.",
        "url": "http://arxiv.org/abs/2602.08725v1",
        "published_date": "2026-02-09T14:34:18+00:00",
        "updated_date": "2026-02-09T14:34:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongwen Lai",
            "Chaoqun Wang",
            "Shaobo Min"
        ],
        "tldr": "FusionEdit is a training-free image editing framework using semantic fusion and attention modulation to achieve precise and controllable edits without hard mask artifacts, outperforming existing methods.",
        "tldr_zh": "FusionEdit 是一种无需训练的图像编辑框架，它利用语义融合和注意力调制来实现精确且可控的编辑，消除了硬掩码伪影，并且优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation",
        "summary": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.",
        "url": "http://arxiv.org/abs/2602.08682v1",
        "published_date": "2026-02-09T14:06:03+00:00",
        "updated_date": "2026-02-09T14:06:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Guo",
            "Qijun Gan",
            "Yifu Zhang",
            "Jinlai Liu",
            "Yifei Hu",
            "Pan Xie",
            "Dongjun Qian",
            "Yu Zhang",
            "Ruiqi Li",
            "Yuqi Zhang",
            "Ruibiao Lu",
            "Xiaofeng Mei",
            "Bo Han",
            "Xiang Yin",
            "Bingyue Peng",
            "Zehuan Yuan"
        ],
        "tldr": "ALIVE is a new audio-video generation model built upon a pretrained T2V model, enabling Text-to-Video&Audio and Reference-to-Video&Audio capabilities, outperforming existing open-source models.",
        "tldr_zh": "ALIVE是一个新的音视频生成模型，建立在预训练的T2V模型之上，实现了文本到视频音频和参考到视频音频的功能，并且优于现有的开源模型。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving",
        "summary": "Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus",
        "url": "http://arxiv.org/abs/2602.08524v1",
        "published_date": "2026-02-09T11:15:01+00:00",
        "updated_date": "2026-02-09T11:15:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linger Deng",
            "Yuliang Liu",
            "Wenwen Yu",
            "Zujia Zhang",
            "Jianzhong Ju",
            "Zhenbo Luo",
            "Xiang Bai"
        ],
        "tldr": "GeoFocus enhances geometry problem-solving in LMMs by using a Critical Local Perceptor and VertexLang to improve local feature coverage and global topology recognition, respectively, resulting in better accuracy and robustness.",
        "tldr_zh": "GeoFocus通过使用关键局部感知器和VertexLang来增强LMM中的几何问题解决能力，分别提高局部特征覆盖率和全局拓扑识别，从而提高准确性和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
        "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
        "url": "http://arxiv.org/abs/2602.08439v1",
        "published_date": "2026-02-09T09:51:29+00:00",
        "updated_date": "2026-02-09T09:51:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Dong",
            "Shulin Tian",
            "Shuai Liu",
            "Shuangrui Ding",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Yuhang Cao",
            "Jiaqi Wang",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces a new task, benchmark (Demo-ICL-Bench), and MLLM (Demo-ICL) for in-context learning of procedural knowledge from videos, using demonstrations to answer questions about target videos. It addresses the gap in current video understanding benchmarks that focus on static knowledge rather than adaptation from dynamic contexts.",
        "tldr_zh": "该论文介绍了一项新的任务、基准测试（Demo-ICL-Bench）和一个多模态大型语言模型（Demo-ICL），用于从视频中进行情境学习，学习程序性知识，通过演示来回答关于目标视频的问题。它弥补了当前视频理解基准测试的不足，这些基准测试侧重于静态知识，而不是从动态环境中进行适应。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.",
        "url": "http://arxiv.org/abs/2602.08392v1",
        "published_date": "2026-02-09T08:47:14+00:00",
        "updated_date": "2026-02-09T08:47:14+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xin Wu",
            "Zhixuan Liang",
            "Yue Ma",
            "Mengkang Hu",
            "Zhiyuan Qin",
            "Xiu Li"
        ],
        "tldr": "The paper introduces BiManiBench, a hierarchical benchmark for evaluating the bimanual coordination abilities of MLLMs, revealing their limitations in dual-arm spatial grounding and control despite proficiency in high-level reasoning.",
        "tldr_zh": "该论文介绍了BiManiBench，一个用于评估多模态大语言模型双臂协调能力的层级基准，揭示了它们在高层次推理方面表现出色，但在双臂空间定位和控制方面存在局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Language-Guided Transformer Tokenizer for Human Motion Generation",
        "summary": "In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.",
        "url": "http://arxiv.org/abs/2602.08337v1",
        "published_date": "2026-02-09T07:22:14+00:00",
        "updated_date": "2026-02-09T07:22:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Yan",
            "Yong Wang",
            "Xin Du",
            "Junsong Yuan",
            "Mengyuan Liu"
        ],
        "tldr": "This paper introduces Language-Guided Tokenization (LG-Tok), a Transformer-based tokenizer that aligns natural language with motion for efficient human motion generation, achieving state-of-the-art results on HumanML3D and Motion-X datasets.",
        "tldr_zh": "本文介绍了一种语言引导的 Tokenization (LG-Tok) 方法，这是一种基于 Transformer 的 tokenizer，它将自然语言与运动对齐，以实现高效的人体运动生成，并在 HumanML3D 和 Motion-X 数据集上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning",
        "summary": "Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.",
        "url": "http://arxiv.org/abs/2602.08167v1",
        "published_date": "2026-02-09T00:10:17+00:00",
        "updated_date": "2026-02-09T00:10:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Milan Ganai",
            "Katie Luo",
            "Jonas Frey",
            "Clark Barrett",
            "Marco Pavone"
        ],
        "tldr": "The paper introduces R&B-EnCoRe, a self-supervised method for bootstrapping embodied reasoning in VLA models by refining reasoning training data without external rewards or human annotation, leading to performance improvements across various embodied tasks.",
        "tldr_zh": "该论文介绍了R&B-EnCoRe，一种自监督方法，通过提炼推理训练数据来引导VLA模型中的具身推理，无需外部奖励或人工标注，从而在各种具身任务中实现性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]