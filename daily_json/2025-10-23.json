[
    {
        "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
        "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
        "url": "http://arxiv.org/abs/2510.19808v1",
        "published_date": "2025-10-22T17:43:15+00:00",
        "updated_date": "2025-10-22T17:43:15+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Yusu Qian",
            "Eli Bocek-Rivele",
            "Liangchen Song",
            "Jialing Tong",
            "Yinfei Yang",
            "Jiasen Lu",
            "Wenze Hu",
            "Zhe Gan"
        ],
        "tldr": "The paper introduces Pico-Banana-400K, a large-scale dataset of 400K real images with text-guided editing instructions, designed to facilitate research on text-guided image editing tasks, including multi-turn editing and instruction summarization.",
        "tldr_zh": "该论文介绍了Pico-Banana-400K，一个包含40万张真实图像的大规模数据集，附带文本引导的编辑指令，旨在促进文本引导图像编辑任务的研究，包括多轮编辑和指令总结。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement Learning and Local Zoom",
        "summary": "General-purpose large Vision-Language Models (VLMs) demonstrate strong\ncapabilities in generating detailed descriptions for natural images. However,\ntheir performance in the medical domain remains suboptimal, even for relatively\nstraightforward tasks, primarily due to the lack of large-scale, high-quality,\nspecialized medical imaging datasets and the neglect of the diagnostic process\nthat progresses from coarse to fine-grained. To address the first issue, we\nconstruct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second\nissue, we propose MedReason-R1, a medical VLM with explicit reasoning process\nfor disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds\nzoom-in disease region-of-interest areas into the image, highlighting the\ncrucial role of both global localization and disease-specific details in\nenhancing the model's diagnostic performance. Furthermore, we introduce the\nGRPO reinforcement learning framework to MedReason-R1, which enables effective\nreasoning without relying on costly manual annotations. Compared to recent\ngeneral-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art\nperformance in CT disease diagnosis while retaining generalization. The code,\ncheckpoints, and dataset are available at:\nhttps://github.com/Leevan001/MedReason-R1",
        "url": "http://arxiv.org/abs/2510.19626v1",
        "published_date": "2025-10-22T14:21:59+00:00",
        "updated_date": "2025-10-22T14:21:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Li",
            "Fenghe Tang",
            "Yingtai Li",
            "Shaohua Kevin Zhou"
        ],
        "tldr": "The paper introduces MedReason-R1, a medical VLM for CT diagnosis using reinforcement learning and a zoom-in strategy, along with a new CT-RATE-VQA dataset. It achieves state-of-the-art performance and generalization in CT disease diagnosis.",
        "tldr_zh": "该论文介绍了MedReason-R1，一种用于CT诊断的医学VLM，它使用强化学习和局部缩放策略，并构建了一个新的CT-RATE-VQA数据集。它在CT疾病诊断方面实现了最先进的性能和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography",
        "summary": "Vision-language models (VLMs) have recently shown remarkable zero-shot\nperformance in medical image understanding, yet their grounding ability, the\nextent to which textual concepts align with visual evidence, remains\nunderexplored. In the medical domain, however, reliable grounding is essential\nfor interpretability and clinical adoption. In this work, we present the first\nsystematic benchmark for evaluating cross-modal interpretability in chest\nX-rays across seven CLIP-style VLM variants. We generate visual explanations\nusing cross-attention and similarity-based localization maps, and\nquantitatively assess their alignment with radiologist-annotated regions across\nmultiple pathologies. Our analysis reveals that: (1) while all VLM variants\ndemonstrate reasonable localization for large and well-defined pathologies,\ntheir performance substantially degrades for small or diffuse lesions; (2)\nmodels that are pretrained on chest X-ray-specific datasets exhibit improved\nalignment compared to those trained on general-domain data. (3) The overall\nrecognition ability and grounding ability of the model are strongly correlated.\nThese findings underscore that current VLMs, despite their strong recognition\nability, still fall short in clinically reliable grounding, highlighting the\nneed for targeted interpretability benchmarks before deployment in medical\npractice. XBench code is available at\nhttps://github.com/Roypic/Benchmarkingattention",
        "url": "http://arxiv.org/abs/2510.19599v1",
        "published_date": "2025-10-22T13:52:19+00:00",
        "updated_date": "2025-10-22T13:52:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haozhe Luo",
            "Shelley Zixin Shu",
            "Ziyu Zhou",
            "Sebastian Otalora",
            "Mauricio Reyes"
        ],
        "tldr": "The paper introduces XBench, a new benchmark for evaluating visual grounding in vision-language models (VLMs) applied to chest X-rays, revealing limitations in current VLMs' ability to accurately localize pathologies, especially small ones.",
        "tldr_zh": "该论文介绍了XBench，一个新的基准，用于评估视觉语言模型（VLMs）在胸部X光片中的视觉定位能力，揭示了当前VLMs在准确定位病变（尤其是小病变）方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation",
        "summary": "Multimodal large language models (MLLMs) demonstrate strong video\nunderstanding by attending to visual tokens relevant to textual queries. To\ndirectly adapt this for localization in a training-free manner, we cast video\nreasoning segmentation as a video QA task and extract attention maps via\nrollout mechanism. However, raw attention maps are noisy and poorly aligned\nwith object regions. We propose Decomposed Attention Fusion (DecAF), which\nrefines these maps through two mechanisms: (1) contrastive object-background\nfusion and (2) complementary video-frame fusion. This method suppresses\nirrelevant activations and enhances object-focused cues, enabling direct\nconversion of attention maps into coarse segmentation masks. In addition, we\nintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.\nUnlike existing methods that jointly train MLLMs with SAM, our method operates\nentirely without retraining. DecAF outperforms training-free methods and\nachieves performance comparable to training-based methods on both referring and\nreasoning VOS benchmarks. The code will be available at\nhttps://github.com/HYUNJS/DecAF.",
        "url": "http://arxiv.org/abs/2510.19592v1",
        "published_date": "2025-10-22T13:42:59+00:00",
        "updated_date": "2025-10-22T13:42:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Su Ho Han",
            "Jeongseok Hyun",
            "Pilhyeon Lee",
            "Minho Shim",
            "Dongyoon Wee",
            "Seon Joo Kim"
        ],
        "tldr": "The paper introduces Decomposed Attention Fusion (DecAF), a training-free method for video reasoning segmentation in MLLMs, refining attention maps for improved localization and achieving competitive performance compared to training-based methods.",
        "tldr_zh": "该论文介绍了分解注意力融合（DecAF），一种MLLM中用于视频推理分割的免训练方法，通过优化注意力图来提高定位效果，并实现了与基于训练的方法相当的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CARES: Context-Aware Resolution Selector for VLMs",
        "summary": "Large vision-language models (VLMs) commonly process images at native or high\nresolution to remain effective across tasks. This inflates visual tokens ofter\nto 97-99% of total tokens, resulting in high compute and latency, even when\nlow-resolution images would suffice. We introduce \\emph{CARES}-a\n\\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a\nlightweight preprocessing module that, given an image-query pair, predicts the\n\\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to\nextract features and predict when a target pretrained VLM's response converges\nto its peak ability to answer correctly. Though trained as a discrete\nclassifier over a set of optional resolutions, CARES interpolates continuous\nresolutions at inference for fine-grained control. Across five multimodal\nbenchmarks spanning documents and natural images, as well as diverse target\nVLMs, CARES preserves task performance while reducing compute by up to 80%.",
        "url": "http://arxiv.org/abs/2510.19496v1",
        "published_date": "2025-10-22T11:44:31+00:00",
        "updated_date": "2025-10-22T11:44:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Moshe Kimhi",
            "Nimrod Shabtay",
            "Raja Giryes",
            "Chaim Baskin",
            "Eli Schwartz"
        ],
        "tldr": "The paper introduces CARES, a context-aware resolution selector for VLMs that dynamically predicts the minimal sufficient input resolution, reducing computation by up to 80% while maintaining performance across various tasks.",
        "tldr_zh": "该论文介绍了一种名为CARES的上下文感知分辨率选择器，用于视觉语言模型，它可以动态预测最小足够的输入分辨率，在保持各种任务性能的同时，将计算量减少高达80%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
        "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.",
        "url": "http://arxiv.org/abs/2510.19430v1",
        "published_date": "2025-10-22T09:57:13+00:00",
        "updated_date": "2025-10-22T09:57:13+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "GigaBrain Team",
            "Angen Ye",
            "Boyuan Wang",
            "Chaojun Ni",
            "Guan Huang",
            "Guosheng Zhao",
            "Haoyun Li",
            "Jie Li",
            "Jiagang Zhu",
            "Lv Feng",
            "Peng Li",
            "Qiuping Deng",
            "Runqi Ouyang",
            "Wenkang Qin",
            "Xinze Chen",
            "Xiaofeng Wang",
            "Yang Wang",
            "Yifan Li",
            "Yilong Li",
            "Yiran Ding",
            "Yuan Xu",
            "Yun Ye",
            "Yukun Zhou",
            "Zhehao Dong",
            "Zhenan Wang",
            "Zhichao Liu",
            "Zheng Zhu"
        ],
        "tldr": "GigaBrain-0 is a Vision-Language-Action model trained using world model-generated data to improve generalization and reduce reliance on real robot data, demonstrating superior performance on real-world robotic tasks. A lightweight version, GigaBrain-0-Small, is also presented for efficient deployment.",
        "tldr_zh": "GigaBrain-0是一个视觉-语言-动作模型，它使用世界模型生成的数据进行训练，以提高泛化能力并减少对真实机器人数据的依赖，并在现实世界的机器人任务中表现出卓越的性能。此外，还推出了轻量级版本GigaBrain-0-Small，以便于高效部署。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes",
        "summary": "Vision-language models (VLMs) are essential to Embodied AI, enabling robots\nto perceive, reason, and act in complex environments. They also serve as the\nfoundation for the recent Vision-Language-Action (VLA) models. Yet most\nevaluations of VLMs focus on single-view settings, leaving their ability to\nintegrate multi-view information underexplored. At the same time, multi-camera\nsetups are increasingly standard in robotic platforms, as they provide\ncomplementary perspectives to mitigate occlusion and depth ambiguity. Whether\nVLMs can effectively leverage such multi-view inputs for robotic reasoning\ntherefore remains an open question. To bridge this gap, we introduce\nMV-RoboBench, a benchmark specifically designed to evaluate the multi-view\nspatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench\nconsists of 1.7k manually curated QA items across eight subtasks, divided into\ntwo primary categories: spatial understanding and robotic execution. We\nevaluate a diverse set of existing VLMs, including both open-source and\nclosed-source models, along with enhanced versions incorporating CoT-inspired\ntechniques. The results show that state-of-the-art models remain far below\nhuman performance, underscoring the substantial challenges VLMs face in\nmulti-view robotic perception. Additionally, our analysis uncovers two key\nfindings: (i) spatial intelligence and robotic task execution are positively\ncorrelated in multi-view robotic scenarios; and (ii) strong performance on\nexisting general-purpose single-view spatial understanding benchmarks does not\nreliably translate to success in the robotic spatial tasks assessed by our\nbenchmark. We release MV-RoboBench as an open resource to foster progress in\nspatially grounded VLMs and VLAs, providing not only data but also a\nstandardized evaluation protocol for multi-view embodied reasoning.",
        "url": "http://arxiv.org/abs/2510.19400v1",
        "published_date": "2025-10-22T09:20:09+00:00",
        "updated_date": "2025-10-22T09:20:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyuan Feng",
            "Zhaolu Kang",
            "Qijie Wang",
            "Zhiying Du",
            "Jiongrui Yan",
            "Shubin Shi",
            "Chengbo Yuan",
            "Huizhi Liang",
            "Yu Deng",
            "Qixiu Li",
            "Rushuai Yang",
            "Arctanx An",
            "Leqi Zheng",
            "Weijie Wang",
            "Shawn Chen",
            "Sicheng Xu",
            "Yaobo Liang",
            "Jiaolong Yang",
            "Baining Guo"
        ],
        "tldr": "The paper introduces MV-RoboBench, a new benchmark for evaluating the multi-view spatial reasoning abilities of vision-language models in robotic manipulation, revealing that current models struggle with this task and that single-view performance doesn't guarantee multi-view success.",
        "tldr_zh": "该论文介绍了MV-RoboBench，这是一个新的基准，用于评估视觉语言模型在机器人操作中多视角的空间推理能力。结果表明，当前的模型在此任务中表现不佳，并且单视角性能不能保证多视角成功。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.",
        "url": "http://arxiv.org/abs/2510.19307v1",
        "published_date": "2025-10-22T07:12:14+00:00",
        "updated_date": "2025-10-22T07:12:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Byung-Kwan Lee",
            "Ryo Hachiuma",
            "Yong Man Ro",
            "Yu-Chiang Frank Wang",
            "Yueh-Hua Wu"
        ],
        "tldr": "This paper introduces a Unified Reinforcement and Imitation Learning (RIL) algorithm to train efficient, lightweight Vision-Language Models (VLMs) that can compete with larger models by combining reinforcement learning and adversarial imitation learning.",
        "tldr_zh": "本文介绍了一种统一的强化学习和模仿学习（RIL）算法，旨在训练高效、轻量级的视觉语言模型（VLM），通过结合强化学习和对抗模仿学习，使其能够与更大的模型竞争。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning",
        "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
        "url": "http://arxiv.org/abs/2510.19183v1",
        "published_date": "2025-10-22T02:41:07+00:00",
        "updated_date": "2025-10-22T02:41:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fengyuan Sun",
            "Hui Chen",
            "Xinhao Xu",
            "Dandan Zheng",
            "Jingdong Chen",
            "Jun Zhou",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "tldr": "This paper introduces PruneHal, a training-free method that uses adaptive KV cache pruning to reduce hallucinations in MLLMs by improving attention to visual tokens, showing strong results on hallucination benchmarks.",
        "tldr_zh": "该论文介绍了一种名为PruneHal的免训练方法，该方法通过自适应KV缓存修剪来减少MLLM中的幻觉，通过增强对视觉tokens的注意力，在幻觉基准测试中表现出强大的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions",
        "summary": "While vision-language models (VLMs) have advanced into detailed image\ndescription, evaluation remains a challenge. Standard metrics (e.g. CIDEr,\nSPICE) were designed for short texts and tuned to recognize errors that are now\nuncommon, such as object misidentification. In contrast, long texts require\nsensitivity to attribute and relation attachments and scores that localize\nerrors to particular text spans. In this work, we introduce PoSh, a metric for\ndetailed image description that uses scene graphs as structured rubrics to\nguide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained\nerrors (e.g. mistakes in compositional understanding). PoSh is replicable,\ninterpretable and a better proxy for human raters than existing metrics\n(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new\ndataset, DOCENT. This novel benchmark contains artwork, paired with\nexpert-written references, and model-generated descriptions, augmented with\ngranular and coarse judgments of their quality from art history students. Thus,\nDOCENT enables evaluating both detailed image description metrics and detailed\nimage description itself in a challenging new domain. We show that PoSh\nachieves stronger correlations (+0.05 Spearman $\\rho$) with the human judgments\nin DOCENT than the best open-weight alternatives, is robust to image type\n(using CapArena, an existing dataset of web imagery) and is a capable reward\nfunction, outperforming standard supervised fine-tuning. Then, using PoSh, we\ncharacterize the performance of open and closed models in describing the\npaintings, sketches and statues in DOCENT and find that foundation models\nstruggle to achieve full, error-free coverage of images with rich scene\ndynamics, establishing a demanding new task to gauge VLM progress. Through both\nPoSh and DOCENT, we hope to enable advances in important areas such as\nassistive text generation.",
        "url": "http://arxiv.org/abs/2510.19060v1",
        "published_date": "2025-10-21T20:30:20+00:00",
        "updated_date": "2025-10-21T20:30:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Amith Ananthram",
            "Elias Stengel-Eskin",
            "Lorena A. Bradford",
            "Julia Demarest",
            "Adam Purvis",
            "Keith Krut",
            "Robert Stein",
            "Rina Elster Pantalony",
            "Mohit Bansal",
            "Kathleen McKeown"
        ],
        "tldr": "The paper introduces PoSh, a new metric using scene graphs and LLMs to evaluate detailed image descriptions, and a new dataset, DOCENT, for artwork description, demonstrating PoSh's superiority over existing metrics and highlighting challenges for VLMs in complex scenes.",
        "tldr_zh": "本文介绍了一种新的评估指标 PoSh，它使用场景图和大型语言模型来评估详细的图像描述，并引入了一个新的数据集 DOCENT 用于艺术作品描述。实验表明，PoSh 优于现有指标，并突出了视觉语言模型在复杂场景中面临的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "olmOCR 2: Unit Test Rewards for Document OCR",
        "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
        "url": "http://arxiv.org/abs/2510.19817v1",
        "published_date": "2025-10-22T17:53:02+00:00",
        "updated_date": "2025-10-22T17:53:02+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jake Poznanski",
            "Luca Soldaini",
            "Kyle Lo"
        ],
        "tldr": "The paper introduces olmOCR 2, a 7B VLM for OCR trained with reinforcement learning using unit test rewards, achieving state-of-the-art results on an OCR benchmark, especially in math formula, table, and multi-column layout parsing. The model, data, and code are released under open licenses.",
        "tldr_zh": "本文介绍了olmOCR 2，一个用于OCR的7B VLM，通过强化学习和单元测试奖励进行训练，在OCR基准测试上取得了最先进的结果，尤其是在数学公式、表格和多列布局解析方面。该模型、数据和代码均以开放许可发布。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization\nthrough large-scale image-text pretraining, yet their performance can drop once\nthe deployment distribution diverges from the training distribution. To address\nthis, Test-Time Adaptation (TTA) methods update models using unlabeled target\ndata. However, existing approaches often ignore two key challenges: prototype\ndegradation in long-tailed distributions and confusion between semantically\nsimilar classes. To tackle these issues, we propose \\textbf{C}lass-Aware\n\\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative\n\\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed\nspecifically for VLMs to enhance generalization under distribution shifts.\nCPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that\ndynamically adjusts per-class capacity based on test-time frequency and\nactivation history, with a rejuvenation mechanism for inactive classes to\nretain rare-category knowledge. Additionally, a \\textit{Negative Contrastive\nLearning} Mechanism identifies and constrains hard visual-textual negatives to\nimprove class separability. The framework employs asymmetric optimization,\nrefining only textual prototypes while anchoring on stable visual features.\nExperiments on 15 benchmarks show that CPL-NC consistently outperforms prior\nTTA methods across both ResNet-50 and ViT-B/16 backbones.",
        "url": "http://arxiv.org/abs/2510.19802v1",
        "published_date": "2025-10-22T17:38:35+00:00",
        "updated_date": "2025-10-22T17:38:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaozhen Qiao",
            "Jingkai Zhao",
            "Yuqiu Jiang",
            "Xianda Guo",
            "Zhe Sun",
            "Hongyuan Zhang",
            "Xuelong Li"
        ],
        "tldr": "This paper introduces CPL-NC, a Test-Time Adaptation (TTA) framework for Vision-Language Models (VLMs) that uses class-aware prototype learning and negative contrastive learning to improve performance under distribution shifts, especially in long-tailed distributions.",
        "tldr_zh": "该论文介绍了一种针对视觉-语言模型（VLM）的测试时自适应（TTA）框架CPL-NC，它利用类别感知的原型学习和负对比学习来提高在分布偏移下的性能，尤其是在长尾分布中。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "[De|Re]constructing VLMs' Reasoning in Counting",
        "summary": "Vision-Language Models (VLMs) have recently gained attention due to their\ncompetitive performance on multiple downstream tasks, achieved by following\nuser-input instructions. However, VLMs still exhibit several limitations in\nvisual reasoning, such as difficulties in identifying relations (e.g., spatial,\ntemporal, and among objects), understanding temporal sequences (e.g., frames),\nand counting objects. In this work, we go beyond score-level benchmark\nevaluations of VLMs by investigating the underlying causes of their failures\nand proposing a targeted approach to improve their reasoning capabilities. We\nstudy the reasoning skills of seven state-of-the-art VLMs in the counting task\nunder controlled experimental conditions. Our experiments show that VLMs are\nhighly sensitive to the number and type of objects, their spatial arrangement,\nand the co-occurrence of distractors. A layer-wise analysis reveals that errors\nare due to incorrect mapping of the last-layer representation into the output\nspace. Our targeted training shows that fine-tuning just the output layer\nimproves accuracy by up to 21%. We corroborate these findings by achieving\nconsistent improvements on real-world datasets.",
        "url": "http://arxiv.org/abs/2510.19555v1",
        "published_date": "2025-10-22T13:08:47+00:00",
        "updated_date": "2025-10-22T13:08:47+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Simone Alghisi",
            "Gabriel Roccabruna",
            "Massimo Rizzoli",
            "Seyed Mahed Mousavi",
            "Giuseppe Riccardi"
        ],
        "tldr": "This paper analyzes the reasoning abilities of VLMs in counting tasks, identifies factors affecting their performance, and proposes a targeted fine-tuning approach to improve accuracy.",
        "tldr_zh": "本文分析了视觉语言模型在计数任务中的推理能力，确定了影响模型性能的因素，并提出了一种有针对性的微调方法来提高准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance across various objective multimodal perception tasks, yet their\napplication to subjective, emotionally nuanced domains, such as psychological\nanalysis, remains largely unexplored. In this paper, we introduce PICK, a\nmulti-step framework designed for Psychoanalytical Image Comprehension through\nhierarchical analysis and Knowledge injection with MLLMs, specifically focusing\non the House-Tree-Person (HTP) Test, a widely used psychological assessment in\nclinical practice. First, we decompose drawings containing multiple instances\ninto semantically meaningful sub-drawings, constructing a hierarchical\nrepresentation that captures spatial structure and content across three levels:\nsingle-object level, multi-object level, and whole level. Next, we analyze\nthese sub-drawings at each level with a targeted focus, extracting\npsychological or emotional insights from their visual cues. We also introduce\nan HTP knowledge base and design a feature extraction module, trained with\nreinforcement learning, to generate a psychological profile for single-object\nlevel analysis. This profile captures both holistic stylistic features and\ndynamic object-specific features (such as those of the house, tree, or person),\ncorrelating them with psychological states. Finally, we integrate these\nmulti-faceted information to produce a well-informed assessment that aligns\nwith expert-level reasoning. Our approach bridges the gap between MLLMs and\nspecialized expert domains, offering a structured and interpretable framework\nfor understanding human mental states through visual expression. Experimental\nresults demonstrate that the proposed PICK significantly enhances the\ncapability of MLLMs in psychological analysis. It is further validated as a\ngeneral framework through extensions to emotion understanding tasks.",
        "url": "http://arxiv.org/abs/2510.19451v1",
        "published_date": "2025-10-22T10:29:14+00:00",
        "updated_date": "2025-10-22T10:29:14+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xueqi Ma",
            "Yanbei Jiang",
            "Sarah Erfani",
            "James Bailey",
            "Weifeng Liu",
            "Krista A. Ehinger",
            "Jey Han Lau"
        ],
        "tldr": "The paper introduces PICK, a multi-step framework using MLLMs for psychoanalytical image comprehension, specifically the HTP test, by hierarchical analysis and knowledge injection, showing enhanced capabilities in psychological analysis.",
        "tldr_zh": "本文介绍了一种名为PICK的多步骤框架，该框架利用多模态大型语言模型（MLLM）通过分层分析和知识注入来进行精神分析图像理解，特别是针对房树人（HTP）测试，并展示了其在心理分析方面的增强能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents",
        "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
        "url": "http://arxiv.org/abs/2510.19336v1",
        "published_date": "2025-10-22T07:57:59+00:00",
        "updated_date": "2025-10-22T07:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Shi",
            "Jun Yang",
            "Ni Yang",
            "Binqiang Pan",
            "Qingsong Xie",
            "Chao Zhang",
            "Zhenyu Yang",
            "Tianhuang Su",
            "Haonan Lu"
        ],
        "tldr": "The paper introduces DaMo, a data mixing optimizer for fine-tuning MLLMs in mobile phone agents, along with PhoneAgentBench, a new benchmark for evaluating MLLMs on multimodal mobile phone tasks. DaMo improves performance by optimizing data mixtures for fine-tuning.",
        "tldr_zh": "该论文介绍了一种数据混合优化器DaMo，用于微调手机代理中的多模态大语言模型（MLLM），并提出了PhoneAgentBench，这是一个用于评估MLLM在多模态手机任务中的新基准。 DaMo 通过优化用于微调的数据混合比例来提高性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP",
        "summary": "This paper presents a novel training-free framework for open-vocabulary image\nsegmentation and object recognition (OVSR), which leverages EfficientNetB0, a\nconvolutional neural network, for unsupervised segmentation and CLIP, a\nvision-language model, for open-vocabulary object recognition. The proposed\nframework adopts a two stage pipeline: unsupervised image segmentation followed\nby segment-level recognition via vision-language alignment. In the first stage,\npixel-wise features extracted from EfficientNetB0 are decomposed using singular\nvalue decomposition to obtain latent representations, which are then clustered\nusing hierarchical clustering to segment semantically meaningful regions. The\nnumber of clusters is adaptively determined by the distribution of singular\nvalues. In the second stage, the segmented regions are localized and encoded\ninto image embeddings using the Vision Transformer backbone of CLIP. Text\nembeddings are precomputed using CLIP's text encoder from category-specific\nprompts, including a generic something else prompt to support open set\nrecognition. The image and text embeddings are concatenated and projected into\na shared latent feature space via SVD to enhance cross-modal alignment.\nRecognition is performed by computing the softmax over the similarities between\nthe projected image and text embeddings. The proposed method is evaluated on\nstandard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving\nstate-of-the-art performance in terms of Hungarian mIoU, precision, recall, and\nF1-score. These results demonstrate the effectiveness, flexibility, and\ngeneralizability of the proposed framework.",
        "url": "http://arxiv.org/abs/2510.19333v1",
        "published_date": "2025-10-22T07:54:18+00:00",
        "updated_date": "2025-10-22T07:54:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Dai",
            "Wei Yu Chen"
        ],
        "tldr": "This paper introduces a training-free open-vocabulary image segmentation and recognition framework using EfficientNetB0 for unsupervised segmentation and CLIP for object recognition, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "本文提出了一种无需训练的开放词汇图像分割和识别框架，该框架利用EfficientNetB0进行无监督分割，利用CLIP进行对象识别，并在标准基准测试中取得了最先进的成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BrainMCLIP: Brain Image Decoding with Multi-Layer feature Fusion of CLIP",
        "summary": "Decoding images from fMRI often involves mapping brain activity to CLIP's\nfinal semantic layer. To capture finer visual details, many approaches add a\nparameter-intensive VAE-based pipeline. However, these approaches overlook rich\nobject information within CLIP's intermediate layers and contradicts the\nbrain's functionally hierarchical. We introduce BrainMCLIP, which pioneers a\nparameter-efficient, multi-layer fusion approach guided by human visual\nsystem's functional hierarchy, eliminating the need for such a separate VAE\npathway. BrainMCLIP aligns fMRI signals from functionally distinct visual areas\n(low-/high-level) to corresponding intermediate and final CLIP layers,\nrespecting functional hierarchy. We further introduce a Cross-Reconstruction\nstrategy and a novel multi-granularity loss. Results show BrainMCLIP achieves\nhighly competitive performance, particularly excelling on high-level semantic\nmetrics where it matches or surpasses SOTA(state-of-the-art) methods, including\nthose using VAE pipelines. Crucially, it achieves this with substantially fewer\nparameters, demonstrating a reduction of\n71.7\\%(Table.\\ref{tab:compare_clip_vae}) compared to top VAE-based SOTA\nmethods, by avoiding the VAE pathway. By leveraging intermediate CLIP features,\nit effectively captures visual details often missed by CLIP-only approaches,\nstriking a compelling balance between semantic accuracy and detail fidelity\nwithout requiring a separate VAE pipeline.",
        "url": "http://arxiv.org/abs/2510.19332v1",
        "published_date": "2025-10-22T07:51:52+00:00",
        "updated_date": "2025-10-22T07:51:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tian Xia",
            "Zihan Ma",
            "Xinlong Wang",
            "Qing Liu",
            "Xiaowei He",
            "Tianming Liu",
            "Yudan Ren"
        ],
        "tldr": "BrainMCLIP decodes fMRI brain activity into images by fusing multi-layer features from CLIP, guided by the human visual system's hierarchy, achieving SOTA semantic accuracy with fewer parameters compared to VAE-based methods.",
        "tldr_zh": "BrainMCLIP通过融合CLIP的多层特征，并以人类视觉系统的层级结构为指导，将fMRI脑部活动解码为图像，在实现SOTA语义准确率的同时，相比于基于VAE的方法，减少了参数量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts",
        "summary": "We present a two-phase vision-language QA system for autonomous driving that\nanswers high-level perception, prediction, and planning questions. In Phase-1,\na large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a\nshort temporal window of history, and a chain-of-thought prompt with few-shot\nexemplars. A self-consistency ensemble (multiple sampled reasoning chains)\nfurther improves answer reliability. In Phase-2, we augment the prompt with\nnuScenes scene metadata (object annotations, ego-vehicle state, etc.) and\ncategory-specific question instructions (separate prompts for perception,\nprediction, planning tasks). In experiments on a driving QA benchmark, our\napproach significantly outperforms the baseline Qwen2.5 models. For example,\nusing 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall\naccuracy (vs.62.61% with zero-shot); applying self-consistency raises this to\n66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%\naccuracy under severe visual corruption. These results demonstrate that\ncarefully engineered prompts and contextual grounding can greatly enhance\nhigh-level driving QA with pretrained vision-language models.",
        "url": "http://arxiv.org/abs/2510.19001v1",
        "published_date": "2025-10-21T18:24:59+00:00",
        "updated_date": "2025-10-21T18:24:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Seungjun Yu",
            "Junsung Park",
            "Youngsun Lim",
            "Hyunjung Shim"
        ],
        "tldr": "This paper presents a two-phase vision-language QA system for autonomous driving that uses metadata-grounded context and task-specific prompts to significantly improve accuracy, particularly under visual corruption, using a large multimodal LLM.",
        "tldr_zh": "本文提出了一个用于自动驾驶的双阶段视觉语言QA系统，该系统使用元数据支持的上下文和特定于任务的提示，显著提高了准确性，尤其是在视觉损坏的情况下，使用了大型多模态LLM。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection",
        "summary": "As object detection models are increasingly deployed in cyber-physical\nsystems such as autonomous vehicles (AVs) and surveillance platforms, ensuring\ntheir security against adversarial threats is essential. While prior work has\nexplored adversarial attacks in the image domain, those attacks in the video\ndomain remain largely unexamined, especially in the no-box setting. In this\npaper, we present {\\alpha}-Cloak, the first no-box adversarial attack on object\ndetectors that operates entirely through the alpha channel of RGBA videos.\n{\\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with\na benign video, resulting in a fused video that appears innocuous to human\nviewers but consistently fools object detectors. Our attack requires no access\nto model architecture, parameters, or outputs, and introduces no perceptible\nartifacts. We systematically study the support for alpha channels across common\nvideo formats and playback applications, and design a fusion algorithm that\nensures visual stealth and compatibility. We evaluate {\\alpha}-Cloak on five\nstate-of-the-art object detectors, a vision-language model, and a multi-modal\nlarge language model (Gemini-2.0-Flash), demonstrating a 100% attack success\nrate across all scenarios. Our findings reveal a previously unexplored\nvulnerability in video-based perception systems, highlighting the urgent need\nfor defenses that account for the alpha channel in adversarial settings.",
        "url": "http://arxiv.org/abs/2510.19574v1",
        "published_date": "2025-10-22T13:27:02+00:00",
        "updated_date": "2025-10-22T13:27:02+00:00",
        "categories": [
            "cs.CV",
            "cs.CR"
        ],
        "authors": [
            "Ariana Yi",
            "Ce Zhou",
            "Liyang Xiao",
            "Qiben Yan"
        ],
        "tldr": "The paper introduces α-Cloak, a novel no-box adversarial attack on video object detectors that exploits the alpha channel in RGBA videos, achieving a 100% success rate against various models without being visually perceptible.",
        "tldr_zh": "该论文介绍了 α-Cloak，一种新型的针对视频目标检测器的无盒对抗攻击，它利用 RGBA 视频中的 alpha 通道，在视觉上不可察觉的情况下，对各种模型实现了 100% 的攻击成功率。",
        "relevance_score": 4,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 6
    }
]