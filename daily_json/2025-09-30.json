[
    {
        "title": "TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models",
        "summary": "Existing medical reasoning benchmarks for vision-language models primarily\nfocus on analyzing a patient's condition based on an image from a single visit.\nHowever, this setting deviates significantly from real-world clinical practice,\nwhere doctors typically refer to a patient's historical conditions to provide a\ncomprehensive assessment by tracking their changes over time. In this paper, we\nintroduce TemMed-Bench, the first benchmark designed for analyzing changes in\npatients' conditions between different clinical visits, which challenges large\nvision-language models (LVLMs) to reason over temporal medical images.\nTemMed-Bench consists of a test set comprising three tasks - visual\nquestion-answering (VQA), report generation, and image-pair selection - and a\nsupplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we\nconduct an evaluation of six proprietary and six open-source LVLMs. Our results\nshow that most LVLMs lack the ability to analyze patients' condition changes\nover temporal medical images, and a large proportion perform only at a\nrandom-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini\nand Claude 3.5 Sonnet demonstrate comparatively decent performance, though they\nhave yet to reach the desired level. Furthermore, we explore augmenting the\ninput with both retrieved visual and textual modalities in the medical domain.\nWe also show that multi-modal retrieval augmentation yields notably higher\nperformance gains than no retrieval and textual retrieval alone across most\nmodels on our benchmark, with the VQA task showing an average improvement of\n2.59%. Overall, we compose a benchmark grounded on real-world clinical\npractice, and it reveals LVLMs' limitations in temporal medical image\nreasoning, as well as highlighting the use of multi-modal retrieval\naugmentation as a potentially promising direction worth exploring to address\nthis challenge.",
        "url": "http://arxiv.org/abs/2509.25143v1",
        "published_date": "2025-09-29T17:51:26+00:00",
        "updated_date": "2025-09-29T17:51:26+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Junyi Zhang",
            "Jia-Chen Gu",
            "Wenbo Hu",
            "Yu Zhou",
            "Robinson Piramuthu",
            "Nanyun Peng"
        ],
        "tldr": "The paper introduces TemMed-Bench, a new benchmark for evaluating vision-language models' ability to reason over temporal medical images, revealing limitations in current LVLMs and suggesting multi-modal retrieval augmentation as a promising solution.",
        "tldr_zh": "该论文介绍了TemMed-Bench，一个新的用于评估视觉语言模型在时序医学图像上推理能力的基准，揭示了当前LVLM的局限性，并提出了多模态检索增强作为一种有希望的解决方案。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
        "summary": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.",
        "url": "http://arxiv.org/abs/2509.25139v1",
        "published_date": "2025-09-29T17:51:01+00:00",
        "updated_date": "2025-09-29T17:51:01+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yue Zhang",
            "Tianyi Ma",
            "Zun Wang",
            "Yanyuan Qiao",
            "Parisa Kordjamshidi"
        ],
        "tldr": "This paper introduces a VLN agent that utilizes analogical textual descriptions with LLMs to improve contextual understanding and spatial reasoning, leading to enhanced navigation performance on the R2R dataset.",
        "tldr_zh": "本文介绍了一种VLN智能体，该智能体利用带有LLM的类比文本描述来提高上下文理解和空间推理能力，从而在R2R数据集上实现了增强的导航性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
        "summary": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL.",
        "url": "http://arxiv.org/abs/2509.25033v1",
        "published_date": "2025-09-29T16:52:47+00:00",
        "updated_date": "2025-09-29T16:52:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.4.9"
        ],
        "authors": [
            "Wenhao Li",
            "Qiangchang Wang",
            "Xianjing Meng",
            "Zhibin Wu",
            "Yilong Yin"
        ],
        "tldr": "This paper proposes VT-FSL, a novel few-shot learning framework that leverages LLMs to generate precise cross-modal prompts conditioned on both class names and support images, achieving state-of-the-art performance across multiple benchmarks.",
        "tldr_zh": "本文提出了一种名为VT-FSL的新型少样本学习框架，该框架利用大型语言模型（LLM）生成基于类别名称和支持图像的精确跨模态提示，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning",
        "summary": "Recent advances in reinforcement learning (RL) have delivered strong\nreasoning capabilities in natural image domains, yet their potential for Earth\nObservation (EO) remains largely unexplored. EO tasks introduce unique\nchallenges, spanning referred object detection, image or region captioning,\nchange detection, grounding, and temporal analysis, that demand task aware\nreasoning. We propose a novel post training framework that incorporates task\naware rewards to enable effective adaptation of reasoning based RL models to\ndiverse EO tasks. This training strategy enhances reasoning capabilities for\nremote sensing images, stabilizes optimization, and improves robustness.\nExtensive experiments across multiple EO benchmarks show consistent performance\ngains over state of the art generic and specialized vision language models.\nCode and models will be released publicly at\nhttps://mustansarfiaz.github.io/GeoVLM-R1/ .",
        "url": "http://arxiv.org/abs/2509.25026v1",
        "published_date": "2025-09-29T16:48:54+00:00",
        "updated_date": "2025-09-29T16:48:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mustansar Fiaz",
            "Hiyam Debary",
            "Paolo Fraccaro",
            "Danda Paudel",
            "Luc Van Gool",
            "Fahad Khan",
            "Salman Khan"
        ],
        "tldr": "The paper introduces a reinforcement learning-based fine-tuning framework (GeoVLM-R1) to improve the reasoning capabilities of Vision Language Models (VLMs) for various Earth Observation tasks, demonstrating consistent performance gains on EO benchmarks.",
        "tldr_zh": "该论文介绍了一种基于强化学习的微调框架 (GeoVLM-R1)，旨在提高视觉语言模型 (VLM) 在各种地球观测任务中的推理能力，并在 EO 基准测试中展示了一致的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents",
        "summary": "Long videos, characterized by temporal complexity and sparse task-relevant\ninformation, pose significant reasoning challenges for AI systems. Although\nvarious Large Language Model (LLM)-based approaches have advanced long video\nunderstanding, they still struggle to achieve both completeness and efficiency\nin capturing task-critical information. Inspired by human progressive visual\ncognition, we propose CogniGPT, a framework that leverages an interactive loop\nbetween Multi-Granular Perception Agent (MGPA) and Verification-Enhanced\nReflection Agent (VERA) for efficient and reliable long video understanding.\nSpecifically, MGPA mimics human visual divergent and focused attention to\ncapture task-related information, while VERA verifies perceived key clues to\nmitigate hallucination and optimize subsequent perception strategies. Through\nthis interactive process, CogniGPT explores a minimal set of informative and\nreliable task-related clues. Extensive experiments on EgoSchema, Video-MME,\nNExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both\naccuracy and efficiency. Notably, on EgoSchema, it surpasses existing\ntraining-free methods using only 11.2 frames and achieves performance\ncomparable to Gemini 1.5-Pro.",
        "url": "http://arxiv.org/abs/2509.24943v1",
        "published_date": "2025-09-29T15:42:55+00:00",
        "updated_date": "2025-09-29T15:42:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahua Li",
            "Kun Wei",
            "Zhe Xu",
            "Zibo Su",
            "Xu Yang",
            "Cheng Deng"
        ],
        "tldr": "The paper introduces CogniGPT, a framework using interactive agents (Multi-Granular Perception Agent and Verification-Enhanced Reflection Agent) to efficiently and reliably understand long videos, demonstrating state-of-the-art performance on several datasets.",
        "tldr_zh": "该论文介绍了CogniGPT，一个利用交互式代理（多粒度感知代理和验证增强反射代理）高效可靠地理解长视频的框架，并在多个数据集上展示了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation",
        "summary": "Source-free active domain adaptation (SFADA) enhances knowledge transfer from\na source model to an unlabeled target domain using limited manual labels\nselected via active learning. While recent domain adaptation studies have\nintroduced Vision-and-Language (ViL) models to improve pseudo-label quality or\nfeature alignment, they often treat ViL-based and data supervision as separate\nsources, lacking effective fusion. To overcome this limitation, we propose Dual\nActive learning with Multimodal (DAM) foundation model, a novel framework that\nintegrates multimodal supervision from a ViL model to complement sparse human\nannotations, thereby forming a dual supervisory signal. DAM initializes stable\nViL-guided targets and employs a bidirectional distillation mechanism to foster\nmutual knowledge exchange between the target model and the dual supervisions\nduring iterative adaptation. Extensive experiments demonstrate that DAM\nconsistently outperforms existing methods and sets a new state-of-the-art\nacross multiple SFADA benchmarks and active learning strategies.",
        "url": "http://arxiv.org/abs/2509.24896v1",
        "published_date": "2025-09-29T15:06:56+00:00",
        "updated_date": "2025-09-29T15:06:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xi Chen",
            "Hongxun Yao",
            "Zhaopan Xu",
            "Kui Jiang"
        ],
        "tldr": "The paper introduces DAM, a novel source-free active domain adaptation framework that fuses multimodal supervision from a Vision-and-Language model with sparse human annotations using a dual active learning approach and bidirectional distillation, achieving state-of-the-art results.",
        "tldr_zh": "该论文提出了DAM，一种新颖的无源主动域自适应框架，它使用双重主动学习方法和双向蒸馏，将来自视觉语言模型的多模态监督与稀疏的人工注释融合，从而实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment",
        "summary": "Magnetic resonance imaging (MRI) quality assessment is crucial for clinical\ndecision-making, yet remains challenging due to data scarcity and protocol\nvariability. Traditional approaches face fundamental trade-offs: signal-based\nmethods like MRIQC provide quantitative metrics but lack semantic\nunderstanding, while deep learning approaches achieve high accuracy but\nsacrifice interpretability. To address these limitations, we introduce the\nMultimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration\nof multimodal large language models (MLLMs) with acquisition-aware signal\nprocessing. MMRQA combines three key innovations: robust metric extraction via\nMRQy augmented with simulated artifacts, structured transformation of metrics\ninto question-answer pairs using Qwen, and parameter-efficient fusion through\nLow-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI,\nand MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with\nstrong zero-shot generalization, as validated by comprehensive ablation\nstudies. By bridging quantitative analysis with semantic reasoning, our\nframework generates clinically interpretable outputs that enhance quality\ncontrol in dynamic medical settings.",
        "url": "http://arxiv.org/abs/2509.24888v1",
        "published_date": "2025-09-29T15:00:19+00:00",
        "updated_date": "2025-09-29T15:00:19+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Fankai Jia",
            "Daisong Gan",
            "Zhe Zhang",
            "Zhaochi Wen",
            "Chenchen Dan",
            "Dong Liang",
            "Haifeng Wang"
        ],
        "tldr": "The paper introduces MMRQA, a multimodal MRI quality assessment framework that integrates signal processing with large language models to improve accuracy, interpretability, and zero-shot generalization in MRI quality control.",
        "tldr_zh": "该论文介绍了MMRQA，一个多模态MRI质量评估框架，它将信号处理与大型语言模型相结合，以提高MRI质量控制的准确性、可解释性和零样本泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StreamForest: Efficient Online Video Understanding with Persistent Event Memory",
        "summary": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable\nprogress in video understanding. However, their effectiveness in real-time\nstreaming scenarios remains limited due to storage constraints of historical\nvisual features and insufficient real-time spatiotemporal reasoning. To address\nthese challenges, we propose StreamForest, a novel architecture specifically\ndesigned for streaming video understanding. Central to StreamForest is the\nPersistent Event Memory Forest, a memory mechanism that adaptively organizes\nvideo frames into multiple event-level tree structures. This process is guided\nby penalty functions based on temporal distance, content similarity, and merge\nfrequency, enabling efficient long-term memory retention under limited\ncomputational resources. To enhance real-time perception, we introduce a\nFine-grained Spatiotemporal Window, which captures detailed short-term visual\ncues to improve current scene perception. Additionally, we present OnlineIT, an\ninstruction-tuning dataset tailored for streaming video tasks. OnlineIT\nsignificantly boosts MLLM performance in both real-time perception and future\nprediction. To evaluate generalization in practical applications, we introduce\nODV-Bench, a new benchmark focused on real-time streaming video understanding\nin autonomous driving scenarios. Experimental results demonstrate that\nStreamForest achieves the state-of-the-art performance, with accuracies of\n77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In\nparticular, even under extreme visual token compression (limited to 1024\ntokens), the model retains 96.8% of its average accuracy in eight benchmarks\nrelative to the default setting. These results underscore the robustness,\nefficiency, and generalizability of StreamForest for streaming video\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.24871v1",
        "published_date": "2025-09-29T14:53:57+00:00",
        "updated_date": "2025-09-29T14:53:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Zeng",
            "Kefan Qiu",
            "Qingyu Zhang",
            "Xinhao Li",
            "Jing Wang",
            "Jiaxin Li",
            "Ziang Yan",
            "Kun Tian",
            "Meng Tian",
            "Xinhai Zhao",
            "Yi Wang",
            "Limin Wang"
        ],
        "tldr": "StreamForest is a novel architecture with Persistent Event Memory Forest and Fine-grained Spatiotemporal Window designed for efficient real-time streaming video understanding, achieving SOTA performance on several benchmarks and demonstrating robustness under visual token compression.",
        "tldr_zh": "StreamForest是一种新颖的架构，具有持久事件记忆森林和细粒度时空窗口，专为高效的实时流视频理解而设计，在多个基准测试中实现了SOTA性能，并在视觉标记压缩下展示了鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models",
        "summary": "Large Vision-Language Models (VLMs) enable strong multimodal reasoning but\nincur heavy inference costs from redundant visual tokens. Token pruning\nalleviates this issue, yet existing approaches face limitations.\nAttention-based methods rely on raw attention scores, which are often unstable\nacross layers and heads and can lead to redundant selections. Diversity-based\nmethods improve robustness by selecting tokens far apart in feature space but\nrisk dropping regions needed for accurate prediction. We propose \\ours, a\ntraining-free framework built on a simple intuition: tokens with higher\nsensitivity are more likely to influence the model's output, and they should\nalso capture complementary visual cues rather than overlapping information. To\nachieve this, we estimate token sensitivity using zeroth-order perturbations at\nthe projection layer, a shallow and computationally light component of the\nmodel. This approach measures how small random perturbations affect the\nprojection outputs, allowing us to approximate each token's influence through\nlightweight forward passes without backpropagation. Extensive experiments\nacross multiple VLMs and benchmarks show that \\ours consistently outperforms\nprior methods, pruning up to 94.4\\% of tokens while maintaining accuracy and\nsignificantly improving efficiency, achieving up to 2.30x faster end-to-end\ninference over the baseline.",
        "url": "http://arxiv.org/abs/2509.24837v1",
        "published_date": "2025-09-29T14:20:05+00:00",
        "updated_date": "2025-09-29T14:20:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youngeun Kim",
            "Youjia Zhang",
            "Huiling Liu",
            "Aecheon Jung",
            "Sunwoo Lee",
            "Sungeun Hong"
        ],
        "tldr": "This paper introduces a training-free token pruning method for Vision-Language Models (VLMs) based on zeroth-order gradient estimation, achieving significant token reduction and inference speedup while maintaining accuracy.",
        "tldr_zh": "本文提出了一种基于零阶梯度估计的视觉语言模型（VLM）的免训练token剪枝方法，该方法在保持精度的同时，实现了显著的token减少和推理加速。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs",
        "summary": "In this work, we introduce SPLICE, a human-curated benchmark derived from the\nCOIN instructional video dataset, designed to probe event-based reasoning\nacross multiple dimensions: temporal, causal, spatial, contextual, and general\nknowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories\nand 180 sub-categories, such as sports, engineering, and housework. These\nvideos are segmented into a total of 11,423 event clips. We evaluate both human\nparticipants and state-of-the-art vision-language models (VLMs) on the task of\nrearranging these clips into coherent event sequences to assess visual\nreasoning capabilities. Results reveal a significant gap: VLMs struggle to\nmatch human performance. While human-annotated textual descriptions improve\nmodel accuracy, they do not affect human performance, suggesting that models\nrely more on language priors than on visual understanding. Even with\nannotations, VLMs fall short of human-level reasoning, underscoring persistent\nchallenges in visual reasoning. A deeper analysis across sub-categories shows\nthat VLMs perform relatively better on videos where temporal and causal\nreasoning are dominant, compared to those where contextual and spatial\nreasoning are dominant. They also perform better on everyday tasks than on\nspecialized ones.",
        "url": "http://arxiv.org/abs/2509.24640v1",
        "published_date": "2025-09-29T11:50:18+00:00",
        "updated_date": "2025-09-29T11:50:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamad Ballout",
            "Okajevo Wilfred",
            "Seyedalireza Yaghoubi",
            "Nohayr Muhammad Abdelmoneim",
            "Julius Mayer",
            "Elia Bruni"
        ],
        "tldr": "The paper introduces SPLICE, a human-curated benchmark for evaluating event-based reasoning in VLMs using instructional videos, revealing a significant performance gap between VLMs and humans, even with textual descriptions.",
        "tldr_zh": "该论文介绍了SPLICE，一个人工策划的基准，用于评估视觉语言模型在事件推理方面的能力，使用教学视频，揭示了即使有文本描述，视觉语言模型和人类之间仍然存在显著的性能差距。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreeRet: MLLMs as Training-Free Retrievers",
        "summary": "Multimodal large language models (MLLMs) are emerging as versatile\nfoundations for mixed-modality retrieval. Yet, they often require heavy\npost-hoc training to convert them into contrastive encoders for retrieval. This\nwork asks: Can off-the-shelf MLLMs serve as powerful retrievers without\nadditional training? We present FreeRet, a plug-and-play framework that turns\nany MLLM into a two-stage retriever. FreeRet first derives semantically\ngrounded embeddings directly from the model for fast candidate search, and then\nexploits its reasoning ability for precise reranking. The framework contributes\nthree advances: bypassing lexical alignment layers to obtain semantically\nfaithful embeddings, conditioning representation generation with explicit\npriors, and mitigating framing effect in reranking via neutral choice framing.\nOn the MMEB and MMEB-V2 benchmarks spanning 46 datasets, FreeRet substantially\noutperforms models trained on millions of pairs. Beyond benchmarks, FreeRet is\nmodel-agnostic and scales seamlessly across MLLM families and sizes, preserves\ntheir generative abilities, supports arbitrary modality combinations, and\nunifies retrieval, reranking, and generation into end-to-end RAG within a\nsingle model. Our findings demonstrate that pretrained MLLMs, when carefully\nharnessed, can serve as strong retrieval engines without training, closing a\ncritical gap in their role as generalists.",
        "url": "http://arxiv.org/abs/2509.24621v1",
        "published_date": "2025-09-29T11:28:42+00:00",
        "updated_date": "2025-09-29T11:28:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhan Zhu",
            "Xiangyu Zeng",
            "Chenting Wang",
            "Xinhao Li",
            "Yicheng Xu",
            "Ziang Yan",
            "Yi Wang",
            "Limin Wang"
        ],
        "tldr": "The paper introduces FreeRet, a training-free framework leveraging MLLMs for multimodal retrieval, achieving strong performance on benchmarks and offering several advantages like model agnosticism and unified RAG.",
        "tldr_zh": "该论文介绍了FreeRet，一个无需训练的框架，利用多模态大型语言模型进行多模态检索，在基准测试中表现出色，并具有模型无关性和统一的RAG等优点。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Instruction Guided Multi Object Image Editing with Quantity and Layout Consistency",
        "summary": "Instruction driven image editing with standard CLIP text encoders often fails\nin complex scenes with many objects. We present QL-Adapter, a framework for\nmultiple object editing that tackles two challenges: enforcing object counts\nand spatial layouts, and accommodating diverse categories. QL-Adapter consists\nof two core modules: the Image-Layout Fusion Module (ILFM) and the Cross-Modal\nAugmentation Module (CMAM). ILFM fuses layout priors with ViT patch tokens from\nthe CLIP image encoder to strengthen spatial structure understanding. CMAM\ninjects image features into the text branch to enrich textual embeddings and\nimprove instruction following. We further build QL-Dataset, a benchmark that\nspans broad category, layout, and count variations, and define the task of\nquantity and layout consistent image editing (QL-Edit). Extensive experiments\nshow that QL-Adapter achieves state of the art performance on QL-Edit and\nsignificantly outperforms existing models.",
        "url": "http://arxiv.org/abs/2509.24514v1",
        "published_date": "2025-09-29T09:33:51+00:00",
        "updated_date": "2025-09-29T09:33:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Tan",
            "Fangyu Li",
            "Yang Liu"
        ],
        "tldr": "The paper introduces QL-Adapter, a framework for instruction-driven multi-object image editing that addresses challenges in object count, spatial layout consistency, and category diversity, along with a new benchmark dataset (QL-Dataset). It outperforms existing methods on the defined QL-Edit task.",
        "tldr_zh": "该论文介绍了QL-Adapter，一个用于指令驱动的多对象图像编辑框架，解决了对象计数、空间布局一致性和类别多样性方面的挑战，并提出了一个新的基准数据集（QL-Dataset）。在定义的QL-Edit任务上，它优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks",
        "summary": "Spatial intelligence spans a rich suite of abilities, including visualising\nand transforming shapes, mentally rotating objects, judging relational\npositions and containment, and estimating numerosity. However, it still remains\na critical unresolved challenge for Multimodal Large Language Models (MLLMs).To\nfill this gap, we propose to treat Euclidean geometry problem-solving as a\nsurrogate task. Specifically, we meticulously constructed a curated multimodal\ndataset, called Euclid30K, comprising approximately 30K plane and solid\ngeometry problems. To enable the model to acquire and apply Euclidean\nprinciples from these geometry problems, we employed Group Relative Policy\nOptimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,\ninspiring the models to identify shapes, count, and relate entities, and\nperform multi-step deductive reasoning using Euclidean principles. Our\nexperiments demonstrate that the resulting models achieve substantial zero-shot\ngains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,\nVSI-Bench, and MindCube) without any task-specific adaptations. Notably, after\ntraining on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models\nrose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,\nRoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous\nstate-of-the-art model, Spatial-MLLM.To our knowledge, this is the first\nsystematic study showing that geometry-centric fine-tuning can confer\nvision-language models with broadly transferable spatial skills. Code and\nEuclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
        "url": "http://arxiv.org/abs/2509.24473v1",
        "published_date": "2025-09-29T08:49:21+00:00",
        "updated_date": "2025-09-29T08:49:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Shijie Lian",
            "Changti Wu",
            "Laurence Tianruo Yang",
            "Hang Yuan",
            "Bin Yu",
            "Lei Zhang",
            "Kai Chen"
        ],
        "tldr": "The paper introduces Euclid30K, a geometry problem dataset, and uses it to finetune VLMs via Group Relative Policy Optimization (GRPO), significantly improving their spatial reasoning abilities across several benchmarks.",
        "tldr_zh": "该论文介绍了一个几何问题数据集Euclid30K，并使用它通过群相对策略优化（GRPO）来微调VLM，从而显著提高了它们在多个基准测试中的空间推理能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
        "summary": "Recent developments in Multimodal Large Language Models (MLLMs) have\nsignificantly improved Vision-Language (VL) reasoning in 2D domains. However,\nextending these capabilities to 3D scene understanding remains a major\nchallenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend\non 3D data inputs, which limits scalability and generalization. To address this\nlimitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes\nvideo inputs without requiring external 3D data, making it practical for\nreal-world deployment. In our method, the geometric prior are directly used to\nimprove the performance of the sceen perception. To integrate the geometric\ncues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to\nalign the 3D geometric priors with the vision-language representations. To\nensure geometric consistency and integrity, we introduce a Metric Depth Model\nthat recovers real-scale geometry from the reconstruction outputs. Finally, the\nmodel is fine-tuned with a two-stage distillation optimization strategy,\nrealizing fast convergence and stabilizes training. Extensive experiments\nacross diverse benchmarks verified the effectiveness of our method on 3D\nQuestion Answering, 3D Dense Captioning and 3D Visual Grounding tasks,\ndemonstrating the superior multi-task capabilities.",
        "url": "http://arxiv.org/abs/2509.24385v1",
        "published_date": "2025-09-29T07:34:18+00:00",
        "updated_date": "2025-09-29T07:34:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haijier Chen",
            "Bo Xu",
            "Shoujian Zhang",
            "Haoze Liu",
            "Jiaxuan Lin",
            "Jingrong Wang"
        ],
        "tldr": "Vid-LLM is a video-based 3D Multimodal LLM that processes video inputs directly without relying on external 3D data, enhancing 3D scene understanding and multi-task capabilities through a novel Cross-Task Adapter and metric depth model.",
        "tldr_zh": "Vid-LLM 是一种基于视频的3D多模态LLM，可以直接处理视频输入，无需依赖外部3D数据，通过新颖的跨任务适配器和度量深度模型，增强了3D场景理解和多任务能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models",
        "summary": "Unified Multimodal Models (UMMs) built on shared autoregressive (AR)\ntransformers are attractive for their architectural simplicity. However, we\nidentify a critical limitation: when trained on multimodal inputs,\nmodality-shared transformers suffer from severe gradient conflicts between\nvision and text, particularly in shallow and deep layers. We trace this issue\nto the fundamentally different low-level statistical properties of images and\ntext, while noting that conflicts diminish in middle layers where\nrepresentations become more abstract and semantically aligned. To overcome this\nchallenge, we propose Uni-X, a two-end-separated, middle-shared architecture.\nUni-X dedicates its initial and final layers to modality-specific processing,\nwhile maintaining shared parameters in the middle layers for high-level\nsemantic fusion. This X-shaped design not only eliminates gradient conflicts at\nboth ends but also further alleviates residual conflicts in the shared layers.\nExtensive experiments validate the effectiveness of Uni-X. Under identical\ntraining conditions, Uni-X achieves superior training efficiency compared to\nstrong baselines. When scaled to 3B parameters with larger training data, Uni-X\nmatches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for\nimage generation alongside strong performance in text and vision understanding\ntasks. These results establish Uni-X as a parameter-efficient and scalable\nfoundation for future unified multimodal modeling. Our code is available at\nhttps://github.com/CURRENTF/Uni-X",
        "url": "http://arxiv.org/abs/2509.24365v1",
        "published_date": "2025-09-29T07:05:10+00:00",
        "updated_date": "2025-09-29T07:05:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jitai Hao",
            "Hao Liu",
            "Xinyan Xiao",
            "Qiang Huang",
            "Jun Yu"
        ],
        "tldr": "The paper introduces Uni-X, a two-end-separated, middle-shared architecture for unified multimodal models to mitigate gradient conflicts between vision and text, achieving superior training efficiency and performance compared to existing models.",
        "tldr_zh": "本文介绍了一种用于统一多模态模型的 Uni-X 架构，该架构采用两端分离、中间共享的设计，旨在缓解视觉和文本之间的梯度冲突，与现有模型相比，实现了更高的训练效率和性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "When MLLMs Meet Compression Distortion: A Coding Paradigm Tailored to MLLMs",
        "summary": "The increasing deployment of powerful Multimodal Large Language Models\n(MLLMs), typically hosted on cloud platforms, urgently requires effective\ncompression techniques to efficiently transmit signal inputs (e.g., images,\nvideos) from edge devices with minimal bandwidth usage. However, conventional\nimage codecs are optimized for fidelity to serve the Human Visual System (HVS)\nand ill-suited for MLLMs, in which diverse downstream tasks are jointly\nconsidered. In this paper, we first systematically analyze the impact of\ncompression artifacts on several mainstream MLLMs. We find that: Compression\ndistortion unevenly impacts different-level image features, leading to varying\neffects on MLLMs' downstream tasks depending on their feature-level reliance.\nMotivated by this discovery, we propose an image Codec TAilored to MLLMs\n(CoTAM) designed to adaptively protect multi-level features and suit different\ndemands of downstream tasks. The encoder leverages CLIP's shallow-layer\nattention to generate an importance map for bit allocation, preserving critical\nsemantic regions. Concurrently, the decoder integrates a lightweight adapter\nwith a multi-level loss function to ensure the faithful reconstruction both of\nlow-level details and high-level semantic context for robust synthesis of\ncross-level features. Extensive experiments validate that our method achieves\nup to 35.99\\% bitrate saving while maintaining the same performance on the MLLM\ntasks, outperforming previous SOTA neural codecs.",
        "url": "http://arxiv.org/abs/2509.24258v1",
        "published_date": "2025-09-29T04:07:52+00:00",
        "updated_date": "2025-09-29T04:07:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinming Liu",
            "Zhaoyang Jia",
            "Jiahao Li",
            "Bin Li",
            "Xin Jin",
            "Wenjun Zeng",
            "Yan Lu"
        ],
        "tldr": "This paper introduces a novel image codec (CoTAM) tailored for MLLMs, addressing the inefficiency of traditional codecs by adaptively protecting multi-level features based on downstream task demands, achieving significant bitrate savings.",
        "tldr_zh": "本文介绍了一种专为多模态大型语言模型（MLLM）设计的新型图像编解码器（CoTAM），通过自适应地保护多层次特征以满足下游任务的需求，解决了传统编解码器的效率问题，并实现了显著的比特率节省。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Environment-Aware Satellite Image Generation with Diffusion Models",
        "summary": "Diffusion-based foundation models have recently garnered much attention in\nthe field of generative modeling due to their ability to generate images of\nhigh quality and fidelity. Although not straightforward, their recent\napplication to the field of remote sensing signaled the first successful trials\ntowards harnessing the large volume of publicly available datasets containing\nmultimodal information. Despite their success, existing methods face\nconsiderable limitations: they rely on limited environmental context, struggle\nwith missing or corrupted data, and often fail to reliably reflect user\nintentions in generated outputs. In this work, we propose a novel diffusion\nmodel conditioned on environmental context, that is able to generate satellite\nimages by conditioning from any combination of three different control signals:\na) text, b) metadata, and c) visual data. In contrast to previous works, the\nproposed method is i) to our knowledge, the first of its kind to condition\nsatellite image generation on dynamic environmental conditions as part of its\ncontrol signals, and ii) incorporating a metadata fusion strategy that models\nattribute embedding interactions to account for partially corrupt and/or\nmissing observations. Our method outperforms previous methods both\nqualitatively (robustness to missing metadata, higher responsiveness to control\ninputs) and quantitatively (higher fidelity, accuracy, and quality of\ngenerations measured using 6 different metrics) in the trials of single-image\nand temporal generation. The reported results support our hypothesis that\nconditioning on environmental context can improve the performance of foundation\nmodels for satellite imagery, and render our model a promising candidate for\nusage in downstream tasks. The collected 3-modal dataset is to our knowledge,\nthe first publicly-available dataset to combine data from these three different\nmediums.",
        "url": "http://arxiv.org/abs/2509.24875v1",
        "published_date": "2025-09-29T14:54:53+00:00",
        "updated_date": "2025-09-29T14:54:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nikos Kostagiolas",
            "Pantelis Georgiades",
            "Yannis Panagakis",
            "Mihalis A. Nicolaou"
        ],
        "tldr": "This paper introduces a novel diffusion model for satellite image generation conditioned on environmental context (text, metadata, and visual data), addressing limitations of existing methods and presenting a new multimodal dataset.",
        "tldr_zh": "该论文介绍了一种新型扩散模型，用于生成以环境背景（文本、元数据和视觉数据）为条件的卫星图像，解决了现有方法的局限性，并提出了一个新的多模态数据集。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity",
        "summary": "Multimodal learning plays a pivotal role in advancing artificial intelligence\nsystems by incorporating information from multiple modalities to build a more\ncomprehensive representation. Despite its importance, current state-of-the-art\nmodels still suffer from severe limitations that prevent the successful\ndevelopment of a fully multimodal model. Such methods may not provide\nindicators that all the involved modalities are effectively aligned. As a\nresult, some modalities may not be aligned, undermining the effectiveness of\nthe model in downstream tasks where multiple modalities should provide\nadditional information that the model fails to exploit. In this paper, we\npresent TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed\nsimilarity measure that is directly computed in the higher-dimensional space\nspanned by the modality embeddings. TRIANGLE improves the joint alignment of\nthree modalities via a triangle-area similarity, avoiding additional fusion\nlayers or pairwise similarities. When incorporated in contrastive losses\nreplacing cosine similarity, TRIANGLE significantly boosts the performance of\nmultimodal modeling, while yielding interpretable alignment rationales.\nExtensive evaluation in three-modal tasks such as video-text and audio-text\nretrieval or audio-video classification, demonstrates that TRIANGLE achieves\nstate-of-the-art results across different datasets improving the performance of\ncosine-based methods up to 9 points of Recall@1.",
        "url": "http://arxiv.org/abs/2509.24734v1",
        "published_date": "2025-09-29T12:58:46+00:00",
        "updated_date": "2025-09-29T12:58:46+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Giordano Cicchetti",
            "Eleonora Grassucci",
            "Danilo Comminiello"
        ],
        "tldr": "This paper introduces TRIANGLE, a novel similarity measure for multimodal alignment that leverages triangle area in the embedding space, achieving state-of-the-art results in three-modal tasks by improving alignment.",
        "tldr_zh": "本文介绍了一种新的多模态对齐相似度度量方法TRIANGLE，它利用嵌入空间中的三角形面积，通过改进对齐，在三模态任务中实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "NeMo: Needle in a Montage for Video-Language Understanding",
        "summary": "Recent advances in video large language models (VideoLLMs) call for new\nevaluation protocols and benchmarks for complex temporal reasoning in\nvideo-language understanding. Inspired by the needle in a haystack test widely\nused by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed\nto assess VideoLLMs' critical reasoning capabilities, including long-context\nrecall and temporal grounding. To generate video question answering data for\nour task, we develop a scalable automated data generation pipeline that\nfacilitates high-quality data synthesis. Built upon the proposed pipeline, we\npresent NeMoBench, a video-language benchmark centered on our task.\nSpecifically, our full set of NeMoBench features 31,378 automatically generated\nquestion-answer (QA) pairs from 13,486 videos with various durations ranging\nfrom seconds to hours. Experiments demonstrate that our pipeline can reliably\nand automatically generate high-quality evaluation data, enabling NeMoBench to\nbe continuously updated with the latest videos. We evaluate 20 state-of-the-art\nmodels on our benchmark, providing extensive results and key insights into\ntheir capabilities and limitations. Our project page is available at:\nhttps://lavi-lab.github.io/NeMoBench.",
        "url": "http://arxiv.org/abs/2509.24563v1",
        "published_date": "2025-09-29T10:16:05+00:00",
        "updated_date": "2025-09-29T10:16:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zi-Yuan Hu",
            "Shuo Liang",
            "Duo Zheng",
            "Yanyang Li",
            "Yeyao Tao",
            "Shijia Huang",
            "Wei Feng",
            "Jia Qin",
            "Jianguang Yu",
            "Jing Huang",
            "Meng Fang",
            "Yin Li",
            "Liwei Wang"
        ],
        "tldr": "The paper introduces NeMoBench, a new video-language benchmark and task (Needle in a Montage) for evaluating long-context recall and temporal grounding in VideoLLMs, along with an automated data generation pipeline.",
        "tldr_zh": "该论文介绍了一个新的视频语言基准和任务（NeMoBench的Needle in a Montage），用于评估视频语言模型中的长上下文回忆和时间定位能力，并提出了一个自动数据生成流程。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D",
        "summary": "3D scene understanding is fundamental for embodied AI and robotics,\nsupporting reliable perception for interaction and navigation. Recent\napproaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning\nembedding vectors to 2D class-agnostic masks generated via vision-language\nmodels (VLMs) and projecting these into 3D. However, these methods often\nproduce fragmented masks and inaccurate semantic assignments due to the direct\nuse of raw masks, limiting their effectiveness in complex environments. To\naddress this, we leverage SemanticSAM with progressive granularity refinement\nto generate more accurate and numerous object-level masks, mitigating the\nover-segmentation commonly observed in mask generation models such as vanilla\nSAM, and improving downstream 3D semantic segmentation. To further enhance\nsemantic context, we employ a context-aware CLIP encoding strategy that\nintegrates multiple contextual views of each mask using empirically determined\nweighting, providing much richer visual context. We evaluate our approach on\nmultiple 3D scene understanding tasks, including 3D semantic segmentation and\nobject retrieval from language queries, across several benchmark datasets.\nExperimental results demonstrate significant improvements over existing\nmethods, highlighting the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2509.24528v1",
        "published_date": "2025-09-29T09:43:00+00:00",
        "updated_date": "2025-09-29T09:43:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamad Amin Mirzaei",
            "Pantea Amoie",
            "Ali Ekhterachian",
            "Matin Mirzababaei"
        ],
        "tldr": "The paper introduces CORE-3D, a method for open-vocabulary 3D semantic mapping that improves mask generation and semantic context encoding to enhance 3D scene understanding tasks. It leverages SemanticSAM and context-aware CLIP encoding.",
        "tldr_zh": "该论文介绍了CORE-3D，一种开放词汇的3D语义映射方法，通过改进掩码生成和语义上下文编码来增强3D场景理解任务。它利用SemanticSAM和上下文感知的CLIP编码。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DINOReg: Strong Point Cloud Registration with Vision Foundation Model",
        "summary": "Point cloud registration is a fundamental task in 3D computer vision. Most\nexisting methods rely solely on geometric information for feature extraction\nand matching. Recently, several studies have incorporated color information\nfrom RGB-D data into feature extraction. Although these methods achieve\nremarkable improvements, they have not fully exploited the abundant texture and\nsemantic information in images, and the feature fusion is performed in an\nimage-lossy manner, which limit their performance. In this paper, we propose\nDINOReg, a registration network that sufficiently utilizes both visual and\ngeometric information to solve the point cloud registration problem. Inspired\nby advances in vision foundation models, we employ DINOv2 to extract\ninformative visual features from images, and fuse visual and geometric features\nat the patch level. This design effectively combines the rich texture and\nglobal semantic information extracted by DINOv2 with the detailed geometric\nstructure information captured by the geometric backbone. Additionally, a mixed\npositional embedding is proposed to encode positional information from both\nimage space and point cloud space, which enhances the model's ability to\nperceive spatial relationships between patches. Extensive experiments on the\nRGBD-3DMatch and RGBD-3DLoMatch datasets demonstrate that our method achieves\nsignificant improvements over state-of-the-art geometry-only and multi-modal\nregistration methods, with a 14.2% increase in patch inlier ratio and a 15.7%\nincrease in registration recall. The code is publicly available at\nhttps://github.com/ccjccjccj/DINOReg.",
        "url": "http://arxiv.org/abs/2509.24370v1",
        "published_date": "2025-09-29T07:15:47+00:00",
        "updated_date": "2025-09-29T07:15:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Congjia Chen",
            "Yufu Qu"
        ],
        "tldr": "DINOReg uses DINOv2 to extract visual features from images and fuses them with geometric features for improved point cloud registration, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "DINOReg使用DINOv2从图像中提取视觉特征，并将其与几何特征融合，以改进点云配准，在标准基准测试上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UI-UG: A Unified MLLM for UI Understanding and Generation",
        "summary": "Although Multimodal Large Language Models (MLLMs) have been widely applied\nacross domains, they are still facing challenges in domain-specific tasks, such\nas User Interface (UI) understanding accuracy and UI generation quality. In\nthis paper, we introduce UI-UG (a unified MLLM for UI Understanding and\nGeneration), integrating both capabilities. For understanding tasks, we employ\nSupervised Fine-tuning (SFT) combined with Group Relative Policy Optimization\n(GRPO) to enhance fine-grained understanding on the modern complex UI data. For\ngeneration tasks, we further use Direct Preference Optimization (DPO) to make\nour model generate human-preferred UIs. In addition, we propose an industrially\neffective workflow, including the design of an LLM-friendly domain-specific\nlanguage (DSL), training strategies, rendering processes, and evaluation\nmetrics. In experiments, our model achieves state-of-the-art (SOTA) performance\non understanding tasks, outperforming both larger general-purpose MLLMs and\nsimilarly-sized UI-specialized models. Our model is also on par with these\nlarger MLLMs in UI generation performance at a fraction of the computational\ncost. We also demonstrate that integrating understanding and generation tasks\ncan improve accuracy and quality for both tasks. Code and Model:\nhttps://github.com/neovateai/UI-UG",
        "url": "http://arxiv.org/abs/2509.24361v2",
        "published_date": "2025-09-29T06:59:09+00:00",
        "updated_date": "2025-09-30T07:45:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Hao Yang",
            "Weijie Qiu",
            "Ru Zhang",
            "Zhou Fang",
            "Ruichao Mao",
            "Xiaoyu Lin",
            "Maji Huang",
            "Zhaosong Huang",
            "Teng Guo",
            "Shuoyang Liu",
            "Hai Rao"
        ],
        "tldr": "The paper introduces UI-UG, a unified MLLM for UI understanding and generation, achieving SOTA performance in understanding and competitive generation with smaller computational cost, demonstrating task integration benefits.",
        "tldr_zh": "该论文介绍了UI-UG，一个统一的MLLM模型，用于UI理解和生成，在理解任务中取得了SOTA性能，并在生成任务中以更小的计算成本实现了有竞争力的性能，证明了任务集成的好处。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SVAC: Scaling Is All You Need For Referring Video Object Segmentation",
        "summary": "Referring Video Object Segmentation (RVOS) aims to segment target objects in\nvideo sequences based on natural language descriptions. While recent advances\nin Multi-modal Large Language Models (MLLMs) have improved RVOS performance\nthrough enhanced text-video understanding, several challenges remain, including\ninsufficient exploitation of MLLMs' prior knowledge, prohibitive computational\nand memory costs for long-duration videos, and inadequate handling of complex\ntemporal dynamics. In this work, we propose SVAC, a unified model that improves\nRVOS by scaling up input frames and segmentation tokens to enhance\nvideo-language interaction and segmentation precision. To address the resulting\ncomputational challenges, SVAC incorporates the Anchor-Based Spatio-Temporal\nCompression (ASTC) module to compress visual tokens while preserving essential\nspatio-temporal structure. Moreover, the Clip-Specific Allocation (CSA)\nstrategy is introduced to better handle dynamic object behaviors across video\nclips. Experimental results demonstrate that SVAC achieves state-of-the-art\nperformance on multiple RVOS benchmarks with competitive efficiency. Our code\nis available at https://github.com/lizhang1998/SVAC.",
        "url": "http://arxiv.org/abs/2509.24109v1",
        "published_date": "2025-09-28T23:02:09+00:00",
        "updated_date": "2025-09-28T23:02:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Li Zhang",
            "Haoxiang Gao",
            "Zhihao Zhang",
            "Luoxiao Huang",
            "Tao Zhang"
        ],
        "tldr": "The paper introduces SVAC, a unified model for Referring Video Object Segmentation (RVOS) that scales up input frames and segmentation tokens while using Anchor-Based Spatio-Temporal Compression (ASTC) and Clip-Specific Allocation (CSA) to improve performance and efficiency. It achieves state-of-the-art results on multiple RVOS benchmarks.",
        "tldr_zh": "该论文介绍了SVAC，一个统一的Referring Video Object Segmentation (RVOS)模型，通过放大输入帧和分割tokens，并利用基于锚点的时空压缩（ASTC）和片段特定分配（CSA）来提高性能和效率。它在多个RVOS基准测试中实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EVLF-FM: Explainable Vision Language Foundation Model for Medicine",
        "summary": "Despite the promise of foundation models in medical AI, current systems\nremain limited - they are modality-specific and lack transparent reasoning\nprocesses, hindering clinical adoption. To address this gap, we present\nEVLF-FM, a multimodal vision-language foundation model (VLM) designed to unify\nbroad diagnostic capability with fine-grain explainability. The development and\ntesting of EVLF-FM encompassed over 1.3 million total samples from 23 global\ndatasets across eleven imaging modalities related to six clinical specialties:\ndermatology, hepatology, ophthalmology, pathology, pulmonology, and radiology.\nExternal validation employed 8,884 independent test samples from 10 additional\ndatasets across five imaging modalities. Technically, EVLF-FM is developed to\nassist with multiple disease diagnosis and visual question answering with\npixel-level visual grounding and reasoning capabilities. In internal validation\nfor disease diagnostics, EVLF-FM achieved the highest average accuracy (0.858)\nand F1-score (0.797), outperforming leading generalist and specialist models.\nIn medical visual grounding, EVLF-FM also achieved stellar performance across\nnine modalities with average mIOU of 0.743 and Acc@0.5 of 0.837. External\nvalidations further confirmed strong zero-shot and few-shot performance, with\ncompetitive F1-scores despite a smaller model size. Through a hybrid training\nstrategy combining supervised and visual reinforcement fine-tuning, EVLF-FM not\nonly achieves state-of-the-art accuracy but also exhibits step-by-step\nreasoning, aligning outputs with visual evidence. EVLF-FM is an early\nmulti-disease VLM model with explainability and reasoning capabilities that\ncould advance adoption of and trust in foundation models for real-world\nclinical deployment.",
        "url": "http://arxiv.org/abs/2509.24231v1",
        "published_date": "2025-09-29T03:15:57+00:00",
        "updated_date": "2025-09-29T03:15:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Bai",
            "Haoran Cheng",
            "Yang Zhou",
            "Jun Zhou",
            "Arun Thirunavukarasu",
            "Yuhe Ke",
            "Jie Yao",
            "Kanae Fukutsu",
            "Chrystie Wan Ning Quek",
            "Ashley Hong",
            "Laura Gutierrez",
            "Zhen Ling Teo",
            "Darren Shu Jeng Ting",
            "Brian T. Soetikno",
            "Christopher S. Nielsen",
            "Tobias Elze",
            "Zengxiang Li",
            "Linh Le Dinh",
            "Hiok Hong Chan",
            "Victor Koh",
            "Marcus Tan",
            "Kelvin Z. Li",
            "Leonard Yip",
            "Ching Yu Cheng",
            "Yih Chung Tham",
            "Gavin Siew Wei Tan",
            "Leopold Schmetterer",
            "Marcus Ang",
            "Rahat Hussain",
            "Jod Mehta",
            "Tin Aung",
            "Lionel Tim-Ee Cheng",
            "Tran Nguyen Tuan Anh",
            "Chee Leong Cheng",
            "Tien Yin Wong",
            "Nan Liu",
            "Iain Beehuat Tan",
            "Soon Thye Lim",
            "Eyal Klang",
            "Tony Kiat Hon Lim",
            "Rick Siow Mong Goh",
            "Yong Liu",
            "Daniel Shu Wei Ting"
        ]
    },
    {
        "title": "BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation",
        "summary": "Vision foundation models like the Segment Anything Model (SAM), pretrained on\nlarge-scale natural image datasets, often struggle in medical image\nsegmentation due to a lack of domain-specific adaptation. In clinical practice,\nfine-tuning such models efficiently for medical downstream tasks with minimal\nresource demands, while maintaining strong performance, is challenging. To\naddress these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation\nframework that enhances SAM for medical imaging. It combines three tailored\ncomponents: (1) a Complementary Detail Enhancement Network (CDEN) using\ndepthwise separable convolutions and multi-scale fusion to capture\nboundary-sensitive features essential for accurate segmentation; (2) low-rank\nadapters integrated into SAM's Vision Transformer blocks to optimize feature\nrepresentation and attention for medical contexts, while simultaneously\nsignificantly reducing the parameter space; and (3) a low-rank tensor attention\nmechanism in the mask decoder, cutting memory usage by 75% and boosting\ninference speed. Experiments on standard medical segmentation datasets show\nthat BALR-SAM, without requiring prompts, outperforms several state-of-the-art\n(SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8%\n(11.7M) of its parameters.",
        "url": "http://arxiv.org/abs/2509.24204v1",
        "published_date": "2025-09-29T02:36:09+00:00",
        "updated_date": "2025-09-29T02:36:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zelin Liu",
            "Sicheng Dong",
            "Bocheng Li",
            "Yixuan Yang",
            "Jiacheng Ruan",
            "Chenxu Zhou",
            "Suncheng Xiang"
        ]
    },
    {
        "title": "UniVid: The Open-Source Unified Video Model",
        "summary": "Unified video modeling that combines generation and understanding\ncapabilities is increasingly important but faces two key challenges:\nmaintaining semantic faithfulness during flow-based generation due to\ntext-visual token imbalance and the limitations of uniform cross-modal\nattention across the flow trajectory, and efficiently extending image-centric\nMLLMs to video without costly retraining. We present UniVid, a unified\narchitecture that couples an MLLM with a diffusion decoder through a\nlightweight adapter, enabling both video understanding and generation. We\nintroduce Temperature Modality Alignment to improve prompt adherence and\nPyramid Reflection for efficient temporal reasoning via dynamic keyframe\nselection. Extensive experiments on standard benchmarks demonstrate\nstate-of-the-art performance, achieving a 2.2% improvement on VBench-Long total\nscore compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA\nand ActivityNet-QA, respectively, compared with the best prior 7B baselines.\nCode: https://github.com/AIGeeksGroup/UniVid. Website:\nhttps://aigeeksgroup.github.io/UniVid.",
        "url": "http://arxiv.org/abs/2509.24200v2",
        "published_date": "2025-09-29T02:31:36+00:00",
        "updated_date": "2025-09-30T09:46:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiabin Luo",
            "Junhui Lin",
            "Zeyu Zhang",
            "Biao Wu",
            "Meng Fang",
            "Ling Chen",
            "Hao Tang"
        ]
    },
    {
        "title": "Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection",
        "summary": "While vision-language models (VLMs) have made significant progress in\nmultimodal perception (e.g., open-vocabulary object detection) with simple\nlanguage queries, state-of-the-art VLMs still show limited ability to perceive\ncomplex queries involving descriptive attributes and relational clauses. Our\nin-depth analysis shows that these limitations mainly stem from text encoders\nin VLMs. Such text encoders behave like bags-of-words and fail to separate\ntarget objects from their descriptive attributes and relations in complex\nqueries, resulting in frequent false positives. To address this, we propose\nrestructuring linguistic representations according to the hierarchical\nrelations within sentences for language-based object detection. A key insight\nis the necessity of disentangling textual tokens into core components-objects,\nattributes, and relations (\"talk in pieces\")-and subsequently aggregating them\ninto hierarchically structured sentence-level representations (\"see in whole\").\nBuilding on this principle, we introduce the TaSe framework with three main\ncontributions: (1) a hierarchical synthetic captioning dataset spanning three\ntiers from category names to descriptive sentences; (2) Talk in Pieces, the\nthree-component disentanglement module guided by a novel disentanglement loss\nfunction, transforms text embeddings into subspace compositions; and (3) See in\nWhole, which learns to aggregate disentangled components into hierarchically\nstructured embeddings with the guide of proposed hierarchical objectives. The\nproposed TaSe framework strengthens the inductive bias of hierarchical\nlinguistic structures, resulting in fine-grained multimodal representations for\nlanguage-based object detection. Experimental results under the OmniLabel\nbenchmark show a 24% performance improvement, demonstrating the importance of\nlinguistic compositionality.",
        "url": "http://arxiv.org/abs/2509.24192v1",
        "published_date": "2025-09-29T02:14:26+00:00",
        "updated_date": "2025-09-29T02:14:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sojung An",
            "Kwanyong Park",
            "Yong Jae Lee",
            "Donghyun Kim"
        ]
    },
    {
        "title": "Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding",
        "summary": "Grounding natural language queries in graphical user interfaces (GUIs)\npresents a challenging task that requires models to comprehend diverse UI\nelements across various applications and systems, while also accurately\npredicting the spatial coordinates for the intended operation. To tackle this\nproblem, we propose GMS: Generalist Scanner Meets Specialist Locator, a\nsynergistic coarse-to-fine framework that effectively improves GUI grounding\nperformance. GMS leverages the complementary strengths of general\nvision-language models (VLMs) and small, task-specific GUI grounding models by\nassigning them distinct roles within the framework. Specifically, the general\nVLM acts as a 'Scanner' to identify potential regions of interest, while the\nfine-tuned grounding model serves as a 'Locator' that outputs precise\ncoordinates within these regions. This design is inspired by how humans perform\nGUI grounding, where the eyes scan the interface and the brain focuses on\ninterpretation and localization. Our whole framework consists of five stages\nand incorporates hierarchical search with cross-modal communication to achieve\npromising prediction results. Experimental results on the ScreenSpot-Pro\ndataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\\%$\nand $3.7\\%$ accuracy respectively when used independently, their integration\nwithin GMS framework yields an overall accuracy of $35.7\\%$, representing a $10\n\\times$ improvement. Additionally, GMS significantly outperforms other strong\nbaselines under various settings, demonstrating its robustness and potential\nfor general-purpose GUI grounding.",
        "url": "http://arxiv.org/abs/2509.24133v1",
        "published_date": "2025-09-29T00:06:31+00:00",
        "updated_date": "2025-09-29T00:06:31+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zhecheng Li",
            "Guoxian Song",
            "Yiwei Wang",
            "Zhen Xiong",
            "Junsong Yuan",
            "Yujun Cai"
        ]
    }
]