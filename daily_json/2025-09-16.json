[
    {
        "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs",
        "summary": "We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.\nSimilar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,\nwhich enables it to process images at their original variable resolutions. This\ndesign avoids the degradation caused by fixed-resolution tiling while\npreserving fine-grained details and global layouts, which is crucial for\nvisually dense content such as complex charts and diagrams. To ensure the\nsmooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a\ndistributed multimodal training framework tailored for Ascend NPUs. To maintain\ntraining accuracy, we implement equivalent replacements for certain operators.\nMindVL undergoes a three-phase training process, namely the warm-up phase,\nmultitask training phase, and supervised instruction tuning phase, to gradually\nenhance its capabilities. This process starts with basic visual and multimodal\npre-training, followed by large-scale multiask trainging and instruction\ntuning. We also adopt multimodal data packaging and hybrid parallelism\ntechniques, which significantly improve end-to-end training speed. To further\nboost model performance, we specifically introduce test-time resolution search\nand model weight averaging. Notably, despite using about 1/10 of the training\ndata required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL\nin evaluations of general multimodal understanding and document/table\ncomprehension. Beyond overall scores, MindVL also delivers leading performance\nin OCR assessments.",
        "url": "http://arxiv.org/abs/2509.11662v1",
        "published_date": "2025-09-15T08:00:31+00:00",
        "updated_date": "2025-09-15T08:00:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "eess.IV"
        ],
        "authors": [
            "Feilong Chen",
            "Yijiang Liu",
            "Yi Huang",
            "Hao Wang",
            "Miren Tian",
            "Ya-Qi Yu",
            "Minghui Liao",
            "Jihao Wu"
        ],
        "tldr": "MindVL is a multimodal large language model trained on Ascend NPUs, achieving comparable performance to Qwen2.5-VL with significantly less training data, particularly excelling in OCR tasks.",
        "tldr_zh": "MindVL 是一个在昇腾 NPU 上训练的多模态大型语言模型，以明显更少的训练数据实现了与 Qwen2.5-VL 相当的性能，尤其在 OCR 任务中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "How Auxiliary Reasoning Unleashes GUI Grounding in VLMs",
        "summary": "Graphical user interface (GUI) grounding is a fundamental task for building\nGUI agents. However, general vision-language models (VLMs) struggle with this\ntask due to a lack of specific optimization. We identify a key gap in this\npaper: while VLMs exhibit significant latent grounding potential, as\ndemonstrated by their performance measured by Pointing Game, they underperform\nwhen tasked with outputting explicit coordinates. To address this discrepancy,\nand bypass the high data and annotation costs of current fine-tuning\napproaches, we propose three zero-shot auxiliary reasoning methods. By\nproviding explicit spatial cues such as axes, grids and labeled intersections\nas part of the input image, these methods enable VLMs to articulate their\nimplicit spatial understanding capabilities. We evaluate these methods on four\nGUI grounding benchmarks across seven open-source and proprietary VLMs. The\nevaluation results demonstrate that the proposed methods substantially improve\nthe performance of GUI grounding.",
        "url": "http://arxiv.org/abs/2509.11548v1",
        "published_date": "2025-09-15T03:28:29+00:00",
        "updated_date": "2025-09-15T03:28:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiming Li",
            "Yan Shao",
            "Jing Yang",
            "Yujing Lu",
            "Ling Zhong",
            "Yuhan Wang",
            "Manni Duan"
        ],
        "tldr": "The paper introduces zero-shot auxiliary reasoning methods to improve GUI grounding in VLMs by providing spatial cues, enhancing their ability to articulate implicit spatial understanding without fine-tuning.",
        "tldr_zh": "该论文介绍了一种零样本辅助推理方法，通过提供空间线索来改进VLMs中的GUI定位，从而增强它们表达隐式空间理解的能力，而无需进行微调。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection",
        "summary": "This paper introduces a new Segment Anything Model (SAM) that leverages\nreverse parameter configuration and test-time training to enhance its\nperformance on Camouflaged Object Detection (COD), named SAM-TTT. While most\nexisting SAM-based COD models primarily focus on enhancing SAM by extracting\nfavorable features and amplifying its advantageous parameters, a crucial gap is\nidentified: insufficient attention to adverse parameters that impair SAM's\nsemantic understanding in downstream tasks. To tackle this issue, the Reverse\nSAM Parameter Configuration Module is proposed to effectively mitigate the\ninfluence of adverse parameters in a train-free manner by configuring SAM's\nparameters. Building on this foundation, the T-Visioner Module is unveiled to\nstrengthen advantageous parameters by integrating Test-Time Training layers,\noriginally developed for language tasks, into vision tasks. Test-Time Training\nlayers represent a new class of sequence modeling layers characterized by\nlinear complexity and an expressive hidden state. By integrating two modules,\nSAM-TTT simultaneously suppresses adverse parameters while reinforcing\nadvantageous ones, significantly improving SAM's semantic understanding in COD\ntask. Our experimental results on various COD benchmarks demonstrate that the\nproposed approach achieves state-of-the-art performance, setting a new\nbenchmark in the field. The code will be available at\nhttps://github.com/guobaoxiao/SAM-TTT.",
        "url": "http://arxiv.org/abs/2509.11884v1",
        "published_date": "2025-09-15T13:02:27+00:00",
        "updated_date": "2025-09-15T13:02:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenni Yu",
            "Li Zhao",
            "Guobao Xiao",
            "Xiaoqin Zhang"
        ],
        "tldr": "The paper introduces SAM-TTT, a new Segment Anything Model for Camouflaged Object Detection, enhancing performance through reverse parameter configuration and test-time training by suppressing adverse parameters and reinforcing advantageous ones.",
        "tldr_zh": "该论文介绍了一种新的用于伪装对象检测的分割任何模型SAM-TTT，通过反向参数配置和测试时训练来增强性能，具体方法是抑制不利参数并加强有利参数。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
        "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
        "url": "http://arxiv.org/abs/2509.11815v1",
        "published_date": "2025-09-15T11:53:56+00:00",
        "updated_date": "2025-09-15T11:53:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haiduo Huang",
            "Fuwei Yang",
            "Zhenhua Liu",
            "Xuanwu Yin",
            "Dong Li",
            "Pengju Ren",
            "Emad Barsoum"
        ],
        "tldr": "The paper introduces SpecVLM, a system that accelerates vision-language model inference using speculative decoding and an elastic visual compressor, achieving 2.5-2.9x speedups with an online distillation protocol.",
        "tldr_zh": "该论文介绍了SpecVLM，一个通过推测解码和弹性视觉压缩器加速视觉语言模型推理的系统，并通过在线蒸馏协议实现了2.5-2.9倍的加速。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Domain-Adaptive Pretraining Improves Primate Behavior Recognition",
        "summary": "Computer vision for animal behavior offers promising tools to aid research in\necology, cognition, and to support conservation efforts. Video camera traps\nallow for large-scale data collection, but high labeling costs remain a\nbottleneck to creating large-scale datasets. We thus need data-efficient\nlearning approaches. In this work, we show that we can utilize self-supervised\nlearning to considerably improve action recognition on primate behavior. On two\ndatasets of great ape behavior (PanAf and ChimpACT), we outperform published\nstate-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.\nmAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and\napplying domain-adaptive pretraining (DAP), i.e. continuing the pretraining\nwith in-domain data. We show that most of the performance gain stems from the\nDAP. Our method promises great potential for improving the recognition of\nanimal behavior, as DAP does not require labeled samples. Code is available at\nhttps://github.com/ecker-lab/dap-behavior",
        "url": "http://arxiv.org/abs/2509.12193v1",
        "published_date": "2025-09-15T17:54:20+00:00",
        "updated_date": "2025-09-15T17:54:20+00:00",
        "categories": [
            "cs.CV",
            "I.4.8; I.2.10; I.5"
        ],
        "authors": [
            "Felix B. Mueller",
            "Timo Lueddecke",
            "Richard Vogg",
            "Alexander S. Ecker"
        ],
        "tldr": "This paper improves primate behavior recognition by using domain-adaptive pretraining (DAP) with a V-JEPA model, achieving significant performance gains on two datasets without requiring labeled samples for DAP.",
        "tldr_zh": "该论文通过使用V-JEPA模型和领域自适应预训练 (DAP) 改进了灵长类动物行为识别，在两个数据集上取得了显著的性能提升，且DAP过程不需要标记样本。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]