[
    {
        "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation",
        "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
        "url": "http://arxiv.org/abs/2509.05263v1",
        "published_date": "2025-09-05T17:22:33+00:00",
        "updated_date": "2025-09-05T17:22:33+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yinglin Duan",
            "Zhengxia Zou",
            "Tongwei Gu",
            "Wei Jia",
            "Zhan Zhao",
            "Luyi Xu",
            "Xinzhu Liu",
            "Hao Jiang",
            "Kang Chen",
            "Shuang Qiu"
        ],
        "tldr": "The paper introduces LatticeWorld, a framework leveraging LLMs (LLaMA-2-7B) and Unreal Engine 5 to generate interactive 3D worlds from multimodal (text and visual) inputs, demonstrating improved scene layout accuracy, visual fidelity, and industrial production efficiency.",
        "tldr_zh": "该论文介绍了 LatticeWorld，一个利用 LLMs (LLaMA-2-7B) 和 Unreal Engine 5 从多模态（文本和视觉）输入生成交互式 3D 世界的框架，展示了改进的场景布局准确性、视觉保真度和工业生产效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Group Detection using VLM-augmented Temporal Groupness Graph",
        "summary": "This paper proposes dynamic human group detection in videos. For detecting\ncomplex groups, not only the local appearance features of in-group members but\nalso the global context of the scene are important. Such local and global\nappearance features in each frame are extracted using a Vision-Language Model\n(VLM) augmented for group detection in our method. For further improvement, the\ngroup structure should be consistent over time. While previous methods are\nstabilized on the assumption that groups are not changed in a video, our method\ndetects dynamically changing groups by global optimization using a graph with\nall frames' groupness probabilities estimated by our groupness-augmented CLIP\nfeatures. Our experimental results demonstrate that our method outperforms\nstate-of-the-art group detection methods on public datasets. Code:\nhttps://github.com/irajisamurai/VLM-GroupDetection.git",
        "url": "http://arxiv.org/abs/2509.04758v1",
        "published_date": "2025-09-05T02:37:01+00:00",
        "updated_date": "2025-09-05T02:37:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaname Yokoyama",
            "Chihiro Nakatani",
            "Norimichi Ukita"
        ],
        "tldr": "This paper introduces a VLM-augmented method for dynamic human group detection in videos, using a temporal groupness graph for global optimization and achieving state-of-the-art results.",
        "tldr_zh": "本文提出了一种使用VLM增强的视频中动态人群检测方法，该方法使用时间群体图进行全局优化，并实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Sample-efficient Integration of New Modalities into Large Language Models",
        "summary": "Multimodal foundation models can process several modalities. However, since\nthe space of possible modalities is large and evolving over time, training a\nmodel from scratch to encompass all modalities is unfeasible. Moreover,\nintegrating a modality into a pre-existing foundation model currently requires\na significant amount of paired data, which is often not available for\nlow-resource modalities. In this paper, we introduce a method for\nsample-efficient modality integration (SEMI) into Large Language Models (LLMs).\nTo this end, we devise a hypernetwork that can adapt a shared projector --\nplaced between modality-specific encoders and an LLM -- to any modality. The\nhypernetwork, trained on high-resource modalities (i.e., text, speech, audio,\nvideo), is conditioned on a few samples from any arbitrary modality at\ninference time to generate a suitable adapter. To increase the diversity of\ntraining modalities, we artificially multiply the number of encoders through\nisometric transformations. We find that SEMI achieves a significant boost in\nsample efficiency during few-shot integration of new modalities (i.e.,\nsatellite images, astronomical images, inertial measurements, and molecules)\nwith encoders of arbitrary embedding dimensionality. For instance, to reach the\nsame accuracy as 32-shot SEMI, training the projector from scratch needs\n64$\\times$ more data. As a result, SEMI holds promise to extend the modality\ncoverage of foundation models.",
        "url": "http://arxiv.org/abs/2509.04606v1",
        "published_date": "2025-09-04T18:41:59+00:00",
        "updated_date": "2025-09-04T18:41:59+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Osman Batur İnce",
            "André F. T. Martins",
            "Oisin Mac Aodha",
            "Edoardo M. Ponti"
        ],
        "tldr": "The paper introduces a sample-efficient method (SEMI) using hypernetworks to integrate new modalities into LLMs, demonstrating significant improvements in few-shot learning for low-resource modalities by adapting a shared projector between modality encoders and the LLM.",
        "tldr_zh": "本文介绍了一种名为SEMI的样本高效方法，该方法利用超网络将新模态集成到大型语言模型中，通过调整模态编码器和LLM之间的共享投影器，显著提高了低资源模态的少样本学习效果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework",
        "summary": "Most existing infrared-visible image fusion (IVIF) methods assume\nhigh-quality inputs, and therefore struggle to handle dual-source degraded\nscenarios, typically requiring manual selection and sequential application of\nmultiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion\npipeline inevitably leads to error accumulation and performance degradation. To\novercome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),\na novel framework that synergistically integrates vision-language models (VLMs)\nfor degradation perception with dual-domain (frequency/spatial) joint\noptimization. Concretely, the designed Guided Frequency Modality-Specific\nExtraction (GFMSE) module performs frequency-domain degradation perception and\nsuppression and discriminatively extracts fusion-relevant sub-band features.\nMeanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries\nout cross-modal degradation filtering and adaptive multi-source feature\naggregation in the spatial domain to enhance modality complementarity and\nstructural consistency. Extensive qualitative and quantitative experiments\ndemonstrate that GD^2Fusion achieves superior fusion performance compared with\nexisting algorithms and strategies in dual-source degraded scenarios. The code\nwill be publicly released after acceptance of this paper.",
        "url": "http://arxiv.org/abs/2509.05000v1",
        "published_date": "2025-09-05T10:48:46+00:00",
        "updated_date": "2025-09-05T10:48:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianpei Zhang",
            "Jufeng Zhao",
            "Yiming Zhu",
            "Guangmang Cui"
        ],
        "tldr": "This paper introduces GD^2Fusion, a novel infrared and visible image fusion framework using vision-language models for degradation perception and dual-domain optimization, achieving superior performance in degraded scenarios.",
        "tldr_zh": "本文介绍了一种名为GD^2Fusion的新型红外和可见图像融合框架，该框架利用视觉语言模型进行退化感知和双域优化，在退化场景中实现了卓越的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing",
        "summary": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser.",
        "url": "http://arxiv.org/abs/2509.04908v1",
        "published_date": "2025-09-05T08:24:12+00:00",
        "updated_date": "2025-09-05T08:24:12+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Hongyi Jing",
            "Jiafu Chen",
            "Chen Rao",
            "Ziqiang Dang",
            "Jiajie Teng",
            "Tianyi Chu",
            "Juncheng Mo",
            "Shuo Fang",
            "Huaizhong Lin",
            "Rui Lv",
            "Chenguang Ma",
            "Lei Zhao"
        ],
        "tldr": "SparkUI-Parser addresses limitations in existing GUI perception MLLMs by using continuous coordinate modeling and a rejection mechanism, achieving higher accuracy and speed with a new benchmark dataset, ScreenParse.",
        "tldr_zh": "SparkUI-Parser通过使用连续坐标建模和拒绝机制，解决了现有GUI感知MLLM的局限性，实现了更高的准确性和速度，并提出了一个新的基准数据集ScreenParse。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SynGen-Vision: Synthetic Data Generation for training industrial vision models",
        "summary": "We propose an approach to generate synthetic data to train computer vision\n(CV) models for industrial wear and tear detection. Wear and tear detection is\nan important CV problem for predictive maintenance tasks in any industry.\nHowever, data curation for training such models is expensive and time-consuming\ndue to the unavailability of datasets for different wear and tear scenarios.\nOur approach employs a vision language model along with a 3D simulation and\nrendering engine to generate synthetic data for varying rust conditions. We\nevaluate our approach by training a CV model for rust detection using the\ngenerated dataset and tested the trained model on real images of rusted\nindustrial objects. The model trained with the synthetic data generated by our\napproach, outperforms the other approaches with a mAP50 score of 0.87. The\napproach is customizable and can be easily extended to other industrial wear\nand tear detection scenarios",
        "url": "http://arxiv.org/abs/2509.04894v1",
        "published_date": "2025-09-05T08:15:46+00:00",
        "updated_date": "2025-09-05T08:15:46+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.4"
        ],
        "authors": [
            "Alpana Dubey",
            "Suma Mani Kuriakose",
            "Nitish Bhardwaj"
        ],
        "tldr": "This paper introduces SynGen-Vision, a synthetic data generation approach using VLMs and 3D rendering for training computer vision models in industrial wear and tear detection. It achieves a mAP50 of 0.87 in rust detection.",
        "tldr_zh": "该论文介绍了SynGen-Vision，一种利用视觉语言模型和3D渲染生成合成数据的方法，用于训练工业磨损检测的计算机视觉模型。在锈蚀检测中，该方法实现了0.87的mAP50。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval",
        "summary": "The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by\ntextual queries with the same semantic meanings. Recent CLIP-based approaches\nhave explored two frameworks: Two-Tower versus Single-Tower framework, yet the\nformer suffers from low effectiveness, while the latter suffers from low\nefficiency. In this study, we explore a new Hybrid-Tower framework that can\nhybridize the advantages of the Two-Tower and Single-Tower framework, achieving\nhigh effectiveness and efficiency simultaneously. We propose a novel hybrid\nmethod, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,\nwhich includes a new pseudo-query generator designed to generate a pseudo-query\nfor each video. This enables the video feature and the textual features of\npseudo-query to interact in a fine-grained manner, similar to the Single-Tower\napproaches to hold high effectiveness, even before the real textual query is\nreceived. Simultaneously, our method introduces no additional storage or\ncomputational overhead compared to the Two-Tower framework during the inference\nstage, thus maintaining high efficiency. Extensive experiments on five commonly\nused text-video retrieval benchmarks demonstrate that our method achieves a\nsignificant improvement over the baseline, with an increase of $1.6\\% \\sim\n3.9\\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower\nmodels while achieving near state-of-the-art performance, highlighting the\nadvantages of the Hybrid-Tower framework.",
        "url": "http://arxiv.org/abs/2509.04773v1",
        "published_date": "2025-09-05T03:05:50+00:00",
        "updated_date": "2025-09-05T03:05:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bangxiang Lan",
            "Ruobing Xie",
            "Ruixiang Zhao",
            "Xingwu Sun",
            "Zhanhui Kang",
            "Gang Yang",
            "Xirong Li"
        ],
        "tldr": "The paper introduces a novel Hybrid-Tower framework (PIG) for text-to-video retrieval, combining the effectiveness of Single-Tower and efficiency of Two-Tower architectures using a pseudo-query generation method. It achieves significant performance improvements with comparable efficiency to Two-Tower models.",
        "tldr_zh": "该论文提出了一种新颖的混合塔结构（PIG）用于文本到视频检索，结合了单塔结构的有效性和双塔结构的效率，通过伪查询生成方法实现了这一点。该方法在效率与双塔模型相当的情况下，取得了显著的性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph",
        "summary": "Timely and accurate floodwater depth estimation is critical for road\naccessibility and emergency response. While recent computer vision methods have\nenabled flood detection, they suffer from both accuracy limitations and poor\ngeneralization due to dependence on fixed object detectors and task-specific\ntraining. To enable accurate depth estimation that can generalize across\ndiverse flood scenarios, this paper presents FloodVision, a zero-shot framework\nthat combines the semantic reasoning abilities of the foundation\nvision-language model GPT-4o with a structured domain knowledge graph. The\nknowledge graph encodes canonical real-world dimensions for common urban\nobjects including vehicles, people, and infrastructure elements to ground the\nmodel's reasoning in physical reality. FloodVision dynamically identifies\nvisible reference objects in RGB images, retrieves verified heights from the\nknowledge graph to mitigate hallucination, estimates submergence ratios, and\napplies statistical outlier filtering to compute final depth values. Evaluated\non 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean\nabsolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and\nsurpassing prior CNN-based methods. The system generalizes well across varying\nscenes and operates in near real-time, making it suitable for future\nintegration into digital twin platforms and citizen-reporting apps for smart\ncity flood resilience.",
        "url": "http://arxiv.org/abs/2509.04772v1",
        "published_date": "2025-09-05T03:05:18+00:00",
        "updated_date": "2025-09-05T03:05:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhangding Liu",
            "Neda Mohammadi",
            "John E. Taylor"
        ],
        "tldr": "FloodVision uses GPT-4o and a domain knowledge graph to estimate urban flood depth from images, achieving improved accuracy and generalization compared to prior methods.",
        "tldr_zh": "FloodVision 结合 GPT-4o 和领域知识图谱，通过图像估计城市洪水深度，与现有方法相比，实现了更高的准确性和泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Guideline-Consistent Segmentation via Multi-Agent Refinement",
        "summary": "Semantic segmentation in real-world applications often requires not only\naccurate masks but also strict adherence to textual labeling guidelines. These\nguidelines are typically complex and long, and both human and automated\nlabeling often fail to follow them faithfully. Traditional approaches depend on\nexpensive task-specific retraining that must be repeated as the guidelines\nevolve. Although recent open-vocabulary segmentation methods excel with simple\nprompts, they often fail when confronted with sets of paragraph-length\nguidelines that specify intricate segmentation rules. To address this, we\nintroduce a multi-agent, training-free framework that coordinates\ngeneral-purpose vision-language models within an iterative Worker-Supervisor\nrefinement architecture. The Worker performs the segmentation, the Supervisor\ncritiques it against the retrieved guidelines, and a lightweight reinforcement\nlearning stop policy decides when to terminate the loop, ensuring\nguideline-consistent masks while balancing resource use. Evaluated on the Waymo\nand ReasonSeg datasets, our method notably outperforms state-of-the-art\nbaselines, demonstrating strong generalization and instruction adherence.",
        "url": "http://arxiv.org/abs/2509.04687v1",
        "published_date": "2025-09-04T22:32:57+00:00",
        "updated_date": "2025-09-04T22:32:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vanshika Vats",
            "Ashwani Rathee",
            "James Davis"
        ],
        "tldr": "This paper presents a multi-agent, training-free framework for semantic segmentation that uses vision-language models to ensure adherence to complex, paragraph-length labeling guidelines, outperforming SOTA baselines on Waymo and ReasonSeg datasets.",
        "tldr_zh": "本文提出了一种多智能体、免训练的语义分割框架，该框架使用视觉语言模型来确保符合复杂的段落长度标注指南，并在 Waymo 和 ReasonSeg 数据集上优于 SOTA 基线。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation",
        "summary": "Vision-language models and their adaptations to image segmentation tasks\npresent enormous potential for producing highly accurate and interpretable\nresults. However, implementations based on CLIP and BiomedCLIP are still\nlagging behind more sophisticated architectures such as CRIS. In this work,\ninstead of focusing on text prompt engineering as is the norm, we attempt to\nnarrow this gap by showing how to ensemble vision-language segmentation models\n(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice\nscore improvement of 6.3% on the BKAI polyp dataset using the ensembled\nBiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.\nFurthermore, we provide initial results on additional four radiology and\nnon-radiology datasets. We conclude that ensembling works differently across\nthese datasets (from outperforming to underperforming the CRIS model),\nindicating a topic for future investigation by the community. The code is\navailable at https://github.com/juliadietlmeier/VLSM-Ensemble.",
        "url": "http://arxiv.org/abs/2509.05154v1",
        "published_date": "2025-09-05T14:48:19+00:00",
        "updated_date": "2025-09-05T14:48:19+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Julia Dietlmeier",
            "Oluwabukola Grace Adegboro",
            "Vayangi Ganepola",
            "Claudia Mazo",
            "Noel E. O'Connor"
        ],
        "tldr": "This paper explores ensembling CLIP-based vision-language models with a simple CNN for medical image segmentation, achieving Dice score improvements, particularly on the BKAI polyp dataset. Results vary across datasets, suggesting further research.",
        "tldr_zh": "本文探讨了将基于CLIP的视觉语言模型与简单的CNN集成用于医学图像分割，实现了Dice得分的提升，尤其是在BKAI息肉数据集上。结果因数据集而异，表明需要进一步研究。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization",
        "summary": "Industrial product inspection is often performed using Anomaly Detection (AD)\nframeworks trained solely on non-defective samples. Although defective samples\ncan be collected during production, leveraging them usually requires\npixel-level annotations, limiting scalability. To address this, we propose\nADClick, an Interactive Image Segmentation (IIS) algorithm for industrial\nanomaly detection. ADClick generates pixel-wise anomaly annotations from only a\nfew user clicks and a brief textual description, enabling precise and efficient\nlabeling that significantly improves AD model performance (e.g., AP = 96.1\\% on\nMVTec AD). We further introduce ADClick-Seg, a cross-modal framework that\naligns visual features and textual prompts via a prototype-based approach for\nanomaly detection and localization. By combining pixel-level priors with\nlanguage-guided cues, ADClick-Seg achieves state-of-the-art results on the\nchallenging ``Multi-class'' AD task (AP = 80.0\\%, PRO = 97.5\\%, Pixel-AUROC =\n99.1\\% on MVTec AD).",
        "url": "http://arxiv.org/abs/2509.05034v1",
        "published_date": "2025-09-05T11:45:17+00:00",
        "updated_date": "2025-09-05T11:45:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingqi Wu",
            "Hanxi Li",
            "Lin Yuanbo Wu",
            "Hao Chen",
            "Deyin Liu",
            "Peng Wang"
        ],
        "tldr": "The paper introduces ADClick and ADClick-Seg, methods for efficient pixel-level anomaly annotation in industrial settings using minimal user input and textual descriptions, achieving state-of-the-art results on the MVTec AD dataset.",
        "tldr_zh": "该论文介绍了 ADClick 和 ADClick-Seg，这两种方法利用最少的用户输入和文本描述，在工业环境中实现高效的像素级异常标注，并在 MVTec AD 数据集上取得了最先进的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]