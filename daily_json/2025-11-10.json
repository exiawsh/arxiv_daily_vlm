[
    {
        "title": "GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding",
        "summary": "Gaze understanding unifies the detection of people, their gaze targets, and\nobjects of interest into a single framework, offering critical insight into\nvisual attention and intent estimation. Although prior research has modelled\ngaze cues in visual scenes, a unified system is still needed for gaze\nunderstanding using both visual and language prompts. This paper introduces\nGazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding\nin images, addressing person detection, gaze target detection, and gaze object\nidentification. While other transformer-based methods exist for gaze analysis,\nGazeVLM represents, to our knowledge, the first application of a VLM to these\ncombined tasks, allowing for selective execution of each task. Through the\nintegration of visual (RGB and depth) and textual modalities, our ablation\nstudy on visual input combinations revealed that a fusion of RGB images with\nHHA-encoded depth maps, guided by text prompts, yields superior performance. We\nalso introduce an object-level gaze detection metric for gaze object\nidentification ($AP_{ob}$). Through experiments, GazeVLM demonstrates\nsignificant improvements, notably achieving state-of-the-art evaluation scores\non GazeFollow and VideoAttentionTarget datasets.",
        "url": "http://arxiv.org/abs/2511.06348v1",
        "published_date": "2025-11-09T12:07:40+00:00",
        "updated_date": "2025-11-09T12:07:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Athul M. Mathew",
            "Haithem Hermassi",
            "Thariq Khalid",
            "Arshad Ali Khan",
            "Riad Souissi"
        ],
        "tldr": "GazeVLM is a novel Vision-Language Model for multi-task gaze understanding, achieving state-of-the-art results on GazeFollow and VideoAttentionTarget datasets by fusing RGB and HHA-encoded depth maps guided by text prompts.",
        "tldr_zh": "GazeVLM是一种新颖的视觉-语言模型，用于多任务的视线理解。通过融合RGB和HHA编码的深度图，并在文本提示的指导下，在GazeFollow和VideoAttentionTarget数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving",
        "summary": "Recent advancements in language-grounded autonomous driving have been\nsignificantly promoted by the sophisticated cognition and reasoning\ncapabilities of large language models (LLMs). However, current LLM-based\napproaches encounter critical challenges: (1) Failure analysis reveals that\nfrequent collisions and obstructions, stemming from limitations in visual\nrepresentations, remain primary obstacles to robust driving performance. (2)\nThe substantial parameters of LLMs pose considerable deployment hurdles. To\naddress these limitations, we introduce VLDrive, a novel approach featuring a\nlightweight MLLM architecture with enhanced vision components. VLDrive achieves\ncompact visual tokens through innovative strategies, including cycle-consistent\ndynamic visual pruning and memory-enhanced feature aggregation. Furthermore, we\npropose a distance-decoupled instruction attention mechanism to improve joint\nvisual-linguistic feature learning, particularly for long-range visual tokens.\nExtensive experiments conducted in the CARLA simulator demonstrate VLDrive`s\neffectiveness. Notably, VLDrive achieves state-of-the-art driving performance\nwhile reducing parameters by 81% (from 7B to 1.3B), yielding substantial\ndriving score improvements of 15.4%, 16.8%, and 7.6% at tiny, short, and long\ndistances, respectively, in closed-loop evaluations. Code is available at\nhttps://github.com/ReaFly/VLDrive.",
        "url": "http://arxiv.org/abs/2511.06256v1",
        "published_date": "2025-11-09T07:14:53+00:00",
        "updated_date": "2025-11-09T07:14:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruifei Zhang",
            "Wei Zhang",
            "Xiao Tan",
            "Sibei Yang",
            "Xiang Wan",
            "Xiaonan Luo",
            "Guanbin Li"
        ],
        "tldr": "VLDrive introduces a lightweight MLLM architecture with enhanced vision components for language-grounded autonomous driving, achieving state-of-the-art performance with significantly reduced parameters.",
        "tldr_zh": "VLDrive 提出了一种轻量级的 MLLM 架构，具有增强的视觉组件，用于语言引导的自动驾驶，并在显著减少参数的情况下实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoRA: Missing Modality Low-Rank Adaptation for Visual Recognition",
        "summary": "Pre-trained vision language models have shown remarkable performance on\nvisual recognition tasks, but they typically assume the availability of\ncomplete multimodal inputs during both training and inference. In real-world\nscenarios, however, modalities may be missing due to privacy constraints,\ncollection difficulties, or resource limitations. While previous approaches\nhave addressed this challenge using prompt learning techniques, they fail to\ncapture the cross-modal relationships necessary for effective multimodal visual\nrecognition and suffer from inevitable computational overhead. In this paper,\nwe introduce MoRA, a parameter-efficient fine-tuning method that explicitly\nmodels cross-modal interactions while maintaining modality-specific\nadaptations. MoRA introduces modality-common parameters between text and vision\nencoders, enabling bidirectional knowledge transfer. Additionally, combined\nwith the modality-specific parameters, MoRA allows the backbone model to\nmaintain inter-modality interaction and enable intra-modality flexibility.\nExtensive experiments on standard benchmarks demonstrate that MoRA achieves an\naverage performance improvement in missing-modality scenarios by 5.24% and uses\nonly 25.90% of the inference time compared to the SOTA method while requiring\nonly 0.11% of trainable parameters compared to full fine-tuning.",
        "url": "http://arxiv.org/abs/2511.06225v1",
        "published_date": "2025-11-09T04:52:42+00:00",
        "updated_date": "2025-11-09T04:52:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shu Zhao",
            "Nilesh Ahuja",
            "Tan Yu",
            "Tianyi Shen",
            "Vijaykrishnan Narayanan"
        ],
        "tldr": "The paper introduces MoRA, a parameter-efficient fine-tuning method for vision-language models in missing modality scenarios, achieving improved performance and reduced computational overhead compared to existing methods.",
        "tldr_zh": "该论文介绍了MoRA，一种用于视觉语言模型在缺失模态场景下的参数高效微调方法，与现有方法相比，该方法实现了更高的性能和更低的计算开销。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models",
        "summary": "This paper introduces a human-in-the-loop computer vision framework that uses\ngenerative AI to propose micro-scale design interventions in public space and\nsupport more continuous, local participation. Using Grounding DINO and a\ncurated subset of the ADE20K dataset as a proxy for the urban built\nenvironment, the system detects urban objects and builds co-occurrence\nembeddings that reveal common spatial configurations. From this analysis, the\nuser receives five statistically likely complements to a chosen anchor object.\nA vision language model then reasons over the scene image and the selected pair\nto suggest a third object that completes a more complex urban tactic. The\nworkflow keeps people in control of selection and refinement and aims to move\nbeyond top-down master planning by grounding choices in everyday patterns and\nlived experience.",
        "url": "http://arxiv.org/abs/2511.06201v1",
        "published_date": "2025-11-09T03:24:10+00:00",
        "updated_date": "2025-11-09T03:24:10+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Rodrigo Gallardo",
            "Oz Fishman",
            "Alexander Htet Kyaw"
        ],
        "tldr": "The paper presents a human-in-the-loop AI framework using co-occurrence embeddings and a vision-language model (VLM) to suggest micro-scale urban design interventions based on detected urban objects and common spatial configurations.",
        "tldr_zh": "该论文提出了一个人机协作的AI框架，利用共现嵌入和视觉语言模型(VLM)，基于检测到的城市物体和常见的空间配置，为城市设计微观干预提供建议。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models",
        "summary": "Spatial Reasoning is an important component of human cognition and is an area\nin which the latest Vision-language models (VLMs) show signs of difficulty. The\ncurrent analysis works use image captioning tasks and visual question\nanswering. In this work, we propose using the Referring Expression\nComprehension task instead as a platform for the evaluation of spatial\nreasoning by VLMs. This platform provides the opportunity for a deeper analysis\nof spatial comprehension and grounding abilities when there is 1) ambiguity in\nobject detection, 2) complex spatial expressions with a longer sentence\nstructure and multiple spatial relations, and 3) expressions with negation\n('not'). In our analysis, we use task-specific architectures as well as large\nVLMs and highlight their strengths and weaknesses in dealing with these\nspecific situations. While all these models face challenges with the task at\nhand, the relative behaviors depend on the underlying models and the specific\ncategories of spatial semantics (topological, directional, proximal, etc.). Our\nresults highlight these challenges and behaviors and provide insight into\nresearch gaps and future directions.",
        "url": "http://arxiv.org/abs/2511.06146v1",
        "published_date": "2025-11-08T21:43:09+00:00",
        "updated_date": "2025-11-08T21:43:09+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Akshar Tumu",
            "Varad Shinde",
            "Parisa Kordjamshidi"
        ],
        "tldr": "This paper proposes using Referring Expression Comprehension to evaluate spatial reasoning abilities in VLMs, highlighting their strengths and weaknesses in handling ambiguity, complex spatial relations, and negation.",
        "tldr_zh": "本文提出使用指称表达式理解来评估视觉语言模型中的空间推理能力，重点展示了它们在处理歧义、复杂空间关系和否定方面的优势和劣势。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "NOAH: Benchmarking Narrative Prior driven Hallucination and Omission in Video Large Language Models",
        "summary": "Video large language models (Video LLMs) have recently achieved strong\nperformance on tasks such as captioning, summarization, and question answering.\nMany models and training methods explicitly encourage continuity across events\nto enhance narrative coherence. While this improves fluency, it also introduces\nan inductive bias that prioritizes storyline consistency over strict grounding\nin visual evidence. We identify this bias, which we call narrative prior, as a\nkey driver of two errors: hallucinations, where non-existent events are\nintroduced or existing ones are misinterpreted, and omissions, where factual\nevents are suppressed because they are misaligned with surrounding context. To\nsystematically evaluate narrative prior-induced errors, we introduce NOAH, a\nlarge-scale benchmark that constructs composite videos by inserting clips from\nother sources into target videos. By varying semantic similarity and insertion\nposition, our benchmark enables controlled and scalable analysis of narrative\npriors. We design one captioning task with tailored metrics and three QA tasks\n- Existence, Temporal, and Narrative - yielding more than 60K evaluation\nsamples. Extensive experiments yield three key findings: (i) most Video LLMs\nexhibit hallucinations and omissions driven by narrative priors, (ii) the\npatterns of these errors vary across architectures and depend on event\nsimilarity and insertion position, and (iii) reliance on narrative priors\nintensifies under sampling with fewer frames, amplifying errors when event\ncontinuity is weak. We establish NOAH as the first standardized evaluation of\nnarrative prior-induced hallucination and omission in Video LLMs, providing a\nfoundation for developing more reliable and trustworthy models. Our benchmark\nand code are available at https://anonymous550520.github.io/.",
        "url": "http://arxiv.org/abs/2511.06475v1",
        "published_date": "2025-11-09T17:41:11+00:00",
        "updated_date": "2025-11-09T17:41:11+00:00",
        "categories": [
            "cs.CV",
            "I.2.10; I.4.8"
        ],
        "authors": [
            "Kyuho Lee",
            "Euntae Kim",
            "Jinwoo Choi",
            "Buru Chang"
        ],
        "tldr": "The paper introduces NOAH, a benchmark for evaluating narrative prior-induced hallucination and omission errors in Video LLMs, revealing that these models often prioritize narrative consistency over visual evidence, leading to errors. The benchmark uses inserted clips in videos to test models and identify patterns.",
        "tldr_zh": "该论文介绍了NOAH，一个用于评估视频大语言模型中由叙事先验引起的幻觉和遗漏错误的基准。研究表明，这些模型经常优先考虑叙事一致性而非视觉证据，从而导致错误。该基准通过在视频中插入片段来测试模型并识别错误模式。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization",
        "summary": "With the rapid growth of the low-altitude economy, unmanned aerial vehicles\n(UAVs) have become key platforms for measurement and tracking in intelligent\npatrol systems. However, in GNSS-denied environments, localization schemes that\nrely solely on satellite signals are prone to failure. Cross-view image\nretrieval-based localization is a promising alternative, yet substantial\ngeometric and appearance domain gaps exist between oblique UAV views and nadir\nsatellite orthophotos. Moreover, conventional approaches often depend on\ncomplex network architectures, text prompts, or large amounts of annotation,\nwhich hinders generalization. To address these issues, we propose\nDiffusionUavLoc, a cross-view localization framework that is image-prompted,\ntext-free, diffusion-centric, and employs a VAE for unified representation. We\nfirst use training-free geometric rendering to synthesize pseudo-satellite\nimages from UAV imagery as structural prompts. We then design a text-free\nconditional diffusion model that fuses multimodal structural cues to learn\nfeatures robust to viewpoint changes. At inference, descriptors are computed at\na fixed time step t and compared using cosine similarity. On University-1652\nand SUES-200, the method performs competitively for cross-view localization,\nespecially for satellite-to-drone in University-1652.Our data and code will be\npublished at the following URL:\nhttps://github.com/liutao23/DiffusionUavLoc.git.",
        "url": "http://arxiv.org/abs/2511.06422v1",
        "published_date": "2025-11-09T15:27:17+00:00",
        "updated_date": "2025-11-09T15:27:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Liu",
            "Kan Ren",
            "Qian Chen"
        ],
        "tldr": "The paper proposes DiffusionUavLoc, a novel cross-view UAV localization framework using a diffusion model with geometric rendering to address the domain gap between UAV and satellite imagery, achieving competitive results in GNSS-denied environments.",
        "tldr_zh": "该论文提出了 DiffusionUavLoc，一种新颖的跨视角无人机定位框架，使用扩散模型和几何渲染来解决无人机和卫星图像之间的域差距，在无 GNSS 环境中取得了具有竞争力的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AesTest: Measuring Aesthetic Intelligence from Perception to Production",
        "summary": "Perceiving and producing aesthetic judgments is a fundamental yet\nunderexplored capability for multimodal large language models (MLLMs). However,\nexisting benchmarks for image aesthetic assessment (IAA) are narrow in\nperception scope or lack the diversity needed to evaluate systematic aesthetic\nproduction. To address this gap, we introduce AesTest, a comprehensive\nbenchmark for multimodal aesthetic perception and production, distinguished by\nthe following features: 1) It consists of curated multiple-choice questions\nspanning ten tasks, covering perception, appreciation, creation, and\nphotography. These tasks are grounded in psychological theories of generative\nlearning. 2) It integrates data from diverse sources, including professional\nediting workflows, photographic composition tutorials, and crowdsourced\npreferences. It ensures coverage of both expert-level principles and real-world\nvariation. 3) It supports various aesthetic query types, such as\nattribute-based analysis, emotional resonance, compositional choice, and\nstylistic reasoning. We evaluate both instruction-tuned IAA MLLMs and general\nMLLMs on AesTest, revealing significant challenges in building aesthetic\nintelligence. We will publicly release AesTest to support future research in\nthis area.",
        "url": "http://arxiv.org/abs/2511.06360v1",
        "published_date": "2025-11-09T12:44:10+00:00",
        "updated_date": "2025-11-09T12:44:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guolong Wang",
            "Heng Huang",
            "Zhiqiang Zhang",
            "Wentian Li",
            "Feilong Ma",
            "Xin Jin"
        ],
        "tldr": "The paper introduces AesTest, a new benchmark for evaluating the aesthetic perception and production capabilities of Multimodal Large Language Models (MLLMs) across various tasks related to image aesthetics.",
        "tldr_zh": "本文介绍了一个名为AesTest的新基准，用于评估多模态大型语言模型（MLLMs）在图像美学相关各种任务中的美学感知和生成能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks",
        "summary": "While Vision Language Models (VLMs) have demonstrated remarkable capabilities\nin general visual understanding, their application in the chemical domain has\nbeen limited, with previous works predominantly focusing on text and thus\noverlooking critical visual information, such as molecular structures. Current\napproaches that directly adopt standard VLMs for chemical tasks suffer from two\nprimary issues: (i) computational inefficiency of processing entire chemical\nimages with non-informative backgrounds. (ii) a narrow scope on molecular-level\ntasks that restricts progress in chemical reasoning. In this work, we propose\n\\textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages\nvisual token reduction and reaction-level tasks to improve model efficiency and\nreasoning capacity. Also, we propose \\textbf{ChemRxn-V}, a reaction-level\nbenchmark for assessing vision-based reaction recognition and prediction tasks.\nDirectly predicting reaction products from molecular images poses a non-trivial\nchallenge, as it requires models to integrate both recognition and reasoning\ncapacities. Our results demonstrate that with only 4B parameters, TinyChemVL\nachieves superior performance on both molecular and reaction tasks while\ndemonstrating faster inference and training speeds compared to existing models.\nNotably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the\nvisual tokens. This work builds efficient yet powerful VLMs for chemical\ndomains by co-designing model architecture and task complexity.",
        "url": "http://arxiv.org/abs/2511.06283v1",
        "published_date": "2025-11-09T08:37:18+00:00",
        "updated_date": "2025-11-09T08:37:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuanle Zhao",
            "Shuxin Zeng",
            "Yinyuan Cai",
            "Xiang Cheng",
            "Duzhen Zhang",
            "Xiuyi Chen",
            "Bo Xu"
        ],
        "tldr": "TinyChemVL introduces a chemical vision-language model with efficient visual token reduction and a new reaction-level benchmark (ChemRxn-V), achieving superior performance and efficiency compared to existing models.",
        "tldr_zh": "TinyChemVL 提出了一个化学视觉语言模型，通过高效的视觉令牌减少和新的反应级别基准测试 (ChemRxn-V)，与现有模型相比，实现了卓越的性能和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]