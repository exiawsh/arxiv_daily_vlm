[
    {
        "title": "SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM",
        "summary": "Industrial vision inspection requires high accuracy under stringent resource\nconstraints, yet existing approaches face a fundamental trade-off. Multimodal\nLLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive\ncomputational costs, while lightweight edge models often fail on complex cases.\nIn this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative\nindustrial vision inspection framework with MLLM. The framework is composed of\nthree synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect\nInspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)\nAdaptive Edge-Cloud Scheduler. Together, these modules enable robust defect\ndetection by tailoring multimodal reasoning to scene complexity and dynamically\nbalancing computation between edge and cloud resources. Experimental results on\nMVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%\naccuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It\nalso reduces runtime by up to 22.4% and cuts energy per correct decision by\n40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.",
        "url": "http://arxiv.org/abs/2509.17136v1",
        "published_date": "2025-09-21T15:58:31+00:00",
        "updated_date": "2025-09-21T15:58:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuhao Tian",
            "Zheming Yang"
        ],
        "tldr": "The paper presents SAEC, an edge-cloud collaborative industrial vision inspection framework using fine-tuned multimodal LLMs, scene complexity estimation, and adaptive scheduling to improve accuracy and reduce runtime and energy consumption compared to other MLLMs.",
        "tldr_zh": "该论文提出了SAEC，一个边缘-云协同的工业视觉检测框架，它利用微调的多模态LLM、场景复杂性估计和自适应调度，相较于其他MLLM，提高了准确率，并降低了运行时间和能耗。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models",
        "summary": "Visual affordance learning is crucial for robots to understand and interact\neffectively with the physical world. Recent advances in this field attempt to\nleverage pre-trained knowledge of vision-language foundation models to learn\naffordance properties with limited training data, providing a novel paradigm\nfor visual affordance learning. However, these methods overlook the\nsignificance of maintaining feature alignment between visual images and\nlanguage descriptions for identifying affordance areas with textual guidance,\nand thus may lead to suboptimal results. In this paper, we present an\ninformative framework for text-guided affordance learning, which involves\ninformation-based constraints to achieve text-image alignment at feature level.\nSpecifically, we design an affordance mutual information constraint that helps\nlearn appropriate textual prompts and task-oriented visual features\nsimultaneously by maximizing the mutual information between the features of the\naffordance areas in the input images and the corresponding textual prompts. In\naddition, we propose an object-level information constraint that maximizes the\nmutual information between the visual features of a given object and the text\nfeatures of the category it belongs to. This enables the model to capture\nhigh-quality representations for the object, providing more reliable semantic\npriors for identifying affordance regions. Experimental results on the AGD20K\ndataset show that the proposed method outperforms existing approaches and\nachieves the new state-of-the-art in one-shot affordance learning.",
        "url": "http://arxiv.org/abs/2509.17074v1",
        "published_date": "2025-09-21T13:21:16+00:00",
        "updated_date": "2025-09-21T13:21:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qian Zhang",
            "Lin Zhang",
            "Xing Fang",
            "Mingxin Zhang",
            "Zhiyuan Wei",
            "Ran Song",
            "Wei Zhang"
        ],
        "tldr": "This paper introduces an information-based framework for text-guided affordance learning, improving text-image alignment at the feature level by maximizing mutual information between affordance areas and textual prompts, as well as between objects and their categories. The method achieves state-of-the-art results on the AGD20K dataset for one-shot affordance learning.",
        "tldr_zh": "本文提出了一种基于信息的文本引导的自适应学习框架，通过最大化自适应区域与文本提示之间以及对象与其类别之间的互信息，从而在特征层面改进文本图像对齐。该方法在 AGD20K 数据集上实现了单样本自适应学习的最新成果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AgriDoctor: A Multimodal Intelligent Assistant for Agriculture",
        "summary": "Accurate crop disease diagnosis is essential for sustainable agriculture and\nglobal food security. Existing methods, which primarily rely on unimodal models\nsuch as image-based classifiers and object detectors, are limited in their\nability to incorporate domain-specific agricultural knowledge and lack support\nfor interactive, language-based understanding. Recent advances in large\nlanguage models (LLMs) and large vision-language models (LVLMs) have opened new\navenues for multimodal reasoning. However, their performance in agricultural\ncontexts remains limited due to the absence of specialized datasets and\ninsufficient domain adaptation. In this work, we propose AgriDoctor, a modular\nand extensible multimodal framework designed for intelligent crop disease\ndiagnosis and agricultural knowledge interaction. As a pioneering effort to\nintroduce agent-based multimodal reasoning into the agricultural domain,\nAgriDoctor offers a novel paradigm for building interactive and domain-adaptive\ncrop health solutions. It integrates five core components: a router,\nclassifier, detector, knowledge retriever and LLMs. To facilitate effective\ntraining and evaluation, we construct AgriMM, a comprehensive benchmark\ncomprising 400000 annotated disease images, 831 expert-curated knowledge\nentries, and 300000 bilingual prompts for intent-driven tool selection.\nExtensive experiments demonstrate that AgriDoctor, trained on AgriMM,\nsignificantly outperforms state-of-the-art LVLMs on fine-grained agricultural\ntasks, establishing a new paradigm for intelligent and sustainable farming\napplications.",
        "url": "http://arxiv.org/abs/2509.17044v1",
        "published_date": "2025-09-21T11:51:57+00:00",
        "updated_date": "2025-09-21T11:51:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingqing Zhang",
            "Zhuoning Xu",
            "Peijie Wang",
            "Rongji Li",
            "Liang Wang",
            "Qiang Liu",
            "Jian Xu",
            "Xuyao Zhang",
            "Shu Wu",
            "Liang Wang"
        ],
        "tldr": "The paper introduces AgriDoctor, a multimodal framework for crop disease diagnosis leveraging LLMs and LVLMs, along with AgriMM, a new benchmark dataset to address the limitations of existing methods in agricultural contexts.",
        "tldr_zh": "该论文介绍了 AgriDoctor，一个利用 LLM 和 LVLM 进行作物疾病诊断的多模态框架，以及 AgriMM，一个新的基准数据集，旨在解决现有方法在农业环境中的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA",
        "summary": "Referring video object segmentation (RVOS) requires segmenting and tracking\nobjects in videos conditioned on natural-language expressions, demanding\nfine-grained understanding of both appearance and motion. Building on Sa2VA,\nwhich couples a Multi-modal Large Language Model (MLLM) with the video\nsegmentation model SAM2, we identify two key bottlenecks that limit\nsegmentation performance: sparse frame sampling and reliance on a single [SEG]\ntoken for an entire video. We propose Segmentation Augmented and Selective\nAveraged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge\n(RVOS track), SaSaSa2VA achieves a $J\\&F$ of 67.45, ranking first and\nsurpassing the runner-up by 2.80 points. This result and ablation studies\ndemonstrate that efficient segmentation augmentation and test-time ensembling\nsubstantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA\nrepository: https://github.com/magic-research/Sa2VA.",
        "url": "http://arxiv.org/abs/2509.16972v1",
        "published_date": "2025-09-21T08:08:17+00:00",
        "updated_date": "2025-09-21T08:08:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Quanzhu Niu",
            "Dengxian Gong",
            "Shihao Chen",
            "Tao Zhang",
            "Yikang Zhou",
            "Haobo Yuan",
            "Lu Qi",
            "Xiangtai Li",
            "Shunping Ji"
        ],
        "tldr": "The paper presents SaSaSa2VA, a new approach for referring video object segmentation (RVOS) that improves upon Sa2VA by addressing sparse frame sampling and single token reliance, achieving state-of-the-art results on the 7th LSVOS Challenge (RVOS track).",
        "tldr_zh": "该论文提出了SaSaSa2VA，一种用于指代视频对象分割（RVOS）的新方法，通过解决稀疏帧采样和单令牌依赖问题改进了Sa2VA，并在第七届LSVOS挑战赛（RVOS track）上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception",
        "summary": "Multimodal Large Language Models (MLLMs) require high-resolution visual\ninformation to perform fine-grained perception, yet processing entire\nhigh-resolution images is computationally prohibitive. While recent methods\nleverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they\ntypically present a difficult trade-off: training-based approaches depend on\nlarge-scale annotated datasets, while training-free methods that utilize the\nmodel's internal attention are computationally inefficient and less accurate,\nrequiring either multi-pass prefill stages or reliance on the slow\nauto-regressive decoding process. In this paper, we propose an efficient,\nannotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves\nthis trade-off. The SD-RPN is built around a pipeline that transforms the noisy\nattention maps from the MLLM's middle layers into high-quality pseudo-RoI\nlabels by explicitly denoising the signal and resolving ambiguity. We use these\nlabels to train a lightweight Region Proposal Network (RPN) that learns a more\nprecise localization. This RPN is also highly efficient, predicting the RoI in\na single forward pass using features from the MLLM's middle layers, decoupling\nRoI identification from the auto-regressive generation and avoiding costly\nmulti-pass operations.To validate our approach, we integrate the framework into\nthe LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)\nquestion-answer pairs, our method demonstrates exceptional data efficiency and\ngeneralization, achieving over a 10% absolute accuracy improvement on unseen\nbenchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a\npractical and scalable solution for enhancing the fine-grained perception of\nMLLMs without requiring costly supervision or full model fine-tuning. Code is\navailable at https://github.com/YuHengsss/SD-RPN.",
        "url": "http://arxiv.org/abs/2509.16944v1",
        "published_date": "2025-09-21T06:54:04+00:00",
        "updated_date": "2025-09-21T06:54:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuheng Shi",
            "Xiaohuan Pei",
            "Minjing Dong",
            "Chang Xu"
        ],
        "tldr": "This paper introduces an efficient, annotation-free self-distilled region proposal network (SD-RPN) to improve fine-grained perception in MLLMs by focusing on relevant regions of high-resolution images, achieving significant accuracy improvements with limited training data.",
        "tldr_zh": "本文介绍了一种高效、无标注的自蒸馏区域建议网络 (SD-RPN)，通过关注高分辨率图像的相关区域来提高 MLLM 中细粒度的感知能力，并在有限的训练数据下实现了显著的准确性提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions",
        "summary": "We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/",
        "url": "http://arxiv.org/abs/2509.17177v1",
        "published_date": "2025-09-21T17:53:30+00:00",
        "updated_date": "2025-09-21T17:53:30+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Bowen Qin",
            "Chen Yue",
            "Fang Yin",
            "Hui Wang",
            "JG Yao",
            "Jiakang Liu",
            "Jing-Shu Zheng",
            "Miguel Hu Chen",
            "Richeng Xuan",
            "Shibei Meng",
            "Shiqi Zhou",
            "Teng Dai",
            "Tong-Shuai Ren",
            "Wei Cui",
            "Xi Yang",
            "Xialin Du",
            "Xiaojing Xu",
            "Xue Sun",
            "Xuejing Li",
            "Yaming Liu",
            "Yesheng Liu",
            "Ying Liu",
            "Yonghua Lin",
            "Yu Zhao",
            "Yunduo Zhang",
            "Yuwen Luo",
            "Zheqi He",
            "Zhiyuan He",
            "Zhongyuan Wang"
        ],
        "tldr": "This paper introduces ROME, a benchmark for evaluating vision-language models' reasoning abilities using visual clues, and presents preliminary evaluation findings of large reasoning models.",
        "tldr_zh": "本文介绍了ROME，一个用于评估视觉语言模型基于视觉线索的推理能力的基准，并展示了对大型推理模型的初步评估结果。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors",
        "summary": "Video action recognition is a fundamental task in computer vision, but\nstate-of-the-art models are often computationally expensive and rely on\nextensive video pre-training. In parallel, large-scale vision-language models\nlike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot\ncapabilities on static images, while motion vectors (MV) provide highly\nefficient temporal information directly from compressed video streams. To\nsynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple\nyet powerful two-stream late fusion framework for efficient video recognition.\nOur approach combines features from a frozen CLIP image encoder with features\nfrom a lightweight, supervised network trained on raw MV. During fusion, both\nbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is\ntrained, ensuring extreme efficiency. Through comprehensive experiments on the\nUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,\nsignificantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)\nbaselines. Our work provides a new, highly efficient baseline for video\nunderstanding that effectively bridges the gap between large static models and\ndynamic, low-cost motion cues. Our code and models are available at\nhttps://github.com/microa/MoCLIP-Lite.",
        "url": "http://arxiv.org/abs/2509.17084v1",
        "published_date": "2025-09-21T14:02:38+00:00",
        "updated_date": "2025-09-21T14:02:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Binhua Huang",
            "Nan Wang",
            "Arjun Parakash",
            "Soumyabrata Dev"
        ],
        "tldr": "MoCLIP-Lite is a computationally efficient video recognition framework that fuses a frozen CLIP image encoder with a lightweight network trained on motion vectors, achieving high accuracy on UCF101 with minimal training.",
        "tldr_zh": "MoCLIP-Lite是一个计算效率高的视频识别框架，它将冻结的CLIP图像编码器与在运动矢量上训练的轻量级网络融合，以极少的训练在UCF101上实现了高精度。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner",
        "summary": "Echocardiography is a vital non-invasive modality for cardiac assessment,\nwith left ventricular ejection fraction (LVEF) serving as a key indicator of\nheart function. Existing LVEF estimation methods depend on large-scale\nannotated video datasets, which are costly and limit adaptability across\nvarious clinical settings. Recent vision-language models for echocardiography,\nsuch as EchoCLIP, apply image-to-text pretraining but fail to capture crucial\ntemporal dynamics and localized cardiac structures essential for accurate\ndiagnosis. To address these challenges, we propose CardiacCLIP, a video-based\nframework that enhances LVEF prediction through attention-based frame\naggregation and multi-resolution input scaling. Specifically, we introduce MFL\n(Multi Frame Learning), a novel attention-based mechanism for selectively\nfusing informative frames, and EchoZoom, a multi-scale feature extraction\nstrategy that refines spatial representations of cardiac structures. As a novel\nadaptation of CLIP models for few-shot echocardiogram video analysis, our\napproach significantly improves diagnostic accuracy, reducing MAE by 2.07 on\nthe EchoNet-Dynamic dataset under 1-shot setting. The code is available at\nhttps://github.com/xmed-lab/CardiacCLIP.",
        "url": "http://arxiv.org/abs/2509.17065v1",
        "published_date": "2025-09-21T12:52:08+00:00",
        "updated_date": "2025-09-21T12:52:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yao Du",
            "Jiarong Guo",
            "Xiaomeng Li"
        ],
        "tldr": "CardiacCLIP introduces a video-based CLIP adaptation for LVEF prediction in echocardiograms, using attention-based frame aggregation and multi-resolution input scaling to improve accuracy in few-shot settings.",
        "tldr_zh": "CardiacCLIP 提出了一种基于视频的 CLIP 改编方法，用于在超声心动图中预测 LVEF，通过基于注意力的帧聚合和多分辨率输入缩放，提高了在少样本设置下的准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models",
        "summary": "Large Vision-Language Models (LVLMs) have achieved strong performance on\nvision-language tasks, particularly Visual Question Answering (VQA). While\nprior work has explored unimodal biases in VQA, the problem of selection bias\nin Multiple-Choice Question Answering (MCQA), where models may favor specific\noption tokens (e.g., \"A\") or positions, remains underexplored. In this paper,\nwe investigate both the presence and nature of selection bias in LVLMs through\nfine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,\ndefined by the semantic similarity of the options. We further propose an\ninference-time logit-level debiasing method that estimates an ensemble bias\nvector from general and contextual prompts and applies confidence-adaptive\ncorrections to the model's output. Our method mitigates bias without retraining\nand is compatible with frozen LVLMs. Extensive experiments across several\nstate-of-the-art models reveal consistent selection biases that intensify with\ntask difficulty, and show that our mitigation approach significantly reduces\nbias while improving accuracy in challenging settings. This work offers new\ninsights into the limitations of LVLMs in MCQA and presents a practical\napproach to improve their robustness in fine-grained visual reasoning. Datasets\nand code are available at:\nhttps://github.com/Atabuzzaman/Selection-Bias-of-LVLMs",
        "url": "http://arxiv.org/abs/2509.16805v1",
        "published_date": "2025-09-20T20:45:47+00:00",
        "updated_date": "2025-09-20T20:45:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md. Atabuzzaman",
            "Ali Asgarov",
            "Chris Thomas"
        ],
        "tldr": "This paper identifies and mitigates selection bias in Large Vision-Language Models (LVLMs) for multiple-choice question answering (MCQA) tasks by proposing an inference-time debiasing method, improving accuracy in challenging settings.",
        "tldr_zh": "该论文识别并缓解了大型视觉语言模型（LVLM）在多项选择题回答（MCQA）任务中的选择偏差，通过提出一种推理时去偏方法，提高了在具有挑战性环境中的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation",
        "summary": "Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,\nmetaverse, and robotics. However, most methods represent the target object as a\nclosed mesh devoid of any structural information, limiting editing, animation,\nand semantic understanding. Part-aware 3D generation addresses this problem by\ndecomposing objects into meaningful components, but existing pipelines face\nchallenges: in existing methods, the user has no control over which objects are\nseparated and how model imagine the occluded parts in isolation phase. In this\npaper, we introduce MMPart, an innovative framework for generating part-aware\n3D models from a single image. We first use a VLM to generate a set of prompts\nbased on the input image and user descriptions. In the next step, a generative\nmodel generates isolated images of each object based on the initial image and\nthe previous step's prompts as supervisor (which control the pose and guide\nmodel how imagine previously occluded areas). Each of those images then enters\nthe multi-view generation stage, where a number of consistent images from\ndifferent views are generated. Finally, a reconstruction model converts each of\nthese multi-view images into a 3D model.",
        "url": "http://arxiv.org/abs/2509.16768v1",
        "published_date": "2025-09-20T18:25:14+00:00",
        "updated_date": "2025-09-20T18:25:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Omid Bonakdar",
            "Nasser Mozayani"
        ],
        "tldr": "The paper introduces MMPart, a framework using VLMs to generate part-aware 3D models from a single image by generating prompts, isolated part images, multi-view images, and finally reconstructing 3D models of each part.",
        "tldr_zh": "该论文介绍了一个名为MMPart的框架，它利用视觉语言模型（VLM）从单个图像生成具有部件感知能力的3D模型，方法是生成提示、孤立的部件图像、多视角图像，并最终重建每个部件的3D模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]