[
    {
        "title": "EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning",
        "summary": "Recent advances in foundation models highlight a clear trend toward\nunification and scaling, showing emergent capabilities across diverse domains.\nWhile image generation and editing have rapidly transitioned from task-specific\nto unified frameworks, video generation and editing remain fragmented due to\narchitectural limitations and data scarcity. In this work, we introduce\nEditVerse, a unified framework for image and video generation and editing\nwithin a single model. By representing all modalities, i.e., text, image, and\nvideo, as a unified token sequence, EditVerse leverages self-attention to\nachieve robust in-context learning, natural cross-modal knowledge transfer, and\nflexible handling of inputs and outputs with arbitrary resolutions and\ndurations. To address the lack of video editing training data, we design a\nscalable data pipeline that curates 232K video editing samples and combines\nthem with large-scale image and video datasets for joint training. Furthermore,\nwe present EditVerseBench, the first benchmark for instruction-based video\nediting covering diverse tasks and resolutions. Extensive experiments and user\nstudies demonstrate that EditVerse achieves state-of-the-art performance,\nsurpassing existing open-source and commercial models, while exhibiting\nemergent editing and generation abilities across modalities.",
        "url": "http://arxiv.org/abs/2509.20360v1",
        "published_date": "2025-09-24T17:59:30+00:00",
        "updated_date": "2025-09-24T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuan Ju",
            "Tianyu Wang",
            "Yuqian Zhou",
            "He Zhang",
            "Qing Liu",
            "Nanxuan Zhao",
            "Zhifei Zhang",
            "Yijun Li",
            "Yuanhao Cai",
            "Shaoteng Liu",
            "Daniil Pakhomov",
            "Zhe Lin",
            "Soo Ye Kim",
            "Qiang Xu"
        ],
        "tldr": "The paper introduces EditVerse, a unified framework for image and video generation and editing using in-context learning with a novel data pipeline and benchmark, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了一种名为EditVerse的统一框架，用于图像和视频生成和编辑，该框架利用上下文学习，并配备了新的数据管道和基准测试，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Video models are zero-shot learners and reasoners",
        "summary": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models.",
        "url": "http://arxiv.org/abs/2509.20328v1",
        "published_date": "2025-09-24T17:17:27+00:00",
        "updated_date": "2025-09-24T17:17:27+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Thaddäus Wiedemer",
            "Yuxuan Li",
            "Paul Vicol",
            "Shixiang Shane Gu",
            "Nick Matarese",
            "Kevin Swersky",
            "Been Kim",
            "Priyank Jaini",
            "Robert Geirhos"
        ],
        "tldr": "The paper demonstrates that the Veo 3 video model exhibits emergent zero-shot capabilities across various vision tasks, suggesting video models are progressing towards becoming generalist vision foundation models.",
        "tldr_zh": "本文展示了Veo 3视频模型在各种视觉任务中表现出新兴的零样本能力，表明视频模型正朝着通用视觉基础模型的方向发展。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving",
        "summary": "Human vision is capable of transforming two-dimensional observations into an\negocentric three-dimensional scene understanding, which underpins the ability\nto translate complex scenes and exhibit adaptive behaviors. This capability,\nhowever, remains lacking in current autonomous driving systems, where\nmainstream approaches primarily rely on depth-based 3D reconstruction rather\nthan true scene understanding. To address this limitation, we propose a novel\nhuman-like framework called OmniScene. First, we introduce the OmniScene\nVision-Language Model (OmniVLM), a vision-language framework that integrates\nmulti-view and temporal perception for holistic 4D scene understanding. Then,\nharnessing a teacher-student OmniVLM architecture and knowledge distillation,\nwe embed textual representations into 3D instance features for semantic\nsupervision, enriching feature learning, and explicitly capturing human-like\nattentional semantics. These feature representations are further aligned with\nhuman driving behaviors, forming a more human-like\nperception-understanding-action architecture. In addition, we propose a\nHierarchical Fusion Strategy (HFS) to address imbalances in modality\ncontributions during multimodal integration. Our approach adaptively calibrates\nthe relative significance of geometric and semantic features at multiple\nabstraction levels, enabling the synergistic use of complementary cues from\nvisual and textual modalities. This learnable dynamic fusion enables a more\nnuanced and effective exploitation of heterogeneous information. We evaluate\nOmniScene comprehensively on the nuScenes dataset, benchmarking it against over\nten state-of-the-art models across various tasks. Our approach consistently\nachieves superior results, establishing new benchmarks in perception,\nprediction, planning, and visual question answering.",
        "url": "http://arxiv.org/abs/2509.19973v1",
        "published_date": "2025-09-24T10:28:06+00:00",
        "updated_date": "2025-09-24T10:28:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pei Liu",
            "Hongliang Lu",
            "Haichao Liu",
            "Haipeng Liu",
            "Xin Liu",
            "Ruoyu Yao",
            "Shengbo Eben Li",
            "Jun Ma"
        ],
        "tldr": "The paper introduces OmniScene, a novel vision-language framework for autonomous driving that integrates multi-view and temporal perception for 4D scene understanding, using attention and hierarchical fusion to achieve superior performance on the nuScenes dataset.",
        "tldr_zh": "该论文介绍了OmniScene，一种用于自动驾驶的新型视觉语言框架，它集成了多视角和时间感知以实现4D场景理解，并使用注意力机制和分层融合在nuScenes数据集上实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Universal Camouflage Attack on Vision-Language Models for Autonomous Driving",
        "summary": "Visual language modeling for automated driving is emerging as a promising\nresearch direction with substantial improvements in multimodal reasoning\ncapabilities. Despite its advanced reasoning abilities, VLM-AD remains\nvulnerable to serious security threats from adversarial attacks, which involve\nmisleading model decisions through carefully crafted perturbations. Existing\nattacks have obvious challenges: 1) Physical adversarial attacks primarily\ntarget vision modules. They are difficult to directly transfer to VLM-AD\nsystems because they typically attack low-level perceptual components. 2)\nAdversarial attacks against VLM-AD have largely concentrated on the digital\nlevel. To address these challenges, we propose the first Universal Camouflage\nAttack (UCA) framework for VLM-AD. Unlike previous methods that focus on\noptimizing the logit layer, UCA operates in the feature space to generate\nphysically realizable camouflage textures that exhibit strong generalization\nacross different user commands and model architectures. Motivated by the\nobserved vulnerability of encoder and projection layers in VLM-AD, UCA\nintroduces a feature divergence loss (FDL) that maximizes the representational\ndiscrepancy between clean and adversarial images. In addition, UCA incorporates\na multi-scale learning strategy and adjusts the sampling ratio to enhance its\nadaptability to changes in scale and viewpoint diversity in real-world\nscenarios, thereby improving training stability. Extensive experiments\ndemonstrate that UCA can induce incorrect driving commands across various\nVLM-AD models and driving scenarios, significantly surpassing existing\nstate-of-the-art attack methods (improving 30\\% in 3-P metrics). Furthermore,\nUCA exhibits strong attack robustness under diverse viewpoints and dynamic\nconditions, indicating high potential for practical deployment.",
        "url": "http://arxiv.org/abs/2509.20196v1",
        "published_date": "2025-09-24T14:52:01+00:00",
        "updated_date": "2025-09-24T14:52:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dehong Kong",
            "Sifan Yu",
            "Siyuan Liang",
            "Jiawei Liang",
            "Jianhou Gan",
            "Aishan Liu",
            "Wenqi Ren"
        ],
        "tldr": "This paper introduces a Universal Camouflage Attack (UCA) framework to generate physically realizable adversarial camouflage textures in the feature space, specifically targeting Vision-Language Models for Autonomous Driving (VLM-AD), and demonstrates its effectiveness and robustness compared to existing methods.",
        "tldr_zh": "该论文提出了一种通用伪装攻击（UCA）框架，用于在特征空间中生成物理上可实现的对抗性伪装纹理，专门针对用于自动驾驶的视觉语言模型（VLM-AD），并证明了其相对于现有方法的有效性和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models",
        "summary": "Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize\nleaderboard accuracy, overlooking reliability and safety. We study sycophancy\n-- models' tendency to uncritically echo user-provided information -- in\nhigh-stakes clinical settings. We introduce EchoBench, a benchmark to\nsystematically evaluate sycophancy in medical LVLMs. It contains 2,122 images\nacross 18 departments and 20 modalities with 90 prompts that simulate biased\ninputs from patients, medical students, and physicians. We evaluate\nmedical-specific, open-source, and proprietary LVLMs. All exhibit substantial\nsycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98%\nsycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95%\nsycophancy despite only moderate accuracy. Fine-grained analyses by bias type,\ndepartment, perceptual granularity, and modality identify factors that increase\nsusceptibility. We further show that higher data quality/diversity and stronger\ndomain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench\nalso serves as a testbed for mitigation: simple prompt-level interventions\n(negative prompting, one-shot, few-shot) produce consistent reductions and\nmotivate training- and decoding-time strategies. Our findings highlight the\nneed for robust evaluation beyond accuracy and provide actionable guidance\ntoward safer, more trustworthy medical LVLMs.",
        "url": "http://arxiv.org/abs/2509.20146v1",
        "published_date": "2025-09-24T14:09:55+00:00",
        "updated_date": "2025-09-24T14:09:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Botai Yuan",
            "Yutian Zhou",
            "Yingjie Wang",
            "Fushuo Huo",
            "Yongcheng Jing",
            "Li Shen",
            "Ying Wei",
            "Zhiqi Shen",
            "Ziwei Liu",
            "Tianwei Zhang",
            "Jie Yang",
            "Dacheng Tao"
        ],
        "tldr": "The paper introduces EchoBench, a benchmark to evaluate sycophancy in medical LVLMs, revealing substantial sycophancy across various models and suggesting mitigation strategies.",
        "tldr_zh": "该论文介绍了EchoBench，一个用于评估医疗LVLM中奉承行为的基准，揭示了各种模型中普遍存在的奉承现象，并提出了缓解策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA",
        "summary": "Scientific visual question answering poses significant challenges for\nvision-language models due to the complexity of scientific figures and their\nmultimodal context. Traditional approaches treat the figure and accompanying\ntext (e.g., questions and answer options) as separate inputs. EXAMS-V\nintroduced a new paradigm by embedding both visual and textual content into a\nsingle image. However, even state-of-the-art proprietary models perform poorly\non this setup in zero-shot settings, underscoring the need for task-specific\nfine-tuning. To address the scarcity of training data in this \"text-in-image\"\nformat, we synthesize a new dataset by converting existing separate image-text\npairs into unified images. Fine-tuning a small multilingual multimodal model on\na mix of our synthetic data and EXAMS-V yields notable gains across 13\nlanguages, demonstrating strong average improvements and cross-lingual\ntransfer.",
        "url": "http://arxiv.org/abs/2509.20119v1",
        "published_date": "2025-09-24T13:41:04+00:00",
        "updated_date": "2025-09-24T13:41:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Belal Shoer",
            "Yova Kementchedjhieva"
        ],
        "tldr": "The paper proposes a data augmentation strategy for scientific VQA by synthesizing training data in a 'text-in-image' format, showing improved performance on multilingual tasks after fine-tuning a multimodal model.",
        "tldr_zh": "该论文提出了一种针对科学VQA的数据增强策略，通过合成“文本嵌入图像”格式的训练数据，并在微调多模态模型后，在多语言任务上显示出性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning",
        "summary": "To enable robots to comprehend high-level human instructions and perform\ncomplex tasks, a key challenge lies in achieving comprehensive scene\nunderstanding: interpreting and interacting with the 3D environment in a\nmeaningful way. This requires a smart map that fuses accurate geometric\nstructure with rich, human-understandable semantics. To address this, we\nintroduce the 3D Queryable Scene Representation (3D QSR), a novel framework\nbuilt on multimedia data that unifies three complementary 3D representations:\n(1) 3D-consistent novel view rendering and segmentation from panoptic\nreconstruction, (2) precise geometry from 3D point clouds, and (3) structured,\nscalable organization via 3D scene graphs. Built on an object-centric design,\nthe framework integrates with large vision-language models to enable semantic\nqueryability by linking multimodal object embeddings, and supporting\nobject-level retrieval of geometric, visual, and semantic information. The\nretrieved data are then loaded into a robotic task planner for downstream\nexecution. We evaluate our approach through simulated robotic task planning\nscenarios in Unity, guided by abstract language instructions and using the\nindoor public dataset Replica. Furthermore, we apply it in a digital duplicate\nof a real wet lab environment to test QSR-supported robotic task planning for\nemergency response. The results demonstrate the framework's ability to\nfacilitate scene understanding and integrate spatial and semantic reasoning,\neffectively translating high-level human instructions into precise robotic task\nplanning in complex 3D environments.",
        "url": "http://arxiv.org/abs/2509.20077v1",
        "published_date": "2025-09-24T12:53:32+00:00",
        "updated_date": "2025-09-24T12:53:32+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Xun Li",
            "Rodrigo Santa Cruz",
            "Mingze Xi",
            "Hu Zhang",
            "Madhawa Perera",
            "Ziwei Wang",
            "Ahalya Ravendran",
            "Brandon J. Matthews",
            "Feng Xu",
            "Matt Adcock",
            "Dadong Wang",
            "Jiajun Liu"
        ],
        "tldr": "The paper introduces a 3D Queryable Scene Representation (3D QSR) framework that integrates geometric, visual, and semantic information to enable robots to understand high-level instructions and perform complex tasks, demonstrated through simulated and real-world robotic task planning.",
        "tldr_zh": "该论文介绍了一种3D可查询场景表示(3D QSR)框架，该框架集成了几何、视觉和语义信息，使机器人能够理解高级指令并执行复杂任务，并通过模拟和真实世界的机器人任务规划进行了演示。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset",
        "summary": "While there exists a lot of work on explainable complaint mining,\narticulating user concerns through text or video remains a significant\nchallenge, often leaving issues unresolved. Users frequently struggle to\nexpress their complaints clearly in text but can easily upload videos depicting\nproduct defects (e.g., vague text such as `worst product' paired with a\n5-second video depicting a broken headphone with the right earcup). This paper\nformulates a new task in the field of complaint mining to aid the common users'\nneed to write an expressive complaint, which is Complaint Description from\nVideos (CoD-V) (e.g., to help the above user articulate her complaint about the\ndefective right earcup). To this end, we introduce ComVID, a video complaint\ndataset containing 1,175 complaint videos and the corresponding descriptions,\nalso annotated with the emotional state of the complainer. Additionally, we\npresent a new complaint retention (CR) evaluation metric that discriminates the\nproposed (CoD-V) task against standard video summary generation and description\ntasks. To strengthen this initiative, we introduce a multimodal\nRetrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to\ngenerate complaints while accounting for the user's emotional state. We conduct\na comprehensive evaluation of several Video Language Models on several tasks\n(pre-trained and fine-tuned versions) with a range of established evaluation\nmetrics, including METEOR, perplexity, and the Coleman-Liau readability score,\namong others. Our study lays the foundation for a new research direction to\nprovide a platform for users to express complaints through video. Dataset and\nresources are available at: https://github.com/sarmistha-D/CoD-V.",
        "url": "http://arxiv.org/abs/2509.19952v1",
        "published_date": "2025-09-24T10:00:05+00:00",
        "updated_date": "2025-09-24T10:00:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sarmistha Das",
            "R E Zera Marveen Lyngkhoi",
            "Kirtan Jain",
            "Vinayak Goyal",
            "Sriparna Saha",
            "Manish Gupta"
        ],
        "tldr": "The paper introduces a new task (CoD-V) and dataset (ComVID) for generating user complaint descriptions from videos, along with a multimodal RAG-based VideoLLaMA2-7b model and a novel complaint retention metric for evaluation. The authors aim to provide a platform for users to express complaints through video.",
        "tldr_zh": "本文介绍了一项新任务 (CoD-V) 和数据集 (ComVID)，用于从视频生成用户投诉描述，以及一个基于多模态 RAG 的 VideoLLaMA2-7b 模型和一个新的投诉保留指标，用于评估。作者旨在为用户提供一个通过视频表达投诉的平台。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection",
        "summary": "Traditional object detection methods face performance degradation challenges\nin complex scenarios such as low-light conditions and heavy occlusions due to a\nlack of high-level semantic understanding. To address this, this paper proposes\nan adaptive guidance-based semantic enhancement edge-cloud collaborative object\ndetection method leveraging Multimodal Large Language Models (MLLM), achieving\nan effective balance between accuracy and efficiency. Specifically, the method\nfirst employs instruction fine-tuning to enable the MLLM to generate structured\nscene descriptions. It then designs an adaptive mapping mechanism that\ndynamically converts semantic information into parameter adjustment signals for\nedge detectors, achieving real-time semantic enhancement. Within an edge-cloud\ncollaborative inference framework, the system automatically selects between\ninvoking cloud-based semantic guidance or directly outputting edge detection\nresults based on confidence scores. Experiments demonstrate that the proposed\nmethod effectively enhances detection accuracy and efficiency in complex\nscenes. Specifically, it can reduce latency by over 79% and computational cost\nby 70% in low-light and highly occluded scenes while maintaining accuracy.",
        "url": "http://arxiv.org/abs/2509.19875v1",
        "published_date": "2025-09-24T08:25:37+00:00",
        "updated_date": "2025-09-24T08:25:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yunqing Hu",
            "Zheming Yang",
            "Chang Zhao",
            "Wen Ji"
        ],
        "tldr": "This paper introduces an edge-cloud collaborative object detection method using MLLMs for semantic enhancement in complex scenarios, adaptively guiding edge detectors and balancing accuracy and efficiency.",
        "tldr_zh": "本文提出了一种利用多模态大语言模型（MLLM）的边缘-云协同对象检测方法，用于在复杂场景中进行语义增强，自适应地引导边缘检测器，并在准确性和效率之间实现平衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
        "summary": "Accurate text recognition for historical documents can greatly advance the\nstudy and preservation of cultural heritage. Existing vision-language models\n(VLMs), however, are designed for modern, standardized texts and are not\nequipped to read the diverse languages and scripts, irregular layouts, and\nfrequent degradation found in historical materials.\n  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for\nhistorical text recognition. The model is trained on CHURRO-DS, the largest\nhistorical text recognition dataset to date. CHURRO-DS unifies 155 historical\ncorpora comprising 99,491 pages, spanning 22 centuries of textual heritage\nacross 46 language clusters, including historical variants and dead languages.\n  We evaluate several open-weight and closed VLMs and optical character\nrecognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all\nother VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and\n70.1% (handwritten) normalized Levenshtein similarity, surpassing the\nsecond-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being\n15.5 times more cost-effective.\n  By releasing the model and dataset, we aim to enable community-driven\nresearch to improve the readability of historical texts and accelerate\nscholarship.",
        "url": "http://arxiv.org/abs/2509.19768v1",
        "published_date": "2025-09-24T05:38:45+00:00",
        "updated_date": "2025-09-24T05:38:45+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Sina J. Semnani",
            "Han Zhang",
            "Xinyan He",
            "Merve Tekgürler",
            "Monica S. Lam"
        ],
        "tldr": "The paper introduces CHURRO, a 3B-parameter open-weight VLM for historical text recognition, trained on a large historical dataset, CHURRO-DS. CHURRO outperforms other VLMs and OCR systems while being more cost-effective.",
        "tldr_zh": "本文介绍CHURRO，一个用于历史文本识别的30亿参数的开源视觉语言模型，该模型在大型历史数据集CHURRO-DS上训练。CHURRO优于其他视觉语言模型和OCR系统，且更具成本效益。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Logics-Parsing Technical Report",
        "summary": "Recent advances in Large Vision-Language models (LVLM) have spurred\nsignificant progress in document parsing task. Compared to traditional\npipeline-based methods, end-to-end paradigms have shown their excellence in\nconverting PDF images into structured outputs through integrated Optical\nCharacter Recognition (OCR), table recognition, mathematical formula\nrecognition and so on. However, the absence of explicit analytical stages for\ndocument layouts and reading orders limits the LVLM's capability in handling\ncomplex document types such as multi-column newspapers or posters. To address\nthis limitation, we propose in this report Logics-Parsing: an end-to-end\nLVLM-based model augmented with reinforcement learning. Our model incorporates\nmeticulously designed reward mechanisms to optimize complex layout analysis and\nreading order inference. In addition, we expand the model's versatility by\nincorporating diverse data types such as chemical formulas and handwritten\nChinese characters into supervised fine-tuning. Finally, to enable rigorous\nevaluation of our approach, we introduce LogicsParsingBench, a curated set of\n1,078 page-level PDF images spanning nine major categories and over twenty\nsub-categories, which will be released later. Comprehensive experiments\nconducted on LogicsParsingBench have validated the efficacy and\nState-of-the-art (SOTA) performance of our proposed model across diverse\ndocument analysis scenarios. Project Page:\nhttps://github.com/alibaba/Logics-Parsing",
        "url": "http://arxiv.org/abs/2509.19760v1",
        "published_date": "2025-09-24T04:54:37+00:00",
        "updated_date": "2025-09-24T04:54:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyang Chen",
            "Shuzhao Li",
            "Xiuwen Zhu",
            "Yongfan Chen",
            "Fan Yang",
            "Cheng Fang",
            "Lin Qu",
            "Xiaoxiao Xu",
            "Hu Wei",
            "Minggang Wu"
        ],
        "tldr": "This paper introduces Logics-Parsing, an end-to-end LVLM model enhanced with reinforcement learning for improved document parsing, especially for complex layouts. They also present a new benchmark dataset, LogicsParsingBench, and demonstrate SOTA performance.",
        "tldr_zh": "本文介绍了Logics-Parsing，一个基于强化学习增强的端到端LVLM模型，用于改进文档解析，尤其是在复杂布局方面。他们还提出了一个新的基准数据集LogicsParsingBench，并展示了SOTA性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CAMILA: Context-Aware Masking for Image Editing with Language Alignment",
        "summary": "Text-guided image editing has been allowing users to transform and synthesize\nimages through natural language instructions, offering considerable\nflexibility. However, most existing image editing models naively attempt to\nfollow all user instructions, even if those instructions are inherently\ninfeasible or contradictory, often resulting in nonsensical output. To address\nthese challenges, we propose a context-aware method for image editing named as\nCAMILA (Context-Aware Masking for Image Editing with Language Alignment).\nCAMILA is designed to validate the contextual coherence between instructions\nand the image, ensuring that only relevant edits are applied to the designated\nregions while ignoring non-executable instructions. For comprehensive\nevaluation of this new method, we constructed datasets for both single- and\nmulti-instruction image editing, incorporating the presence of infeasible\nrequests. Our method achieves better performance and higher semantic alignment\nthan state-of-the-art models, demonstrating its effectiveness in handling\ncomplex instruction challenges while preserving image integrity.",
        "url": "http://arxiv.org/abs/2509.19731v1",
        "published_date": "2025-09-24T03:20:44+00:00",
        "updated_date": "2025-09-24T03:20:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunseung Kim",
            "Chiho Choi",
            "Srikanth Malla",
            "Sai Prahladh Padmanabhan",
            "Saurabh Bagchi",
            "Joon Hee Choi"
        ],
        "tldr": "CAMILA is a context-aware image editing method that validates instruction coherence and applies edits selectively, outperforming state-of-the-art models by handling infeasible instructions effectively.",
        "tldr_zh": "CAMILA 是一种上下文感知的图像编辑方法，它验证指令的一致性并有选择地应用编辑，通过有效地处理不可行的指令，优于最先进的模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment",
        "summary": "Large vision-language models (VLMs) can jointly interpret images and text,\nbut they are also prone to absorbing and reproducing harmful social stereotypes\nwhen visual cues such as age, gender, race, clothing, or occupation are\npresent. To investigate these risks, we introduce a news-image benchmark\nconsisting of 1,343 image-question pairs drawn from diverse outlets, which we\nannotated with ground-truth answers and demographic attributes (age, gender,\nrace, occupation, and sports). We evaluate a range of state-of-the-art VLMs and\nemploy a large language model (LLM) as judge, with human verification. Our\nfindings show that: (i) visual context systematically shifts model outputs in\nopen-ended settings; (ii) bias prevalence varies across attributes and models,\nwith particularly high risk for gender and occupation; and (iii) higher\nfaithfulness does not necessarily correspond to lower bias. We release the\nbenchmark prompts, evaluation rubric, and code to support reproducible and\nfairness-aware multimodal assessment.",
        "url": "http://arxiv.org/abs/2509.19659v1",
        "published_date": "2025-09-24T00:33:58+00:00",
        "updated_date": "2025-09-24T00:33:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aravind Narayanan",
            "Vahid Reza Khazaie",
            "Shaina Raza"
        ],
        "tldr": "This paper introduces a new benchmark to evaluate bias in VLMs, finding that visual context significantly influences model outputs and that higher faithfulness doesn't guarantee lower bias.",
        "tldr_zh": "该论文介绍了一个新的基准来评估视觉语言模型中的偏差，发现视觉上下文显著影响模型输出，并且更高的忠实度并不保证更低的偏差。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models",
        "summary": "The embodiment of emotional reactions from body parts contains rich\ninformation about our affective experiences. We propose a framework that\nutilizes state-of-the-art large vision-language models (LVLMs) to generate\nEmbodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered\ntext outputs, primarily comprising descriptions that focus on the salient body\nparts involved in emotional reactions. We also employ attention maps and\nobserve that contemporary models exhibit a persistent bias towards the facial\nregion. Despite this limitation, we observe that our employed framework can\neffectively recognize embodied emotions in face-masked images, outperforming\nbaselines without any fine-tuning. ELENA opens a new trajectory for embodied\nemotion analysis across the modality of vision and enriches modeling in an\naffect-aware setting.",
        "url": "http://arxiv.org/abs/2509.19595v1",
        "published_date": "2025-09-23T21:34:57+00:00",
        "updated_date": "2025-09-23T21:34:57+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Mohammad Saim",
            "Phan Anh Duong",
            "Cat Luong",
            "Aniket Bhanderi",
            "Tianyu Jiang"
        ],
        "tldr": "The paper introduces ELENA, a framework leveraging LVLMs to generate multi-layered narratives describing embodied emotions, focusing on body parts involved in emotional reactions, and shows its effectiveness even with face-masked images.",
        "tldr_zh": "该论文介绍了一种名为ELENA的框架，该框架利用大型视觉语言模型（LVLM）生成多层叙述，描述具身情感，重点关注参与情感反应的身体部位，并展示了即使在面部遮挡图像中的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "A co-evolving agentic AI system for medical imaging analysis",
        "summary": "Agentic AI is rapidly advancing in healthcare and biomedical research.\nHowever, in medical image analysis, their performance and adoption remain\nlimited due to the lack of a robust ecosystem, insufficient toolsets, and the\nabsence of real-time interactive expert feedback. Here we present \"TissueLab\",\na co-evolving agentic AI system that allows researchers to ask direct\nquestions, automatically plan and generate explainable workflows, and conduct\nreal-time analyses where experts can visualize intermediate results and refine\nthem. TissueLab integrates tool factories across pathology, radiology, and\nspatial omics domains. By standardizing inputs, outputs, and capabilities of\ndiverse tools, the system determines when and how to invoke them to address\nresearch and clinical questions. Across diverse tasks with clinically\nmeaningful quantifications that inform staging, prognosis, and treatment\nplanning, TissueLab achieves state-of-the-art performance compared with\nend-to-end vision-language models (VLMs) and other agentic AI systems such as\nGPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward\nimproved classifiers and more effective decision strategies. With active\nlearning, it delivers accurate results in unseen disease contexts within\nminutes, without requiring massive datasets or prolonged retraining. Released\nas a sustainable open-source ecosystem, TissueLab aims to accelerate\ncomputational research and translational adoption in medical imaging while\nestablishing a foundation for the next generation of medical AI.",
        "url": "http://arxiv.org/abs/2509.20279v1",
        "published_date": "2025-09-24T16:15:28+00:00",
        "updated_date": "2025-09-24T16:15:28+00:00",
        "categories": [
            "cs.CV",
            "q-bio.QM"
        ],
        "authors": [
            "Songhao Li",
            "Jonathan Xu",
            "Tiancheng Bao",
            "Yuxuan Liu",
            "Yuchen Liu",
            "Yihang Liu",
            "Lilin Wang",
            "Wenhui Lei",
            "Sheng Wang",
            "Yinuo Xu",
            "Yan Cui",
            "Jialu Yao",
            "Shunsuke Koga",
            "Zhi Huang"
        ],
        "tldr": "The paper introduces TissueLab, a co-evolving agentic AI system for medical image analysis that integrates diverse tools and learns from clinicians to achieve state-of-the-art performance with explainable workflows and real-time feedback.",
        "tldr_zh": "该论文介绍了一种名为TissueLab的共进化代理AI系统，用于医学图像分析。该系统集成了多种工具，并从临床医生那里学习，从而以可解释的工作流程和实时反馈实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models",
        "summary": "Multimodal pre-trained models (e.g., ImageBind), which align distinct data\nmodalities into a shared embedding space, have shown remarkable success across\ndownstream tasks. However, their increasing adoption raises serious security\nconcerns, especially regarding targeted adversarial attacks. In this paper, we\nshow that existing targeted adversarial attacks on multimodal pre-trained\nmodels still have limitations in two aspects: generalizability and\nundetectability. Specifically, the crafted targeted adversarial examples (AEs)\nexhibit limited generalization to partially known or semantically similar\ntargets in cross-modal alignment tasks (i.e., limited generalizability) and can\nbe easily detected by simple anomaly detection methods (i.e., limited\nundetectability). To address these limitations, we propose a novel method\ncalled Proxy Targeted Attack (PTA), which leverages multiple source-modal and\ntarget-modal proxies to optimize targeted AEs, ensuring they remain evasive to\ndefenses while aligning with multiple potential targets. We also provide\ntheoretical analyses to highlight the relationship between generalizability and\nundetectability and to ensure optimal generalizability while meeting the\nspecified requirements for undetectability. Furthermore, experimental results\ndemonstrate that our PTA can achieve a high success rate across various related\ntargets and remain undetectable against multiple anomaly detection methods.",
        "url": "http://arxiv.org/abs/2509.19994v1",
        "published_date": "2025-09-24T11:00:43+00:00",
        "updated_date": "2025-09-24T11:00:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhifang Zhang",
            "Jiahan Zhang",
            "Shengjie Zhou",
            "Qi Wei",
            "Shuo He",
            "Feng Liu",
            "Lei Feng"
        ],
        "tldr": "The paper introduces a novel adversarial attack (PTA) on multimodal pre-trained models that improves both generalizability to similar targets and undetectability by anomaly detection methods, addressing limitations of existing attacks.",
        "tldr_zh": "该论文提出了一种新的针对多模态预训练模型的对抗攻击方法（PTA），提高了攻击对类似目标的泛化能力和对异常检测方法的不可检测性，解决了现有攻击的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Interpreting ResNet-based CLIP via Neuron-Attention Decomposition",
        "summary": "We present a novel technique for interpreting the neurons in CLIP-ResNet by\ndecomposing their contributions to the output into individual computation\npaths. More specifically, we analyze all pairwise combinations of neurons and\nthe following attention heads of CLIP's attention-pooling layer. We find that\nthese neuron-head pairs can be approximated by a single direction in\nCLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret\neach neuron-head pair by associating it with text. Additionally, we find that\nonly a sparse set of the neuron-head pairs have a significant contribution to\nthe output value, and that some neuron-head pairs, while polysemantic,\nrepresent sub-concepts of their corresponding neurons. We use these\nobservations for two applications. First, we employ the pairs for training-free\nsemantic segmentation, outperforming previous methods for CLIP-ResNet. Second,\nwe utilize the contributions of neuron-head pairs to monitor dataset\ndistribution shifts. Our results demonstrate that examining individual\ncomputation paths in neural networks uncovers interpretable units, and that\nsuch units can be utilized for downstream tasks.",
        "url": "http://arxiv.org/abs/2509.19943v1",
        "published_date": "2025-09-24T09:50:01+00:00",
        "updated_date": "2025-09-24T09:50:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Edmund Bu",
            "Yossi Gandelsman"
        ],
        "tldr": "This paper introduces a neuron-attention decomposition technique to interpret CLIP-ResNet's neurons by analyzing neuron-head pairs, demonstrating its utility in semantic segmentation and dataset shift monitoring.",
        "tldr_zh": "本文提出了一种神经元-注意力分解技术，通过分析神经元-头对来解释CLIP-ResNet中的神经元，并展示了其在语义分割和数据集漂移监控中的应用。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection",
        "summary": "The increasing realism of AI-generated images has raised serious concerns\nabout misinformation and privacy violations, highlighting the urgent need for\naccurate and interpretable detection methods. While existing approaches have\nmade progress, most rely on binary classification without explanations or\ndepend heavily on supervised fine-tuning, resulting in limited generalization.\nIn this paper, we propose ThinkFake, a novel reasoning-based and generalizable\nframework for AI-generated image detection. Our method leverages a Multimodal\nLarge Language Model (MLLM) equipped with a forgery reasoning prompt and is\ntrained using Group Relative Policy Optimization (GRPO) reinforcement learning\nwith carefully designed reward functions. This design enables the model to\nperform step-by-step reasoning and produce interpretable, structured outputs.\nWe further introduce a structured detection pipeline to enhance reasoning\nquality and adaptability. Extensive experiments show that ThinkFake outperforms\nstate-of-the-art methods on the GenImage benchmark and demonstrates strong\nzero-shot generalization on the challenging LOKI benchmark. These results\nvalidate our framework's effectiveness and robustness. Code will be released\nupon acceptance.",
        "url": "http://arxiv.org/abs/2509.19841v1",
        "published_date": "2025-09-24T07:34:09+00:00",
        "updated_date": "2025-09-24T07:34:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tai-Ming Huang",
            "Wei-Tung Lin",
            "Kai-Lung Hua",
            "Wen-Huang Cheng",
            "Junichi Yamagishi",
            "Jun-Cheng Chen"
        ],
        "tldr": "The paper introduces ThinkFake, a novel reasoning-based MLLM framework for detecting AI-generated images, utilizing reinforcement learning and a structured detection pipeline for improved interpretability and generalization, outperforming SOTA methods on GenImage and demonstrating strong zero-shot performance on LOKI.",
        "tldr_zh": "本文介绍了一个名为ThinkFake的新型基于推理的MLLM框架，用于检测AI生成的图像，它利用强化学习和结构化的检测管道来提高可解释性和泛化能力，在GenImage上优于SOTA方法，并在LOKI上表现出强大的零样本性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning",
        "summary": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding.",
        "url": "http://arxiv.org/abs/2509.19552v1",
        "published_date": "2025-09-23T20:25:53+00:00",
        "updated_date": "2025-09-23T20:25:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manyi Yao",
            "Bingbing Zhuang",
            "Sparsh Garg",
            "Amit Roy-Chowdhury",
            "Christian Shelton",
            "Manmohan Chandraker",
            "Abhishek Aich"
        ],
        "tldr": "The paper introduces iFinder, a training-free framework that grounds LLMs in dash-cam video analysis by translating videos into structured data, significantly improving zero-shot performance compared to end-to-end V-VLMs.",
        "tldr_zh": "该论文介绍了一个名为iFinder的免训练框架，通过将行车记录仪视频转换为结构化数据，使LLM能够更好地进行行车记录仪视频分析，与端到端V-VLM相比，显著提高了零样本性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]