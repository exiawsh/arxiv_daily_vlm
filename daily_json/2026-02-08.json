[
    {
        "title": "MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images",
        "summary": "Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page",
        "url": "http://arxiv.org/abs/2602.06965v1",
        "published_date": "2026-02-06T18:59:59+00:00",
        "updated_date": "2026-02-06T18:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ankan Deria",
            "Komal Kumar",
            "Adinath Madhavrao Dukre",
            "Eran Segal",
            "Salman Khan",
            "Imran Razzak"
        ],
        "tldr": "The paper introduces MedMO, a medical multimodal large language model trained on domain-specific data, demonstrating significant improvements in various medical tasks including VQA, report generation, and disease localization, showcasing strong cross-modality generalization.",
        "tldr_zh": "该论文介绍了MedMO，一个在特定领域数据上训练的医学多模态大型语言模型，在包括VQA、报告生成和疾病定位等多种医学任务中表现出显著改进，展示了强大的跨模态泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs",
        "summary": "Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.",
        "url": "http://arxiv.org/abs/2602.06914v1",
        "published_date": "2026-02-06T18:13:01+00:00",
        "updated_date": "2026-02-06T18:13:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Darryl Hannan",
            "John Cooper",
            "Dylan White",
            "Yijing Watkins"
        ],
        "tldr": "This paper investigates the relationship between task complexity and visual compression in VLLMs, finding that training on high-complexity visual data is crucial for improving performance on complex visual tasks.",
        "tldr_zh": "本文研究了VLLM中任务复杂性与视觉压缩之间的关系，发现使用高复杂度视觉数据进行训练对于提高复杂视觉任务的性能至关重要。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]