[
    {
        "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding",
        "summary": "Enabling agents to understand and interact with complex 3D scenes is a\nfundamental challenge for embodied artificial intelligence systems. While\nMultimodal Large Language Models (MLLMs) have achieved significant progress in\n2D image understanding, extending such capabilities to 3D scenes remains\ndifficult: 1) 3D environment involves richer concepts such as spatial\nrelationships, affordances, physics, layout, and so on, 2) the absence of\nlarge-scale 3D vision-language datasets has posed a significant obstacle. In\nthis paper, we introduce Text-Scene, a framework that automatically parses 3D\nscenes into textual descriptions for scene understanding. Given a 3D scene, our\nmodel identifies object attributes and spatial relationships, and then\ngenerates a coherent summary of the whole scene, bridging the gap between 3D\nobservation and language without requiring human-in-the-loop intervention. By\nleveraging both geometric analysis and MLLMs, Text-Scene produces descriptions\nthat are accurate, detailed, and human-interpretable, capturing object-level\ndetails and global-level context. Experimental results on benchmarks\ndemonstrate that our textual parses can faithfully represent 3D scenes and\nbenefit downstream tasks. To evaluate the reasoning capability of MLLMs, we\npresent InPlan3D, a comprehensive benchmark for 3D task planning, consisting of\n3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity\nand accessibility in our approach, aiming to make 3D scene content\nunderstandable through language. Code and datasets will be released.",
        "url": "http://arxiv.org/abs/2509.16721v1",
        "published_date": "2025-09-20T15:10:45+00:00",
        "updated_date": "2025-09-20T15:10:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Haoyuan Li",
            "Rui Liu",
            "Hehe Fan",
            "Yi Yang"
        ],
        "tldr": "The paper introduces Text-Scene, a framework that automatically parses 3D scenes into textual descriptions to improve 3D scene understanding for embodied AI, addressing the lack of 3D vision-language datasets and the complexity of 3D environments.",
        "tldr_zh": "该论文介绍了Text-Scene，一个自动将3D场景解析成文本描述的框架，旨在提升具身人工智能对3D场景的理解，解决了3D视觉-语言数据集的缺乏以及3D环境的复杂性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering",
        "summary": "Visual Question Answering (VQA) is increasingly used in diverse applications\nranging from general visual reasoning to safety-critical domains such as\nmedical imaging and autonomous systems, where models must provide not only\naccurate answers but also explanations that humans can easily understand and\nverify. Prototype-based modeling has shown promise for interpretability by\ngrounding predictions in semantically meaningful regions for purely visual\nreasoning tasks, yet remains underexplored in the context of VQA. We present\nProtoVQA, a unified prototypical framework that (i) learns question-aware\nprototypes that serve as reasoning anchors, connecting answers to\ndiscriminative image regions, (ii) applies spatially constrained matching to\nensure that the selected evidence is coherent and semantically relevant, and\n(iii) supports both answering and grounding tasks through a shared prototype\nbackbone. To assess explanation quality, we propose the Visual-Linguistic\nAlignment Score (VLAS), which measures how well the model's attended regions\nalign with ground-truth evidence. Experiments on Visual7W show that ProtoVQA\nyields faithful, fine-grained explanations while maintaining competitive\naccuracy, advancing the development of transparent and trustworthy VQA systems.",
        "url": "http://arxiv.org/abs/2509.16680v1",
        "published_date": "2025-09-20T13:12:08+00:00",
        "updated_date": "2025-09-20T13:12:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xingjian Diao",
            "Weiyi Wu",
            "Keyi Kong",
            "Peijun Qing",
            "Xinwen Xu",
            "Ming Cheng",
            "Soroush Vosoughi",
            "Jiang Gui"
        ],
        "tldr": "The paper introduces ProtoVQA, a novel prototypical framework for explainable Visual Question Answering, using question-aware prototypes and spatially constrained matching to generate fine-grained explanations.",
        "tldr_zh": "该论文介绍了 ProtoVQA，一种用于可解释视觉问答的新型原型框架，它使用问题感知的原型和空间约束匹配来生成细粒度的解释。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness",
        "summary": "Vision-Language Pre-training (VLP) is drawing increasing interest for its\nability to minimize manual annotation requirements while enhancing semantic\nunderstanding in downstream tasks. However, its reliance on image-text datasets\nposes challenges due to privacy concerns and the high cost of obtaining paired\nannotations. Data augmentation emerges as a viable strategy to address this\nissue, yet existing methods often fall short of capturing the subtle and\ncomplex variations in medical data due to limited diversity. To this end, we\npropose MedCutMix, a novel multi-modal disease-centric data augmentation\nmethod. MedCutMix performs diagnostic sentence CutMix within medical reports\nand establishes the cross-attention between the diagnostic sentence and medical\nimage to guide attentive manifold mix within the imaging modality. Our approach\nsurpasses previous methods across four downstream radiology diagnosis datasets,\nhighlighting its effectiveness in enhancing performance and generalizability in\nradiology VLP.",
        "url": "http://arxiv.org/abs/2509.16673v1",
        "published_date": "2025-09-20T12:51:14+00:00",
        "updated_date": "2025-09-20T12:51:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sinuo Wang",
            "Yutong Xie",
            "Yuyuan Liu",
            "Qi Wu"
        ],
        "tldr": "The paper introduces MedCutMix, a novel data augmentation technique for vision-language pre-training in radiology, which improves performance and generalizability by performing diagnostic sentence CutMix and attentive manifold mix based on cross-attention.",
        "tldr_zh": "该论文介绍了MedCutMix，一种新颖的放射学视觉语言预训练数据增强技术，通过执行诊断语句CutMix和基于交叉注意力的注意力流形混合，提高了性能和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents",
        "summary": "Vision-Language Models (VLMs), with their strong reasoning and planning\ncapabilities, are widely used in embodied decision-making (EDM) tasks in\nembodied agents, such as autonomous driving and robotic manipulation. Recent\nresearch has increasingly explored adversarial attacks on VLMs to reveal their\nvulnerabilities. However, these attacks either rely on overly strong\nassumptions, requiring full knowledge of the victim VLM, which is impractical\nfor attacking VLM-based agents, or exhibit limited effectiveness. The latter\nstems from disrupting most semantic information in the image, which leads to a\nmisalignment between the perception and the task context defined by system\nprompts. This inconsistency interrupts the VLM's reasoning process, resulting\nin invalid outputs that fail to affect interactions in the physical world. To\nthis end, we propose a fine-grained adversarial attack framework, ADVEDM, which\nmodifies the VLM's perception of only a few key objects while preserving the\nsemantics of the remaining regions. This attack effectively reduces conflicts\nwith the task context, making VLMs output valid but incorrect decisions and\naffecting the actions of agents, thus posing a more substantial safety threat\nin the physical world. We design two variants of based on this framework,\nADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific\nobject from the image and add the semantics of a new object into the image. The\nexperimental results in both general scenarios and EDM tasks demonstrate\nfine-grained control and excellent attack performance.",
        "url": "http://arxiv.org/abs/2509.16645v1",
        "published_date": "2025-09-20T11:48:11+00:00",
        "updated_date": "2025-09-20T11:48:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Wang",
            "Hangtao Zhang",
            "Hewen Pan",
            "Ziqi Zhou",
            "Xianlong Wang",
            "Peijin Guo",
            "Lulu Xue",
            "Shengshan Hu",
            "Minghui Li",
            "Leo Yu Zhang"
        ],
        "tldr": "This paper introduces ADVEDM, a fine-grained adversarial attack framework against VLM-based embodied agents. It modifies the perception of a few key objects to mislead the VLM into making incorrect decisions, posing a substantial safety threat.",
        "tldr_zh": "该论文介绍了ADVEDM，一种针对基于VLM的具身代理的细粒度对抗攻击框架。它通过修改少数关键物体的感知来误导VLM做出错误的决策，从而构成重大的安全威胁。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs",
        "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.",
        "url": "http://arxiv.org/abs/2509.16633v1",
        "published_date": "2025-09-20T11:12:23+00:00",
        "updated_date": "2025-09-20T11:12:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Abhirama Subramanyam Penamakuri",
            "Navlika Singh",
            "Piyush Arora",
            "Anand Mishra"
        ],
        "tldr": "This paper introduces Model Parity Aligner (MPA), a novel label-free knowledge transfer framework that improves the performance of small vision-language models (S-VLMs) by aligning them with large vision-language models (L-VLMs) on unlabeled images, addressing the performance gap between them.",
        "tldr_zh": "本文介绍了模型奇偶对齐器（MPA），这是一种新颖的无标签知识迁移框架，通过在无标签图像上将小型视觉语言模型（S-VLMs）与大型视觉语言模型（L-VLMs）对齐，从而提高小型视觉语言模型的性能，解决它们之间的性能差距。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning",
        "summary": "In this study, we introduce Vision-Caption aware Supervised FineTuning\n(VCASFT), a novel learning paradigm designed to enhance the performance of\nsmaller Vision Language Models(VLMs) on scientific visual question\nanswering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts\nalongside question-answer pairs and instruction-tunes models to yield\nsignificant performance improvements. To comprehensively evaluate VCASFT, we\nbenchmark it on ScienceQA, which consists of questions across diverse\nlanguages, subjects, and fields, demonstrating its adaptability and\neffectiveness in a variety of educational contexts. Additionally, to further\ndemonstrate the effectiveness of this technique on lowresource languages, we\ndeveloped HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated\nHindi multimodal Q&A pairs. This dataset addresses the critical need for\nlow-resource language Q&A datasets and serves as a foundation for testing\nVCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to\nevaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness\nsurpassing traditional n-gram matching accuracy metrics. We are committed to\nadvancing the field by open-sourcing all code files and the HiSciVQA dataset\nfor the research community.",
        "url": "http://arxiv.org/abs/2509.16628v1",
        "published_date": "2025-09-20T11:07:36+00:00",
        "updated_date": "2025-09-20T11:07:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Janak Kapuriya",
            "Anwar Shaikh",
            "Arnav Goel",
            "Medha Hira",
            "Apoorv Singh",
            "Jay Saraf",
            "Sanjana",
            "Vaibhav Nauriyal",
            "Avinash Anand",
            "Zhengkui Wang",
            "Rajiv Ratn Shah"
        ],
        "tldr": "The paper introduces VCASFT, a vision-caption aware supervised fine-tuning method to enhance VLMs for scientific VQA, and a new Hindi multimodal Q&A dataset (HiSciVQA) with a novel LLM-based evaluation scheme.",
        "tldr_zh": "该论文介绍了一种视觉字幕感知的监督微调方法 VCASFT，用于增强 VLM 在科学 VQA 方面的能力，并提出了一个新的印地语多模态问答数据集 (HiSciVQA)，以及一种新颖的基于 LLM 的评估方案。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits",
        "summary": "Recent black-box counterfactual generation frameworks fail to take into\naccount the semantic content of the proposed edits, while relying heavily on\ntraining to guide the generation process. We propose a novel, plug-and-play\nblack-box counterfactual generation framework, which suggests step-by-step\nedits based on theoretical guarantees of optimal edits to produce human-level\ncounterfactual explanations with zero training. Our framework utilizes a\npre-trained image editing diffusion model, and operates without access to the\ninternals of the classifier, leading to an explainable counterfactual\ngeneration process. Throughout our experimentation, we showcase the explanatory\ngap between human reasoning and neural model behavior by utilizing both\nConvolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision\nLanguage Model (LVLM) classifiers, substantiated through a comprehensive human\nevaluation.",
        "url": "http://arxiv.org/abs/2509.16567v1",
        "published_date": "2025-09-20T07:53:06+00:00",
        "updated_date": "2025-09-20T07:53:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nikolaos Spanos",
            "Maria Lymperaiou",
            "Giorgos Filandrianos",
            "Konstantinos Thomas",
            "Athanasios Voulodimos",
            "Giorgos Stamou"
        ],
        "tldr": "The paper introduces V-CECE, a training-free, plug-and-play framework for generating visual counterfactual explanations by suggesting semantic image edits using a pre-trained diffusion model, demonstrating differences between human and model reasoning.",
        "tldr_zh": "该论文介绍了V-CECE，一个无需训练的即插即用框架，通过使用预训练的扩散模型提出语义图像编辑来生成视觉反事实解释，展示了人类和模型推理之间的差异。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization",
        "summary": "In text-video retrieval, auxiliary captions are often used to enhance video\nunderstanding, bridging the gap between the modalities. While recent advances\nin multi-modal large language models (MLLMs) have enabled strong zero-shot\ncaption generation, we observe that such captions tend to be generic and\nindistinguishable across visually similar videos, limiting their utility for\nfine-grained retrieval. Moreover, conventional captioning approaches are\ntypically evaluated using language generation metrics, such as BLEU, which are\nnot typically tailored for retrieval tasks that require making discriminative\ndistinctions between candidates. To address this, we propose\n$\\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption\ngeneration using retrieval relevance scores. At its core is Dual-Group Direct\nPreference Optimization (DG-DPO), a novel learning strategy that supervises\ncaptioning by modeling preferences across groups of distinct video and caption\npairs. In addition, we present an MLLM-based retrieval model that incorporates\nrole-embeddings to better distinguish between textual inputs with different\nfunctional roles, such as an auxiliary caption and a text query. Through\nextensive experiments, we demonstrate that CaRe-DPO significantly enhances\nretrieval performance by effectively leveraging auxiliary knowledge to generate\nfine-grained captions for retrieval. Code is available at\nhttps://github.com/mlvlab/CaReDPO.",
        "url": "http://arxiv.org/abs/2509.16560v1",
        "published_date": "2025-09-20T07:36:53+00:00",
        "updated_date": "2025-09-20T07:36:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ji Soo Lee",
            "Byungoh Ko",
            "Jaewon Cho",
            "Howoong Lee",
            "Jaewoon Byun",
            "Hyunwoo J. Kim"
        ],
        "tldr": "The paper introduces CaRe-DPO, a novel retrieval framework using Dual-Group Direct Preference Optimization (DG-DPO) to generate fine-grained, retrieval-optimized captions for text-video retrieval, addressing the generic caption problem of MLLMs.",
        "tldr_zh": "该论文介绍了CaRe-DPO，一种新颖的检索框架，它使用双组直接偏好优化 (DG-DPO) 来生成用于文本-视频检索的细粒度、检索优化的字幕，解决了 MLLM 的通用字幕问题。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs",
        "summary": "Vision-Language Models (VLMs) deliver impressive performance in understanding\nvisual content with language instructions. However, redundancy in vision tokens\nresults in the degenerated inference efficiency of VLMs, which hinders\nreal-time use on edge consumer devices such as AR/VR devices. Existing\nefficiency methods commonly prune visual tokens using learned saliency, sparse\nattention schedules, or controller policies, but they often require\narchitectural modification or access to intermediate activations. These\npipelines add inference-time modules that increase compute and memory and often\nlead to an accuracy trade-off. Moreover, they also suffer from misalignment\nbetween the prompts and the region of interest in the images. Without human\nguidance, the model may focus on the wrong regions and miss small,\nhigh-frequency details when prompts or scenes change. In this paper, we propose\nGazeVLM, a training-free framework that uses the human eye gaze as a natural\nsupervisory signal to allocate computation where it matters. By extracting\ngaze-driven regions of interest (ROIs) and optionally combining them with a\nlow-resolution global view, GazeVLM mimics fovea-periphery perception to cut\nredundant visual tokens while preserving task-relevant details. We evaluate the\nvisual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark\nwith human gaze. Quality of the answer is assessed by GPT-4o pairwise judging\nand a weighted score over coverage, accuracy, details, and fluency. Efficiency\nis measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to\n93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better\nanswer quality relative to full-resolution baselines. Our results show that\naligning model computation with human gaze offers a simple, plug-and-play path\ntoward efficient VLM inference on consumer devices.",
        "url": "http://arxiv.org/abs/2509.16476v1",
        "published_date": "2025-09-20T00:16:48+00:00",
        "updated_date": "2025-09-20T00:16:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinyu Chen",
            "Jiawen Qi"
        ],
        "tldr": "The paper introduces GazeVLM, a training-free method that leverages human eye gaze to reduce computational costs in Vision-Language Models by focusing on regions of interest, achieving significant reductions in tokens and FLOPs while maintaining or improving answer quality.",
        "tldr_zh": "该论文介绍了GazeVLM，一种无需训练的方法，利用人类眼动追踪来减少视觉语言模型的计算成本，通过关注感兴趣区域，在保持或提高答案质量的同时，显著减少token和FLOPs。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models",
        "summary": "Accurate vision-based action recognition is crucial for developing autonomous\nrobots that can operate safely and reliably in complex, real-world\nenvironments. In this work, we advance video-based recognition of indoor daily\nactions for robotic perception by leveraging vision-language models (VLMs)\nenriched with domain-specific knowledge. We adapt a prompt-learning framework\nin which class-level textual descriptions of each action are embedded as\nlearnable prompts into a frozen pre-trained VLM backbone. Several strategies\nfor structuring and encoding these textual descriptions are designed and\nevaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our\nmethod, using only RGB video inputs at test time, achieves over 95\\% accuracy\nand outperforms state-of-the-art approaches. These results highlight the\neffectiveness of knowledge-augmented prompts in enabling robust action\nrecognition with minimal supervision.",
        "url": "http://arxiv.org/abs/2509.16452v1",
        "published_date": "2025-09-19T22:12:49+00:00",
        "updated_date": "2025-09-19T22:12:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Son Hai Nguyen",
            "Diwei Wang",
            "Jinhyeok Jang",
            "Hyewon Seo"
        ],
        "tldr": "The paper presents KRAST, a method for robotic action recognition that leverages vision-language models with knowledge-augmented prompts, achieving state-of-the-art accuracy on the ETRI-Activity3D dataset.",
        "tldr_zh": "该论文提出了KRAST，一种利用知识增强提示的视觉语言模型进行机器人动作识别的方法，并在ETRI-Activity3D数据集上实现了最先进的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead",
        "summary": "Real-time understanding of continuous video streams is essential for\nintelligent agents operating in high-stakes environments, including autonomous\nvehicles, surveillance drones, and disaster response robots. Yet, most existing\nvideo understanding and highlight detection methods assume access to the entire\nvideo during inference, making them unsuitable for online or streaming\nscenarios. In particular, current models optimize for offline summarization,\nfailing to support step-by-step reasoning needed for real-time decision-making.\nWe introduce Aha, an autoregressive highlight detection framework that predicts\nthe relevance of each video frame against a task described in natural language.\nWithout accessing future video frames, Aha utilizes a multimodal\nvision-language model and lightweight, decoupled heads trained on a large,\ncurated dataset of human-centric video labels. To enable scalability, we\nintroduce the Dynamic SinkCache mechanism that achieves constant memory usage\nacross infinite-length streams without degrading performance on standard\nbenchmarks. This encourages the hidden representation to capture high-level\ntask objectives, enabling effective frame-level rankings for informativeness,\nrelevance, and uncertainty with respect to the natural language task. Aha\nachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,\nsurpassing even prior offline, full-context approaches and video-language\nmodels by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).\nWe explore Aha's potential for real-world robotics applications given a\ntask-oriented natural language input and a continuous, robot-centric video.\nBoth experiments demonstrate Aha's potential effectiveness as a real-time\nreasoning module for downstream planning and long-horizon understanding.",
        "url": "http://arxiv.org/abs/2509.16421v1",
        "published_date": "2025-09-19T21:03:00+00:00",
        "updated_date": "2025-09-19T21:03:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aiden Chang",
            "Celso De Melo",
            "Stephanie M. Lukin"
        ],
        "tldr": "The paper introduces Aha, an autoregressive vision-language model for online highlight detection in streaming video, achieving state-of-the-art performance without future frame access and constant memory usage using Dynamic SinkCache.",
        "tldr_zh": "该论文介绍了Aha，一个用于在线视频流高光检测的自回归视觉-语言模型。该模型在无需访问未来帧的情况下，利用动态SinkCache实现了恒定的内存使用，并达到了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?",
        "summary": "Vision-Language Models (VLMs) have recently shown remarkable progress in\nmultimodal reasoning, yet their applications in autonomous driving remain\nlimited. In particular, the ability to understand road topology, a key\nrequirement for safe navigation, has received relatively little attention.\nWhile some recent works have begun to explore VLMs in driving contexts, their\nperformance on topology reasoning is far from satisfactory. In this work, we\nsystematically evaluate VLMs' capabilities in road topology understanding.\nSpecifically, multi-view images are projected into unified ground-plane\ncoordinate system and fused into bird's-eye-view (BEV) lanes. Based on these\nBEV lanes, we formulate four topology-related diagnostic VQA tasks, which\ntogether capture essential components of spatial topology reasoning. Through\nextensive evaluation, we find that while frontier closed-source models (e.g.,\nGPT-4o) achieve relatively high accuracy in some tasks, they still fail in some\ntemporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in\nvector, a two-class classification problem). Furthermore, we find open-source\nVLMs, even at 30B scale, struggle significantly. These results indicate that\nspatial reasoning remains a fundamental bottleneck for current VLMs. We also\nfind that the model's capability is positively correlated with model size,\nlength of reasoning tokens and shots provided as examples, showing direction\nfor future research.",
        "url": "http://arxiv.org/abs/2509.16654v1",
        "published_date": "2025-09-20T12:02:39+00:00",
        "updated_date": "2025-09-20T12:02:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Chen",
            "Jia He",
            "Maozheng Li",
            "Dongliang Xu",
            "Tianyu Wang",
            "Yixiao Chen",
            "Zhixin Lin",
            "Yue Yao"
        ],
        "tldr": "This paper evaluates the capability of VLMs, including GPT-4o and open-source models, in understanding road topology for autonomous driving using diagnostic VQA tasks. The results show that while closed-source VLMs perform relatively well, they still struggle with temporal reasoning, and open-source VLMs perform poorly, indicating a spatial reasoning bottleneck.",
        "tldr_zh": "该论文评估了VLMs（包括GPT-4o和开源模型）在理解自动驾驶中的道路拓扑结构的能力，使用了诊断性VQA任务。结果表明，虽然闭源VLMs表现相对较好，但仍然难以处理时间推理，而开源VLMs表现不佳，表明存在空间推理瓶颈。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery",
        "summary": "In recent years, Visual Question Localized-Answering in robotic surgery\n(Surgical-VQLA) has gained significant attention for its potential to assist\nmedical students and junior doctors in understanding surgical scenes. Recently,\nthe rapid development of Large Language Models (LLMs) has provided more\npromising solutions for this task. However, current methods struggle to\nestablish complex dependencies between text and visual details, and have\ndifficulty perceiving the spatial information of surgical scenes. To address\nthese challenges, we propose a novel method, Surgical-MambaLLM, which is the\nfirst to combine Mamba2 with LLM in the surgical domain, that leverages\nMamba2's ability to effectively capture cross-modal dependencies and perceive\nspatial information in surgical scenes, thereby enhancing the LLMs'\nunderstanding of surgical images. Specifically, we propose the Cross-modal\nBidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective\nmultimodal fusion, with its cross-modal integration capabilities. Additionally,\ntailored to the geometric characteristics of surgical scenes, we design the\nSurgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the\nsurgical images, enhancing the model's spatial understanding of the surgical\nscene. Extensive experiments demonstrate that our Surgical-MambaLLM model\noutperforms the state-of-the-art methods on the EndoVis17-VQLA and\nEndoVis18-VQLA datasets, significantly improving the performance of the\nSurgical-VQLA task.",
        "url": "http://arxiv.org/abs/2509.16618v1",
        "published_date": "2025-09-20T10:42:29+00:00",
        "updated_date": "2025-09-20T10:42:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pengfei Hao",
            "Hongqiu Wang",
            "Shuaibo Li",
            "Zhaohu Xing",
            "Guang Yang",
            "Kaishun Wu",
            "Lei Zhu"
        ],
        "tldr": "The paper introduces Surgical-MambaLLM, a novel multimodal large language model that incorporates Mamba2 to improve Visual Question Localized-Answering in robotic surgery by better capturing cross-modal dependencies and spatial information. It outperforms SOTA on surgical VQLA datasets.",
        "tldr_zh": "本文介绍了一种新型多模态大型语言模型 Surgical-MambaLLM，它结合了 Mamba2 以改进机器人手术中的视觉问题定位回答，通过更好地捕捉跨模态依赖关系和空间信息。 在手术 VQLA 数据集上优于 SOTA。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Describe-to-Score: Text-Guided Efficient Image Complexity Assessment",
        "summary": "Accurately assessing image complexity (IC) is critical for computer vision,\nyet most existing methods rely solely on visual features and often neglect\nhigh-level semantic information, limiting their accuracy and generalization. We\nintroduce vision-text fusion for IC modeling. This approach integrates visual\nand textual semantic features, increasing representational diversity. It also\nreduces the complexity of the hypothesis space, which enhances both accuracy\nand generalization in complexity assessment. We propose the D2S\n(Describe-to-Score) framework, which generates image captions with a\npre-trained vision-language model. We propose the feature alignment and entropy\ndistribution alignment mechanisms, D2S guides semantic information to inform\ncomplexity assessment while bridging the gap between vision and text\nmodalities. D2S utilizes multi-modal information during training but requires\nonly the vision branch during inference, thereby avoiding multi-modal\ncomputational overhead and enabling efficient assessment. Experimental results\ndemonstrate that D2S outperforms existing methods on the IC9600 dataset and\nmaintains competitiveness on no-reference image quality assessment (NR-IQA)\nbenchmark, validating the effectiveness and efficiency of multi-modal fusion in\ncomplexity-related tasks. Code is available at:\nhttps://github.com/xauat-liushipeng/D2S",
        "url": "http://arxiv.org/abs/2509.16609v1",
        "published_date": "2025-09-20T10:17:25+00:00",
        "updated_date": "2025-09-20T10:17:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shipeng Liu",
            "Zhonglin Zhang",
            "Dengfeng Chen",
            "Liang Zhao"
        ],
        "tldr": "The paper introduces D2S, a vision-text fusion framework that uses image captions generated by a pre-trained vision-language model to improve image complexity assessment. It achieves state-of-the-art performance on IC9600 while maintaining competitiveness on NR-IQA benchmarks, with efficient inference.",
        "tldr_zh": "该论文介绍了D2S，一个视觉-文本融合框架，利用预训练的视觉-语言模型生成的图像描述来改进图像复杂性评估。它在IC9600上实现了最先进的性能，同时在NR-IQA基准测试中保持了竞争力，且推理效率高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis",
        "summary": "Video captions offer concise snapshots of actors, objects, and actions within\na video, serving as valuable assets for applications such as question answering\nand event localization. However, acquiring human annotations for video captions\nis costly or even impractical, especially when dealing with diverse video\ndomains. Existing models trained on supervised datasets face challenges in\nevaluating performance across different domains due to the reliance on\nreference-based evaluation protocols, which necessitate ground truth captions.\nThis assumption is unrealistic for evaluating videos in the wild. To address\nthese limitations, we propose a reference-free evaluation framework that does\nnot require ground truth captions, focusing on factual grounding to ensure\naccurate assessment of caption quality. We introduce VC-Inspector, a novel\ncaption quality evaluator that is both reference-free and factually grounded.\nUtilizing large language models, we generate pseudo captions of varying quality\nbased on supervised data, which are subsequently used to train a multimodal\nmodel (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior\nalignment with human judgments on the VATEX-Eval dataset, outperforming\nexisting methods. The performance also generalizes to image caption datasets,\nFlickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.\nOverall, VC-Inspector offers a scalable and generalizable solution for\nevaluating the factual accuracy of video captions, paving the way for more\neffective and objective assessment methodologies in diverse video domains.",
        "url": "http://arxiv.org/abs/2509.16538v1",
        "published_date": "2025-09-20T05:04:41+00:00",
        "updated_date": "2025-09-20T05:04:41+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Shubhashis Roy Dipta",
            "Tz-Ying Wu",
            "Subarna Tripathi"
        ],
        "tldr": "The paper introduces VC-Inspector, a novel reference-free and factually grounded framework for evaluating video caption quality using LLMs to generate pseudo captions and a multimodal model (Qwen2.5-VL) for evaluation, demonstrating superior performance on VATEX-Eval.",
        "tldr_zh": "该论文介绍了一种新颖的、无需参考且基于事实依据的视频字幕质量评估框架VC-Inspector，该框架使用LLM生成伪字幕，并使用多模态模型(Qwen2.5-VL)进行评估，在VATEX-Eval上表现出优异的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding",
        "summary": "Multimodal vision-language models (VLMs) have made substantial progress in\nvarious tasks that require a combined understanding of visual and textual\ncontent, particularly in cultural understanding tasks, with the emergence of\nnew cultural datasets. However, these datasets frequently fall short of\nproviding cultural reasoning while underrepresenting many cultures. In this\npaper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural\nreasoning with a novel approach that requires VLMs to reason on culturally rich\nimages in two stages: i) selecting the correct visual option with\nmultiple-choice visual question answering (VQA), and ii) segmenting the\nrelevant cultural artifact as evidence of reasoning. Visual options in the\nfirst stage are systematically organized into three types: those originating\nfrom the same country, those from different countries, or a mixed group.\nNotably, all options are derived from a singular category for each type.\nProgression to the second stage occurs only after a correct visual option is\nchosen. The SCB benchmark comprises 1,065 images that capture 138 cultural\nartifacts across five categories from seven Southeast Asia countries, whose\ndiverse cultures are often overlooked, accompanied by 3,178 questions, of which\n1,093 are unique and meticulously curated by human annotators. Our evaluation\nof various VLMs reveals the complexities involved in cross-modal cultural\nreasoning and highlights the disparity between visual reasoning and spatial\ngrounding in culturally nuanced scenarios. The SCB serves as a crucial\nbenchmark for identifying these shortcomings, thereby guiding future\ndevelopments in the field of cultural reasoning.\nhttps://github.com/buraksatar/SeeingCulture",
        "url": "http://arxiv.org/abs/2509.16517v1",
        "published_date": "2025-09-20T03:47:49+00:00",
        "updated_date": "2025-09-20T03:47:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "authors": [
            "Burak Satar",
            "Zhixin Ma",
            "Patrick A. Irawan",
            "Wilfried A. Mulyawan",
            "Jing Jiang",
            "Ee-Peng Lim",
            "Chong-Wah Ngo"
        ],
        "tldr": "The paper introduces the Seeing Culture Benchmark (SCB) for evaluating VLM's cultural reasoning abilities using a two-stage approach involving visual question answering and cultural artifact segmentation, focusing on Southeast Asian cultures often underrepresented in existing datasets.",
        "tldr_zh": "本文介绍了“文化视角基准测试（SCB）”，通过视觉问答和文化文物分割两阶段方法评估VLM的文化推理能力，重点关注现有数据集中代表性不足的东南亚文化。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute",
        "summary": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks.",
        "url": "http://arxiv.org/abs/2509.16343v1",
        "published_date": "2025-09-19T18:34:08+00:00",
        "updated_date": "2025-09-19T18:34:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Chung-En",
            "Yu",
            "Brian Jalaian",
            "Nathaniel D. Bastian"
        ],
        "tldr": "The paper introduces Visual Reasoning Agent (VRA), a training-free, agentic reasoning framework that leverages off-the-shelf vision-language models in a Think-Critique-Act loop, achieving significant accuracy gains on visual reasoning benchmarks at the cost of increased test-time computation.",
        "tldr_zh": "该论文介绍了一种名为Visual Reasoning Agent (VRA)的免训练agentic推理框架，它利用现成的视觉-语言模型，通过Think-Critique-Act循环，在视觉推理基准测试中实现了显著的准确率提升，但代价是增加了测试时的计算量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]