[
    {
        "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
        "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/",
        "url": "http://arxiv.org/abs/2510.10274v1",
        "published_date": "2025-10-11T16:20:17+00:00",
        "updated_date": "2025-10-11T16:20:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jinliang Zheng",
            "Jianxiong Li",
            "Zhihao Wang",
            "Dongxiu Liu",
            "Xirui Kang",
            "Yuchun Feng",
            "Yinan Zheng",
            "Jiayin Zou",
            "Yilun Chen",
            "Jia Zeng",
            "Ya-Qin Zhang",
            "Jiangmiao Pang",
            "Jingjing Liu",
            "Tai Wang",
            "Xianyuan Zhan"
        ],
        "tldr": "The paper introduces X-VLA, a novel Vision-Language-Action model that uses soft prompts to effectively train across diverse robotic platforms and achieve state-of-the-art performance in both simulation and real-world scenarios.",
        "tldr_zh": "该论文介绍了X-VLA，一种新型的视觉-语言-动作模型，它使用软提示来有效地跨多个机器人平台进行训练，并在模拟和现实世界场景中实现最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback",
        "summary": "Embodied agents face a fundamental limitation: once deployed in real-world\nenvironments to perform specific tasks, they are unable to acquire new useful\nknowledge to enhance task performance. In this paper, we propose a general\npost-deployment learning framework called Dejavu, which employs an Experience\nFeedback Network (EFN) and augments the frozen Vision-Language-Action (VLA)\npolicy with retrieved execution memories. EFN automatically identifies\ncontextually successful prior action experiences and conditions action\nprediction on this retrieved guidance. We adopt reinforcement learning with\nsemantic similarity rewards on EFN to ensure that the predicted actions align\nwith past successful behaviors under current observations. During deployment,\nEFN continually enriches its memory with new trajectories, enabling the agent\nto exhibit \"learning from experience\" despite fixed weights. Experiments across\ndiverse embodied tasks show that EFN significantly improves adaptability,\nrobustness, and success rates over frozen baselines. These results highlight a\npromising path toward embodied agents that continually refine their behavior\nafter deployment.",
        "url": "http://arxiv.org/abs/2510.10181v1",
        "published_date": "2025-10-11T11:43:58+00:00",
        "updated_date": "2025-10-11T11:43:58+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shaokai Wu",
            "Yanbiao Ji",
            "Qiuchang Li",
            "Zhiyi Zhang",
            "Qichen He",
            "Wenyuan Xie",
            "Guodong Zhang",
            "Bayram Bayramli",
            "Yue Ding",
            "Hongtao Lu"
        ],
        "tldr": "The paper introduces Dejavu, a post-deployment learning framework for embodied agents that uses an Experience Feedback Network to retrieve successful past action experiences and augment a frozen Vision-Language-Action policy, enhancing adaptability and robustness.",
        "tldr_zh": "该论文介绍了Dejavu，一个具身智能体的部署后学习框架，它使用经验反馈网络来检索成功的过去动作经验，并增强冻结的视觉-语言-动作策略，从而提高适应性和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cooperative Pseudo Labeling for Unsupervised Federated Classification",
        "summary": "Unsupervised Federated Learning (UFL) aims to collaboratively train a global\nmodel across distributed clients without sharing data or accessing label\ninformation. Previous UFL works have predominantly focused on representation\nlearning and clustering tasks. Recently, vision language models (e.g., CLIP)\nhave gained significant attention for their powerful zero-shot prediction\ncapabilities. Leveraging this advancement, classification problems that were\npreviously infeasible under the UFL paradigm now present promising new\nopportunities, yet remain largely unexplored. In this paper, we extend UFL to\nthe classification problem with CLIP for the first time and propose a novel\nmethod, \\underline{\\textbf{Fed}}erated \\underline{\\textbf{Co}}operative\n\\underline{\\textbf{P}}seudo \\underline{\\textbf{L}}abeling (\\textbf{FedCoPL}).\nSpecifically, clients estimate and upload their pseudo label distribution, and\nthe server adjusts and redistributes them to avoid global imbalance among\nclasses. Moreover, we introduce a partial prompt aggregation protocol for\neffective collaboration and personalization. In particular, visual prompts\ncontaining general image features are aggregated at the server, while text\nprompts encoding personalized knowledge are retained locally. Extensive\nexperiments demonstrate the superior performance of our FedCoPL compared to\nbaseline methods. Our code is available at\n\\href{https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}.",
        "url": "http://arxiv.org/abs/2510.10100v1",
        "published_date": "2025-10-11T08:18:26+00:00",
        "updated_date": "2025-10-11T08:18:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kuangpu Guo",
            "Lijun Sheng",
            "Yongcan Yu",
            "Jian Liang",
            "Zilei Wang",
            "Ran He"
        ],
        "tldr": "The paper proposes FedCoPL, a novel unsupervised federated learning approach for classification that leverages CLIP and pseudo-labeling with server-side rebalancing and partial prompt aggregation for effective collaboration and personalization.",
        "tldr_zh": "该论文提出了FedCoPL，一种新颖的无监督联邦学习分类方法，利用CLIP和伪标签，通过服务器端重新平衡和部分提示聚合，实现有效的协作和个性化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents",
        "summary": "Large vision-language model (LVLM)-based web agents are emerging as powerful\ntools for automating complex online tasks. However, when deployed in real-world\nenvironments, they face serious security risks, motivating the design of\nsecurity evaluation benchmarks. Existing benchmarks provide only partial\ncoverage, typically restricted to narrow scenarios such as user-level prompt\nmanipulation, and thus fail to capture the broad range of agent\nvulnerabilities. To address this gap, we present \\tool{}, the first holistic\nbenchmark for evaluating the security of LVLM-based web agents. \\tool{} first\nintroduces a unified evaluation suite comprising six simulated but realistic\nweb environments (\\eg, e-commerce platforms, community forums) and includes\n2,970 high-quality trajectories spanning diverse tasks and attack settings. The\nsuite defines a structured taxonomy of six attack vectors spanning both\nuser-level and environment-level manipulations. In addition, we introduce a\nmulti-layered evaluation protocol that analyzes agent failures across three\ncritical dimensions: internal reasoning, behavioral trajectory, and task\noutcome, facilitating a fine-grained risk analysis that goes far beyond simple\nsuccess metrics. Using this benchmark, we conduct large-scale experiments on 9\nrepresentative LVLMs, which fall into three categories: general-purpose,\nagent-specialized, and GUI-grounded. Our results show that all tested agents\nare consistently vulnerable to subtle adversarial manipulations and reveal\ncritical trade-offs between model specialization and security. By providing (1)\na comprehensive benchmark suite with diverse environments and a multi-layered\nevaluation pipeline, and (2) empirical insights into the security challenges of\nmodern LVLM-based web agents, \\tool{} establishes a foundation for advancing\ntrustworthy web agent deployment.",
        "url": "http://arxiv.org/abs/2510.10073v1",
        "published_date": "2025-10-11T07:18:12+00:00",
        "updated_date": "2025-10-11T07:18:12+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Zonghao Ying",
            "Yangguang Shao",
            "Jianle Gan",
            "Gan Xu",
            "Junjie Shen",
            "Wenxin Zhang",
            "Quanchen Zou",
            "Junzheng Shi",
            "Zhenfei Yin",
            "Mingchuan Zhang",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "tldr": "The paper introduces SecureWebArena, a holistic benchmark for evaluating the security vulnerabilities of LVLM-based web agents across diverse tasks and attack vectors, revealing their susceptibility to adversarial manipulations.",
        "tldr_zh": "该论文介绍了SecureWebArena，一个全面的基准，用于评估基于LVLM的Web代理在各种任务和攻击向量中的安全漏洞，揭示了它们对对抗性操纵的敏感性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Think Twice to See More: Iterative Visual Reasoning in Medical VLMs",
        "summary": "Medical vision-language models (VLMs) excel at image-text understanding but\ntypically rely on a single-pass reasoning that neglects localized visual cues.\nIn clinical practice, however, human experts iteratively scan, focus, and\nrefine the regions of interest before reaching a final diagnosis. To narrow\nthis machine-human perception gap, we introduce ViTAR, a novel VLM framework\nthat emulates the iterative reasoning process of human experts through a\ncognitive chain of \"think-act-rethink-answer\". ViTAR treats medical images as\ninteractive objects, enabling models to engage multi-step visual reasoning. To\nsupport this approach, we curate a high-quality instruction dataset comprising\n1K interactive examples that encode expert-like diagnostic behaviors. In\naddition, a 16K visual question answering training data has been curated\ntowards fine-grained visual diagnosis. We introduce a two-stage training\nstrategy that begins with supervised fine-tuning to guide cognitive\ntrajectories, followed by the reinforcement learning to optimize\ndecision-making. Extensive evaluations demonstrate that ViTAR outperforms\nstrong state-of-the-art models. Visual attention analysis reveals that from the\n\"think\" to \"rethink\" rounds, ViTAR increasingly anchors visual grounding to\nclinically critical regions and maintains high attention allocation to visual\ntokens during reasoning, providing mechanistic insight into its improved\nperformance. These findings demonstrate that embedding expert-style iterative\nthinking chains into VLMs enhances both performance and trustworthiness of\nmedical AI.",
        "url": "http://arxiv.org/abs/2510.10052v1",
        "published_date": "2025-10-11T06:39:57+00:00",
        "updated_date": "2025-10-11T06:39:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kaitao Chen",
            "Shaohao Rui",
            "Yankai Jiang",
            "Jiamin Wu",
            "Qihao Zheng",
            "Chunfeng Song",
            "Xiaosong Wang",
            "Mu Zhou",
            "Mianxin Liu"
        ],
        "tldr": "The paper introduces ViTAR, a novel medical VLM framework that mimics expert iterative reasoning through a \"think-act-rethink-answer\" cognitive chain, enhancing performance and trustworthiness in medical AI.",
        "tldr_zh": "该论文介绍了ViTAR，一种新型医疗视觉语言模型框架，通过“思考-行动-再思考-回答”的认知链来模仿专家迭代推理，从而提高医疗人工智能的性能和可信度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning",
        "summary": "Recent advances in video captioning are driven by large-scale pretrained\nmodels, which follow the standard \"pre-training followed by fine-tuning\"\nparadigm, where the full model is fine-tuned for downstream tasks. Although\neffective, this approach becomes computationally prohibitive as the model size\nincreases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a\npromising alternative, but primarily focuses on the language components of\nMultimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains\nunderexplored in multimodal tasks and lacks sufficient understanding of visual\ninformation during fine-tuning the model. To bridge this gap, we propose\nQuery-Adapter (Q-Adapter), a lightweight visual adapter module designed to\nenhance MLLMs by enabling efficient fine-tuning for the video captioning task.\nQ-Adapter introduces learnable query tokens and a gating layer into Vision\nEncoder, enabling effective extraction of sparse, caption-relevant features\nwithout relying on external textual supervision. We evaluate Q-Adapter on two\nwell-known video captioning datasets, MSR-VTT and MSVD, where it achieves\nstate-of-the-art performance among the methods that take the PEFT approach\nacross BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves\ncompetitive performance compared to methods that take the full fine-tuning\napproach while requiring only 1.4% of the parameters. We further analyze the\nimpact of key hyperparameters and design choices on fine-tuning effectiveness,\nproviding insights into optimization strategies for adapter-based learning.\nThese results highlight the strong potential of Q-Adapter in balancing caption\nquality and parameter efficiency, demonstrating its scalability for\nvideo-language modeling.",
        "url": "http://arxiv.org/abs/2510.10022v1",
        "published_date": "2025-10-11T04:58:21+00:00",
        "updated_date": "2025-10-11T04:58:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junan Chen",
            "Trung Thanh Nguyen",
            "Takahiro Komamizu",
            "Ichiro Ide"
        ],
        "tldr": "The paper introduces Q-Adapter, a parameter-efficient fine-tuning method for video captioning using learnable query tokens and a gating layer in the Vision Encoder, achieving state-of-the-art performance among PEFT methods and competitive performance compared to full fine-tuning with significantly fewer parameters.",
        "tldr_zh": "本文介绍了一种名为 Q-Adapter 的参数高效微调方法，用于视频字幕生成。该方法在视觉编码器中使用可学习的查询令牌和门控层，在 PEFT 方法中实现了最先进的性能，并且与全微调相比，以更少的参数实现了具有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output",
        "summary": "Currently, medical vision language models are widely used in medical vision\nquestion answering tasks. However, existing models are confronted with two\nissues: for input, the model only relies on text instructions and lacks direct\nunderstanding of visual clues in the image; for output, the model only gives\ntext answers and lacks connection with key areas in the image. To address these\nissues, we propose a unified medical vision language model MIMO, with visual\nreferring Multimodal Input and pixel grounding Multimodal Output. MIMO can not\nonly combine visual clues and textual instructions to understand complex\nmedical images and semantics, but can also ground medical terminologies in\ntextual output within the image. To overcome the scarcity of relevant data in\nthe medical field, we propose MIMOSeg, a comprehensive medical multimodal\ndataset including 895K samples. MIMOSeg is constructed from four different\nperspectives, covering basic instruction following and complex question\nanswering with multimodal input and multimodal output. We conduct experiments\non several downstream medical multimodal tasks. Extensive experimental results\nverify that MIMO can uniquely combine visual referring and pixel grounding\ncapabilities, which are not available in previous models.",
        "url": "http://arxiv.org/abs/2510.10011v1",
        "published_date": "2025-10-11T04:29:09+00:00",
        "updated_date": "2025-10-11T04:29:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanyuan Chen",
            "Dexuan Xu",
            "Yu Huang",
            "Songkun Zhan",
            "Hanpin Wang",
            "Dongxue Chen",
            "Xueping Wang",
            "Meikang Qiu",
            "Hang Li"
        ],
        "tldr": "The paper introduces MIMO, a medical vision language model that incorporates visual clues in the input and pixel grounding in the output, along with a new multimodal dataset, MIMOSeg, to address limitations in existing medical VQA models.",
        "tldr_zh": "该论文介绍了MIMO，一种医学视觉语言模型，它在输入中结合了视觉线索，在输出中实现了像素级定位。同时，该论文还提出了一个新的多模态数据集MIMOSeg，以解决现有医学视觉问答模型的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation",
        "summary": "Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across\nvarious tasks by pre-training on numerous image-text pairs. These models often\nbenefit from using an ensemble of context prompts to represent a class. Despite\nbeing effective, conventional prompt ensembling that averages textual features\nof context prompts often yields suboptimal results. This is because feature\naveraging shifts the class centroids away from the true class distribution. To\naddress this issue, we propose the Cluster-Aware Prompt Ensemble Learning\n(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL\nclassifies images into one of several class clusters, each represented by a\ndistinct prompt. Instead of ensembling prompts in the feature space, we perform\nensembling in the classification logits space, aligning better with the visual\nfeature distribution. To further optimize prompt fine-tuning while maintaining\ncluster-specific discriminative power, we introduce a cluster-preserving\nregularization term. This ensures that prompts remain distinct and specialized\nfor different clusters, preventing collapse into a uniform direction.\nAdditionally, we integrate an adaptive prompt weighting technique to\ndynamically adjust the attention weights for flawed or ambiguous prompts,\nensuring robust performance across diverse datasets and tasks.",
        "url": "http://arxiv.org/abs/2510.09867v1",
        "published_date": "2025-10-10T20:58:43+00:00",
        "updated_date": "2025-10-10T20:58:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhi Chen",
            "Xin Yu",
            "Xiaohui Tao",
            "Yan Li",
            "Zi Huang"
        ],
        "tldr": "This paper introduces Cluster-Aware Prompt Ensemble Learning (CAPEL) for few-shot vision-language model adaptation, addressing the suboptimal results of conventional prompt ensembling by preserving the cluster nature of context prompts and using cluster-preserving regularization.",
        "tldr_zh": "本文介绍了用于少样本视觉语言模型适配的集群感知提示集成学习（CAPEL），通过保留上下文提示的集群性质并使用集群保持正则化来解决传统提示集成的次优结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Text Prompt Injection of Vision Language Models",
        "summary": "The widespread application of large vision language models has significantly\nraised safety concerns. In this project, we investigate text prompt injection,\na simple yet effective method to mislead these models. We developed an\nalgorithm for this type of attack and demonstrated its effectiveness and\nefficiency through experiments. Compared to other attack methods, our approach\nis particularly effective for large models without high demand for\ncomputational resources.",
        "url": "http://arxiv.org/abs/2510.09849v1",
        "published_date": "2025-10-10T20:26:20+00:00",
        "updated_date": "2025-10-10T20:26:20+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Ruizhe Zhu"
        ],
        "tldr": "This paper introduces a text prompt injection attack method against vision language models, demonstrating its effectiveness and efficiency, particularly for large models with limited computational resources.",
        "tldr_zh": "本文介绍了一种针对视觉语言模型的文本提示注入攻击方法，证明了其有效性和效率，尤其是在计算资源有限的大型模型上。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Task-Aware Resolution Optimization for Visual Large Language Models",
        "summary": "Real-world vision-language applications demand varying levels of perceptual\ngranularity. However, most existing visual large language models (VLLMs), such\nas LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to\nsubpar performance. To address this problem, we first conduct a comprehensive\nand pioneering investigation into the resolution preferences of different\nvision-language tasks, revealing a correlation between resolution preferences\nwith image complexity, and uncertainty variance of the VLLM at different image\ninput resolutions. Building on this insight, we propose an empirical formula to\ndetermine the optimal resolution for a given vision-language task, combining\nthese two factors. Second, based on rigorous experiments, we propose a novel\nparameter-efficient fine-tuning technique to extend the visual input resolution\nof pre-trained VLLMs to the identified optimal resolution. Extensive\nexperiments on various vision-language tasks validate the effectiveness of our\nmethod.",
        "url": "http://arxiv.org/abs/2510.09822v1",
        "published_date": "2025-10-10T19:53:30+00:00",
        "updated_date": "2025-10-10T19:53:30+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Weiqing Luo",
            "Zhen Tan",
            "Yifan Li",
            "Xinyu Zhao",
            "Kwonjoon Lee",
            "Behzad Dariush",
            "Tianlong Chen"
        ],
        "tldr": "This paper investigates task-aware resolution optimization for VLLMs by identifying optimal resolutions based on image complexity and VLLM uncertainty. They propose a fine-tuning method to adapt VLLMs to these resolutions, improving performance across tasks.",
        "tldr_zh": "本文研究了视觉语言模型（VLLM）的任务感知分辨率优化，通过识别基于图像复杂度和VLLM不确定性的最佳分辨率。他们提出了一种微调方法，使VLLM适应这些分辨率，从而提高跨任务的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization",
        "summary": "Advances in image tampering pose serious security threats, underscoring the\nneed for effective image manipulation localization (IML). While supervised IML\nachieves strong performance, it depends on costly pixel-level annotations.\nExisting weakly supervised or training-free alternatives often underperform and\nlack interpretability. We propose the In-Context Forensic Chain (ICFC), a\ntraining-free framework that leverages multi-modal large language models\n(MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule\nconstruction with adaptive filtering to build a reliable knowledge base and a\nmulti-step progressive reasoning pipeline that mirrors expert forensic\nworkflows from coarse proposals to fine-grained forensics results. This design\nenables systematic exploitation of MLLM reasoning for image-level\nclassification, pixel-level localization, and text-level interpretability.\nAcross multiple benchmarks, ICFC not only surpasses state-of-the-art\ntraining-free methods but also achieves competitive or superior performance\ncompared to weakly and fully supervised approaches.",
        "url": "http://arxiv.org/abs/2510.10111v1",
        "published_date": "2025-10-11T08:42:31+00:00",
        "updated_date": "2025-10-11T08:42:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Rui Chen",
            "Bin Liu",
            "Changtao Miao",
            "Xinghao Wang",
            "Yi Li",
            "Tao Gong",
            "Qi Chu",
            "Nenghai Yu"
        ],
        "tldr": "This paper introduces a training-free In-Context Forensic Chain (ICFC) framework for image manipulation localization using multi-modal large language models (MLLMs), achieving state-of-the-art performance without training.",
        "tldr_zh": "本文提出了一种无需训练的In-Context Forensic Chain (ICFC)框架，利用多模态大型语言模型（MLLMs）进行图像篡改定位，在无需训练的情况下实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models",
        "summary": "Recent advances in large language models (LLMs) have demonstrated that\nreinforcement learning with verifiable rewards (RLVR) can significantly enhance\nreasoning abilities by directly optimizing correctness, rather than relying\nsolely on supervised imitation. This paradigm has been extended to multimodal\nLLMs for complex video and image understanding tasks. However, while\noutcome-driven RL improves answer accuracy, it can inadvertently decouple the\nreasoning chain from the final answer, leading to situations where models\nproduce inconsistency between the reasoning trace and final answer. In our\nexperiments on multiple-choice visual question-answering tasks, the standard\nGRPO method yields only 79.7\\% consistency on MMVU between the reasoning steps\nand the chosen answers, indicating frequent mismatches between answers and\nreasoning. To this end, we propose Answer-Consistent Reinforcement Learning\n(ACRE) that modifies the GRPO algorithm with an auxiliary consistency check.\nAfter the model generates a chain of thought and an initial answer for a given\nquestion, we shuffle the answer options and prompt the model again with the\nsame reasoning trace to predict a second answer. We design a\nconsistency-verification reward that grants a high reward only if both the\noriginal and the post-shuffle answers agree and are correct; otherwise, a lower\nreward is assigned accordingly. This mechanism penalizes reasoning-answer\nmisalignment and discourages the model from relying on spurious patterns, such\nas option ordering biases. We evaluate ACRE on challenging Video Reasoning\nbenchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\\%\nand 1.5\\% improvement for Video Reasoning and Math Reasoning tasks over the\nGRPO baseline.",
        "url": "http://arxiv.org/abs/2510.10104v1",
        "published_date": "2025-10-11T08:32:52+00:00",
        "updated_date": "2025-10-11T08:32:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minbin Huang",
            "Runhui Huang",
            "Chuanyang Zheng",
            "Jingyao Li",
            "Guoxuan Chen",
            "Han Shi",
            "Hong Cheng"
        ],
        "tldr": "This paper introduces Answer-Consistent Reinforcement Learning (ACRE), a method to improve the consistency between reasoning traces and answers in multimodal LLMs by incorporating a consistency check into the GRPO algorithm. It shows performance improvements on video and math reasoning benchmarks.",
        "tldr_zh": "本文介绍了答案一致性强化学习（ACRE），该方法通过在GRPO算法中加入一致性检查，提高了多模态LLM中推理过程和答案之间的一致性。实验表明，该方法在视频和数学推理基准测试中取得了性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making",
        "summary": "Accurate, scalable traffic monitoring is critical for real-time and long-term\ntransportation management, particularly during disruptions such as natural\ndisasters, large construction projects, or major policy changes like New York\nCity's first-in-the-nation congestion pricing program. However, widespread\nsensor deployment remains limited due to high installation, maintenance, and\ndata management costs. While traffic cameras offer a cost-effective\nalternative, existing video analytics struggle with dynamic camera viewpoints\nand massive data volumes from large camera networks. This study presents an\nend-to-end AI-based framework leveraging existing traffic camera infrastructure\nfor high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11\nmodel, trained on localized urban scenes, extracts multimodal traffic density\nand classification metrics in real time. To address inconsistencies from\nnon-stationary pan-tilt-zoom cameras, we introduce a novel graph-based\nviewpoint normalization method. A domain-specific large language model was also\nintegrated to process massive data from a 24/7 video stream to generate\nfrequent, automated summaries of evolving traffic patterns, a task far\nexceeding manual capabilities. We validated the system using over 9 million\nimages from roughly 1,000 traffic cameras during the early rollout of NYC\ncongestion pricing in 2025. Results show a 9% decline in weekday passenger\nvehicle density within the Congestion Relief Zone, early truck volume\nreductions with signs of rebound, and consistent increases in pedestrian and\ncyclist activity at corridor and zonal scales. Experiments showed that\nexample-based prompts improved LLM's numerical accuracy and reduced\nhallucinations. These findings demonstrate the framework's potential as a\npractical, infrastructure-ready solution for large-scale, policy-relevant\ntraffic monitoring with minimal human intervention.",
        "url": "http://arxiv.org/abs/2510.09981v1",
        "published_date": "2025-10-11T03:18:42+00:00",
        "updated_date": "2025-10-11T03:18:42+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Fan Zuo",
            "Donglin Zhou",
            "Jingqin Gao",
            "Kaan Ozbay"
        ],
        "tldr": "This paper presents an AI-powered framework utilizing traffic cameras and a domain-specific LLM for large-scale, real-time traffic monitoring, validated by NYC congestion pricing data, showing its effectiveness in automatically generating insightful traffic summaries.",
        "tldr_zh": "本文提出了一个基于人工智能的框架，利用交通摄像头和领域特定的大型语言模型进行大规模、实时的交通监控。通过纽约市拥堵收费数据的验证，表明该框架能够有效地自动生成有见地的交通状况总结。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "TCMA: Text-Conditioned Multi-granularity Alignment for Drone Cross-Modal Text-Video Retrieval",
        "summary": "Unmanned aerial vehicles (UAVs) have become powerful platforms for real-time,\nhigh-resolution data collection, producing massive volumes of aerial videos.\nEfficient retrieval of relevant content from these videos is crucial for\napplications in urban management, emergency response, security, and disaster\nrelief. While text-video retrieval has advanced in natural video domains, the\nUAV domain remains underexplored due to limitations in existing datasets, such\nas coarse and redundant captions. Thus, in this work, we construct the Drone\nVideo-Text Match Dataset (DVTMD), which contains 2,864 videos and 14,320\nfine-grained, semantically diverse captions. The annotations capture multiple\ncomplementary aspects, including human actions, objects, background settings,\nenvironmental conditions, and visual style, thereby enhancing text-video\ncorrespondence and reducing redundancy. Building on this dataset, we propose\nthe Text-Conditioned Multi-granularity Alignment (TCMA) framework, which\nintegrates global video-sentence alignment, sentence-guided frame aggregation,\nand word-guided patch alignment. To further refine local alignment, we design a\nWord and Patch Selection module that filters irrelevant content, as well as a\nText-Adaptive Dynamic Temperature Mechanism that adapts attention sharpness to\ntext type. Extensive experiments on DVTMD and CapERA establish the first\ncomplete benchmark for drone text-video retrieval. Our TCMA achieves\nstate-of-the-art performance, including 45.5% R@1 in text-to-video and 42.8%\nR@1 in video-to-text retrieval, demonstrating the effectiveness of our dataset\nand method. The code and dataset will be released.",
        "url": "http://arxiv.org/abs/2510.10180v1",
        "published_date": "2025-10-11T11:38:01+00:00",
        "updated_date": "2025-10-11T11:38:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixu Zhao",
            "Yang Zhan"
        ],
        "tldr": "The paper introduces a new drone video-text retrieval dataset (DVTMD) and a novel Text-Conditioned Multi-granularity Alignment (TCMA) framework that achieves state-of-the-art performance in drone video retrieval tasks.",
        "tldr_zh": "本文介绍了一个新的无人机视频-文本检索数据集 (DVTMD) 和一个新的文本条件多粒度对齐 (TCMA) 框架，该框架在无人机视频检索任务中实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]