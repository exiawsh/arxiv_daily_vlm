[
    {
        "title": "Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement",
        "summary": "Class-Incremental Semantic Segmentation (CISS) requires continuous learning\nof newly introduced classes while retaining knowledge of past classes. By\nabstracting mainstream methods into two stages (visual feature extraction and\nprototype-feature matching), we identify a more fundamental challenge termed\ncatastrophic semantic entanglement. This phenomenon involves Prototype-Feature\nEntanglement caused by semantic misalignment during the incremental process,\nand Background-Increment Entanglement due to dynamic data evolution. Existing\ntechniques, which rely on visual feature learning without sufficient cues to\ndistinguish targets, introduce significant noise and errors. To address these\nissues, we introduce a Language-inspired Bootstrapped Disentanglement framework\n(LBD). We leverage the prior class semantics of pre-trained visual-language\nmodels (e.g., CLIP) to guide the model in autonomously disentangling features\nthrough Language-guided Prototypical Disentanglement and Manifold Mutual\nBackground Disentanglement. The former guides the disentangling of new\nprototypes by treating hand-crafted text features as topological templates,\nwhile the latter employs multiple learnable prototypes and mask-pooling-based\nsupervision for background-incremental class disentanglement. By incorporating\nsoft prompt tuning and encoder adaptation modifications, we further bridge the\ncapability gap of CLIP between dense and sparse tasks, achieving\nstate-of-the-art performance on both Pascal VOC and ADE20k, particularly in\nmulti-step scenarios.",
        "url": "http://arxiv.org/abs/2509.00527v1",
        "published_date": "2025-08-30T15:18:58+00:00",
        "updated_date": "2025-08-30T15:18:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruitao Wu",
            "Yifan Zhao",
            "Jia Li"
        ],
        "tldr": "This paper introduces a Language-inspired Bootstrapped Disentanglement framework (LBD) to address catastrophic semantic entanglement in class-incremental semantic segmentation by leveraging pre-trained visual-language models like CLIP for better feature disentanglement and achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种语言引导的自举解耦框架（LBD），旨在解决类别增量语义分割中的灾难性语义纠缠问题。该框架利用像CLIP这样的预训练视觉语言模型来实现更好的特征解耦，并取得最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression",
        "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
        "url": "http://arxiv.org/abs/2509.00419v1",
        "published_date": "2025-08-30T08:57:53+00:00",
        "updated_date": "2025-08-30T08:57:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lianyu Hu",
            "Fanhua Shang",
            "Wei Feng",
            "Liang Wan"
        ],
        "tldr": "LightVLM accelerates VLMs during both encoding (pyramid token merging) and decoding (KV cache compression) stages, achieving significant speedups with minimal performance loss.",
        "tldr_zh": "LightVLM通过在编码阶段采用金字塔token合并，解码阶段采用KV缓存压缩，加速VLM的推理过程，在性能损失最小的情况下实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Target-Oriented Single Domain Generalization",
        "summary": "Deep models trained on a single source domain often fail catastrophically\nunder distribution shifts, a critical challenge in Single Domain Generalization\n(SDG). While existing methods focus on augmenting source data or learning\ninvariant features, they neglect a readily available resource: textual\ndescriptions of the target deployment environment. We propose Target-Oriented\nSingle Domain Generalization (TO-SDG), a novel problem setup that leverages the\ntextual description of the target domain, without requiring any target data, to\nguide model generalization. To address TO-SDG, we introduce Spectral TARget\nAlignment (STAR), a lightweight module that injects target semantics into\nsource features by exploiting visual-language models (VLMs) such as CLIP. STAR\nuses a target-anchored subspace derived from the text embedding of the target\ndescription to recenter image features toward the deployment domain, then\nutilizes spectral projection to retain directions aligned with target cues\nwhile discarding source-specific noise. Moreover, we use a vision-language\ndistillation to align backbone features with VLM's semantic geometry. STAR\nfurther employs feature-space Mixup to ensure smooth transitions between source\nand target-oriented representations. Experiments across various image\nclassification and object detection benchmarks demonstrate STAR's superiority.\nThis work establishes that minimal textual metadata, which is a practical and\noften overlooked resource, significantly enhances generalization under severe\ndata constraints, opening new avenues for deploying robust models in target\nenvironments with unseen data.",
        "url": "http://arxiv.org/abs/2509.00351v1",
        "published_date": "2025-08-30T04:21:48+00:00",
        "updated_date": "2025-08-30T04:21:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Marzi Heidari",
            "Yuhong Guo"
        ],
        "tldr": "The paper introduces Target-Oriented Single Domain Generalization (TO-SDG), a novel approach that leverages textual descriptions of target environments and vision-language models (VLMs) to improve model generalization in single domain generalization tasks. The proposed method, STAR, uses target semantics to guide feature alignment and outperforms existing methods on image classification and object detection benchmarks.",
        "tldr_zh": "该论文介绍了一种名为目标导向单域泛化（TO-SDG）的新方法，该方法利用目标环境的文本描述和视觉-语言模型（VLM）来提高单域泛化任务中模型的泛化能力。所提出的方法STAR使用目标语义来引导特征对齐，并且在图像分类和目标检测基准测试中优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Adaptive Visual Token Pruning for Large Multimodal Models",
        "summary": "Large Multimodal Models (LMMs) have achieved significant success across\nvarious tasks. These models usually encode visual inputs into dense token\nsequences, which are then concatenated with textual tokens and jointly\nprocessed by a language model. However, the increased token count substantially\nraises computational and memory costs during inference. Token pruning has\nemerged as a promising approach to address this issue. Existing token pruning\nmethods often rely on costly calibration or suboptimal importance metrics,\nleading to redundant retained tokens. In this paper, we analyze the redundancy\ndifferences between visual and textual tokens and propose pruning exclusively\non visual tokens. Based on this, we propose a visual token pruning strategy\nthat explicitly preserves both cross-modal alignment and intra-modal\ninformational diversity. We introduce a mutual information-based token pruning\nstrategy that removes visual tokens semantically misaligned with textual\ntokens, effectively preserving the alignment between the visual and textual\nmodalities. To further improve the representational quality of the retained\ntokens, we additionally prune redundant visual tokens by maximizing the\nexpected pairwise distances in the embedding space, which is solved efficiently\nwith a greedy algorithm. Extensive experiments demonstrate that our method\nmaintains strong performance while reducing tokens by 88.9% on models such as\nLLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference\nspeed.",
        "url": "http://arxiv.org/abs/2509.00320v1",
        "published_date": "2025-08-30T02:43:50+00:00",
        "updated_date": "2025-08-30T02:43:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Zhang",
            "Mengsi Lyu",
            "Chenrui He",
            "Yulong Ao",
            "Yonghua Lin"
        ],
        "tldr": "This paper proposes an adaptive visual token pruning strategy for large multimodal models (LMMs) that preserves cross-modal alignment and intra-modal diversity, achieving significant token reduction and inference speedup.",
        "tldr_zh": "本文提出了一种用于大型多模态模型（LMM）的自适应视觉令牌剪枝策略，该策略可保持跨模态对齐和模态内信息多样性，从而显著减少令牌数量并提高推理速度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Aware Information Maximization for Transductive Few-Shot CLIP",
        "summary": "Transductive few-shot learning has triggered an abundant literature focusing\non vision-only models, but is still at a nascent stage within the recent\ncontext of foundational vision-language models (VLMs). Only a few recent\nmethods addressed the problem, pointing to the potential of tranduction in VLMs\nand to the need for VLM-tailored methods. Building on this momentum, we\nleverage information-theoretic concepts and recent progress in\nparameter-efficient fine-tuning (PEFT), developing a highly competitive\ntransductive few-shot CLIP method. Specifically, we introduce a novel\nLanguage-aware Information MaximizatiOn (LIMO) loss integrating three\ncomplementary terms: (i) the mutual information between the vision inputs and\nthe textual class descriptions; (ii) a Kullback-Leibler (KL) divergence\npenalizing deviation of the network's probabilistic outputs from the\ntext-driven zero-shot predictions; and (iii) a standard cross-entropy loss\nbased on the labeled shots. Furthermore, we challenge the commonly followed\nfine-tuning practices in the context of transductive few-shot learning, and\nexplore PEFT strategies, completely overlooked in this context. Surprisingly,\nwe observe substantial boosts in performances, which points to the potential of\nadapting a subset of the model's parameters in the transductive few-shot\nsetting. We report comprehensive evaluations, which show that LIMO outperforms\nthe very recent transductive few-shot CLIP methods by a large margin and yields\nsignificant gains over the best-performing inductive methods. Our code is\npublicly available at:\\[\n\\href{https://github.com/ghassenbaklouti/LIMO}{\\text{here}} \\]",
        "url": "http://arxiv.org/abs/2509.00305v1",
        "published_date": "2025-08-30T01:46:31+00:00",
        "updated_date": "2025-08-30T01:46:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ghassen Baklouti",
            "Maxime Zanella",
            "Ismail Ben Ayed"
        ],
        "tldr": "This paper introduces LIMO, a language-aware information maximization loss for transductive few-shot CLIP, and explores parameter-efficient fine-tuning strategies to achieve significant performance improvements over existing methods.",
        "tldr_zh": "本文介绍了LIMO，一种用于transductive few-shot CLIP的语言感知信息最大化损失，并探索了参数高效的微调策略，以实现比现有方法显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Generative AI for Industrial Contour Detection: A Language-Guided Vision System",
        "summary": "Industrial computer vision systems often struggle with noise, material\nvariability, and uncontrolled imaging conditions, limiting the effectiveness of\nclassical edge detectors and handcrafted pipelines. In this work, we present a\nlanguage-guided generative vision system for remnant contour detection in\nmanufacturing, designed to achieve CAD-level precision. The system is organized\ninto three stages: data acquisition and preprocessing, contour generation using\na conditional GAN, and multimodal contour refinement through vision-language\nmodeling, where standardized prompts are crafted in a human-in-the-loop process\nand applied through image-text guided synthesis. On proprietary FabTrack\ndatasets, the proposed system improved contour fidelity, enhancing edge\ncontinuity and geometric alignment while reducing manual tracing. For the\nrefinement stage, we benchmarked several vision-language models, including\nGoogle's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided\nworkflow, and open-source baselines. Under standardized conditions, GPT-image-1\nconsistently outperformed Gemini 2.0 Flash in both structural accuracy and\nperceptual quality. These findings demonstrate the promise of VLM-guided\ngenerative workflows for advancing industrial computer vision beyond the\nlimitations of classical pipelines.",
        "url": "http://arxiv.org/abs/2509.00284v1",
        "published_date": "2025-08-29T23:58:08+00:00",
        "updated_date": "2025-08-29T23:58:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Liang Gong",
            "Tommy",
            "Wang",
            "Sara Chaker",
            "Yanchen Dong",
            "Fouad Bousetouane",
            "Brenden Morton",
            "Mark Mendez"
        ],
        "tldr": "This paper introduces a language-guided generative vision system for industrial contour detection, leveraging conditional GANs and vision-language models to improve contour fidelity and geometric alignment in manufacturing settings, demonstrating the superiority of GPT-image-1 over Gemini 2.0 Flash in this context.",
        "tldr_zh": "本文介绍了一种用于工业轮廓检测的语言引导生成视觉系统，该系统利用条件GAN和视觉语言模型来提高制造环境中的轮廓保真度和几何对齐，并证明了GPT-image-1在这方面优于Gemini 2.0 Flash。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language tasks. However, these models often infer and\nreveal sensitive biometric attributes - such as race, gender, age, body weight,\nand eye color - even when such information is not explicitly requested. This\nraises critical concerns, particularly in real-world applications and\nsocially-sensitive domains. Despite increasing awareness, no publicly available\ndataset or benchmark exists to comprehensively evaluate or mitigate biometric\nleakage in MLLMs. To address this gap, we introduce PRISM (Privacy-aware\nEvaluation of Responses in Sensitive Modalities), a new benchmark designed to\nassess MLLMs on two fronts: (1) refuse biometric-related queries and (2)\nimplicit biometric leakage in general responses while maintaining semantic\nfaithfulness. Further, we conduct a detailed audit of the widely used LLaVA\ndatasets and uncover extensive biometric leakage across pretraining and\ninstruction data. To address this, we present Safe-LLaVA dataset, the first\nprivacy-preserving MLLM training dataset constructed by systematically removing\nexplicit and implicit biometric information from LLaVA dataset. Our evaluations\non PRISM reveal biometric leakages across MLLMs for different attributes,\nhighlighting the detailed privacy-violations. We also fine-tune a model on\nSafe-LLaVA dataset and show that it substantially reduces the biometric\nleakages. Together, Safe-LLaVA & PRISM set a new standard for privacy-aligned\ndevelopment and evaluation of MLLMs. The Safe-LLaVA dataset & PRISM benchmark\nare publicly available at https://huggingface.co/datasets/kyh9191/Safe-LLaVA,\nand the source code is available at\nhttps://github.com/Kimyounggun99/Safe-LLaVA.git.",
        "url": "http://arxiv.org/abs/2509.00192v1",
        "published_date": "2025-08-29T18:54:57+00:00",
        "updated_date": "2025-08-29T18:54:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Younggun Kim",
            "Sirnam Swetha",
            "Fazil Kagdi",
            "Mubarak Shah"
        ],
        "tldr": "The paper introduces Safe-LLaVA, a privacy-preserving dataset, and PRISM, a benchmark, to address and evaluate biometric leakage in Vision-Language Models. They also fine-tune a model on Safe-LLaVA, demonstrating reduced biometric leakage.",
        "tldr_zh": "该论文介绍了Safe-LLaVA（一个保护隐私的数据集）和PRISM（一个基准测试），旨在解决并评估视觉语言模型中的生物特征泄露问题。他们还在Safe-LLaVA上微调了一个模型，证明可以减少生物特征泄露。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders",
        "summary": "This work explores text-to-image retrieval for queries that specify or\ndescribe a semantic category. While vision-and-language models (VLMs) like CLIP\noffer a straightforward open-vocabulary solution, they map text and images to\ndistant regions in the representation space, limiting retrieval performance. To\nbridge this modality gap, we propose a two-step approach. First, we transform\nthe text query into a visual query using a generative diffusion model. Then, we\nestimate image-to-image similarity with a vision model. Additionally, we\nintroduce an aggregation network that combines multiple generated images into a\nsingle vector representation and fuses similarity scores across both query\nmodalities. Our approach leverages advancements in vision encoders, VLMs, and\ntext-to-image generation models. Extensive evaluations show that it\nconsistently outperforms retrieval methods relying solely on text queries.\nSource code is available at: https://github.com/faixan-khan/cletir",
        "url": "http://arxiv.org/abs/2509.00177v1",
        "published_date": "2025-08-29T18:24:38+00:00",
        "updated_date": "2025-08-29T18:24:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Faizan Farooq Khan",
            "Vladan Stojnić",
            "Zakaria Laskar",
            "Mohamed Elhoseiny",
            "Giorgos Tolias"
        ],
        "tldr": "The paper proposes a two-step approach to improve category-level text-to-image retrieval by using a diffusion model to transform text queries into visual queries and then using a vision model to estimate image similarity, effectively bridging the modality gap between text and images.",
        "tldr_zh": "该论文提出了一种两步法，通过使用扩散模型将文本查询转换为视觉查询，然后使用视觉模型来估计图像相似度，从而改进类别级文本到图像的检索，有效地弥合了文本和图像之间的模态差距。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments",
        "summary": "Recent advancements in Large Language Models (LLMs) have paved the way for\nVision Large Language Models (VLLMs) capable of performing a wide range of\nvisual understanding tasks. While LLMs have demonstrated impressive performance\non standard natural images, their capabilities have not been thoroughly\nexplored in cluttered datasets where there is complex environment having\ndeformed shaped objects. In this work, we introduce a novel dataset\nspecifically designed for waste classification in real-world scenarios,\ncharacterized by complex environments and deformed shaped objects. Along with\nthis dataset, we present an in-depth evaluation approach to rigorously assess\nthe robustness and accuracy of VLLMs. The introduced dataset and comprehensive\nanalysis provide valuable insights into the performance of VLLMs under\nchallenging conditions. Our findings highlight the critical need for further\nadvancements in VLLM's robustness to perform better in complex environments.\nThe dataset and code for our experiments will be made publicly available.",
        "url": "http://arxiv.org/abs/2509.00176v1",
        "published_date": "2025-08-29T18:22:48+00:00",
        "updated_date": "2025-08-29T18:22:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Muhammad Ali",
            "Salman Khan"
        ],
        "tldr": "The paper introduces Waste-Bench, a new benchmark dataset for evaluating Vision-Language Models (VLLMs) in cluttered, real-world waste classification scenarios. It highlights VLLMs' performance limitations in complex environments and calls for further robustness improvements.",
        "tldr_zh": "该论文介绍了Waste-Bench，一个新的基准数据集，用于评估视觉语言模型（VLLM）在杂乱的现实世界垃圾分类场景中的表现。 它强调了VLLM在复杂环境中的性能限制，并呼吁进一步提高鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment",
        "summary": "Achieving human-like reasoning in deep learning models for complex tasks in\nunknown environments remains a critical challenge in embodied intelligence.\nWhile advanced vision-language models (VLMs) excel in static scene\nunderstanding, their limitations in spatio-temporal reasoning and adaptation to\ndynamic, open-set tasks like task-oriented navigation and embodied question\nanswering (EQA) persist due to inadequate modeling of fine-grained\nspatio-temporal cues and physical world comprehension. To address this, we\npropose VEME, a novel cross-modal alignment method that enhances generalization\nin unseen scenes by learning an ego-centric, experience-centered world model.\nOur framework integrates three key components: (1) a cross-modal alignment\nframework bridging objects, spatial representations, and visual semantics with\nspatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,\nimplicit cognitive map activated by world embedding to enable task-relevant\ngeometric-semantic memory recall; and (3) an instruction-based navigation and\nreasoning framework leveraging embodied priors for long-term planning and\nefficient exploration. By embedding geometry-aware spatio-temporal episodic\nexperiences, our method significantly improves reasoning and planning in\ndynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate\n1%-3% accuracy and exploration efficiency improvement compared to traditional\napproaches.",
        "url": "http://arxiv.org/abs/2509.00210v1",
        "published_date": "2025-08-29T19:47:25+00:00",
        "updated_date": "2025-08-29T19:47:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinzhou Tang",
            "Jusheng zhang",
            "Sidi Liu",
            "Waikit Xiu",
            "Qinhan Lv",
            "Xiying Li"
        ],
        "tldr": "The paper introduces VEME, a cross-modal alignment method that integrates spatio-temporal cues and geometric-semantic world priors into VLMs for improved reasoning and planning in dynamic embodied AI tasks. Experimental results demonstrate a modest improvement in accuracy and exploration efficiency compared to traditional approaches.",
        "tldr_zh": "该论文介绍了VEME，一种跨模态对齐方法，将时空线索和几何语义世界先验知识整合到VLMs中，以提高动态具身AI任务中的推理和规划能力。实验结果表明，与传统方法相比，准确性和探索效率略有提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    }
]