[
    {
        "title": "Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models",
        "summary": "Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.",
        "url": "http://arxiv.org/abs/2601.17918v1",
        "published_date": "2026-01-25T17:36:53+00:00",
        "updated_date": "2026-01-25T17:36:53+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Dain Kim",
            "Jiwoo Lee",
            "Jaehoon Yun",
            "Yong Hoe Koo",
            "Qingyu Chen",
            "Hyunjae Kim",
            "Jaewoo Kang"
        ],
        "tldr": "This paper benchmarks various Direct Preference Optimization (DPO) methods for medical Vision-Language Models (VLMs), identifies limitations, and proposes a targeted preference construction strategy to improve visual question answering.",
        "tldr_zh": "本文对医学视觉语言模型（VLM）的各种直接偏好优化（DPO）方法进行了基准测试，指出了局限性，并提出了一种有针对性的偏好构建策略来改善视觉问答。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation",
        "summary": "Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.\n  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.\n  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.\n  Project website: https://peafowlvla.github.io/.",
        "url": "http://arxiv.org/abs/2601.17885v1",
        "published_date": "2026-01-25T15:29:32+00:00",
        "updated_date": "2026-01-25T15:29:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Qingyu Fan",
            "Zhaoxiang Li",
            "Yi Lu",
            "Wang Chen",
            "Qiu Shen",
            "Xiao-xiao Long",
            "Yinghao Cai",
            "Tao Lu",
            "Shuo Wang",
            "Xun Cao"
        ],
        "tldr": "PEAfowl is a new vision-language-action model for bimanual manipulation that improves spatial reasoning and instruction grounding through depth distribution prediction, 3D lifting, and a Perceiver-style text-aware readout, demonstrating significant performance gains in simulation and real-world robot experiments.",
        "tldr_zh": "PEAfowl 是一种新的双臂操作视觉-语言-动作模型，通过深度分布预测、3D提升和 Perceiver 风格的文本感知读取，提高了空间推理和指令定位能力，并在模拟和现实机器人实验中表现出显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding",
        "summary": "Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.",
        "url": "http://arxiv.org/abs/2601.17868v1",
        "published_date": "2026-01-25T15:02:01+00:00",
        "updated_date": "2026-01-25T15:02:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhihao He",
            "Tieyuan Chen",
            "Kangyu Wang",
            "Ziran Qin",
            "Yang Shao",
            "Chaofan Gan",
            "Shijie Li",
            "Zuxuan Wu",
            "Weiyao Lin"
        ],
        "tldr": "VidLaDA is a Video LLM utilizing bidirectional attention via Diffusion Language Model and MARS-Cache to accelerate video understanding, outperforming diffusion baselines and rivaling autoregressive models with significant speedup.",
        "tldr_zh": "VidLaDA是一个视频大型语言模型，它利用扩散语言模型的双向注意力机制和MARS-Cache来加速视频理解，在性能上优于扩散基线模型，并能与自回归模型相媲美，同时显著提升速度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction",
        "summary": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.",
        "url": "http://arxiv.org/abs/2601.17857v1",
        "published_date": "2026-01-25T14:31:23+00:00",
        "updated_date": "2026-01-25T14:31:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lan Yang",
            "Minghan Yang",
            "Ke Li",
            "Honggang Zhang",
            "Kaiyue Pang",
            "Yi-Zhe Song"
        ],
        "tldr": "The paper introduces SynMind, a framework leveraging grounded VLMs to generate semantic descriptions from fMRI signals, improving image reconstruction by reducing semantic hallucination and outperforming state-of-the-art methods with smaller models.",
        "tldr_zh": "本文介绍了一种名为SynMind的框架，该框架利用接地的VLMs从fMRI信号生成语义描述，通过减少语义幻觉来改进图像重建，并使用更小的模型优于最先进的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning",
        "summary": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.",
        "url": "http://arxiv.org/abs/2601.17818v1",
        "published_date": "2026-01-25T12:47:30+00:00",
        "updated_date": "2026-01-25T12:47:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wen Luo",
            "Peng Chen",
            "Xiaotao Huang",
            "LiQun Huang"
        ],
        "tldr": "The paper introduces ViTCoP, a novel pruning framework for LVLMs that combines vision encoder redundancy filtering with LLM-based co-pruning, achieving state-of-the-art performance with reduced latency and memory consumption, especially at high pruning rates.",
        "tldr_zh": "该论文介绍了ViTCoP，一种用于LVLMs的新型剪枝框架，它结合了视觉编码器冗余过滤和基于LLM的协同剪枝，实现了最先进的性能，并降低了延迟和内存消耗，尤其是在高剪枝率下。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation",
        "summary": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip",
        "url": "http://arxiv.org/abs/2601.17657v1",
        "published_date": "2026-01-25T02:32:01+00:00",
        "updated_date": "2026-01-25T02:32:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taewan Cho",
            "Taeryang Kim",
            "Andrew Jaeyong Choi"
        ],
        "tldr": "SPACE-CLIP introduces a dual-pathway decoder to extract geometric knowledge directly from a frozen CLIP vision encoder for monocular depth estimation, outperforming previous CLIP-based methods. It offers a new spatial perception module for embodied AI systems.",
        "tldr_zh": "SPACE-CLIP 引入了一个双路径解码器，用于直接从冻结的 CLIP 视觉编码器中提取几何知识，以进行单目深度估计，性能优于之前的基于 CLIP 的方法。它为具身人工智能系统提供了一个新的空间感知模块。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting",
        "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.",
        "url": "http://arxiv.org/abs/2601.17666v1",
        "published_date": "2026-01-25T03:07:17+00:00",
        "updated_date": "2026-01-25T03:07:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyue Pan",
            "Yuhao Chen",
            "Fengqing Zhu"
        ],
        "tldr": "The paper introduces Prompt Grafting (PG), a training-free framework that improves the compositional generation of food images by using explicit spatial cues in text prompts to control food separation and reduce object entanglement in text-to-image diffusion models.",
        "tldr_zh": "该论文介绍了一种名为Prompt Grafting (PG)的免训练框架，它通过在文本提示中使用明确的空间线索来控制食物分离，从而改进食物图像的组合生成，并减少文本到图像扩散模型中的对象纠缠。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]