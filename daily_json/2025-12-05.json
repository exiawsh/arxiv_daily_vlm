[
    {
        "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
        "summary": "Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.",
        "url": "http://arxiv.org/abs/2512.04981v1",
        "published_date": "2025-12-04T16:52:45+00:00",
        "updated_date": "2025-12-04T16:52:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "NaHyeon Park",
            "Namin An",
            "Kunhee Kim",
            "Soyeon Yoon",
            "Jiahao Huo",
            "Hyunjung Shim"
        ],
        "tldr": "This paper identifies system prompts in LVLM-based text-to-image models as a key source of social bias and proposes FairPro, a meta-prompting framework to mitigate this bias.",
        "tldr_zh": "该论文指出，基于LVLM的文本到图像模型中的系统提示是社会偏见的主要来源，并提出了FairPro，一个元提示框架来减轻这种偏见。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
        "summary": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.",
        "url": "http://arxiv.org/abs/2512.04810v1",
        "published_date": "2025-12-04T14:01:53+00:00",
        "updated_date": "2025-12-04T14:01:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin He",
            "Longhui Wei",
            "Jianbo Ouyang",
            "Lingxi Xie",
            "Qi Tian"
        ],
        "tldr": "The paper introduces EMMA, a unified and efficient architecture for multimodal understanding, generation, and editing, achieving state-of-the-art performance with fewer parameters compared to existing models.",
        "tldr_zh": "该论文介绍了EMMA，一个统一且高效的多模态理解、生成和编辑架构。与现有模型相比，它以更少的参数实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
        "summary": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.",
        "url": "http://arxiv.org/abs/2512.04763v1",
        "published_date": "2025-12-04T12:56:30+00:00",
        "updated_date": "2025-12-04T12:56:30+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Massimo Bini",
            "Ondrej Bohdal",
            "Umberto Michieli",
            "Zeynep Akata",
            "Mete Ozay",
            "Taha Ceritli"
        ],
        "tldr": "The paper introduces MemLoRA and MemLoRA-V, memory-augmented systems using distilled expert adapters for small language and vision-language models, enabling efficient on-device performance and visual understanding.",
        "tldr_zh": "该论文介绍了MemLoRA和MemLoRA-V，这是一种使用专家适配器蒸馏方法增强小型语言和视觉语言模型内存的系统，从而实现高效的设备端性能和视觉理解。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Cross-View Point Correspondence in Vision-Language Models",
        "summary": "Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of \"perceive\", \"reason\", and \"correspond\". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.",
        "url": "http://arxiv.org/abs/2512.04686v1",
        "published_date": "2025-12-04T11:30:31+00:00",
        "updated_date": "2025-12-04T11:30:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yipu Wang",
            "Yuheng Ji",
            "Yuyang Liu",
            "Enshen Zhou",
            "Ziqiang Yang",
            "Yuxuan Tian",
            "Ziheng Qin",
            "Yue Liu",
            "Huajie Tan",
            "Cheng Chi",
            "Zhiyuan Ma",
            "Daniel Dajun Zeng",
            "Xiaolong Zheng"
        ],
        "tldr": "This paper introduces a new benchmark (CrossPoint-Bench) and dataset (CrossPoint-378K) for evaluating and improving cross-view point correspondence in VLMs, highlighting the current gap between VLMs and human performance and proposing a new model (CroPond) that achieves state-of-the-art results.",
        "tldr_zh": "该论文介绍了一个新的基准测试(CrossPoint-Bench)和数据集(CrossPoint-378K)，用于评估和提高视觉语言模型中的跨视图点对应能力。论文强调了当前视觉语言模型与人类性能之间的差距，并提出了一种新的模型(CroPond)，该模型取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding",
        "summary": "Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2512.04643v1",
        "published_date": "2025-12-04T10:17:20+00:00",
        "updated_date": "2025-12-04T10:17:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Chang-Hsun Wu",
            "Kai-Po Chang",
            "Yu-Yang Sheng",
            "Hung-Kai Chung",
            "Kuei-Chun Wang",
            "Yu-Chiang Frank Wang"
        ],
        "tldr": "The paper introduces SEASON, a training-free method to mitigate temporal hallucination in VideoLLMs by adaptively enhancing temporal and spatial faithfulness via self-diagnostic contrastive decoding.",
        "tldr_zh": "该论文介绍了一种名为SEASON的免训练方法，通过自诊断对比解码自适应地增强时间和空间上的真实性，从而减轻VideoLLM中的时间幻觉问题。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot",
        "summary": "Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.",
        "url": "http://arxiv.org/abs/2512.04599v1",
        "published_date": "2025-12-04T09:18:14+00:00",
        "updated_date": "2025-12-04T09:18:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Hang",
            "Chaoxiang He",
            "Hongsheng Hu",
            "Hanqing Hu",
            "Bin Benjamin Zhu",
            "Shi-Feng Sun",
            "Dawu Gu",
            "Shuo Wang"
        ],
        "tldr": "This paper introduces a zero-shot pipeline for detecting harmful visual content by identifying and localizing malicious elements within images using a vision-language segmentation fusion approach, demonstrating robustness against adversarial attacks.",
        "tldr_zh": "本文介绍了一种零样本管道，通过使用视觉-语言分割融合方法识别和定位图像中的恶意元素，来检测有害视觉内容，并展示了对抗攻击的鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAM3-I: Segment Anything with Instructions",
        "summary": "Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.",
        "url": "http://arxiv.org/abs/2512.04585v1",
        "published_date": "2025-12-04T09:00:25+00:00",
        "updated_date": "2025-12-04T09:00:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingjing Li",
            "Yue Feng",
            "Yuchen Guo",
            "Jincai Huang",
            "Yongri Piao",
            "Qi Bi",
            "Miao Zhang",
            "Xiaoqi Zhao",
            "Qiang Chen",
            "Shihao Zou",
            "Wei Ji",
            "Huchuan Lu",
            "Li Cheng"
        ],
        "tldr": "The paper introduces SAM3-I, an extension of the Segment Anything Model (SAM3) that enables it to directly follow complex natural-language instructions for segmentation, improving upon its original noun-phrase concept-driven capabilities.",
        "tldr_zh": "该论文介绍了SAM3-I，它是Segment Anything Model (SAM3)的扩展，使其能够直接遵循复杂的自然语言指令进行分割，从而改进了其原有的基于名词短语概念驱动的能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
        "summary": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91\\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92\\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
        "url": "http://arxiv.org/abs/2512.04563v1",
        "published_date": "2025-12-04T08:26:04+00:00",
        "updated_date": "2025-12-04T08:26:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zefeng Zhang",
            "Xiangzhao Hao",
            "Hengzhu Tang",
            "Zhenyu Zhang",
            "Jiawei Sheng",
            "Xiaodong Li",
            "Zhenyang Li",
            "Li Gao",
            "Daiting Shi",
            "Dawei Yin",
            "Tingwen Liu"
        ],
        "tldr": "The paper introduces COOPER, a unified MLLM that integrates depth and segmentation to improve spatial reasoning through auxiliary modality generation and adaptive interleaved reasoning, achieving significant performance gains.",
        "tldr_zh": "该论文介绍了COOPER，一个统一的MLLM，它整合了深度和分割信息，通过辅助模态生成和自适应交错推理来提高空间推理能力，并取得了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management",
        "summary": "Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.",
        "url": "http://arxiv.org/abs/2512.04540v1",
        "published_date": "2025-12-04T07:42:13+00:00",
        "updated_date": "2025-12-04T07:42:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongbo Jin",
            "Qingyuan Wang",
            "Wenhao Zhang",
            "Yang Liu",
            "Sijie Cheng"
        ],
        "tldr": "The paper introduces VideoMem, a novel framework for ultra-long video understanding that uses adaptive memory management and a progressive reinforcement learning approach to overcome limitations of VLMs on long videos, showing superior performance on various benchmarks.",
        "tldr_zh": "该论文介绍了VideoMem，一种用于超长视频理解的新框架，它采用自适应内存管理和渐进式强化学习方法，以克服VLM在长视频上的局限性，并在各种基准测试中表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement",
        "summary": "Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.",
        "url": "http://arxiv.org/abs/2512.04532v1",
        "published_date": "2025-12-04T07:28:56+00:00",
        "updated_date": "2025-12-04T07:28:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yu-Wei Zhan",
            "Xin Wang",
            "Hong Chen",
            "Tongtong Feng",
            "Wei Feng",
            "Ren Wang",
            "Guangyao Li",
            "Qing Li",
            "Wenwu Zhu"
        ],
        "tldr": "PhyVLLM introduces a physics-guided video language model that disentangles motion and appearance using a dual-branch encoder and Neural ODE to improve physical reasoning in videos through self-supervised learning.",
        "tldr_zh": "PhyVLLM 引入了一种物理引导的视频语言模型，该模型使用双分支编码器和神经常微分方程来分离运动和外观，通过自监督学习来提高视频中的物理推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning",
        "summary": "The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.",
        "url": "http://arxiv.org/abs/2512.04459v1",
        "published_date": "2025-12-04T05:05:41+00:00",
        "updated_date": "2025-12-04T05:05:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingzi Ma",
            "Yulong Cao",
            "Wenhao Ding",
            "Shuibai Zhang",
            "Yan Wang",
            "Boris Ivanovic",
            "Ming Jiang",
            "Marco Pavone",
            "Chaowei Xiao"
        ],
        "tldr": "This paper introduces dVLM-AD, a diffusion-based vision-language model for autonomous driving that enhances consistency and controllability between reasoning and planning, demonstrating improved performance on nuScenes and WOD-E2E datasets compared to AR-based VLMs.",
        "tldr_zh": "本文介绍了dVLM-AD，一种基于扩散的视觉语言模型，用于自动驾驶，它增强了推理和规划之间的一致性和可控性，并在nuScenes和WOD-E2E数据集上表现出比基于AR的VLM更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios",
        "summary": "As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.",
        "url": "http://arxiv.org/abs/2512.04451v1",
        "published_date": "2025-12-04T04:48:16+00:00",
        "updated_date": "2025-12-04T04:48:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifei Wang",
            "Zhenkai Li",
            "Tianwen Qian",
            "Huanran Zheng",
            "Zheng Wang",
            "Yuqian Fu",
            "Xiaoling Wang"
        ],
        "tldr": "The paper introduces StreamEQA, a new benchmark for evaluating streaming video question answering in embodied scenarios, highlighting the limitations of current video-LLMs in understanding streaming embodied environments.",
        "tldr_zh": "该论文介绍了StreamEQA，一个新的基准，用于评估具身场景中的流视频问答，强调了当前视频LLM在理解流式具身环境方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving",
        "summary": "End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of \"context simulation - candidate generation - multi-objective trade-off\". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned \"what-if\" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.",
        "url": "http://arxiv.org/abs/2512.04441v1",
        "published_date": "2025-12-04T04:16:10+00:00",
        "updated_date": "2025-12-04T04:16:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Suna",
            "Yaoguang Caob",
            "Yan Wanga",
            "Rui Wanga",
            "Jiachen Shanga",
            "Xiejie Fenga",
            "Jiayi Lu",
            "Jia Shi",
            "Shichun Yang",
            "Xiaoyu Yane",
            "Ziying Song"
        ],
        "tldr": "The paper introduces MindDrive, a framework integrating world models and vision-language models for end-to-end autonomous driving, achieving state-of-the-art performance on NAVSIM benchmarks by combining future-aware trajectory generation with VLM-based multi-objective evaluation.",
        "tldr_zh": "该论文介绍了MindDrive，一个集成了世界模型和视觉语言模型的端到端自动驾驶框架。它通过结合未来感知轨迹生成和基于VLM的多目标评估，在NAVSIM基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models",
        "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2512.04395v1",
        "published_date": "2025-12-04T02:32:55+00:00",
        "updated_date": "2025-12-04T02:32:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hieu Dinh Trung Pham",
            "Huy Minh Nhat Nguyen",
            "Cuong Tuan Nguyen"
        ],
        "tldr": "The paper introduces Fourier-Attentive Representation Learning (FARL), a novel framework that disentangles visual representations in VLMs using Fourier analysis to improve few-shot generalization.",
        "tldr_zh": "该论文介绍了傅里叶注意力表示学习（FARL），这是一种新颖的框架，使用傅里叶分析来解开VLMs中的视觉表示，以提高少样本泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
        "summary": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.",
        "url": "http://arxiv.org/abs/2512.04356v1",
        "published_date": "2025-12-04T01:05:16+00:00",
        "updated_date": "2025-12-04T01:05:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Kai-Po Chang",
            "Wei-Yuan Cheng",
            "Chi-Pin Huang",
            "Fu-En Yang",
            "Yu-Chiang Frank Wang"
        ],
        "tldr": "This paper introduces SANTA, a framework that mitigates object and action hallucinations in multimodal LLMs for videos by using self-augmented contrastive alignment to improve faithfulness to visual facts.",
        "tldr_zh": "本文介绍了一个名为SANTA的框架，该框架通过使用自增强对比对齐来提高对视觉事实的忠实度，从而减轻多模态LLM中视频的对象和动作幻觉。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "How (Mis)calibrated is Your Federated CLIP and What To Do About It?",
        "summary": "While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\\text{FL}^2\\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\\text{FL}^2\\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.",
        "url": "http://arxiv.org/abs/2512.04305v1",
        "published_date": "2025-12-03T22:40:19+00:00",
        "updated_date": "2025-12-03T22:40:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mainak Singha",
            "Masih Aminbeidokhti",
            "Paolo Casari",
            "Elisa Ricci",
            "Subhankar Roy"
        ],
        "tldr": "This paper investigates the impact of federated learning on CLIP model calibration, finds that existing methods degrade calibration, and proposes FL^2oRA, a LoRA-based approach that naturally improves calibration in FL settings.",
        "tldr_zh": "该论文研究了联邦学习对CLIP模型校准的影响，发现现有方法会降低校准效果，并提出了FL^2oRA，一种基于LoRA的方法，可以自然地提高联邦学习环境下的模型校准。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models",
        "summary": "Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about \"typical\" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.",
        "url": "http://arxiv.org/abs/2512.04238v1",
        "published_date": "2025-12-03T20:10:22+00:00",
        "updated_date": "2025-12-03T20:10:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Leon Mayer",
            "Piotr Kalinowski",
            "Caroline Ebersbach",
            "Marcel Knopp",
            "Tim Rädsch",
            "Evangelia Christodoulou",
            "Annika Reinke",
            "Fiona R. Kolbinger",
            "Lena Maier-Hein"
        ],
        "tldr": "This paper introduces AdversarialAnatomyBench, a new benchmark to evaluate VLMs' performance on rare anatomical variants, revealing a significant drop in accuracy compared to typical anatomy and highlighting limitations in current VLM generalization.",
        "tldr_zh": "该论文介绍了AdversarialAnatomyBench，一个新的基准，用于评估VLM在罕见解剖变异上的性能，揭示了与典型解剖结构相比，准确性显著下降，并强调了当前VLM泛化能力的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
        "summary": "Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.",
        "url": "http://arxiv.org/abs/2512.04069v1",
        "published_date": "2025-12-03T18:50:04+00:00",
        "updated_date": "2025-12-03T18:50:04+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Siyi Chen",
            "Mikaela Angelina Uy",
            "Chan Hee Song",
            "Faisal Ladhak",
            "Adithyavairavan Murali",
            "Qing Qu",
            "Stan Birchfield",
            "Valts Blukis",
            "Jonathan Tremblay"
        ],
        "tldr": "The paper introduces SpaceTools, a VLM that uses Double Interactive Reinforcement Learning (DIRL) to effectively coordinate multiple tools for enhanced spatial reasoning, achieving state-of-the-art results on spatial understanding benchmarks and demonstrating real-world robotic manipulation.",
        "tldr_zh": "该论文介绍了SpaceTools，一个使用双重交互强化学习（DIRL）的VLM，可以有效地协调多个工具以增强空间推理，在空间理解基准测试中取得了最先进的结果，并展示了真实的机器人操作。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Jina-VLM: Small Multilingual Vision Language Model",
        "summary": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm .",
        "url": "http://arxiv.org/abs/2512.04032v2",
        "published_date": "2025-12-03T18:13:41+00:00",
        "updated_date": "2025-12-04T12:45:29+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Andreas Koukounas",
            "Georgios Mastrapas",
            "Florian Hönicke",
            "Sedigheh Eslami",
            "Guillaume Roncari",
            "Scott Martens",
            "Han Xiao"
        ],
        "tldr": "Jina-VLM is a 2.4B parameter multilingual vision-language model that achieves state-of-the-art performance on multilingual visual question answering among open 2B-scale VLMs, using a SigLIP2 vision encoder and a Qwen3 language backbone.",
        "tldr_zh": "Jina-VLM是一个24亿参数的多语言视觉语言模型，通过使用SigLIP2视觉编码器和Qwen3语言骨干，在开放的2B规模视觉语言模型中实现了最先进的多语言视觉问题解答性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking the Use of Vision Transformers for AI-Generated Image Detection",
        "summary": "Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.",
        "url": "http://arxiv.org/abs/2512.04969v1",
        "published_date": "2025-12-04T16:37:47+00:00",
        "updated_date": "2025-12-04T16:37:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "NaHyeon Park",
            "Kunhee Kim",
            "Junsuk Choe",
            "Hyunjung Shim"
        ],
        "tldr": "The paper analyzes layer-wise features of CLIP-ViT for AI-generated image detection, finding earlier layers more effective and proposing a multi-layer dynamic integration method (MoLD) for improved performance and generalization.",
        "tldr_zh": "该论文分析了 CLIP-ViT 在 AI 生成图像检测中各层特征的作用，发现早期层更有效，并提出了一种多层动态集成方法 (MoLD)，以提高性能和泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization",
        "summary": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.",
        "url": "http://arxiv.org/abs/2512.04952v1",
        "published_date": "2025-12-04T16:21:38+00:00",
        "updated_date": "2025-12-04T16:21:38+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yicheng Liu",
            "Shiduo Zhang",
            "Zibin Dong",
            "Baijun Ye",
            "Tianyuan Yuan",
            "Xiaopeng Yu",
            "Linqi Yin",
            "Chenhao Lu",
            "Junhao Shi",
            "Luca Jiang-Tao Yu",
            "Liangtao Zheng",
            "Tao Jiang",
            "Jingjing Gong",
            "Xipeng Qiu",
            "Hang Zhao"
        ],
        "tldr": "The paper introduces FASTer, a novel framework for efficient autoregressive vision-language-action modeling using a learned tokenizer that improves inference speed and task performance in robotic manipulation tasks.",
        "tldr_zh": "该论文介绍了FASTer，一种新颖的框架，它使用学习到的标记器来实现高效的自回归视觉-语言-动作建模，从而提高了机器人操作任务中的推理速度和任务性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection",
        "summary": "Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.",
        "url": "http://arxiv.org/abs/2512.04875v1",
        "published_date": "2025-12-04T15:05:04+00:00",
        "updated_date": "2025-12-04T15:05:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qing Xu",
            "Yanqian Wang",
            "Xiangjian Hea",
            "Yue Li",
            "Yixuan Zhang",
            "Rong Qu",
            "Wenting Duan",
            "Zhen Chen"
        ],
        "tldr": "The paper proposes SP-Det, a self-prompted detection framework for multi-label lesion detection in chest X-rays, using a dual-text prompt generator and bidirectional feature enhancer to eliminate the need for manual annotations.",
        "tldr_zh": "该论文提出了 SP-Det，一种用于胸部 X 光片多标签病灶检测的自提示检测框架，它使用双文本提示生成器和双向特征增强器，从而消除了对手动标注的需求。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving",
        "summary": "End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.",
        "url": "http://arxiv.org/abs/2512.04733v1",
        "published_date": "2025-12-04T12:17:25+00:00",
        "updated_date": "2025-12-04T12:17:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yihong Tang",
            "Haicheng Liao",
            "Tong Nie",
            "Junlin He",
            "Ao Qu",
            "Kehua Chen",
            "Wei Ma",
            "Zhenning Li",
            "Lijun Sun",
            "Chengzhong Xu"
        ],
        "tldr": "The paper introduces E3AD, an emotion-aware vision-language-action model for autonomous driving that considers passenger emotion via a VAD model and dual-pathway spatial reasoning, demonstrating improved performance in visual grounding, waypoint planning, and emotion estimation.",
        "tldr_zh": "本文介绍了一种名为E3AD的情绪感知视觉-语言-动作模型，用于自动驾驶，通过VAD模型和双路径空间推理来考虑乘客情绪，并在视觉定位、航点规划和情绪估计方面表现出改进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild",
        "summary": "Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.",
        "url": "http://arxiv.org/abs/2512.04728v1",
        "published_date": "2025-12-04T12:13:18+00:00",
        "updated_date": "2025-12-04T12:13:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yigui Feng",
            "Qinglin Wang",
            "Haotian Mo",
            "Yang Liu",
            "Ke Liu",
            "Gencheng Liu",
            "Xinhai Chen",
            "Siqi Shen",
            "Songzhu Mei",
            "Jie Liu"
        ],
        "tldr": "The paper introduces MIND, a disentanglement model and a benchmark dataset (ConvoInsight-DB) with associated evaluation metrics (PRISM) for improved psychological analysis of in-the-wild conversations, specifically addressing the Articulatory-Affective Ambiguity problem in VLMs.",
        "tldr_zh": "该论文介绍了一种解缠模型 MIND，一个基准数据集 (ConvoInsight-DB) 以及相关的评估指标 (PRISM)，用于改进对实际对话中的心理分析，特别解决了 VLM 中的发音-情感歧义问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
        "summary": "Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.",
        "url": "http://arxiv.org/abs/2512.04082v1",
        "published_date": "2025-12-03T18:59:37+00:00",
        "updated_date": "2025-12-03T18:59:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiazhe Wei",
            "Ken Li",
            "Tianyu Lao",
            "Haofan Wang",
            "Liang Wang",
            "Caifeng Shan",
            "Chenyang Si"
        ],
        "tldr": "PosterCopilot introduces a three-stage training strategy for LMMs to improve geometric accuracy and aesthetic reasoning in graphic design layout generation, enabling layer-controllable editing.",
        "tldr_zh": "PosterCopilot提出了一种LMM的三阶段训练策略，以提高图形设计布局生成中的几何精度和审美推理能力，从而实现图层可控的编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
        "summary": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
        "url": "http://arxiv.org/abs/2512.04025v1",
        "published_date": "2025-12-03T18:02:11+00:00",
        "updated_date": "2025-12-03T18:02:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xiaolong Li",
            "Youping Gu",
            "Xi Lin",
            "Weijie Wang",
            "Bohan Zhuang"
        ],
        "tldr": "The paper introduces Pyramid Sparse Attention (PSA), a novel attention mechanism that uses multi-level pooled KV representations for efficient video understanding and generation, achieving better efficiency-quality trade-offs compared to existing sparse attention methods.",
        "tldr_zh": "本文介绍了一种新的金字塔稀疏注意力（PSA）机制，该机制采用多级池化的KV表示，用于高效的视频理解和生成，与现有的稀疏注意力方法相比，实现了更好的效率-质量权衡。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance",
        "summary": "Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.",
        "url": "http://arxiv.org/abs/2512.04511v1",
        "published_date": "2025-12-04T06:45:20+00:00",
        "updated_date": "2025-12-04T06:45:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yinghui Xing",
            "Xiaoting Su",
            "Shizhou Zhang",
            "Donghao Chu",
            "Di Xu"
        ],
        "tldr": "The paper introduces DuGI-MAE, a Dual-domain Guided Infrared foundation model based on MAE, pre-trained on a new large-scale infrared dataset (Inf-590K), to improve performance on infrared image interpretation tasks. It addresses limitations of existing MAEs when applied to infrared data.",
        "tldr_zh": "该论文介绍了DuGI-MAE，一种基于MAE的双域引导红外基础模型，该模型在一个新的大型红外数据集(Inf-590K)上进行预训练，以提高红外图像解释任务的性能。 它解决了现有MAE应用于红外数据时的局限性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]