[
    {
        "title": "PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology",
        "summary": "Accurate analysis of pathological images is essential for automated tumor\ndiagnosis but remains challenging due to high structural similarity and subtle\nmorphological variations in tissue images. Current vision-language (VL) models\noften struggle to capture the complex reasoning required for interpreting\nstructured pathological reports. To address these limitations, we propose\nPathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in\nhierarchical semantic understanding and compositional reasoning within the\npathology domain. Results of this benchmark reveal that existing VL models fail\nto effectively model intricate cross-modal relationships, hence limiting their\napplicability in clinical setting. To overcome this, we further introduce a\npathology-specific VL training scheme that generates enhanced and perturbed\nsamples for multimodal contrastive learning. Experimental evaluations\ndemonstrate that our approach achieves state-of-the-art performance on\nPathoHR-Bench and six additional pathology datasets, highlighting its\neffectiveness in fine-grained pathology representation.",
        "url": "http://arxiv.org/abs/2509.06105v1",
        "published_date": "2025-09-07T15:42:38+00:00",
        "updated_date": "2025-09-07T15:42:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yating Huang",
            "Ziyan Huang",
            "Lintao Xiang",
            "Qijun Yang",
            "Hujun Yin"
        ],
        "tldr": "The paper introduces PathoHR-Bench, a new benchmark for evaluating vision-language models in pathology, and proposes a pathology-specific VL training scheme to improve performance on this benchmark and other pathology datasets.",
        "tldr_zh": "该论文介绍了一个新的病理学视觉-语言模型评估基准PathoHR-Bench，并提出了一种病理学特定的视觉-语言训练方案，以提高模型在该基准和其他病理学数据集上的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge",
        "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.",
        "url": "http://arxiv.org/abs/2509.06079v1",
        "published_date": "2025-09-07T14:47:32+00:00",
        "updated_date": "2025-09-07T14:47:32+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Hao Liang",
            "Ruitao Wu",
            "Bohan Zeng",
            "Junbo Niu",
            "Wentao Zhang",
            "Bin Dong"
        ],
        "tldr": "The paper introduces a caption-assisted reasoning framework that bridges visual and textual modalities, achieving first place in the ICML 2025 SeePhys challenge and demonstrating generalization on the MathVerse benchmark. It addresses the difficulty current VLMs have with multimodal reasoning.",
        "tldr_zh": "该论文介绍了一种基于字幕辅助的推理框架，该框架桥接了视觉和文本模态，在 ICML 2025 SeePhys 挑战赛中获得第一名，并在 MathVerse 基准测试中展示了泛化能力。 它解决了当前 VLM 在多模态推理方面遇到的困难。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance",
        "summary": "Vision-language models have demonstrated impressive capabilities in\ngenerating 2D images under various conditions; however the impressive\nperformance of these models in 2D is largely enabled by extensive, readily\navailable pretrained foundation models. Critically, comparable pretrained\nfoundation models do not exist for 3D, significantly limiting progress in this\ndomain. As a result, the potential of vision-language models to produce\nhigh-resolution 3D counterfactual medical images conditioned solely on natural\nlanguage descriptions remains completely unexplored. Addressing this gap would\nenable powerful clinical and research applications, such as personalized\ncounterfactual explanations, simulation of disease progression scenarios, and\nenhanced medical training by visualizing hypothetical medical conditions in\nrealistic detail. Our work takes a meaningful step toward addressing this\nchallenge by introducing a framework capable of generating high-resolution 3D\ncounterfactual medical images of synthesized patients guided by free-form\nlanguage prompts. We adapt state-of-the-art 3D diffusion models with\nenhancements from Simple Diffusion and incorporate augmented conditioning to\nimprove text alignment and image quality. To our knowledge, this represents the\nfirst demonstration of a language-guided native-3D diffusion model applied\nspecifically to neurological imaging data, where faithful three-dimensional\nmodeling is essential to represent the brain's three-dimensional structure.\nThrough results on two distinct neurological MRI datasets, our framework\nsuccessfully simulates varying counterfactual lesion loads in Multiple\nSclerosis (MS), and cognitive states in Alzheimer's disease, generating\nhigh-quality images while preserving subject fidelity in synthetically\ngenerated medical images. Our results lay the groundwork for prompt-driven\ndisease progression analysis within 3D medical imaging.",
        "url": "http://arxiv.org/abs/2509.05978v1",
        "published_date": "2025-09-07T08:52:18+00:00",
        "updated_date": "2025-09-07T08:52:18+00:00",
        "categories": [
            "eess.IV",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mohamed Mohamed",
            "Brennan Nichyporuk",
            "Douglas L. Arnold",
            "Tal Arbel"
        ],
        "tldr": "This paper introduces a language-guided 3D diffusion model for generating counterfactual medical images, specifically in neurological imaging, demonstrating its application in simulating disease progression for Multiple Sclerosis and Alzheimer's disease.",
        "tldr_zh": "本文介绍了一种语言引导的3D扩散模型，用于生成反事实医学图像，特别是在神经影像领域。该模型展示了其在模拟多发性硬化症和阿尔茨海默病疾病进展中的应用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AttriPrompt: Dynamic Prompt Composition Learning for CLIP",
        "summary": "The evolution of prompt learning methodologies has driven exploration of\ndeeper prompt designs to enhance model performance. However, current deep text\nprompting approaches suffer from two critical limitations: Over-reliance on\nconstrastive learning objectives that prioritize high-level semantic alignment,\nneglecting fine-grained feature optimization; Static prompts across all input\ncategories, preventing content-aware adaptation. To address these limitations,\nwe propose AttriPrompt-a novel framework that enhances and refines textual\nsemantic representations by leveraging the intermediate-layer features of\nCLIP's vision encoder. We designed an Attribute Retrieval module that first\nclusters visual features from each layer. The aggregated visual features\nretrieve semantically similar prompts from a prompt pool, which are then\nconcatenated to the input of every layer in the text encoder. Leveraging\nhierarchical visual information embedded in prompted text features, we\nintroduce Dual-stream Contrastive Learning to realize fine-grained alignment.\nFurthermore, we introduce a Self-Regularization mechanism by applying explicit\nregularization constraints between the prompted and non-prompted text features\nto prevent overfitting on limited training data. Extensive experiments across\nthree benchmarks demonstrate AttriPrompt's superiority over state-of-the-art\nmethods, achieving up to 7.37\\% improvement in the base-to-novel setting. The\nobserved strength of our method in cross-domain knowledge transfer positions\nvision-language pre-trained models as more viable solutions for real-world\nimplementation.",
        "url": "http://arxiv.org/abs/2509.05949v1",
        "published_date": "2025-09-07T07:07:59+00:00",
        "updated_date": "2025-09-07T07:07:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiqi Zhan",
            "Shiwei Li",
            "Qingjie Liu",
            "Yunhong Wang"
        ],
        "tldr": "The paper introduces AttriPrompt, a novel framework that dynamically composes prompts for CLIP by leveraging visual features to enhance fine-grained feature optimization and content-aware adaptation, demonstrating improved performance in vision-language tasks.",
        "tldr_zh": "该论文介绍了 AttriPrompt，一种新颖的框架，通过利用视觉特征动态组合 CLIP 的提示，以增强细粒度的特征优化和内容感知的适应性，并在视觉语言任务中表现出改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models",
        "summary": "Recent deep learning-based methods for lossy image compression achieve\ncompetitive rate-distortion performance through extensive end-to-end training\nand advanced architectures. However, emerging applications increasingly\nprioritize semantic preservation over pixel-level reconstruction and demand\nrobust performance across diverse data distributions and downstream tasks.\nThese challenges call for advanced semantic compression paradigms. Motivated by\nthe zero-shot and representational capabilities of multimodal foundation\nmodels, we propose a novel semantic compression method based on the contrastive\nlanguage-image pretraining (CLIP) model. Rather than compressing images for\nreconstruction, we propose compressing the CLIP feature embeddings into minimal\nbits while preserving semantic information across different tasks. Experiments\nshow that our method maintains semantic integrity across benchmark datasets,\nachieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This\nis less than 5% of the bitrate required by mainstream image compression\napproaches for comparable performance. Remarkably, even under extreme\ncompression, the proposed approach exhibits zero-shot robustness across diverse\ndata distributions and downstream tasks.",
        "url": "http://arxiv.org/abs/2509.05925v1",
        "published_date": "2025-09-07T04:49:25+00:00",
        "updated_date": "2025-09-07T04:49:25+00:00",
        "categories": [
            "cs.CV",
            "cs.IT",
            "math.IT"
        ],
        "authors": [
            "Ruiqi Shen",
            "Haotian Wu",
            "Wenjing Zhang",
            "Jiangjing Hu",
            "Deniz Gunduz"
        ],
        "tldr": "This paper proposes a novel semantic image compression method using CLIP embeddings, achieving significant compression rates while preserving semantic information and maintaining zero-shot robustness across various tasks.",
        "tldr_zh": "本文提出了一种新颖的语义图像压缩方法，该方法使用CLIP嵌入，在保持语义信息的同时实现了显著的压缩率，并在各种任务中保持了零样本鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Analysis of Blood Report Images Using General Purpose Vision-Language Models",
        "summary": "The reliable analysis of blood reports is important for health knowledge, but\nindividuals often struggle with interpretation, leading to anxiety and\noverlooked issues. We explore the potential of general-purpose Vision-Language\nModels (VLMs) to address this challenge by automatically analyzing blood report\nimages. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini\n2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of\n100 diverse blood report images. Each model was prompted with clinically\nrelevant questions adapted to each blood report. The answers were then\nprocessed using Sentence-BERT to compare and evaluate how closely the models\nresponded. The findings suggest that general-purpose VLMs are a practical and\npromising technology for developing patient-facing tools for preliminary blood\nreport analysis. Their ability to provide clear interpretations directly from\nimages can improve health literacy and reduce the limitations to understanding\ncomplex medical information. This work establishes a foundation for the future\ndevelopment of reliable and accessible AI-assisted healthcare applications.\nWhile results are encouraging, they should be interpreted cautiously given the\nlimited dataset size.",
        "url": "http://arxiv.org/abs/2509.06033v1",
        "published_date": "2025-09-07T12:31:16+00:00",
        "updated_date": "2025-09-07T12:31:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nadia Bakhsheshi",
            "Hamid Beigy"
        ],
        "tldr": "This paper explores the use of general-purpose Vision-Language Models (VLMs) for analyzing blood report images, finding them promising for patient-facing tools despite a limited dataset. It evaluates Qwen-VL-Max, Gemini 2.5 Pro, and Llama 4 Maverick.",
        "tldr_zh": "本文探讨了使用通用视觉语言模型 (VLM) 分析血液报告图像，发现它们有希望用于面向患者的工具，但数据集有限。它评估了 Qwen-VL-Max、Gemini 2.5 Pro 和 Llama 4 Maverick。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users",
        "summary": "Visual Question Answering (VQA) holds great potential for assisting Blind and\nLow Vision (BLV) users, yet real-world usage remains challenging. Due to visual\nimpairments, BLV users often take blurry or poorly framed photos and face\ndifficulty in articulating specific questions about what they cannot fully see.\nAs a result, their visual questions are frequently ambiguous, and different\nusers may interpret them in diverse ways. This leads to multiple valid answers,\neach grounded in different image regions-posing a mismatch with conventional\nVQA systems that assume a single answer and region. To bridge this gap, we\npresent BLaVe-CoT, a VQA framework designed to reason about answer consistency\nin the face of ambiguity. Our method proposes diverse candidate answers using a\nLoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer,\nand finally applies a chain-of-thought reasoning module to assess whether the\nanswers refer to the same or different regions. Evaluated on the\nVQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves\nmore robust to the ambiguity and visual noise common in assistive settings.\nThis work highlights the need for VQA systems that can adapt to real human\nuncertainty and provide inclusive support for BLV users. To foster further\nresearch and accessibility applications, we have made the code publicly\navailable at https://github.com/Accecwan/BLaVe-CoT.",
        "url": "http://arxiv.org/abs/2509.06010v1",
        "published_date": "2025-09-07T10:58:17+00:00",
        "updated_date": "2025-09-07T10:58:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wanyin Cheng",
            "Zanxi Ruan"
        ],
        "tldr": "The paper introduces BLaVe-CoT, a VQA framework for Blind and Low Vision (BLV) users that addresses ambiguity in visual questions by generating diverse answers, grounding them spatially, and using chain-of-thought reasoning to assess answer consistency. It outperforms existing methods on the VQA-AnswerTherapy benchmark.",
        "tldr_zh": "该论文介绍了BLaVe-CoT，一个为盲人和低视力（BLV）用户设计的VQA框架，通过生成不同的答案、在空间上定位它们，并使用链式思维推理来评估答案一致性，从而解决视觉问题中的歧义。该框架在VQA-AnswerTherapy基准测试中优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model",
        "summary": "Bi-temporal satellite imagery supports critical applications such as urban\ndevelopment monitoring and disaster assessment. Although powerful multimodal\nlarge language models (MLLMs) have been applied in bi-temporal change analysis,\nprevious methods process image pairs through direct concatenation, inadequately\nmodeling temporal correlations and spatial semantic changes. This deficiency\nhampers visual-semantic alignment in change understanding, thereby constraining\nthe overall effectiveness of current approaches. To address this gap, we\npropose BTCChat, a multi-temporal MLLM with advanced bi-temporal change\nunderstanding capability. BTCChat supports bi-temporal change captioning and\nretains single-image interpretation capability. To better capture temporal\nfeatures and spatial semantic changes in image pairs, we design a Change\nExtraction module. Moreover, to enhance the model's attention to spatial\ndetails, we introduce a Prompt Augmentation mechanism, which incorporates\ncontextual clues into the prompt to enhance model performance. Experimental\nresults demonstrate that BTCChat achieves state-of-the-art performance on\nchange captioning and visual question answering tasks.",
        "url": "http://arxiv.org/abs/2509.05895v1",
        "published_date": "2025-09-07T02:16:18+00:00",
        "updated_date": "2025-09-07T02:16:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujie Li",
            "Wenjia Xu",
            "Yuanben Zhang",
            "Zhiwei Wei",
            "Mugen Peng"
        ],
        "tldr": "The paper introduces BTCChat, a multi-temporal Multimodal Large Language Model (MLLM) designed for bi-temporal remote sensing change captioning, which improves upon existing methods by better modeling temporal correlations and spatial semantic changes using a Change Extraction module and Prompt Augmentation.",
        "tldr_zh": "该论文介绍了BTCChat，一种用于双时相遥感变化描述的多时相多模态大型语言模型（MLLM），通过使用变化提取模块和提示增强，更好地建模时间相关性和空间语义变化，从而改进了现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]