[
    {
        "title": "FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing",
        "summary": "Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE",
        "url": "http://arxiv.org/abs/2601.00785v1",
        "published_date": "2026-01-02T18:40:41+00:00",
        "updated_date": "2026-01-02T18:40:41+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Sunny Gupta",
            "Amit Sethi"
        ],
        "tldr": "FedHypeVAE introduces a differentially private federated learning framework using hypernetworks to generate client-specific conditional VAEs for embedding sharing, addressing non-IID data and gradient leakage issues.",
        "tldr_zh": "FedHypeVAE 提出了一种使用超网络生成客户端特定条件 VAE 的差分隐私联邦学习框架，用于嵌入共享，解决了非 IID 数据和梯度泄漏问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection",
        "summary": "While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.",
        "url": "http://arxiv.org/abs/2601.00777v1",
        "published_date": "2026-01-02T18:17:22+00:00",
        "updated_date": "2026-01-02T18:17:22+00:00",
        "categories": [
            "cs.SD",
            "cs.CV"
        ],
        "authors": [
            "Akanksha Chuchra",
            "Shukesh Reddy",
            "Sudeepta Mishra",
            "Abhijit Das",
            "Abhinav Dhall"
        ],
        "tldr": "This paper investigates the use of Multi-modal Large Language Models (MLLMs) for audio deepfake detection, finding promising results with fine-tuning but poor generalization without it. It suggests a multi-prompt approach combined with audio might be viable.",
        "tldr_zh": "本文研究了多模态大型语言模型（MLLMs）在音频深度伪造检测中的应用，发现通过微调可以获得有希望的结果，但在没有微调的情况下泛化能力较差。它表明，将多提示方法与音频结合使用可能是可行的。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]