[
    {
        "title": "Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations",
        "summary": "Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in\nclinical diagnosis and research, yet its complexity and heterogeneity pose\nchallenges for automated analysis, particularly in scalable and generalizable\nmachine learning applications. While foundation models have revolutionized\nnatural language and vision tasks, their application to MRI remains limited due\nto data scarcity and narrow anatomical focus. In this work, we present\nDecipher-MR, a 3D MRI-specific vision-language foundation model trained on a\nlarge-scale dataset comprising 200,000 MRI series from over 22,000 studies\nspanning diverse anatomical regions, sequences, and pathologies. Decipher-MR\nintegrates self-supervised vision learning with report-guided text supervision\nto build robust, generalizable representations, enabling effective adaptation\nacross broad applications. To enable robust and diverse clinical tasks with\nminimal computational overhead, Decipher-MR supports a modular design that\nenables tuning of lightweight, task-specific decoders attached to a frozen\npretrained encoder. Following this setting, we evaluate Decipher-MR across\ndiverse benchmarks including disease classification, demographic prediction,\nanatomical localization, and cross-modal retrieval, demonstrating consistent\nperformance gains over existing foundation models and task-specific approaches.\nOur results establish Decipher-MR as a scalable and versatile foundation for\nMRI-based AI, facilitating efficient development across clinical and research\ndomains.",
        "url": "http://arxiv.org/abs/2509.21249v1",
        "published_date": "2025-09-25T14:43:33+00:00",
        "updated_date": "2025-09-25T14:43:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zhijian Yang",
            "Noel DSouza",
            "Istvan Megyeri",
            "Xiaojian Xu",
            "Amin Honarmandi Shandiz",
            "Farzin Haddadpour",
            "Krisztian Koos",
            "Laszlo Rusko",
            "Emanuele Valeriano",
            "Bharadwaj Swaninathan",
            "Lei Wu",
            "Parminder Bhatia",
            "Taha Kass-Hout",
            "Erhan Bas"
        ],
        "tldr": "The paper introduces Decipher-MR, a 3D MRI vision-language foundation model trained on a large dataset, demonstrating strong performance across various MRI analysis tasks. It shows promise as a versatile foundation for MRI-based AI.",
        "tldr_zh": "该论文介绍了Decipher-MR，一个基于大型数据集训练的3D MRI视觉-语言基础模型，在各种MRI分析任务中表现出色。它展示了作为MRI人工智能通用基础的潜力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection",
        "summary": "Facing scaling laws, video data from the internet becomes increasingly\nimportant. However, collecting extensive videos that meet specific needs is\nextremely labor-intensive and time-consuming. In this work, we study the way to\nexpedite this collection process and propose VC-Agent, the first interactive\nagent that is able to understand users' queries and feedback, and accordingly\nretrieve/scale up relevant video clips with minimal user input. Specifically,\nconsidering the user interface, our agent defines various user-friendly ways\nfor the user to specify requirements based on textual descriptions and\nconfirmations. As for agent functions, we leverage existing multi-modal large\nlanguage models to connect the user's requirements with the video content. More\nimportantly, we propose two novel filtering policies that can be updated when\nuser interaction is continually performed. Finally, we provide a new benchmark\nfor personalized video dataset collection, and carefully conduct the user study\nto verify our agent's usage in various real scenarios. Extensive experiments\ndemonstrate the effectiveness and efficiency of our agent for customized video\ndataset collection. Project page: https://allenyidan.github.io/vcagent_page/.",
        "url": "http://arxiv.org/abs/2509.21291v1",
        "published_date": "2025-09-25T15:08:28+00:00",
        "updated_date": "2025-09-25T15:08:28+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yidan Zhang",
            "Mutian Xu",
            "Yiming Hao",
            "Kun Zhou",
            "Jiahao Chang",
            "Xiaoqiang Liu",
            "Pengfei Wan",
            "Hongbo Fu",
            "Xiaoguang Han"
        ],
        "tldr": "The paper introduces VC-Agent, an interactive agent leveraging multi-modal large language models to efficiently collect customized video datasets based on user queries and feedback. It includes novel filtering policies and a new benchmark for evaluation.",
        "tldr_zh": "该论文介绍了 VC-Agent，一个交互式代理，利用多模态大型语言模型，基于用户查询和反馈高效收集定制视频数据集。它包括新的过滤策略和一个新的评估基准。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources",
        "summary": "Large multimodal reasoning models have achieved rapid progress, but their\nadvancement is constrained by two major limitations: the absence of open,\nlarge-scale, high-quality long chain-of-thought (CoT) data, and the instability\nof reinforcement learning (RL) algorithms in post-training. Group Relative\nPolicy Optimization (GRPO), the standard framework for RL fine-tuning, is prone\nto gradient vanishing when reward variance is low, which weakens optimization\nsignals and impairs convergence. This work makes three contributions: (1) We\npropose Variance-Aware Sampling (VAS), a data selection strategy guided by\nVariance Promotion Score (VPS) that combines outcome variance and trajectory\ndiversity to promote reward variance and stabilize policy optimization. (2) We\nrelease large-scale, carefully curated resources containing ~1.6M long CoT\ncold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,\nand diversity, along with a fully reproducible end-to-end training codebase.\n(3) We open-source a family of multimodal reasoning models in multiple scales,\nestablishing standardized baselines for the community. Experiments across\nmathematical reasoning benchmarks demonstrate the effectiveness of both the\ncurated data and the proposed VAS. Comprehensive ablation studies and analyses\nprovide further insight into the contributions of each component. In addition,\nwe theoretically establish that reward variance lower-bounds the expected\npolicy gradient magnitude, with VAS serving as a practical mechanism to realize\nthis guarantee. Our code, data, and checkpoints are available at\nhttps://github.com/LengSicong/MMR1.",
        "url": "http://arxiv.org/abs/2509.21268v1",
        "published_date": "2025-09-25T14:58:29+00:00",
        "updated_date": "2025-09-25T14:58:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicong Leng",
            "Jing Wang",
            "Jiaxi Li",
            "Hao Zhang",
            "Zhiqiang Hu",
            "Boqiang Zhang",
            "Yuming Jiang",
            "Hang Zhang",
            "Xin Li",
            "Lidong Bing",
            "Deli Zhao",
            "Wei Lu",
            "Yu Rong",
            "Aixin Sun",
            "Shijian Lu"
        ],
        "tldr": "This paper introduces Variance-Aware Sampling (VAS) to improve multimodal reasoning models, alongside a large-scale open dataset and a family of open-source models, demonstrating effectiveness on mathematical reasoning benchmarks.",
        "tldr_zh": "该论文提出了方差感知采样（VAS）方法来改进多模态推理模型，同时发布了一个大规模的开放数据集和一系列开源模型，并在数学推理基准测试中展示了有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning",
        "summary": "The field of vision-language understanding has been actively researched in\nrecent years, thanks to the development of Large Language Models~(LLMs).\nHowever, it still needs help with problems requiring multi-step reasoning, even\nfor very simple questions. Recent studies adopt LLMs to tackle this problem by\niteratively generating sub-questions and answers. However, there are\ndisadvantages such as 1) the fine-grained visual contents of images are not\navailable using LLMs that cannot read visual information, 2) internal\nmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.\nTo solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,\nwhich improves inference performance by generating image-aware informative\nsub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists\nof a Questioner, Answerer, and Reasoner that share the same architecture.\nQuestioner and Answerer generate sub-questions and sub-answers to help infer\nthe main-question, and Reasoner performs reasoning on the main-question\nconsidering the generated sub-question information. Our experiments show that\nthe proposed method SQ-InstructBLIP, which uses the generated sub-questions as\nadditional information when solving the VQA task, performs more accurate\nreasoning than the previous works.",
        "url": "http://arxiv.org/abs/2509.21251v1",
        "published_date": "2025-09-25T14:45:06+00:00",
        "updated_date": "2025-09-25T14:45:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "You-Won Jang",
            "Yu-Jung Heo",
            "Jaeseok Kim",
            "Minsu Lee",
            "Du-Seong Chang",
            "Byoung-Tak Zhang"
        ],
        "tldr": "The paper introduces SQ-InstructBLIP, a novel framework for multimodal reasoning that iteratively generates image-aware sub-questions and answers to improve performance on Vision Question Answering (VQA) tasks, addressing limitations of black-box LLMs.",
        "tldr_zh": "该论文介绍了SQ-InstructBLIP，一种用于多模态推理的新框架，它迭代地生成图像感知的子问题和答案，从而提高视觉问答（VQA）任务的性能，并解决了黑盒LLM的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Learning to Look: Cognitive Attention Alignment with Vision-Language Models",
        "summary": "Convolutional Neural Networks (CNNs) frequently \"cheat\" by exploiting\nsuperficial correlations, raising concerns about whether they make predictions\nfor the right reasons. Inspired by cognitive science, which highlights the role\nof attention in robust human perception, recent methods have sought to guide\nmodel attention using concept-based supervision and explanation regularization.\nHowever, these techniques depend on labor-intensive, expert-provided\nannotations, limiting their scalability. We propose a scalable framework that\nleverages vision-language models to automatically generate semantic attention\nmaps using natural language prompts. By introducing an auxiliary loss that\naligns CNN attention with these language-guided maps, our approach promotes\nmore reliable and cognitively plausible decision-making without manual\nannotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,\nshow that our method achieves state-of-the-art performance on ColorMNIST and\nremains competitive with annotation-heavy baselines on DecoyMNIST,\ndemonstrating improved generalization, reduced shortcut reliance, and model\nattention that better reflects human intuition.",
        "url": "http://arxiv.org/abs/2509.21247v1",
        "published_date": "2025-09-25T14:40:48+00:00",
        "updated_date": "2025-09-25T14:40:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ryan L. Yang",
            "Dipkamal Bhusal",
            "Nidhi Rastogi"
        ],
        "tldr": "This paper proposes a scalable method to improve CNN attention by aligning it with semantic attention maps generated by vision-language models using natural language prompts, leading to better generalization and reduced shortcut reliance without manual annotation.",
        "tldr_zh": "该论文提出了一种可扩展的方法，通过将CNN注意力与视觉-语言模型使用自然语言提示生成的语义注意力图对齐，来改善CNN注意力，从而在无需手动注释的情况下实现更好的泛化和减少对捷径的依赖。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding",
        "summary": "While table understanding increasingly relies on pixel-only settings where\ntables are processed as visual representations, current benchmarks\npredominantly use synthetic renderings that lack the complexity and visual\ndiversity of real-world tables. Additionally, existing visual table\nunderstanding (VTU) datasets offer fixed examples with single visualizations\nand pre-defined instructions, providing no access to underlying serialized data\nfor reformulation. We introduce TABLET, a large-scale VTU dataset with 4\nmillion examples across 20 tasks, grounded in 2 million unique tables where 88%\npreserve original visualizations. Each example includes paired image-HTML\nrepresentations, comprehensive metadata, and provenance information linking\nback to the source datasets. Fine-tuning vision-language models like\nQwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while\nincreasing robustness on real-world table visualizations. By preserving\noriginal visualizations and maintaining example traceability in a unified\nlarge-scale collection, TABLET establishes a foundation for robust training and\nextensible evaluation of future VTU models.",
        "url": "http://arxiv.org/abs/2509.21205v1",
        "published_date": "2025-09-25T14:14:27+00:00",
        "updated_date": "2025-09-25T14:14:27+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Iñigo Alonso",
            "Imanol Miranda",
            "Eneko Agirre",
            "Mirella Lapata"
        ],
        "tldr": "The paper introduces TABLET, a large-scale visual table understanding dataset with 4 million examples based on real-world tables, designed to improve the robustness of vision-language models in VTU tasks.",
        "tldr_zh": "该论文介绍了一个大规模视觉表格理解数据集TABLET，包含4百万个基于真实世界表格的例子，旨在提高视觉语言模型在VTU任务中的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Human-like Navigation in a World Built for Humans",
        "summary": "When navigating in a man-made environment they haven't visited before--like\nan office building--humans employ behaviors such as reading signs and asking\nothers for directions. These behaviors help humans reach their destinations\nefficiently by reducing the need to search through large areas. Existing robot\nnavigation systems lack the ability to execute such behaviors and are thus\nhighly inefficient at navigating within large environments. We present\nReasonNav, a modular navigation system which integrates these human-like\nnavigation skills by leveraging the reasoning capabilities of a vision-language\nmodel (VLM). We design compact input and output abstractions based on\nnavigation landmarks, allowing the VLM to focus on language understanding and\nreasoning. We evaluate ReasonNav on real and simulated navigation tasks and\nshow that the agent successfully employs higher-order reasoning to navigate\nefficiently in large, complex buildings.",
        "url": "http://arxiv.org/abs/2509.21189v1",
        "published_date": "2025-09-25T14:04:17+00:00",
        "updated_date": "2025-09-25T14:04:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Bhargav Chandaka",
            "Gloria X. Wang",
            "Haozhe Chen",
            "Henry Che",
            "Albert J. Zhai",
            "Shenlong Wang"
        ],
        "tldr": "The paper introduces ReasonNav, a navigation system that leverages a vision-language model to mimic human-like navigation behaviors (reading signs, asking for directions) in complex environments, improving efficiency compared to traditional robot navigation.",
        "tldr_zh": "该论文介绍了ReasonNav，一种利用视觉语言模型模仿人类导航行为（如阅读标志、询问方向）的导航系统，在复杂环境中比传统的机器人导航更有效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
        "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
        "url": "http://arxiv.org/abs/2509.21153v1",
        "published_date": "2025-09-25T13:39:16+00:00",
        "updated_date": "2025-09-25T13:39:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Moshe Kimhi",
            "Erez Koifman",
            "Ehud Rivlin",
            "Eli Schwartz",
            "Chaim Baskin"
        ],
        "tldr": "WAVECLIP introduces wavelet-based tokenization for adaptive-resolution CLIP, enabling efficient inference by dynamically adjusting resolution based on confidence and reusing computations.",
        "tldr_zh": "WAVECLIP引入了基于小波变换的令牌化方法，用于自适应分辨率的CLIP模型，通过基于置信度的动态调整分辨率和重用计算，实现了高效的推理。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning",
        "summary": "Video reasoning has emerged as a critical capability for multimodal large\nlanguage models (MLLMs), requiring models to move beyond static perception\ntoward coherent understanding of temporal dynamics in complex scenes. Yet\nexisting MLLMs often exhibit process inconsistency, where intermediate\nreasoning drifts from video dynamics even when the final answer is correct,\nundermining interpretability and robustness. To address this issue, we\nintroduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time\nWarping (DTW)-based process reward. This rule-based reward aligns reasoning\ntraces with temporally grounded references, enabling efficient process\nsupervision without auxiliary reward models. We further identify dynamic state\nprediction as a key measure of video reasoning and construct MOSS-Video, a\nbenchmark with annotated reasoning traces, where the training split is used to\nfine-tune MOSS-ChatV and the held-out split is reserved for evaluation.\nMOSS-ChatV achieves 87.2\\% on MOSS-Video (test) and improves performance on\ngeneral video benchmarks such as MVBench and MMVU. The framework consistently\nyields gains across different architectures, including Qwen2.5-VL and Phi-2,\nconfirming its broad applicability. Evaluations with GPT-4o-as-judge further\nshow that MOSS-ChatV produces more consistent and stable reasoning traces.",
        "url": "http://arxiv.org/abs/2509.21113v1",
        "published_date": "2025-09-25T12:59:13+00:00",
        "updated_date": "2025-09-25T12:59:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicheng Tao",
            "Jungang Li",
            "Yibo Yan",
            "Junyan Zhang",
            "Yubo Gao",
            "Hanqian Li",
            "ShuHang Xun",
            "Yuxuan Fan",
            "Hong Chen",
            "Jianxiang He",
            "Xuming Hu"
        ],
        "tldr": "The paper introduces MOSS-ChatV, a reinforcement learning framework using a DTW-based process reward, to improve the temporal reasoning consistency of video understanding in MLLMs. It also presents MOSS-Video, a new benchmark for evaluating reasoning traces.",
        "tldr_zh": "该论文介绍了MOSS-ChatV，一个使用基于DTW的过程奖励的强化学习框架，旨在提高MLLM中视频理解的时间推理一致性。此外，它还提出了MOSS-Video，一个新的用于评估推理轨迹的基准。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-Modal Instructions for Robot Motion Generation",
        "summary": "Teaching robots novel behaviors typically requires motion demonstrations via\nteleoperation or kinaesthetic teaching, that is, physically guiding the robot.\nWhile recent work has explored using human sketches to specify desired\nbehaviors, data collection remains cumbersome, and demonstration datasets are\ndifficult to scale. In this paper, we introduce an alternative paradigm,\nLearning from Cross-Modal Instructions, where robots are shaped by\ndemonstrations in the form of rough annotations, which can contain free-form\ntext labels, and are used in lieu of physical motion. We introduce the\nCrossInstruct framework, which integrates cross-modal instructions as examples\ninto the context input to a foundational vision-language model (VLM). The VLM\nthen iteratively queries a smaller, fine-tuned model, and synthesizes the\ndesired motion over multiple 2D views. These are then subsequently fused into a\ncoherent distribution over 3D motion trajectories in the robot's workspace. By\nincorporating the reasoning of the large VLM with a fine-grained pointing\nmodel, CrossInstruct produces executable robot behaviors that generalize beyond\nthe environment of in the limited set of instruction examples. We then\nintroduce a downstream reinforcement learning pipeline that leverages\nCrossInstruct outputs to efficiently learn policies to complete fine-grained\ntasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and\nreal hardware, demonstrating effectiveness without additional fine-tuning and\nproviding a strong initialization for policies subsequently refined via\nreinforcement learning.",
        "url": "http://arxiv.org/abs/2509.21107v1",
        "published_date": "2025-09-25T12:54:00+00:00",
        "updated_date": "2025-09-25T12:54:00+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "William Barron",
            "Xiaoxiang Dong",
            "Matthew Johnson-Roberson",
            "Weiming Zhi"
        ],
        "tldr": "This paper introduces CrossInstruct, a framework that uses cross-modal instructions (text labels) with a Vision-Language Model (VLM) to generate robot motions, which are then refined via reinforcement learning, demonstrating effective performance on simulation and real hardware.",
        "tldr_zh": "本文介绍了CrossInstruct，一个使用跨模态指令（文本标签）和视觉语言模型（VLM）来生成机器人运动的框架，并通过强化学习进行优化，在模拟和真实硬件上表现出有效的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception",
        "summary": "Inducing reasoning in multimodal large language models (MLLMs) is critical\nfor achieving human-level perception and understanding. Existing methods mainly\nleverage LLM reasoning to analyze parsed visuals, often limited by static\nperception stages. This paper introduces Visual Test-Time Scaling (VTTS), a\nnovel approach to enhance MLLMs' reasoning via iterative perception during\ninference. VTTS mimics humans' hierarchical attention by progressively refining\nfocus on high-confidence spatio-temporal regions, guided by updated textual\npredictions. Specifically, VTTS employs an Iterative Perception (ITP)\nmechanism, incorporating reinforcement learning with spatio-temporal\nsupervision to optimize reasoning. To support this paradigm, we also present\nVTTS-80K, a dataset tailored for iterative perception. These designs allows a\nMLLM to enhance its performance by increasing its perceptual compute. Extensive\nexperiments validate VTTS's effectiveness and generalization across diverse\ntasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved\nremarkable improvements, with an average increase of over 5\\%, compared to\nrobust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks\nthat encompass video conversation, video reasoning, and spatio-temporal\nperception.",
        "url": "http://arxiv.org/abs/2509.21100v1",
        "published_date": "2025-09-25T12:46:46+00:00",
        "updated_date": "2025-09-25T12:46:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziang Yan",
            "Xinhao Li",
            "Yinan He",
            "Zhengrong Yue",
            "Xiangyu Zeng",
            "Yali Wang",
            "Yu Qiao",
            "Limin Wang",
            "Yi Wang"
        ],
        "tldr": "The paper introduces Visual Test-Time Scaling (VTTS), an iterative perception mechanism reinforced with spatio-temporal supervision, to enhance multimodal large language models' reasoning capabilities, achieving improved performance across various video-related benchmarks with the Videochat-R1.5 model.",
        "tldr_zh": "该论文介绍了视觉测试时缩放（VTTS），一种通过时空监督增强的迭代感知机制，旨在提高多模态大型语言模型的推理能力。通过Videochat-R1.5模型，在多个视频相关基准测试中取得了显著提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering",
        "summary": "Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual\nQuestion Answering (KB-VQA). Recent methods use large language models (LLMs) as\nknowledge engines for answering. These methods generally employ image captions\nas visual text descriptions to assist LLMs in interpreting images. However, the\ncaptions frequently include excessive noise irrelevant to the question, and\nLLMs generally do not comprehend VQA tasks, limiting their reasoning\ncapabilities. To address this issue, we propose the Summarized Caption-Rerank\nAugmented VQA (SCRA-VQA), which employs a pre-trained visual language model to\nconvert images into captions. Moreover, SCRA-VQA generates contextual examples\nfor the captions while simultaneously summarizing and reordering them to\nexclude unrelated information. The caption-rerank process enables LLMs to\nunderstand the image information and questions better, thus enhancing the\nmodel's reasoning ability and task adaptability without expensive end-to-end\ntraining. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently\non two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving\naccuracies of 38.8% and 34.6%. Our code is available at\nhttps://github.com/HubuKG/SCRA-VQA.",
        "url": "http://arxiv.org/abs/2509.20871v1",
        "published_date": "2025-09-25T08:01:28+00:00",
        "updated_date": "2025-09-25T08:01:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yan Zhang",
            "Jiaqing Lin",
            "Miao Zhang",
            "Kui Xiao",
            "Xiaoju Hou",
            "Yue Zhao",
            "Zhifei Li"
        ],
        "tldr": "The paper introduces SCRA-VQA, a method using summarized and re-ranked captions to augment LLMs for knowledge-based VQA, achieving improved accuracy on OK-VQA and A-OKVQA datasets without end-to-end training.",
        "tldr_zh": "该论文介绍了SCRA-VQA，一种利用总结和重新排序的字幕来增强LLM的知识型VQA方法，在OK-VQA和A-OKVQA数据集上实现了更高的准确率，且无需端到端训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Poisoning Prompt-Guided Sampling in Video Large Language Models",
        "summary": "Video Large Language Models (VideoLLMs) have emerged as powerful tools for\nunderstanding videos, supporting tasks such as summarization, captioning, and\nquestion answering. Their performance has been driven by advances in frame\nsampling, progressing from uniform-based to semantic-similarity-based and, most\nrecently, prompt-guided strategies. While vulnerabilities have been identified\nin earlier sampling strategies, the safety of prompt-guided sampling remains\nunexplored. We close this gap by presenting PoisonVID, the first black-box\npoisoning attack that undermines prompt-guided sampling in VideoLLMs. PoisonVID\ncompromises the underlying prompt-guided sampling mechanism through a\nclosed-loop optimization strategy that iteratively optimizes a universal\nperturbation to suppress harmful frame relevance scores, guided by a depiction\nset constructed from paraphrased harmful descriptions leveraging a shadow\nVideoLLM and a lightweight language model, i.e., GPT-4o-mini. Comprehensively\nevaluated on three prompt-guided sampling strategies and across three advanced\nVideoLLMs, PoisonVID achieves 82% - 99% attack success rate, highlighting the\nimportance of developing future advanced sampling strategies for VideoLLMs.",
        "url": "http://arxiv.org/abs/2509.20851v1",
        "published_date": "2025-09-25T07:40:16+00:00",
        "updated_date": "2025-09-25T07:40:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxin Cao",
            "Wei Song",
            "Jingling Xue",
            "Jin Song Dong"
        ],
        "tldr": "This paper introduces PoisonVID, a black-box poisoning attack against prompt-guided sampling in VideoLLMs, demonstrating its high success rate across various models and sampling strategies.",
        "tldr_zh": "本文介绍了PoisonVID，一种针对视频大型语言模型中提示引导采样的黑盒投毒攻击，证明了其在各种模型和采样策略中的高成功率。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Federated Domain Generalization with Domain-specific Soft Prompts Generation",
        "summary": "Prompt learning has become an efficient paradigm for adapting CLIP to\ndownstream tasks. Compared with traditional fine-tuning, prompt learning\noptimizes a few parameters yet yields highly competitive results, especially\nappealing in federated learning for computational efficiency. engendering\ndomain shift among clients and posing a formidable challenge for\ndownstream-task adaptation. Existing federated domain generalization (FDG)\nmethods based on prompt learning typically learn soft prompts from training\nsamples, replacing manually designed prompts to enhance the generalization\nability of federated models. However, these learned prompts exhibit limited\ndiversity and tend to ignore information from unknown domains. We propose a\nnovel and effective method from a generative perspective for handling FDG\ntasks, namely federated domain generalization with domain-specific soft prompts\ngeneration (FedDSPG). Specifically, during training, we introduce\ndomain-specific soft prompts (DSPs) for each domain and integrate content and\ndomain knowledge into the generative model among clients. In the inference\nphase, the generator is utilized to obtain DSPs for unseen target domains, thus\nguiding downstream tasks in unknown domains. Comprehensive evaluations across\nseveral public datasets confirm that our method outperforms existing strong\nbaselines in FDG, achieving state-of-the-art results.",
        "url": "http://arxiv.org/abs/2509.20807v1",
        "published_date": "2025-09-25T06:41:48+00:00",
        "updated_date": "2025-09-25T06:41:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianhan Wu",
            "Xiaoyang Qu",
            "Zhangcheng Huang",
            "Jianzong Wang"
        ],
        "tldr": "The paper introduces FedDSPG, a federated domain generalization method that uses a generative model to create domain-specific soft prompts for adapting CLIP models to downstream tasks in unseen domains, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了 FedDSPG，一种联邦域泛化方法，它使用生成模型来创建特定于域的软提示，以使 CLIP 模型适应于未见域中的下游任务，并实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On",
        "summary": "We present InstructVTON, an instruction-following interactive virtual try-on\nsystem that allows fine-grained and complex styling control of the resulting\ngeneration, guided by natural language, on single or multiple garments. A\ncomputationally efficient and scalable formulation of virtual try-on formulates\nthe problem as an image-guided or image-conditioned inpainting task. These\ninpainting-based virtual try-on models commonly use a binary mask to control\nthe generation layout. Producing a mask that yields desirable result is\ndifficult, requires background knowledge, might be model dependent, and in some\ncases impossible with the masking-based approach (e.g. trying on a long-sleeve\nshirt with \"sleeves rolled up\" styling on a person wearing long-sleeve shirt\nwith sleeves down, where the mask will necessarily cover the entire sleeve).\nInstructVTON leverages Vision Language Models (VLMs) and image segmentation\nmodels for automated binary mask generation. These masks are generated based on\nuser-provided images and free-text style instructions. InstructVTON simplifies\nthe end-user experience by removing the necessity of a precisely drawn mask,\nand by automating execution of multiple rounds of image generation for try-on\nscenarios that cannot be achieved with masking-based virtual try-on models\nalone. We show that InstructVTON is interoperable with existing virtual try-on\nmodels to achieve state-of-the-art results with styling control.",
        "url": "http://arxiv.org/abs/2509.20524v1",
        "published_date": "2025-09-24T19:52:40+00:00",
        "updated_date": "2025-09-24T19:52:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Julien Han",
            "Shuwen Qiu",
            "Qi Li",
            "Xingzi Xu",
            "Mehmet Saygin Seyfioglu",
            "Kavosh Asadi",
            "Karim Bouyarmane"
        ],
        "tldr": "InstructVTON is an instruction-following virtual try-on system that utilizes VLMs and image segmentation for automated mask generation based on user-provided images and free-text style instructions, overcoming limitations of traditional masking-based approaches.",
        "tldr_zh": "InstructVTON是一个指令跟随的虚拟试穿系统，它利用视觉语言模型和图像分割技术，基于用户提供的图像和自由文本风格指令自动生成掩码，克服了传统基于掩码方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows",
        "summary": "Agentic systems offer a potential path to solve complex clinical tasks\nthrough collaboration among specialized agents, augmented by tool use and\nexternal knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,\nprevailing methods remain limited: (i) reasoning is frequently neither\nclinically interpretable nor aligned with guidelines, reflecting mere\naggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,\nyielding text-only rationales that are not visually grounded; and (iii) systems\nrarely detect or resolve cross-tool inconsistencies and provide no principled\nverification mechanisms. To bridge the above gaps, we present RadAgents, a\nmulti-agent framework for CXR interpretation that couples clinical priors with\ntask-aware multimodal reasoning. In addition, we integrate grounding and\nmultimodal retrieval-augmentation to verify and resolve context conflicts,\nresulting in outputs that are more reliable, transparent, and consistent with\nclinical practice.",
        "url": "http://arxiv.org/abs/2509.20490v1",
        "published_date": "2025-09-24T19:08:01+00:00",
        "updated_date": "2025-09-24T19:08:01+00:00",
        "categories": [
            "cs.MA",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Kai Zhang",
            "Corey D Barrett",
            "Jangwon Kim",
            "Lichao Sun",
            "Tara Taghavi",
            "Krishnaram Kenthapadi"
        ],
        "tldr": "RadAgents is a multi-agent framework for chest X-ray interpretation that uses multimodal reasoning, grounding, and retrieval-augmentation to improve reliability, transparency, and consistency with clinical practice.",
        "tldr_zh": "RadAgents是一个用于胸部X光片解读的多智能体框架，它采用多模态推理、基础和检索增强来提高可靠性、透明度，并与临床实践保持一致。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy",
        "summary": "The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.",
        "url": "http://arxiv.org/abs/2509.21173v1",
        "published_date": "2025-09-25T13:54:34+00:00",
        "updated_date": "2025-09-25T13:54:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Aymen Bouguerra",
            "Daniel Montoya",
            "Alexandra Gomez-Villa",
            "Fabio Arnez",
            "Chokri Mraidha"
        ],
        "tldr": "This paper evaluates the impact of quantization on CLIP models beyond accuracy, finding that it can improve calibration and OOD detection in specific scenarios, especially when using quantization-aware training.",
        "tldr_zh": "本文评估了量化对CLIP模型在精度之外的影响，发现在特定情况下，它可以改善校准和OOD检测，特别是在使用量化感知训练时。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in Vision-Language Models",
        "summary": "Understanding what deep learning (DL) models learn is essential for the safe\ndeployment of artificial intelligence (AI) in clinical settings. While previous\nwork has focused on pixel-based explainability methods, less attention has been\npaid to the textual concepts learned by these models, which may better reflect\nthe reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first\nconcept-based explainability framework for systematically dissecting DL vision\nmodels trained for mammography. Leveraging a mammography-specific\nvision-language model (Mammo-CLIP) as a \"dissector,\" our approach labels\nneurons at specified layers with human-interpretable textual concepts and\nquantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we\ninvestigate three key questions: (1) how concept learning differs between DL\nvision models trained on general image datasets versus mammography-specific\ndatasets; (2) how fine-tuning for downstream mammography tasks affects concept\nspecialisation; and (3) which mammography-relevant concepts remain\nunderrepresented. We show that models trained on mammography data capture more\nclinically relevant concepts and align more closely with radiologists'\nworkflows than models not trained on mammography data. Fine-tuning for\ntask-specific classification enhances the capture of certain concept categories\n(e.g., benign calcifications) but can reduce coverage of others (e.g.,\ndensity-related features), indicating a trade-off between specialisation and\ngeneralisation. Our findings show that Mammo-CLIP Dissect provides insights\ninto how convolutional neural networks (CNNs) capture mammography-specific\nknowledge. By comparing models across training data and fine-tuning regimes, we\nreveal how domain-specific training and task-specific adaptation shape concept\nlearning. Code and concept set are available:\nhttps://github.com/Suaiba/Mammo-CLIP-Dissect.",
        "url": "http://arxiv.org/abs/2509.21102v1",
        "published_date": "2025-09-25T12:47:27+00:00",
        "updated_date": "2025-09-25T12:47:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Suaiba Amina Salahuddin",
            "Teresa Dorszewski",
            "Marit Almenning Martiniussen",
            "Tone Hovda",
            "Antonio Portaluri",
            "Solveig Thrun",
            "Michael Kampffmeyer",
            "Elisabeth Wetzer",
            "Kristoffer Wickstrøm",
            "Robert Jenssen"
        ],
        "tldr": "The paper introduces Mammo-CLIP Dissect, a concept-based explainability framework for mammography DL models using a mammography-specific VLM (Mammo-CLIP) to analyze learned textual concepts and their alignment with domain knowledge, revealing insights into concept learning differences and the impact of fine-tuning.",
        "tldr_zh": "该论文介绍了Mammo-CLIP Dissect，这是一个基于概念的可解释性框架，用于分析乳腺X线摄影深度学习模型。它使用乳腺X线摄影特定的视觉-语言模型(Mammo-CLIP)来分析学习到的文本概念及其与领域知识的对齐，揭示了概念学习的差异以及微调的影响。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos",
        "summary": "The dynamic propagation of social media has broadened the reach of financial\nadvisory content through podcast videos, yet extracting insights from lengthy,\nmultimodal segments (30-40 minutes) remains challenging. We introduce FASTER\n(Financial Advisory Summariser with Textual Embedded Relevant images), a\nmodular framework that tackles three key challenges: (1) extracting\nmodality-specific features, (2) producing optimized, concise summaries, and (3)\naligning visual keyframes with associated textual points. FASTER employs BLIP\nfor semantic visual descriptions, OCR for textual patterns, and Whisper-based\ntranscription with Speaker diarization as BOS features. A modified Direct\nPreference Optimization (DPO)-based loss function, equipped with BOS-specific\nfact-checking, ensures precision, relevance, and factual consistency against\nthe human-aligned summary. A ranker-based retrieval mechanism further aligns\nkeyframes with summarized content, enhancing interpretability and cross-modal\ncoherence. To acknowledge data resource scarcity, we introduce Fin-APT, a\ndataset comprising 470 publicly accessible financial advisory pep-talk videos\nfor robust multimodal research. Comprehensive cross-domain experiments confirm\nFASTER's strong performance, robustness, and generalizability when compared to\nLarge Language Models (LLMs) and Vision-Language Models (VLMs). By establishing\na new standard for multimodal summarization, FASTER makes financial advisory\ncontent more accessible and actionable, thereby opening new avenues for\nresearch. The dataset and code are available at:\nhttps://github.com/sarmistha-D/FASTER",
        "url": "http://arxiv.org/abs/2509.20961v1",
        "published_date": "2025-09-25T09:54:19+00:00",
        "updated_date": "2025-09-25T09:54:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sarmistha Das",
            "R E Zera Marveen Lyngkhoi",
            "Sriparna Saha",
            "Alka Maurya"
        ],
        "tldr": "The paper introduces FASTER, a multimodal summarization framework for financial advisory videos, along with a new dataset, Fin-APT, and demonstrates its superior performance compared to LLMs and VLMs.",
        "tldr_zh": "该论文介绍了一个用于金融咨询视频的多模态摘要框架 FASTER，以及一个新的数据集 Fin-APT，并证明了其优于 LLM 和 VLM 的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ArchGPT: Understanding the World's Architectures with Large Multimodal Models",
        "summary": "Architecture embodies aesthetic, cultural, and historical values, standing as\na tangible testament to human civilization. Researchers have long leveraged\nvirtual reality (VR), mixed reality (MR), and augmented reality (AR) to enable\nimmersive exploration and interpretation of architecture, enhancing\naccessibility, public understanding, and creative workflows around architecture\nin education, heritage preservation, and professional design practice. However,\nexisting VR/MR/AR systems are often developed case-by-case, relying on\nhard-coded annotations and task-specific interactions that do not scale across\ndiverse built environments. In this work, we present ArchGPT, a multimodal\narchitectural visual question answering (VQA) model, together with a scalable\ndata-construction pipeline for curating high-quality, architecture-specific VQA\nannotations. This pipeline yields Arch-300K, a domain-specialized dataset of\napproximately 315,000 image-question-answer triplets. Arch-300K is built via a\nmulti-stage process: first, we curate architectural scenes from Wikimedia\nCommons and filter unconstrained tourist photo collections using a novel\ncoarse-to-fine strategy that integrates 3D reconstruction and semantic\nsegmentation to select occlusion-free, structurally consistent architectural\nimages. To mitigate noise and inconsistency in raw textual metadata, we propose\nan LLM-guided text verification and knowledge-distillation pipeline to generate\nreliable, architecture-specific question-answer pairs. Using these curated\nimages and refined metadata, we further synthesize formal analysis\nannotations-including detailed descriptions and aspect-guided conversations-to\nprovide richer semantic variety while remaining faithful to the data. We\nperform supervised fine-tuning of an open-source multimodal backbone\n,ShareGPT4V-7B, on Arch-300K, yielding ArchGPT.",
        "url": "http://arxiv.org/abs/2509.20858v1",
        "published_date": "2025-09-25T07:49:43+00:00",
        "updated_date": "2025-09-25T07:49:43+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yuze Wang",
            "Luo Yang",
            "Junyi Wang",
            "Yue Qi"
        ],
        "tldr": "The paper introduces ArchGPT, a multimodal VQA model for architectural understanding, trained on a newly created dataset, Arch-300K, built using a novel data curation and annotation pipeline.",
        "tldr_zh": "本文介绍了ArchGPT，一个用于建筑理解的多模态VQA模型，它在一个新创建的数据集Arch-300K上进行训练，该数据集是使用一种新的数据管理和标注流水线构建的。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CaTS-Bench: Can Language Models Describe Numeric Time Series?",
        "summary": "Time series captioning, the task of describing numeric time series in natural\nlanguage, requires numerical reasoning, trend interpretation, and contextual\nunderstanding. Existing benchmarks, however, often rely on synthetic data or\noverly simplistic captions, and typically neglect metadata and visual\nrepresentations. To close this gap, we introduce CaTS-Bench, the first\nlarge-scale, real-world benchmark for Context-aware Time Series captioning.\nCaTS-Bench is derived from 11 diverse datasets reframed as captioning and Q&A\ntasks, comprising roughly 465k training and 105k test timestamps. Each sample\nincludes a numeric series segment, contextual metadata, a line-chart image, and\na caption. A key contribution of this work is the scalable pipeline used to\ngenerate reference captions: while most references are produced by an oracle\nLLM and verified through factual checks, human indistinguishability studies,\nand diversity analyses, we also provide a human-revisited subset of 579 test\ncaptions, refined from LLM outputs to ensure accuracy and human-like style.\nBeyond captioning, CaTS-Bench offers 460 multiple-choice questions targeting\ndeeper aspects of time series reasoning. We further propose new tailored\nevaluation metrics and benchmark leading VLMs, highlighting both their\nstrengths and persistent limitations. Together, these contributions establish\nCaTS-Bench and its captioning pipeline as a reliable and extensible foundation\nfor future research at the intersection of time series analysis and foundation\nmodels.",
        "url": "http://arxiv.org/abs/2509.20823v1",
        "published_date": "2025-09-25T07:10:03+00:00",
        "updated_date": "2025-09-25T07:10:03+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Luca Zhou",
            "Pratham Yashwante",
            "Marshall Fisher",
            "Alessio Sampieri",
            "Zihao Zhou",
            "Fabio Galasso",
            "Rose Yu"
        ],
        "tldr": "The paper introduces CaTS-Bench, a large-scale, real-world benchmark for context-aware time series captioning and Q&A, designed to evaluate and improve VLMs' ability to reason about and describe numeric time series data.",
        "tldr_zh": "该论文介绍了一个名为CaTS-Bench的大规模、真实世界的基准测试，用于上下文感知的时序数据描述和问答，旨在评估和提高VLM对数值时序数据进行推理和描述的能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion",
        "summary": "Recently remarkable progress has been made in synthesizing realistic human\nphotos using text-to-image diffusion models. However, current approaches face\ndegraded scenes, insufficient control, and suboptimal perceptual identity. We\nintroduce CustomEnhancer, a novel framework to augment existing identity\ncustomization models. CustomEnhancer is a zero-shot enhancement pipeline that\nleverages face swapping techniques, pretrained diffusion model, to obtain\nadditional representations in a zeroshot manner for encoding into personalized\nmodels. Through our proposed triple-flow fused PerGeneration approach, which\nidentifies and combines two compatible counter-directional latent spaces to\nmanipulate a pivotal space of personalized model, we unify the generation and\nreconstruction processes, realizing generation from three flows. Our pipeline\nalso enables comprehensive training-free control over the generation process of\npersonalized models, offering precise controlled personalization for them and\neliminating the need for controller retraining for per-model. Besides, to\naddress the high time complexity of null-text inversion (NTI), we introduce\nResInversion, a novel inversion method that performs noise rectification via a\npre-diffusion mechanism, reducing the inversion time by 129 times. Experiments\ndemonstrate that CustomEnhancer reach SOTA results at scene diversity, identity\nfidelity, training-free controls, while also showing the efficiency of our\nResInversion over NTI. The code will be made publicly available upon paper\nacceptance.",
        "url": "http://arxiv.org/abs/2509.20775v1",
        "published_date": "2025-09-25T06:00:34+00:00",
        "updated_date": "2025-09-25T06:00:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Maoye Ren",
            "Praneetha Vaddamanu",
            "Jianjin Xu",
            "Fernando De la Torre Frade"
        ],
        "tldr": "The paper introduces CustomEnhancer, a zero-shot pipeline for enhancing personalized photo generation models via face swapping, a novel latent space manipulation technique, and ResInversion for faster noise rectification, achieving state-of-the-art results in scene diversity, identity fidelity, and training-free controls.",
        "tldr_zh": "该论文介绍了 CustomEnhancer，一个零样本流程，通过人脸交换、一种新颖的潜在空间操纵技术和 ResInversion 来增强个性化照片生成模型，在场景多样性、身份保真度和免训练控制方面实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems",
        "summary": "In this work, we present a retrieval-augmented generation (RAG)-based system\nfor provenance analysis of archaeological artifacts, designed to support expert\nreasoning by integrating multimodal retrieval and large vision-language models\n(VLMs). The system constructs a dual-modal knowledge base from reference texts\nand images, enabling raw visual, edge-enhanced, and semantic retrieval to\nidentify stylistically similar objects. Retrieved candidates are synthesized by\nthe VLM to generate structured inferences, including chronological,\ngeographical, and cultural attributions, alongside interpretive justifications.\nWe evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from\nthe British Museum. Expert evaluation demonstrates that the system produces\nmeaningful and interpretable outputs, offering scholars concrete starting\npoints for analysis and significantly alleviating the cognitive burden of\nnavigating vast comparative corpora.",
        "url": "http://arxiv.org/abs/2509.20769v1",
        "published_date": "2025-09-25T05:52:13+00:00",
        "updated_date": "2025-09-25T05:52:13+00:00",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Tuo Zhang",
            "Yuechun Sun",
            "Ruiliang Liu"
        ],
        "tldr": "The paper presents a RAG-based system leveraging VLMs for provenance analysis of archaeological artifacts, aiding experts by integrating multimodal retrieval and generating structured inferences.",
        "tldr_zh": "该论文提出了一个基于RAG的系统，利用视觉语言模型进行考古文物溯源分析，通过整合多模态检索并生成结构化推论来帮助专家。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning",
        "summary": "Conventional SLAM pipelines for legged robot navigation are fragile under\nrapid motion, calibration demands, and sensor drift, while offering limited\nsemantic reasoning for task-driven exploration. To deal with these issues, we\npropose a vision-only, SLAM-free navigation framework that replaces dense\ngeometry with semantic reasoning and lightweight topological representations. A\nhierarchical vision-language perception module fuses scene-level context with\nobject-level cues for robust semantic inference. And a semantic-probabilistic\ntopological map supports coarse-to-fine planning: LLM-based global reasoning\nfor subgoal selection and vision-based local planning for obstacle avoidance.\nIntegrated with reinforcement-learning locomotion controllers, the framework is\ndeployable across diverse legged robot platforms. Experiments in simulation and\nreal-world settings demonstrate consistent improvements in semantic accuracy,\nplanning quality, and navigation success, while ablation studies further\nshowcase the necessity of both hierarchical perception and fine local planning.\nThis work introduces a new paradigm for SLAM-free, vision-language-driven\nnavigation, shifting robotic exploration from geometry-centric mapping to\nsemantics-driven decision making.",
        "url": "http://arxiv.org/abs/2509.20739v1",
        "published_date": "2025-09-25T04:38:45+00:00",
        "updated_date": "2025-09-25T04:38:45+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Guoyang Zhao",
            "Yudong Li",
            "Weiqing Qi",
            "Kai Zhang",
            "Bonan Liu",
            "Kai Chen",
            "Haoang Li",
            "Jun Ma"
        ],
        "tldr": "This paper presents a SLAM-free visual navigation framework for legged robots that uses hierarchical vision-language perception and coarse-to-fine semantic topological planning for robust and task-driven exploration, validated in simulation and real-world scenarios.",
        "tldr_zh": "本文提出了一种用于腿式机器人的无 SLAM 视觉导航框架，该框架采用分层视觉语言感知和粗到细的语义拓扑规划，以实现稳健且任务驱动的探索，并在模拟和现实环境中进行了验证。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery",
        "summary": "Building-level occupancy after disasters is vital for triage, inspections,\nutility re-energization, and equitable resource allocation. Overhead imagery\nprovides rapid coverage but often misses facade and access cues that determine\nhabitability, while street-view imagery captures those details but is sparse\nand difficult to align with parcels. We present FacadeTrack, a street-level,\nlanguage-guided framework that links panoramic video to parcels, rectifies\nviews to facades, and elicits interpretable attributes (for example, entry\nblockage, temporary coverings, localized debris) that drive two decision\nstrategies: a transparent one-stage rule and a two-stage design that separates\nperception from conservative reasoning. Evaluated across two post-Hurricane\nHelene surveys, the two-stage approach achieves a precision of 0.927, a recall\nof 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a\nprecision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond\naccuracy, intermediate attributes and spatial diagnostics reveal where and why\nresidual errors occur, enabling targeted quality control. The pipeline provides\nauditable, scalable occupancy assessments suitable for integration into\ngeospatial and emergency-management workflows.",
        "url": "http://arxiv.org/abs/2509.20628v1",
        "published_date": "2025-09-25T00:01:38+00:00",
        "updated_date": "2025-09-25T00:01:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Xiao",
            "Archit Gupta",
            "Miguel Esparza",
            "Yu-Hsuan Ho",
            "Antonia Sebastian",
            "Hannah Weas",
            "Rose Houck",
            "Ali Mostafavi"
        ],
        "tldr": "The paper introduces FacadeTrack, a vision-language framework using street view imagery to assess building occupancy after disasters, achieving high precision and recall in post-hurricane surveys.",
        "tldr_zh": "该论文介绍了FacadeTrack，一个利用街景图像和视觉语言模型来评估灾后建筑物占用情况的框架，并在飓风后的调查中实现了较高的精确度和召回率。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Quantized Visual Geometry Grounded Transformer",
        "summary": "Learning-based 3D reconstruction models, represented by Visual Geometry\nGrounded Transformers (VGGTs), have made remarkable progress with the use of\nlarge-scale transformers. Their prohibitive computational and memory costs\nseverely hinder real-world deployment. Post-Training Quantization (PTQ) has\nbecome a common practice for compressing and accelerating models. However, we\nempirically observe that PTQ faces unique obstacles when compressing\nbillion-scale VGGTs: the data-independent special tokens induce heavy-tailed\nactivation distributions, while the multi-view nature of 3D data makes\ncalibration sample selection highly unstable. This paper proposes the first\nQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on two\ntechnical contributions: First, we introduce Dual-Smoothed Fine-Grained\nQuantization, which integrates pre-global Hadamard rotation and post-local\nchannel smoothing to mitigate heavy-tailed distributions and inter-channel\nvariance robustly. Second, we design Noise-Filtered Diverse Sampling, which\nfilters outliers via deep-layer statistics and constructs frame-aware diverse\ncalibration clusters to ensure stable quantization ranges. Comprehensive\nexperiments demonstrate that QuantVGGT achieves the state-of-the-art results\nacross different benchmarks and bit-width, surpassing the previous\nstate-of-the-art generic quantization method with a great margin. We highlight\nthat our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and\n2.5$\\times$ acceleration in real-hardware inference, while maintaining\nreconstruction accuracy above 98\\% of its full-precision counterpart. This\ndemonstrates the vast advantages and practicality of QuantVGGT in\nresource-constrained scenarios. Our code is released in\nhttps://github.com/wlfeng0509/QuantVGGT.",
        "url": "http://arxiv.org/abs/2509.21302v1",
        "published_date": "2025-09-25T15:17:11+00:00",
        "updated_date": "2025-09-25T15:17:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weilun Feng",
            "Haotong Qin",
            "Mingqiang Wu",
            "Chuanguang Yang",
            "Yuqi Li",
            "Xiangqi Li",
            "Zhulin An",
            "Libo Huang",
            "Yulun Zhang",
            "Michele Magno",
            "Yongjun Xu"
        ],
        "tldr": "This paper introduces QuantVGGT, a novel quantization framework for Visual Geometry Grounded Transformers (VGGTs), which significantly reduces memory footprint and accelerates inference while maintaining high reconstruction accuracy, making it suitable for resource-constrained environments.",
        "tldr_zh": "本文介绍了 QuantVGGT，一种用于视觉几何基础 Transformer (VGGT) 的新型量化框架，可显著减少内存占用并加速推理，同时保持较高的重建精度，使其适用于资源受限的环境。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Large Pre-Trained Models for Bimanual Manipulation in 3D",
        "summary": "We investigate the integration of attention maps from a pre-trained Vision\nTransformer into voxel representations to enhance bimanual robotic\nmanipulation. Specifically, we extract attention maps from DINOv2, a\nself-supervised ViT model, and interpret them as pixel-level saliency scores\nover RGB images. These maps are lifted into a 3D voxel grid, resulting in\nvoxel-level semantic cues that are incorporated into a behavior cloning policy.\nWhen integrated into a state-of-the-art voxel-based policy, our\nattention-guided featurization yields an average absolute improvement of 8.2%\nand a relative gain of 21.9% across all tasks in the RLBench bimanual\nbenchmark.",
        "url": "http://arxiv.org/abs/2509.20579v1",
        "published_date": "2025-09-24T21:38:42+00:00",
        "updated_date": "2025-09-24T21:38:42+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Hanna Yurchyk",
            "Wei-Di Chang",
            "Gregory Dudek",
            "David Meger"
        ],
        "tldr": "This paper explores using attention maps from a pre-trained Vision Transformer (DINOv2) to enhance bimanual robotic manipulation by incorporating semantic cues into a voxel-based behavior cloning policy, achieving significant performance improvements on the RLBench benchmark.",
        "tldr_zh": "本文探讨了使用预训练的 Vision Transformer (DINOv2) 的注意力图来增强双臂机器人操作，通过将语义线索整合到基于体素的行为克隆策略中，从而在 RLBench 基准测试中实现了显著的性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]