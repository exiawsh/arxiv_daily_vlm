[
    {
        "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation",
        "summary": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.",
        "url": "http://arxiv.org/abs/2601.01513v1",
        "published_date": "2026-01-04T12:46:35+00:00",
        "updated_date": "2026-01-04T12:46:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Gen Li",
            "Peiyu Liu"
        ],
        "tldr": "The paper introduces VideoSpeculateRAG, a novel and efficient VLM-based RAG framework for video QA, which uses speculative decoding and similarity-based filtering to accelerate inference and improve answer accuracy.",
        "tldr_zh": "该论文介绍了 VideoSpeculateRAG，一种新颖高效的基于 VLM 的 RAG 框架，用于视频问答，它使用推测解码和基于相似性的过滤来加速推理并提高答案准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization",
        "summary": "Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.",
        "url": "http://arxiv.org/abs/2601.01483v1",
        "published_date": "2026-01-04T11:09:33+00:00",
        "updated_date": "2026-01-04T11:09:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyu Qiu",
            "Heng Jia",
            "Zhengwen Zeng",
            "Shuheng Shen",
            "Changhua Meng",
            "Yi Yang",
            "Linchao Zhu"
        ],
        "tldr": "The paper introduces Advantage Decoupled Preference Optimization (ADPO), a reinforcement learning framework for jointly training generation and self-verification within a VLM, improving verification AUC and accuracy while reducing inference time.",
        "tldr_zh": "该论文提出了一种名为优势解耦偏好优化（ADPO）的强化学习框架，用于在视觉语言模型中联合训练生成和自我验证，从而提高验证AUC和准确率，同时减少推理时间。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval",
        "summary": "Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot",
        "url": "http://arxiv.org/abs/2601.01416v1",
        "published_date": "2026-01-04T07:38:51+00:00",
        "updated_date": "2026-01-04T07:38:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Zhou",
            "Ran Ding",
            "Xue Yang",
            "Xue Jiang",
            "Xingzhao Liu"
        ],
        "tldr": "The paper introduces AirSpatialBot, a spatially-aware aerial agent for fine-grained vehicle attribute recognition and retrieval, along with a new dataset, AirSpatial, to address the spatial understanding limitations of existing VLMs in remote sensing.",
        "tldr_zh": "该论文介绍了AirSpatialBot，一个用于细粒度车辆属性识别和检索的空间感知空中智能体，以及一个新的数据集AirSpatial，旨在解决现有VLM在遥感中空间理解的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LinMU: Multimodal Understanding Made Linear",
        "summary": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.",
        "url": "http://arxiv.org/abs/2601.01322v1",
        "published_date": "2026-01-04T01:17:36+00:00",
        "updated_date": "2026-01-04T01:17:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "eess.IV"
        ],
        "authors": [
            "Hongjie Wang",
            "Niraj K. Jha"
        ],
        "tldr": "LinMU presents a linear-complexity VLM architecture using a novel M-MATE block and a three-stage distillation framework to achieve comparable performance to quadratic-complexity VLMs while significantly improving speed and throughput, especially for long-context videos.",
        "tldr_zh": "LinMU提出了一种线性复杂度的VLM架构，使用新型的M-MATE模块和一个三阶段的蒸馏框架，在实现与二次复杂度VLM相当的性能的同时，显著提高了速度和吞吐量，尤其是在长上下文视频方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding",
        "summary": "Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and \"average\" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.",
        "url": "http://arxiv.org/abs/2601.01352v1",
        "published_date": "2026-01-04T03:41:55+00:00",
        "updated_date": "2026-01-04T03:41:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yixuan Lai",
            "He Wang",
            "Kun Zhou",
            "Tianjia Shao"
        ],
        "tldr": "This paper presents a method for generating identity-preserving videos from short reference videos using a diffusion-transformer architecture with a novel identity encoding scheme that captures temporal dynamics.",
        "tldr_zh": "本文提出了一种从短参考视频生成身份保持视频的方法，该方法使用扩散-Transformer架构和一种新颖的身份编码方案来捕获时间动态。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]