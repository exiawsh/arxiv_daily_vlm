[
    {
        "title": "GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration",
        "summary": "Video detailed captioning aims to generate comprehensive video descriptions\nto facilitate video understanding. Recently, most efforts in the video detailed\ncaptioning community have been made towards a local-to-global paradigm, which\nfirst generates local captions from video clips and then summarizes them into a\nglobal caption. However, we find this paradigm leads to less detailed and\ncontextual-inconsistent captions, which can be attributed to (1) no mechanism\nto ensure fine-grained captions, and (2) weak interaction between local and\nglobal captions. To remedy the above two issues, we propose GLaVE-Cap, a\nGlobal-Local aligned framework with Vision Expert integration for Captioning,\nwhich consists of two core modules: TrackFusion enables comprehensive local\ncaption generation, by leveraging vision experts to acquire cross-frame visual\nprompts, coupled with a dual-stream structure; while CaptionBridge establishes\na local-global interaction, by using global context to guide local captioning,\nand adaptively summarizing local captions into a coherent global caption.\nBesides, we construct GLaVE-Bench, a comprehensive video captioning benchmark\nfeaturing 5X more queries per video than existing benchmarks, covering diverse\nvisual dimensions to facilitate reliable evaluation. We further provide a\ntraining dataset GLaVE-1.2M containing 16K high-quality fine-grained video\ncaptions and 1.2M related question-answer pairs. Extensive experiments on four\nbenchmarks show that our GLaVE-Cap achieves state-of-the-art performance.\nBesides, the ablation studies and student model analyses further validate the\neffectiveness of the proposed modules and the contribution of GLaVE-1.2M to the\nvideo understanding community. The source code, model weights, benchmark, and\ndataset will be open-sourced.",
        "url": "http://arxiv.org/abs/2509.11360v1",
        "published_date": "2025-09-14T17:25:55+00:00",
        "updated_date": "2025-09-14T17:25:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wan Xu",
            "Feng Zhu",
            "Yihan Zeng",
            "Yuanfan Guo",
            "Ming Liu",
            "Hang Xu",
            "Wangmeng Zuo"
        ],
        "tldr": "The paper introduces GLaVE-Cap, a novel global-local aligned framework for detailed video captioning with vision expert integration, and a new comprehensive video captioning benchmark and dataset, achieving state-of-the-art results.",
        "tldr_zh": "本文介绍了GLaVE-Cap，一种新颖的全局-局部对齐框架，用于集成视觉专家进行详细的视频字幕生成，并提出了一个新的综合视频字幕生成基准和数据集，实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations",
        "summary": "Large Vision-Language Models (LVLMs) suffer from serious hallucination\nproblems, where the model-generated responses are inconsistent with the visual\ninputs. Existing hallucination mitigation methods are mainly based on\npreference alignment and require external human annotations or auxiliary models\nfor preference data collection, which increase costs and limit sustainable\nimprovement. To tackle these challenges, we propose Autonomous Preference\nAlignment via Self-Injection (APASI), a novel and generalizable method that\nmitigates hallucinations without external dependencies. APASI leverages the\ntarget LVLM to self-inject hallucinations into a generated response, creating a\npair of responses with varying preference levels. During the self-injection\nprocess, the dis-preferred response is generated based on three key\nobservations of hallucinations, ensuring it simulates real hallucination\npatterns. This fidelity offers an accurate learning signal for hallucination\nmitigation. Moreover, APASI incorporates an iterative alignment training\nstrategy combined with curriculum learning to periodically update the\npreference data with increasing challenge, enabling stable and continuous\nenhancement of the LVLM. Extensive experiments across six benchmarks show that\nAPASI not only effectively mitigates hallucinations for three baseline models\nbut also achieves comparable or even superior performance to alignment-based\nmethods with external dependency, thereby demonstrating its effectiveness and\ngeneralization capability. The code is available at\nhttps://github.com/davidluciolu/APASI.",
        "url": "http://arxiv.org/abs/2509.11287v1",
        "published_date": "2025-09-14T14:26:53+00:00",
        "updated_date": "2025-09-14T14:26:53+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yifan Lu",
            "Ziqi Zhang",
            "Chunfeng Yuan",
            "Jun Gao",
            "Congxuan Zhang",
            "Xiaojuan Qi",
            "Bing Li",
            "Weiming Hu"
        ],
        "tldr": "This paper introduces APASI, a novel method that mitigates hallucinations in LVLMs by self-injecting hallucinations to generate preference data for alignment training, achieving comparable or superior performance to methods relying on external data.",
        "tldr_zh": "本文介绍了一种名为APASI的新方法，通过自我注入幻觉来生成偏好数据，用于对齐训练，从而减轻大型视觉语言模型（LVLM）中的幻觉问题，其性能与依赖外部数据的方法相当甚至更好。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge",
        "summary": "This report outlines our approach using vision language model systems for the\nDriving with Language track of the CVPR 2024 Autonomous Grand Challenge. We\nhave exclusively utilized the DriveLM-nuScenes dataset for training our models.\nOur systems are built on the LLaVA models, which we enhanced through\nfine-tuning with the LoRA and DoRA methods. Additionally, we have integrated\ndepth information from open-source depth estimation models to enrich the\ntraining and inference processes. For inference, particularly with\nmultiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning\napproach to improve the accuracy of the results. This comprehensive methodology\nenabled us to achieve a top score of 0.7799 on the validation set leaderboard,\nranking 1st on the leaderboard.",
        "url": "http://arxiv.org/abs/2509.11071v1",
        "published_date": "2025-09-14T03:37:17+00:00",
        "updated_date": "2025-09-14T03:37:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Jinghan Peng",
            "Jingwen Wang",
            "Xing Yu",
            "Dehui Du"
        ],
        "tldr": "The paper describes a vision language model system for the CVPR 2024 Autonomous Grand Challenge, achieving a top score using LLaVA models fine-tuned with LoRA/DoRA and incorporating depth information and Chain-of-Thought reasoning.",
        "tldr_zh": "该论文描述了一个用于CVPR 2024自动驾驶挑战赛的视觉语言模型系统，通过使用LoRA/DoRA微调的LLaVA模型，并结合深度信息和思维链推理，取得了最高分。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]