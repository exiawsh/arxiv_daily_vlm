[
    {
        "title": "Text4Seg++: Advancing Image Segmentation via Generative Language Modeling",
        "summary": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities\nin vision-language tasks. However, effectively integrating image segmentation\ninto these models remains a significant challenge. In this work, we propose a\nnovel text-as-mask paradigm that casts image segmentation as a text generation\nproblem, eliminating the need for additional decoders and significantly\nsimplifying the segmentation process. Our key innovation is semantic\ndescriptors, a new textual representation of segmentation masks where each\nimage patch is mapped to its corresponding text label. We first introduce\nimage-wise semantic descriptors, a patch-aligned textual representation of\nsegmentation masks that integrates naturally into the language modeling\npipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding\n(R-RLE), which compresses redundant text sequences, reducing the length of\nsemantic descriptors by 74% and accelerating inference by $3\\times$, without\ncompromising performance. Building upon this, our initial framework Text4Seg\nachieves strong segmentation performance across a wide range of vision tasks.\nTo further improve granularity and compactness, we propose box-wise semantic\ndescriptors, which localizes regions of interest using bounding boxes and\nrepresents region masks via structured mask tokens called semantic bricks. This\nleads to our refined model, Text4Seg++, which formulates segmentation as a\nnext-brick prediction task, combining precision, scalability, and generative\nefficiency. Comprehensive experiments on natural and remote sensing datasets\nshow that Text4Seg++ consistently outperforms state-of-the-art models across\ndiverse benchmarks without any task-specific fine-tuning, while remaining\ncompatible with existing MLLM backbones. Our work highlights the effectiveness,\nscalability, and generalizability of text-driven image segmentation within the\nMLLM framework.",
        "url": "http://arxiv.org/abs/2509.06321v1",
        "published_date": "2025-09-08T04:07:14+00:00",
        "updated_date": "2025-09-08T04:07:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengcheng Lan",
            "Chaofeng Chen",
            "Jiaxing Xu",
            "Zongrui Li",
            "Yiping Ke",
            "Xudong Jiang",
            "Yingchen Yu",
            "Yunqing Zhao",
            "Song Bai"
        ],
        "tldr": "The paper introduces Text4Seg++, a novel framework that casts image segmentation as text generation within Multimodal Large Language Models (MLLMs), achieving state-of-the-art performance across various datasets without task-specific fine-tuning.",
        "tldr_zh": "该论文介绍了 Text4Seg++，一种新颖的框架，将图像分割转化为多模态大型语言模型（MLLM）中的文本生成，在各种数据集上实现了最先进的性能，而无需针对特定任务进行微调。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Interleaving Reasoning for Better Text-to-Image Generation",
        "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
        "url": "http://arxiv.org/abs/2509.06945v1",
        "published_date": "2025-09-08T17:56:23+00:00",
        "updated_date": "2025-09-08T17:56:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Wenxuan Huang",
            "Shuang Chen",
            "Zheyong Xie",
            "Shaosheng Cao",
            "Shixiang Tang",
            "Yufan Shen",
            "Qingyu Yin",
            "Wenbo Hu",
            "Xiaoman Wang",
            "Yuntian Tang",
            "Junbo Qiao",
            "Yue Guo",
            "Yao Hu",
            "Zhenfei Yin",
            "Philip Torr",
            "Yu Cheng",
            "Wanli Ouyang",
            "Shaohui Lin"
        ],
        "tldr": "The paper introduces Interleaving Reasoning Generation (IRG), a framework that alternates between text-based reasoning and image synthesis, trained with Interleaving Reasoning Generation Learning (IRGL) on a new dataset, achieving state-of-the-art performance in text-to-image generation.",
        "tldr_zh": "本文介绍了一种交错推理生成 (IRG) 框架，该框架在基于文本的推理和图像合成之间交替进行，并使用交错推理生成学习 (IRGL) 在新数据集上进行训练，在文本到图像生成方面实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Curia: A Multi-Modal Foundation Model for Radiology",
        "summary": "AI-assisted radiological interpretation is based on predominantly narrow,\nsingle-task models. This approach is impractical for covering the vast spectrum\nof imaging modalities, diseases, and radiological findings. Foundation models\n(FMs) hold the promise of broad generalization across modalities and in\nlow-data settings. However, this potential has remained largely unrealized in\nradiology. We introduce Curia, a foundation model trained on the entire\ncross-sectional imaging output of a major hospital over several years, which to\nour knowledge is the largest such corpus of real-world data-encompassing\n150,000 exams (130 TB). On a newly curated 19-task external validation\nbenchmark, Curia accurately identifies organs, detects conditions like brain\nhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.\nCuria meets or surpasses the performance of radiologists and recent foundation\nmodels, and exhibits clinically significant emergent properties in\ncross-modality, and low-data regimes. To accelerate progress, we release our\nbase model's weights at https://huggingface.co/raidium/curia.",
        "url": "http://arxiv.org/abs/2509.06830v1",
        "published_date": "2025-09-08T16:04:12+00:00",
        "updated_date": "2025-09-08T16:04:12+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Corentin Dancette",
            "Julien Khlaut",
            "Antoine Saporta",
            "Helene Philippe",
            "Elodie Ferreres",
            "Baptiste Callard",
            "Théo Danielou",
            "Léo Alberge",
            "Léo Machado",
            "Daniel Tordjman",
            "Julie Dupuis",
            "Korentin Le Floch",
            "Jean Du Terrail",
            "Mariam Moshiri",
            "Laurent Dercle",
            "Tom Boeken",
            "Jules Gregory",
            "Maxime Ronot",
            "François Legou",
            "Pascal Roux",
            "Marc Sapoval",
            "Pierre Manceron",
            "Paul Hérent"
        ],
        "tldr": "The paper introduces Curia, a multi-modal foundation model for radiology trained on a large real-world dataset, demonstrating performance at or above radiologist level on various tasks and exhibiting emergent properties.",
        "tldr_zh": "该论文介绍了Curia，一个用于放射学的多模态基础模型，它是在大型真实世界数据集上训练的，在各种任务上表现出与放射科医生相当或更高的水平，并展现出涌现特性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
        "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.",
        "url": "http://arxiv.org/abs/2509.06461v1",
        "published_date": "2025-09-08T09:20:04+00:00",
        "updated_date": "2025-09-08T09:20:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuyao Ge",
            "Shenghua Liu",
            "Yiwei Wang",
            "Lingrui Mei",
            "Baolong Bi",
            "Xuanshan Zhou",
            "Jiayu Yao",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "tldr": "This paper introduces CARVE, a training-free method leveraging contrastive attention to enhance VLMs' visual reasoning by extracting task-relevant visual signals, demonstrating significant performance improvements, particularly in complex visual environments.",
        "tldr_zh": "本文介绍了CARVE，一种无需训练的方法，通过对比注意力来增强视觉语言模型的视觉推理能力，提取与任务相关的视觉信号，并展示了显著的性能提升，尤其是在复杂的视觉环境中。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection",
        "summary": "Muzzle patterns are among the most effective biometric traits for cattle\nidentification. Fast and accurate detection of the muzzle region as the region\nof interest is critical to automatic visual cattle identification.. Earlier\napproaches relied on manual detection, which is labor-intensive and\ninconsistent. Recently, automated methods using supervised models like YOLO\nhave become popular for muzzle detection. Although effective, these methods\nrequire extensive annotated datasets and tend to be trained data-dependent,\nlimiting their performance on new or unseen cattle. To address these\nlimitations, this study proposes a zero-shot muzzle detection framework based\non Grounding DINO, a vision-language model capable of detecting muzzles without\nany task-specific training or annotated data. This approach leverages natural\nlanguage prompts to guide detection, enabling scalable and flexible muzzle\nlocalization across diverse breeds and environments. Our model achieves a mean\nAverage Precision (mAP)@0.5 of 76.8\\%, demonstrating promising performance\nwithout requiring annotated data. To our knowledge, this is the first research\nto provide a real-world, industry-oriented, and annotation-free solution for\ncattle muzzle detection. The framework offers a practical alternative to\nsupervised methods, promising improved adaptability and ease of deployment in\nlivestock monitoring applications.",
        "url": "http://arxiv.org/abs/2509.06427v1",
        "published_date": "2025-09-08T08:21:34+00:00",
        "updated_date": "2025-09-08T08:21:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rabin Dulal",
            "Lihong Zheng",
            "Muhammad Ashad Kabir"
        ],
        "tldr": "The paper introduces a zero-shot cattle muzzle detection framework using Grounding DINO, a vision-language model, achieving promising performance without task-specific training data, offering a practical alternative to supervised methods.",
        "tldr_zh": "该论文介绍了一种使用 Grounding DINO（一种视觉语言模型）的零样本牛鼻检测框架，无需特定任务的训练数据即可实现有希望的性能，为监督方法提供了一种实用的替代方案。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models",
        "summary": "Recent progress in vision-language models (VLMs) has led to impressive\nresults in document understanding tasks, but their high computational demands\nremain a challenge. To mitigate the compute burdens, we propose a lightweight\ntoken pruning framework that filters out non-informative background regions\nfrom document images prior to VLM processing. A binary patch-level classifier\nremoves non-text areas, and a max-pooling refinement step recovers fragmented\ntext regions to enhance spatial coherence. Experiments on real-world document\ndatasets demonstrate that our approach substantially lowers computational\ncosts, while maintaining comparable accuracy.",
        "url": "http://arxiv.org/abs/2509.06415v1",
        "published_date": "2025-09-08T08:12:26+00:00",
        "updated_date": "2025-09-08T08:12:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Jaemin Son",
            "Sujin Choi",
            "Inyong Yun"
        ],
        "tldr": "This paper presents a lightweight token pruning method for vision-language models in document understanding, aiming to reduce computational costs by filtering out non-informative regions while preserving accuracy.",
        "tldr_zh": "本文提出了一种用于文档理解的视觉语言模型中的轻量级令牌修剪方法，旨在通过过滤掉非信息区域来降低计算成本，同时保持准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Harnessing Object Grounding for Time-Sensitive Video Understanding",
        "summary": "We propose to improve the time-sensitive video understanding (TSV) capability\nof video large language models (Video-LLMs) with grounded objects (GO). We\nhypothesize that TSV tasks can benefit from GO within frames, which is\nsupported by our preliminary experiments on LITA, a state-of-the-art Video-LLM\nfor reasoning temporal localization. While augmenting prompts with textual\ndescription of these object annotations improves the performance of LITA, it\nalso introduces extra token length and susceptibility to the noise in object\nlevel information. To address this, we propose GO-Tokenizer, a lightweight\nadd-on module for Video-LLMs leveraging off-the-shelf object detectors to\nencode compact object information on the fly. Experimental results demonstrate\nthat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its\ncounterpart utilizing textual description of objects in the prompt. The gain\ngeneralizes across different models, datasets and video understanding tasks\nsuch as reasoning temporal localization and dense captioning.",
        "url": "http://arxiv.org/abs/2509.06335v1",
        "published_date": "2025-09-08T04:52:00+00:00",
        "updated_date": "2025-09-08T04:52:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tz-Ying Wu",
            "Sharath Nittur Sridhar",
            "Subarna Tripathi"
        ],
        "tldr": "The paper introduces GO-Tokenizer, a lightweight module for Video-LLMs that uses object detection to encode object information compactly, improving time-sensitive video understanding and outperforming text-based object descriptions.",
        "tldr_zh": "该论文介绍了一种名为GO-Tokenizer的轻量级模块，用于视频大语言模型（Video-LLM），该模块利用对象检测来紧凑地编码对象信息，从而提高时间敏感的视频理解能力，并且性能优于基于文本的对象描述。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding",
        "summary": "Visual Grounding (VG) aims to utilize given natural language queries to\nlocate specific target objects within images. While current transformer-based\napproaches demonstrate strong localization performance in standard scene (i.e,\nscenarios without any novel objects), they exhibit notable limitations in\nopen-vocabulary scene (i.e, both familiar and novel object categories during\ntesting). These limitations primarily stem from three key factors: (1)\nimperfect alignment between visual and linguistic modalities, (2) insufficient\ncross-modal feature fusion, and (3) ineffective utilization of semantic\nprototype information. To overcome these challenges, we present Prototype-Aware\nMultimodal Learning (PAML), an innovative framework that systematically\naddresses these issues through several key components: First, we leverage ALBEF\nto establish robust cross-modal alignment during initial feature encoding.\nSubsequently, our Visual Discriminative Feature Encoder selectively enhances\nsalient object representations while suppressing irrelevant visual context. The\nframework then incorporates a novel prototype discovering and inheriting\nmechanism that extracts and aggregates multi-neighbor semantic prototypes to\nfacilitate open-vocabulary recognition. These enriched features undergo\ncomprehensive multimodal integration through our Multi-stage Decoder before\nfinal bounding box regression. Extensive experiments across five benchmark\ndatasets validate our approach, showing competitive performance in standard\nscene while achieving state-of-the-art results in open-vocabulary scene. Our\ncode is available at https://github.com/plankXie/PAML.",
        "url": "http://arxiv.org/abs/2509.06291v1",
        "published_date": "2025-09-08T02:27:10+00:00",
        "updated_date": "2025-09-08T02:27:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangnan Xie",
            "Xiaolong Zheng",
            "Liang Zheng"
        ],
        "tldr": "The paper introduces Prototype-Aware Multimodal Learning (PAML) for open-vocabulary visual grounding, addressing limitations in aligning modalities, fusing features, and utilizing semantic prototypes, achieving state-of-the-art results in open-vocabulary scenarios.",
        "tldr_zh": "本文提出了一种原型感知多模态学习（PAML）方法，用于开放词汇视觉定位，解决了对齐模态、融合特征和利用语义原型方面的局限性，并在开放词汇场景中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning",
        "summary": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
        "url": "http://arxiv.org/abs/2509.06771v1",
        "published_date": "2025-09-08T14:55:16+00:00",
        "updated_date": "2025-09-08T14:55:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sai Kartheek Reddy Kasu",
            "Mohammad Zia Ur Rehman",
            "Shahid Shafi Dar",
            "Rishi Bharat Junghare",
            "Dhanvin Sanjay Namboodiri",
            "Nagendra Kumar"
        ],
        "tldr": "The paper introduces a new dataset of Reddit memes annotated for dark humor and presents a reasoning-augmented framework (TCRNet) using a VLM for dark humor detection, target identification, and intensity prediction.",
        "tldr_zh": "该论文介绍了一个新的Reddit模因数据集，该数据集针对黑色幽默进行了注释，并提出了一个使用VLM的推理增强框架（TCRNet），用于黑色幽默检测、目标识别和强度预测。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis",
        "summary": "Vision foundation models like DINOv2 demonstrate remarkable potential in\nmedical imaging despite their origin in natural image domains. However, their\ndesign inherently works best for uni-modal image analysis, limiting their\neffectiveness for multi-modal imaging tasks that are common in many medical\nfields, such as neurology and oncology. While supervised models perform well in\nthis setting, they fail to leverage unlabeled datasets and struggle with\nmissing modalities, a frequent challenge in clinical settings. To bridge these\ngaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the\npre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our\napproach incorporates multi-modal patch embeddings, enabling vision foundation\nmodels to effectively process multi-modal imaging data. To address missing\nmodalities, we employ full-modality masking, which encourages the model to\nlearn robust cross-modality relationships. Furthermore, we leverage\nsemi-supervised learning to harness large unlabeled datasets, enhancing both\nthe accuracy and reliability of medical predictions. Applied to glioma subtype\nclassification from multi-sequence brain MRI, our method achieves a Matthews\nCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassing\nstate-of-the-art supervised approaches by +11.1%. Our work establishes a\nscalable and robust solution for multi-modal medical imaging tasks, leveraging\npowerful vision foundation models pre-trained on natural images while\naddressing real-world clinical challenges such as missing data and limited\nannotations.",
        "url": "http://arxiv.org/abs/2509.06617v1",
        "published_date": "2025-09-08T12:34:15+00:00",
        "updated_date": "2025-09-08T12:34:15+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Daniel Scholz",
            "Ayhan Can Erdur",
            "Viktoria Ehm",
            "Anke Meyer-Baese",
            "Jan C. Peeken",
            "Daniel Rueckert",
            "Benedikt Wiestler"
        ],
        "tldr": "The paper introduces MM-DINOv2, an adapted DINOv2 foundation model for multi-modal medical image analysis, addressing challenges like missing modalities and limited annotations with multi-modal patch embeddings, full-modality masking, and semi-supervised learning.",
        "tldr_zh": "本文介绍了MM-DINOv2，一种改进的DINOv2基础模型，用于多模态医学图像分析，通过多模态补丁嵌入、全模态掩码和半监督学习来解决诸如模态缺失和有限标注等挑战。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement",
        "summary": "Subject-driven image generation (SDIG) aims to manipulate specific subjects\nwithin images while adhering to textual instructions, a task crucial for\nadvancing text-to-image diffusion models. SDIG requires reconciling the tension\nbetween maintaining subject identity and complying with dynamic edit\ninstructions, a challenge inadequately addressed by existing methods. In this\npaper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,\nwhich resolves this tension through target supervision and preference learning\nwithout test-time fine-tuning. TIDE pioneers target-supervised triplet\nalignment, modelling subject adaptation dynamics using a (reference image,\ninstruction, target images) triplet. This approach leverages the Direct Subject\nDiffusion (DSD) objective, training the model with paired \"winning\" (balanced\npreservation-compliance) and \"losing\" (distorted) targets, systematically\ngenerated and evaluated via quantitative metrics. This enables implicit reward\nmodelling for optimal preservation-compliance balance. Experimental results on\nstandard benchmarks demonstrate TIDE's superior performance in generating\nsubject-faithful outputs while maintaining instruction compliance,\noutperforming baseline methods across multiple quantitative metrics. TIDE's\nversatility is further evidenced by its successful application to diverse\ntasks, including structural-conditioned generation, image-to-image generation,\nand text-image interpolation. Our code is available at\nhttps://github.com/KomJay520/TIDE.",
        "url": "http://arxiv.org/abs/2509.06499v1",
        "published_date": "2025-09-08T10:06:37+00:00",
        "updated_date": "2025-09-08T10:06:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jibai Lin",
            "Bo Ma",
            "Yating Yang",
            "Rong Ma",
            "Turghun Osman",
            "Ahtamjan Ahmat",
            "Rui Dong",
            "Lei Wang",
            "Xi Zhou"
        ],
        "tldr": "The paper introduces TIDE, a framework for subject-driven image generation that balances subject identity preservation with instruction compliance using target supervision and preference learning, achieving state-of-the-art results without test-time fine-tuning.",
        "tldr_zh": "该论文介绍了TIDE，一个用于主体驱动图像生成的框架，通过目标监督和偏好学习来平衡主体身份保持和指令遵循，无需测试时微调即可实现最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM",
        "summary": "Video camouflaged object detection (VCOD) is challenging due to dynamic\nenvironments. Existing methods face two main issues: (1) SAM-based methods\nstruggle to separate camouflaged object edges due to model freezing, and (2)\nMLLM-based methods suffer from poor object separability as large language\nmodels merge foreground and background. To address these issues, we propose a\nnovel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the\nseparability of object edge details, we represent video sequences with temporal\nand spatial clues and perform feature fusion via LLM to increase information\ndensity. Next, multiple cues are generated through the dynamic foreground\nvisual token scoring module and the prompt network to adaptively guide and\nfine-tune the SAM model, enabling it to adapt to subtle textures. To enhance\nthe separability of objects and background, we propose a decoupled\nforeground-background learning strategy. By generating foreground and\nbackground cues separately and performing decoupled training, the visual token\ncan effectively integrate foreground and background information independently,\nenabling SAM to more accurately segment camouflaged objects in the video.\nExperiments on the MoCA-Mask dataset show that Phantom-Insight achieves\nstate-of-the-art performance across various metrics. Additionally, its ability\nto detect unseen camouflaged objects on the CAD2016 dataset highlights its\nstrong generalization ability.",
        "url": "http://arxiv.org/abs/2509.06422v1",
        "published_date": "2025-09-08T08:17:47+00:00",
        "updated_date": "2025-09-08T08:17:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hua Zhang",
            "Changjiang Luo",
            "Ruoyu Chen"
        ],
        "tldr": "The paper introduces Phantom-Insight, a novel VCOD method based on SAM and MLLM that addresses limitations in separating camouflaged object edges and foreground/background by using temporal/spatial cues, dynamic token scoring, and decoupled learning. It achieves state-of-the-art performance on MoCA-Mask and demonstrates generalization on CAD2016.",
        "tldr_zh": "本文介绍了一种新的基于SAM和MLLM的VCOD方法，名为Phantom-Insight，它通过使用时间/空间线索、动态令牌评分和解耦学习，解决了分离伪装对象边缘和前景/背景的局限性。该方法在MoCA-Mask上实现了最先进的性能，并在CAD2016上展示了泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing",
        "summary": "Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain\nperformance by employing vision-language models like CLIP. However, existing\nCLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,\nfailing to detect critical spoofing clues. Moreover, these models rely on a\nsingle text prompt per class (e.g., 'live' or 'fake'), which limits\ngeneralization. To address these issues, we propose MVP-FAS, a novel framework\nincorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text\nPatch Alignment (MTPA). Both modules utilize multiple paraphrased texts to\ngenerate generalized features and reduce dependence on domain-specific text.\nMVS extracts local detailed spatial features and global context from patch\nembeddings by leveraging diverse texts with multiple perspectives. MTPA aligns\npatches with multiple text representations to improve semantic robustness.\nExtensive experiments demonstrate that MVP-FAS achieves superior generalization\nperformance, outperforming previous state-of-the-art methods on cross-domain\ndatasets. Code: https://github.com/Elune001/MVP-FAS.",
        "url": "http://arxiv.org/abs/2509.06336v1",
        "published_date": "2025-09-08T04:53:46+00:00",
        "updated_date": "2025-09-08T04:53:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Jeongmin Yu",
            "Susang Kim",
            "Kisu Lee",
            "Taekyoung Kwon",
            "Won-Yong Shin",
            "Ha Young Kim"
        ],
        "tldr": "The paper introduces MVP-FAS, a novel face anti-spoofing framework leveraging Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA) with paraphrased texts to enhance generalization and outperform existing CLIP-based methods in cross-domain scenarios.",
        "tldr_zh": "该论文介绍了 MVP-FAS，一种新颖的人脸防欺骗框架，利用多视图槽注意力 (MVS) 和多文本补丁对齐 (MTPA) 以及释义文本，以增强泛化能力，并在跨域场景中优于现有的基于 CLIP 的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "summary": "Understanding 3D spatial relationships remains a major limitation of current\nVision-Language Models (VLMs). Prior work has addressed this issue by creating\nspatial question-answering (QA) datasets based on single images or indoor\nvideos. However, real-world embodied AI agents such as robots and self-driving\ncars typically rely on ego-centric, multi-view observations. To this end, we\nintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatial\nreasoning abilities of VLMs using ego-centric, multi-view outdoor data.\nEgo3D-Bench comprises over 8,600 QA pairs, created with significant involvement\nfrom human annotators to ensure quality and diversity. We benchmark 16 SOTA\nVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results\nreveal a notable performance gap between human level scores and VLM\nperformance, highlighting that current VLMs still fall short of human level\nspatial understanding. To bridge this gap, we propose Ego3D-VLM, a\npost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM\ngenerates cognitive map based on estimated global 3D coordinates, resulting in\n12% average improvement on multi-choice QA and 56% average improvement on\nabsolute distance estimation. Ego3D-VLM is modular and can be integrated with\nany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for\nadvancing toward human level spatial understanding in real-world, multi-view\nenvironments.",
        "url": "http://arxiv.org/abs/2509.06266v1",
        "published_date": "2025-09-08T01:08:41+00:00",
        "updated_date": "2025-09-08T01:08:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohsen Gholami",
            "Ahmad Rezaei",
            "Zhou Weimin",
            "Yong Zhang",
            "Mohammad Akbari"
        ],
        "tldr": "This paper introduces Ego3D-Bench, a benchmark for evaluating spatial reasoning in VLMs using ego-centric multi-view data, and Ego3D-VLM, a post-training framework to improve spatial reasoning by generating cognitive maps. They show significant improvement in spatial reasoning tasks.",
        "tldr_zh": "该论文介绍了Ego3D-Bench，一个使用自我中心多视角数据评估VLM空间推理能力的基准，以及Ego3D-VLM，一个通过生成认知地图来提高空间推理能力的后训练框架。实验表明，该方法在空间推理任务中取得了显著的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis",
        "summary": "We investigate how both the adaptation of a generic foundation model via\ntransfer learning and the integration of complementary modalities from the\noperating room (OR) can support surgical data science. To this end, we use\nV-JEPA as the single-modality foundation of a multimodal model for minimally\ninvasive surgery support. We analyze how the model's downstream performance can\nbenefit (a) from finetuning on unlabeled surgical video data and (b) from\nproviding additional time-resolved data streams from the OR in a multimodal\nsetup.\n  In an in-house dataset of liver surgery videos, we analyze the tasks of\npredicting hospital length of stay and postoperative complications. In videos\nof the public HeiCo dataset, we analyze the task of surgical phase recognition.\nAs a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on\nunlabeled, held-out videos to investigate its change in performance after\ndomain adaptation. Following the idea of modular decision support networks, we\nintegrate additional data streams from the OR by training a separate encoder to\nform a shared representation space with V-JEPA's embeddings.\n  Our experiments show that finetuning on domain-specific data increases model\nperformance. On the in-house data, integrating additional time-resolved data\nlikewise benefits the model. On the HeiCo data, accuracy of the pretrained\nvideo-only, single-modality baseline setup is on par with the top-performing\nsubmissions of the EndoVis2017 challenge, while finetuning on domain-specific\ndata increases accuracy further. Our results thus demonstrate how surgical data\nscience can leverage public, generic foundation models. Likewise, they indicate\nthe potential of domain adaptation and of integrating suitable complementary\ndata streams from the OR. To support further research, we release our code and\nmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.",
        "url": "http://arxiv.org/abs/2509.06831v1",
        "published_date": "2025-09-08T16:04:19+00:00",
        "updated_date": "2025-09-08T16:04:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Simon Pezold",
            "Jérôme A. Kurylec",
            "Jan S. Liechti",
            "Beat P. Müller",
            "Joël L. Lavanchy"
        ],
        "tldr": "This paper explores adapting a generic foundation model (V-JEPA) for multimodal surgical data analysis, demonstrating performance improvements through finetuning and integration of additional data streams for tasks like predicting hospital stay and surgical phase recognition. The code and model weights are released.",
        "tldr_zh": "本文探讨了如何调整通用基础模型(V-JEPA)以进行多模态手术数据分析，并通过微调和整合额外的数据流来提高模型性能，应用于预测住院时间和手术阶段识别等任务。代码和模型权重已发布。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''",
        "summary": "We investigated the reproducibility of FairCLIP, proposed by Luo et al.\n(2024), for improving the group fairness of CLIP (Radford et al., 2021) by\nminimizing image-text similarity score disparities across sensitive groups\nusing the Sinkhorn distance. The experimental setup of Luo et al. (2024) was\nreproduced to primarily investigate the research findings for FairCLIP. The\nmodel description by Luo et al. (2024) was found to differ from the original\nimplementation. Therefore, a new implementation, A-FairCLIP, is introduced to\nexamine specific design choices. Furthermore, FairCLIP+ is proposed to extend\nthe FairCLIP objective to include multiple attributes. Additionally, the impact\nof the distance minimization on FairCLIP's fairness and performance was\nexplored. In alignment with the original authors, CLIP was found to be biased\ntowards certain demographics when applied to zero-shot glaucoma classification\nusing medical scans and clinical notes from the Harvard-FairVLMed dataset.\nHowever, the experimental results on two datasets do not support their claim\nthat FairCLIP improves the performance and fairness of CLIP. Although the\nregularization objective reduces Sinkhorn distances, both the official\nimplementation and the aligned implementation, A-FairCLIP, were not found to\nimprove performance nor fairness in zero-shot glaucoma classification.",
        "url": "http://arxiv.org/abs/2509.06535v1",
        "published_date": "2025-09-08T10:41:10+00:00",
        "updated_date": "2025-09-08T10:41:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hua Chang Bakker",
            "Stan Fris",
            "Angela Madelon Bernardy",
            "Stan Deutekom"
        ],
        "tldr": "This paper investigates the reproducibility of FairCLIP, finding discrepancies in the original implementation and demonstrating that neither the original nor a corrected implementation improves performance or fairness in zero-shot glaucoma classification.",
        "tldr_zh": "本文调查了FairCLIP的可重复性，发现原始实现中存在差异，并表明原始实现和修正后的实现均未提高零样本青光眼分类的性能或公平性。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 5,
        "overall_priority_score": 5
    }
]