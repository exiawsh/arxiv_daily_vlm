[
    {
        "title": "M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG",
        "summary": "Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.",
        "url": "http://arxiv.org/abs/2512.05959v1",
        "published_date": "2025-12-05T18:55:58+00:00",
        "updated_date": "2025-12-05T18:55:58+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "David Anugraha",
            "Patrick Amadeus Irawan",
            "Anshul Singh",
            "En-Shiun Annie Lee",
            "Genta Indra Winata"
        ],
        "tldr": "The paper introduces M4-RAG, a large multilingual, multi-cultural, and multimodal benchmark for evaluating retrieval-augmented VQA, and finds that RAG benefits smaller VLMs but degrades performance in larger ones.",
        "tldr_zh": "该论文介绍了M4-RAG，一个大型多语言、多文化、多模态的基准，用于评估检索增强的VQA，并发现RAG有利于较小的VLM，但会降低较大VLM的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io",
        "url": "http://arxiv.org/abs/2512.05955v1",
        "published_date": "2025-12-05T18:51:03+00:00",
        "updated_date": "2025-12-05T18:51:03+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haowen Liu",
            "Shaoxiong Yao",
            "Haonan Chen",
            "Jiawei Gao",
            "Jiayuan Mao",
            "Jia-Bin Huang",
            "Yilun Du"
        ],
        "tldr": "SIMPACT enhances Vision-Language Models (VLMs) with physical reasoning by incorporating simulation-in-the-loop for action planning in robotic manipulation tasks, achieving state-of-the-art performance without additional training.",
        "tldr_zh": "SIMPACT通过在机器人操作任务中加入循环模拟，增强了视觉语言模型(VLM)的物理推理能力，从而进行动作规划，无需额外训练即可实现最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]