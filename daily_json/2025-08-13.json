[
    {
        "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer",
        "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
        "url": "http://arxiv.org/abs/2508.09131v1",
        "published_date": "2025-08-12T17:57:04+00:00",
        "updated_date": "2025-08-12T17:57:04+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zixin Yin",
            "Xili Dai",
            "Ling-Hao Chen",
            "Deyu Zhou",
            "Jianan Wang",
            "Duomin Wang",
            "Gang Yu",
            "Lionel M. Ni",
            "Heung-Yeung Shum"
        ],
        "tldr": "The paper introduces ColorCtrl, a training-free method for text-guided color editing using Multi-Modal Diffusion Transformers (MM-DiT), achieving state-of-the-art performance in edit quality and consistency compared to existing methods and commercial models.",
        "tldr_zh": "该论文介绍了一种名为ColorCtrl的免训练方法，使用多模态扩散Transformer（MM-DiT）进行文本引导的颜色编辑，与现有方法和商业模型相比，在编辑质量和一致性方面实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenCUA: Open Foundations for Computer-Use Agents",
        "summary": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.",
        "url": "http://arxiv.org/abs/2508.09123v1",
        "published_date": "2025-08-12T17:52:32+00:00",
        "updated_date": "2025-08-12T17:52:32+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xinyuan Wang",
            "Bowen Wang",
            "Dunjie Lu",
            "Junlin Yang",
            "Tianbao Xie",
            "Junli Wang",
            "Jiaqi Deng",
            "Xiaole Guo",
            "Yiheng Xu",
            "Chen Henry Wu",
            "Zhennan Shen",
            "Zhuokai Li",
            "Ryan Li",
            "Xiaochuan Li",
            "Junda Chen",
            "Boyuan Zheng",
            "Peihang Li",
            "Fangyu Lei",
            "Ruisheng Cao",
            "Yeqiao Fu",
            "Dongchan Shin",
            "Martin Shin",
            "Jiarui Hu",
            "Yuyan Wang",
            "Jixuan Chen",
            "Yuxiao Ye",
            "Danyang Zhang",
            "Dikang Du",
            "Hao Hu",
            "Huarong Chen",
            "Zaida Zhou",
            "Yipu Wang",
            "Heng Wang",
            "Diyi Yang",
            "Victor Zhong",
            "Flood Sung",
            "Y. Charles",
            "Zhilin Yang",
            "Tao Yu"
        ],
        "tldr": "The paper introduces OpenCUA, an open-source framework for computer-use agents (CUAs), including data, models, and tools, and demonstrates SOTA performance surpassing OpenAI's CUA on the OSWorld-Verified benchmark.",
        "tldr_zh": "该论文介绍了OpenCUA，一个用于计算机使用代理（CUA）的开源框架，包括数据、模型和工具，并在OSWorld-Verified基准测试中展示了超越OpenAI的CUA的SOTA性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable success on multimodal\ntasks such as image-text retrieval and zero-shot classification, yet they can\nexhibit demographic biases even when explicit protected attributes are absent\nduring training. In this work, we focus on automated glaucoma screening from\nretinal fundus images, a critical application given that glaucoma is a leading\ncause of irreversible blindness and disproportionately affects underserved\npopulations. Building on a reweighting-based contrastive learning framework, we\nintroduce an attribute-agnostic debiasing method that (i) infers proxy\nsubgroups via unsupervised clustering of image-image embeddings, (ii) computes\ngradient-similarity weights between the CLIP-style multimodal loss and a\nSimCLR-style image-pair contrastive loss, and (iii) applies these weights in a\njoint, top-$k$ weighted objective to upweight underperforming clusters. This\nlabel-free approach adaptively targets the hardest examples, thereby reducing\nsubgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma\nsubset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES\nAUC), and Groupwise AUC to demonstrate equitable performance across inferred\ndemographic subgroups.",
        "url": "http://arxiv.org/abs/2508.09087v1",
        "published_date": "2025-08-12T17:07:58+00:00",
        "updated_date": "2025-08-12T17:07:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ahsan Habib Akash",
            "Greg Murray",
            "Annahita Amireskandari",
            "Joel Palko",
            "Carol Laxson",
            "Binod Bhattarai",
            "Prashnna Gyawali"
        ],
        "tldr": "The paper presents a method to address demographic biases in VLMs for glaucoma detection without requiring protected attribute labels, using unsupervised clustering and gradient-similarity reweighting.",
        "tldr_zh": "该论文提出了一种解决VLM在青光眼检测中人口统计学偏差的方法，该方法无需受保护的属性标签，而是使用无监督聚类和梯度相似性重加权。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception",
        "summary": "Open-set perception in complex traffic environments poses a critical\nchallenge for autonomous driving systems, particularly in identifying\npreviously unseen object categories, which is vital for ensuring safety. Visual\nLanguage Models (VLMs), with their rich world knowledge and strong semantic\nreasoning capabilities, offer new possibilities for addressing this task.\nHowever, existing approaches typically leverage VLMs to extract visual features\nand couple them with traditional object detectors, resulting in multi-stage\nerror propagation that hinders perception accuracy. To overcome this\nlimitation, we propose VLM-3D, the first end-to-end framework that enables VLMs\nto perform 3D geometric perception in autonomous driving scenarios. VLM-3D\nincorporates Low-Rank Adaptation (LoRA) to efficiently adapt VLMs to driving\ntasks with minimal computational overhead, and introduces a joint\nsemantic-geometric loss design: token-level semantic loss is applied during\nearly training to ensure stable convergence, while 3D IoU loss is introduced in\nlater stages to refine the accuracy of 3D bounding box predictions. Evaluations\non the nuScenes dataset demonstrate that the proposed joint semantic-geometric\nloss in VLM-3D leads to a 12.8% improvement in perception accuracy, fully\nvalidating the effectiveness and advancement of our method.",
        "url": "http://arxiv.org/abs/2508.09061v1",
        "published_date": "2025-08-12T16:25:27+00:00",
        "updated_date": "2025-08-12T16:25:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fuhao Chang",
            "Shuxin Li",
            "Yabei Li",
            "Lei He"
        ],
        "tldr": "VLM-3D is an end-to-end framework that uses VLMs for 3D geometric perception in autonomous driving, utilizing LoRA and a joint semantic-geometric loss function, demonstrating a significant improvement in perception accuracy on the nuScenes dataset.",
        "tldr_zh": "VLM-3D是一个端到端的框架，它使用视觉语言模型进行自动驾驶中的3D几何感知，利用LoRA和联合语义-几何损失函数，并在nuScenes数据集上展示了感知精度的显著提高。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Per-Query Visual Concept Learning",
        "summary": "Visual concept learning, also known as Text-to-image personalization, is the\nprocess of teaching new concepts to a pretrained model. This has numerous\napplications from product placement to entertainment and personalized design.\nHere we show that many existing methods can be substantially augmented by\nadding a personalization step that is (1) specific to the prompt and noise\nseed, and (2) using two loss terms based on the self- and cross- attention,\ncapturing the identity of the personalized concept. Specifically, we leverage\nPDM features - previously designed to capture identity - and show how they can\nbe used to improve personalized semantic similarity. We evaluate the benefit\nthat our method gains on top of six different personalization methods, and\nseveral base text-to-image models (both UNet- and DiT-based). We find\nsignificant improvements even over previous per-query personalization methods.",
        "url": "http://arxiv.org/abs/2508.09045v1",
        "published_date": "2025-08-12T16:07:27+00:00",
        "updated_date": "2025-08-12T16:07:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ori Malca",
            "Dvir Samuel",
            "Gal Chechik"
        ],
        "tldr": "The paper introduces a per-query personalization step for visual concept learning that leverages self- and cross-attention and PDM features to improve personalized semantic similarity, demonstrating improvements over existing methods.",
        "tldr_zh": "该论文提出了一种针对视觉概念学习的逐查询个性化步骤，利用自注意力和交叉注意力以及PDM特征来提高个性化语义相似性，并证明了相对于现有方法的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding",
        "summary": "Vision-Language-Action models have demonstrated remarkable capabilities in\npredicting agent movements within virtual environments and real-world scenarios\nbased on visual observations and textual instructions. Although recent research\nhas focused on enhancing spatial and temporal understanding independently, this\npaper presents a novel approach that integrates both aspects through visual\nprompting. We introduce a method that projects visual traces of key points from\nobservations onto depth maps, enabling models to capture both spatial and\ntemporal information simultaneously. The experiments in SimplerEnv show that\nthe mean number of tasks successfully solved increased for 4% compared to\nSpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this\nenhancement can be achieved with minimal training data, making it particularly\nvaluable for real-world applications where data collection is challenging. The\nproject page is available at https://ampiromax.github.io/ST-VLA.",
        "url": "http://arxiv.org/abs/2508.09032v1",
        "published_date": "2025-08-12T15:53:45+00:00",
        "updated_date": "2025-08-12T15:53:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Maxim A. Patratskiy",
            "Alexey K. Kovalev",
            "Aleksandr I. Panov"
        ],
        "tldr": "The paper introduces a novel approach integrating spatial and temporal understanding in Vision-Language-Action models using visual traces projected onto depth maps, achieving improved task success with minimal training data.",
        "tldr_zh": "该论文提出了一种新颖的方法，通过将视觉轨迹投影到深度图中，将空间和时间理解集成到视觉-语言-动作模型中，从而以最少的训练数据成功地提高了任务成功率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding",
        "summary": "Recently, with the emergence of large language models, multimodal LLMs have\ndemonstrated exceptional capabilities in image and video modalities. Despite\nadvancements in video comprehension, the substantial computational demands of\nlong video sequences lead current video LLMs (Vid-LLMs) to employ compression\nstrategies at both the inter-frame level (e.g., uniform sampling of video\nframes) and intra-frame level (e.g., condensing all visual tokens of each frame\ninto a limited number). However, this approach often neglects the uneven\ntemporal distribution of critical information across frames, risking the\nomission of keyframes that contain essential temporal and semantic details. To\ntackle these challenges, we propose KFFocus, a method designed to efficiently\ncompress video tokens and emphasize the informative context present within\nvideo frames. We substitute uniform sampling with a refined approach inspired\nby classic video compression principles to identify and capture keyframes based\non their temporal redundancy. By assigning varying condensation ratios to\nframes based on their contextual relevance, KFFocus efficiently reduces token\nredundancy while preserving informative content details. Additionally, we\nintroduce a spatiotemporal modeling module that encodes both the temporal\nrelationships between video frames and the spatial structure within each frame,\nthus providing Vid-LLMs with a nuanced understanding of spatial-temporal\ndynamics. Extensive experiments on widely recognized video understanding\nbenchmarks, especially long video scenarios, demonstrate that KFFocus\nsignificantly outperforms existing methods, achieving substantial computational\nefficiency and enhanced accuracy.",
        "url": "http://arxiv.org/abs/2508.08989v1",
        "published_date": "2025-08-12T14:57:03+00:00",
        "updated_date": "2025-08-12T14:57:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ming Nie",
            "Chunwei Wang",
            "Hang Xu",
            "Li Zhang"
        ],
        "tldr": "The paper introduces KFFocus, a keyframe selection and spatiotemporal modeling method to improve the efficiency and accuracy of video LLMs, especially for long videos, by focusing on important frames and contextual relevance.",
        "tldr_zh": "该论文介绍了KFFocus，一种关键帧选择和时空建模方法，旨在通过关注重要帧和上下文相关性来提高视频LLM（特别是长视频）的效率和准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs",
        "summary": "Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong\ncapabilities in learning joint representations from text and images. However,\ntheir spatial reasoning remains limited. We introduce 3DFroMLLM, a novel\nframework that enables the generation of 3D object prototypes directly from\nMLLMs, including geometry and part labels. Our pipeline is agentic, comprising\na designer, coder, and visual inspector operating in a refinement loop.\nNotably, our approach requires no additional training data or detailed user\ninstructions. Building on prior work in 2D generation, we demonstrate that\nrendered images produced by our framework can be effectively used for image\nclassification pretraining tasks and outperforms previous methods by 15%. As a\ncompelling real-world use case, we show that the generated prototypes can be\nleveraged to improve fine-grained vision-language models by using the rendered,\npart-labeled prototypes to fine-tune CLIP for part segmentation and achieving a\n55% accuracy improvement without relying on any additional human-labeled data.",
        "url": "http://arxiv.org/abs/2508.08821v1",
        "published_date": "2025-08-12T10:21:59+00:00",
        "updated_date": "2025-08-12T10:21:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Noor Ahmed",
            "Cameron Braunstein",
            "Steffen Eger",
            "Eddy Ilg"
        ],
        "tldr": "The paper introduces 3DFroMLLM, a novel framework for generating 3D object prototypes directly from MLLMs without additional training data, showing improved performance in image classification pretraining and fine-grained vision-language tasks.",
        "tldr_zh": "该论文介绍了3DFroMLLM，一种新颖的框架，可以直接从MLLM生成3D对象原型，无需额外训练数据，并在图像分类预训练和细粒度视觉语言任务中表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SHREC 2025: Retrieval of Optimal Objects for Multi-modal Enhanced Language and Spatial Assistance (ROOMELSA)",
        "summary": "Recent 3D retrieval systems are typically designed for simple, controlled\nscenarios, such as identifying an object from a cropped image or a brief\ndescription. However, real-world scenarios are more complex, often requiring\nthe recognition of an object in a cluttered scene based on a vague, free-form\ndescription. To this end, we present ROOMELSA, a new benchmark designed to\nevaluate a system's ability to interpret natural language. Specifically,\nROOMELSA attends to a specific region within a panoramic room image and\naccurately retrieves the corresponding 3D model from a large database. In\naddition, ROOMELSA includes over 1,600 apartment scenes, nearly 5,200 rooms,\nand more than 44,000 targeted queries. Empirically, while coarse object\nretrieval is largely solved, only one top-performing model consistently ranked\nthe correct match first across nearly all test cases. Notably, a lightweight\nCLIP-based model also performed well, although it struggled with subtle\nvariations in materials, part structures, and contextual cues, resulting in\noccasional errors. These findings highlight the importance of tightly\nintegrating visual and language understanding. By bridging the gap between\nscene-level grounding and fine-grained 3D retrieval, ROOMELSA establishes a new\nbenchmark for advancing robust, real-world 3D recognition systems.",
        "url": "http://arxiv.org/abs/2508.08781v1",
        "published_date": "2025-08-12T09:36:05+00:00",
        "updated_date": "2025-08-12T09:36:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Trong-Thuan Nguyen",
            "Viet-Tham Huynh",
            "Quang-Thuc Nguyen",
            "Hoang-Phuc Nguyen",
            "Long Le Bao",
            "Thai Hoang Minh",
            "Minh Nguyen Anh",
            "Thang Nguyen Tien",
            "Phat Nguyen Thuan",
            "Huy Nguyen Phong",
            "Bao Huynh Thai",
            "Vinh-Tiep Nguyen",
            "Duc-Vu Nguyen",
            "Phu-Hoa Pham",
            "Minh-Huy Le-Hoang",
            "Nguyen-Khang Le",
            "Minh-Chinh Nguyen",
            "Minh-Quan Ho",
            "Ngoc-Long Tran",
            "Hien-Long Le-Hoang",
            "Man-Khoi Tran",
            "Anh-Duong Tran",
            "Kim Nguyen",
            "Quan Nguyen Hung",
            "Dat Phan Thanh",
            "Hoang Tran Van",
            "Tien Huynh Viet",
            "Nhan Nguyen Viet Thien",
            "Dinh-Khoi Vo",
            "Van-Loc Nguyen",
            "Trung-Nghia Le",
            "Tam V. Nguyen",
            "Minh-Triet Tran"
        ],
        "tldr": "The paper introduces ROOMELSA, a new benchmark for 3D object retrieval in complex, real-world scenarios involving natural language descriptions and cluttered panoramic scenes, highlighting the need for improved integration of visual and language understanding.",
        "tldr_zh": "该论文介绍了 ROOMELSA，这是一个新的基准，用于在复杂的真实场景中进行 3D 对象检索，涉及自然语言描述和杂乱的全景场景，强调需要改进视觉和语言理解的集成。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SafeFix: Targeted Model Repair via Controlled Image Generation",
        "summary": "Deep learning models for visual recognition often exhibit systematic errors\ndue to underrepresented semantic subpopulations. Although existing debugging\nframeworks can pinpoint these failures by identifying key failure attributes,\nrepairing the model effectively remains difficult. Current solutions often rely\non manually designed prompts to generate synthetic training images -- an\napproach prone to distribution shift and semantic errors. To overcome these\nchallenges, we introduce a model repair module that builds on an interpretable\nfailure attribution pipeline. Our approach uses a conditional text-to-image\nmodel to generate semantically faithful and targeted images for failure cases.\nTo preserve the quality and relevance of the generated samples, we further\nemploy a large vision-language model (LVLM) to filter the outputs, enforcing\nalignment with the original data distribution and maintaining semantic\nconsistency. By retraining vision models with this rare-case-augmented\nsynthetic dataset, we significantly reduce errors associated with rare cases.\nOur experiments demonstrate that this targeted repair strategy improves model\nrobustness without introducing new bugs. Code is available at\nhttps://github.com/oxu2/SafeFix",
        "url": "http://arxiv.org/abs/2508.08701v1",
        "published_date": "2025-08-12T07:45:25+00:00",
        "updated_date": "2025-08-12T07:45:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ouyang Xu",
            "Baoming Zhang",
            "Ruiyu Mao",
            "Yunhui Guo"
        ],
        "tldr": "SafeFix uses a conditional text-to-image model and a vision-language model (LVLM) to generate targeted, semantically faithful synthetic data for model repair, improving robustness without introducing new bugs.",
        "tldr_zh": "SafeFix 使用条件文本到图像模型和视觉语言模型 (LVLM) 来生成有针对性的、语义真实的合成数据以进行模型修复，从而提高鲁棒性而不会引入新的错误。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision",
        "summary": "Vision-language models (VLMs) have made significant strides in reasoning, yet\nthey often struggle with complex multimodal tasks and tend to generate overly\nverbose outputs. A key limitation is their reliance on chain-of-thought (CoT)\nreasoning, despite many tasks benefiting from alternative topologies like trees\nor graphs. To address this, we introduce STELAR-Vision, a training framework\nfor topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline\nthat enriches training with diverse topological structures. Using supervised\nfine-tuning and reinforcement learning, we post-train Qwen2VL models with both\naccuracy and efficiency in mind. Additionally, we propose Frugal Learning,\nwhich reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H,\nSTELAR-Vision improves accuracy by 9.7% over its base model and surpasses the\nlarger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it\noutperforms Phi-4-Multimodal-Instruct by up to 28.4% and\nLLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong\ngeneralization. Compared to Chain-Only training, our approach achieves 4.3%\nhigher overall accuracy on in-distribution datasets and consistently\noutperforms across all OOD benchmarks. We have released datasets, and code will\nbe available.",
        "url": "http://arxiv.org/abs/2508.08688v1",
        "published_date": "2025-08-12T07:27:50+00:00",
        "updated_date": "2025-08-12T07:27:50+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chen Li",
            "Han Zhang",
            "Zhantao Yang",
            "Fangyi Chen",
            "Zihan Wang",
            "Anudeepsekhar Bolimera",
            "Marios Savvides"
        ],
        "tldr": "The paper introduces STELAR-Vision, a training framework that enhances vision-language models (VLMs) with topology-aware reasoning using synthetic data and reinforcement learning, achieving significant accuracy and efficiency gains on several benchmarks.",
        "tldr_zh": "该论文介绍了STELAR-Vision，一个训练框架，通过使用合成数据和强化学习增强视觉语言模型（VLMs）的拓扑感知推理能力，并在多个基准测试上实现了显著的准确性和效率提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AME: Aligned Manifold Entropy for Robust Vision-Language Distillation",
        "summary": "Knowledge distillation is a long-established technique for knowledge\ntransfer, and has regained attention in the context of the recent emergence of\nlarge vision-language models (VLMs). However, vision-language knowledge\ndistillation often requires sufficient training data to achieve robust\ngeneralization on amples with ambiguous or boundary-adjacent representations,\nwhich are associated with high predictive uncertainty. Critically, collecting\nsuch large-scale, task-specific data for training is often impractical in\nreal-world scenarios. To address this major challenge arising from the\nentanglement of uncertainty and cross-modal feature representation, we propose\nAligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming\nto achieve robust generalization under real-world conditions. AME applies\nentropy minimization over a reconfigured shared manifold, where multi-modal\ndata (i.e., image and text) are bridged through a pair of projection functions,\nconducive to structural compression for cross-modal feature representations.\nThis enables robust knowledge distillation under low-data regimes, while\nrequiring no architectural modifications to the backbone. As a result, it can\nserve as a plug-and-play module compatible with a wide range of vision-language\ndistillation frameworks. Notably, our theoretical analysis reveals that\nintegrating knowledge distillation with entropy minimization over the shared\nmanifold leads to a tighter generalization error bound. Extensive experiments\nacross diverse distillation architectures and training settings demonstrate\nthat AME consistently facilitates robust knowledge distillation, resulting in\nsuperior generalization performance across a wide spectrum of downstream tasks.",
        "url": "http://arxiv.org/abs/2508.08644v1",
        "published_date": "2025-08-12T05:16:00+00:00",
        "updated_date": "2025-08-12T05:16:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guiming Cao",
            "Yuming Ou"
        ],
        "tldr": "The paper introduces Aligned Manifold Entropy (AME) for robust vision-language knowledge distillation, especially in low-data regimes, by minimizing entropy over a reconfigured shared manifold without architectural changes.",
        "tldr_zh": "本文提出了一种对齐流形熵（AME）方法，用于稳健的视觉-语言知识蒸馏，尤其是在低数据情况下，通过最小化重构共享流形上的熵，且无需架构更改。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Transferable Model-agnostic Vision-Language Model Adaptation for Efficient Weak-to-Strong Generalization",
        "summary": "Vision-Language Models (VLMs) have been widely used in various visual\nrecognition tasks due to their remarkable generalization capabilities. As these\nmodels grow in size and complexity, fine-tuning becomes costly, emphasizing the\nneed to reuse adaptation knowledge from 'weaker' models to efficiently enhance\n'stronger' ones. However, existing adaptation transfer methods exhibit limited\ntransferability across models due to their model-specific design and high\ncomputational demands. To tackle this, we propose Transferable Model-agnostic\nadapter (TransMiter), a light-weight adapter that improves vision-language\nmodels 'without backpropagation'. TransMiter captures the knowledge gap between\npre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained,\nthis knowledge can be seamlessly transferred across different models without\nthe need for backpropagation. Moreover, TransMiter consists of only a few\nlayers, inducing a negligible additional inference cost. Notably, supplementing\nthe process with a few labeled data further yields additional performance gain,\noften surpassing a fine-tuned stronger model, with a marginal training cost.\nExperimental results and analyses demonstrate that TransMiter effectively and\nefficiently transfers adaptation knowledge while preserving generalization\nabilities across VLMs of different sizes and architectures in visual\nrecognition tasks.",
        "url": "http://arxiv.org/abs/2508.08604v1",
        "published_date": "2025-08-12T03:37:16+00:00",
        "updated_date": "2025-08-12T03:37:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jihwan Park",
            "Taehoon song",
            "Sanghyeok Lee",
            "Miso Choi",
            "Hyunwoo J. Kim"
        ],
        "tldr": "The paper introduces TransMiter, a lightweight, model-agnostic adapter that transfers adaptation knowledge between vision-language models without backpropagation, enabling efficient weak-to-strong generalization.",
        "tldr_zh": "该论文介绍了一种轻量级的、模型无关的适配器TransMiter，它可以在视觉-语言模型之间无需反向传播地传递适配知识，从而实现高效的弱到强泛化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in document understanding. However, their reasoning processes\nremain largely black-box, making it difficult to ensure reliability and\ntrustworthiness, especially in high-stakes domains such as legal, financial,\nand medical document analysis. Existing methods use fixed Chain-of-Thought\n(CoT) reasoning with supervised fine-tuning (SFT) but suffer from catastrophic\nforgetting, poor adaptability, and limited generalization across domain tasks.\nIn this paper, we propose DocThinker, a rule-based Reinforcement Learning (RL)\nframework for dynamic inference-time reasoning. Instead of relying on static\nCoT templates, DocThinker autonomously refines reasoning strategies via policy\nlearning, generating explainable intermediate results, including structured\nreasoning processes, rephrased questions, regions of interest (RoI) supporting\nthe answer, and the final answer. By integrating multi-objective rule-based\nrewards and KL-constrained optimization, our method mitigates catastrophic\nforgetting and enhances both adaptability and transparency. Extensive\nexperiments on multiple benchmarks demonstrate that DocThinker significantly\nimproves generalization while producing more explainable and\nhuman-understandable reasoning steps. Our findings highlight RL as a powerful\nalternative for enhancing explainability and adaptability in MLLM-based\ndocument understanding. Code will be available at\nhttps://github.com/wenwenyu/DocThinker.",
        "url": "http://arxiv.org/abs/2508.08589v1",
        "published_date": "2025-08-12T03:06:55+00:00",
        "updated_date": "2025-08-12T03:06:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenwen Yu",
            "Zhibo Yang",
            "Yuliang Liu",
            "Xiang Bai"
        ],
        "tldr": "DocThinker introduces a rule-based Reinforcement Learning framework to enhance the explainability and adaptability of Multimodal Large Language Models for document understanding by dynamically refining reasoning strategies at inference time.",
        "tldr_zh": "DocThinker 引入了一个基于规则的强化学习框架，通过在推理时动态调整推理策略，增强多模态大型语言模型在文档理解方面的可解释性和适应性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models",
        "summary": "Vision Language Models (VLMs) are increasingly being used in a broad range of\napplications, bringing their security and behavioral control to the forefront.\nWhile existing approaches for behavioral control or output redirection, like\nsystem prompting in VLMs, are easily detectable and often ineffective,\nactivation-based steering vectors require invasive runtime access to model\ninternals--incompatible with API-based services and closed-source deployments.\nWe introduce VISOR (Visual Input-based Steering for Output Redirection), a\nnovel method that achieves sophisticated behavioral control through optimized\nvisual inputs alone. By crafting universal steering images that induce target\nactivation patterns, VISOR enables practical deployment across all VLM serving\nmodalities while remaining imperceptible compared to explicit textual\ninstructions. We validate VISOR on LLaVA-1.5-7B across three critical alignment\ntasks: refusal, sycophancy and survival instinct. A single 150KB steering image\nmatches steering vector performance within 1-2% for positive behavioral shifts\nwhile dramatically exceeding it for negative steering--achieving up to 25%\nshifts from baseline compared to steering vectors' modest changes. Unlike\nsystem prompting (3-4% shifts), VISOR provides robust bidirectional control\nwhile maintaining 99.9% performance on 14,000 unrelated MMLU tasks. Beyond\neliminating runtime overhead and model access requirements, VISOR exposes a\ncritical security vulnerability: adversaries can achieve sophisticated\nbehavioral manipulation through visual channels alone, bypassing text-based\ndefenses. Our work fundamentally re-imagines multimodal model control and\nhighlights the urgent need for defenses against visual steering attacks.",
        "url": "http://arxiv.org/abs/2508.08521v1",
        "published_date": "2025-08-11T23:25:16+00:00",
        "updated_date": "2025-08-11T23:25:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mansi Phute",
            "Ravikumar Balakrishnan"
        ],
        "tldr": "The paper introduces VISOR, a novel method for controlling VLMs' behavior through optimized visual inputs, demonstrating its effectiveness in redirecting outputs and highlighting a critical security vulnerability. It outperforms text-based approaches and matches steering vector performance.",
        "tldr_zh": "该论文介绍了VISOR，一种通过优化视觉输入来控制VLM行为的新方法，展示了其在重定向输出方面的有效性，并强调了一个关键的安全漏洞。它优于基于文本的方法，并与steering vector的性能相匹配。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling",
        "summary": "Despite recent advances, long-sequence video generation frameworks still\nsuffer from significant limitations: poor assistive capability, suboptimal\nvisual quality, and limited expressiveness. To mitigate these limitations, we\npropose MAViS, an end-to-end multi-agent collaborative framework for\nlong-sequence video storytelling. MAViS orchestrates specialized agents across\nmultiple stages, including script writing, shot designing, character modeling,\nkeyframe generation, video animation, and audio generation. In each stage,\nagents operate under the 3E Principle -- Explore, Examine, and Enhance -- to\nensure the completeness of intermediate outputs. Considering the capability\nlimitations of current generative models, we propose the Script Writing\nGuidelines to optimize compatibility between scripts and generative tools.\nExperimental results demonstrate that MAViS achieves state-of-the-art\nperformance in assistive capability, visual quality, and video expressiveness.\nIts modular framework further enables scalability with diverse generative\nmodels and tools. With just a brief user prompt, MAViS is capable of producing\nhigh-quality, expressive long-sequence video storytelling, enriching\ninspirations and creativity for users. To the best of our knowledge, MAViS is\nthe only framework that provides multimodal design output -- videos with\nnarratives and background music.",
        "url": "http://arxiv.org/abs/2508.08487v1",
        "published_date": "2025-08-11T21:42:41+00:00",
        "updated_date": "2025-08-11T21:42:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Qian Wang",
            "Ziqi Huang",
            "Ruoxi Jia",
            "Paul Debevec",
            "Ning Yu"
        ],
        "tldr": "MAViS is a multi-agent framework for long-sequence video storytelling that addresses limitations in assistive capability, visual quality, and expressiveness by orchestrating specialized agents across multiple stages and optimizing compatibility between scripts and generative tools.",
        "tldr_zh": "MAViS是一个用于长序列视频叙事的多智能体框架，通过协调多个阶段的专门智能体并优化脚本和生成工具之间的兼容性，解决了辅助能力、视觉质量和表现力方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering",
        "summary": "The Earth's surface is constantly changing, and detecting these changes\nprovides valuable insights that benefit various aspects of human society. While\ntraditional change detection methods have been employed to detect changes from\nbi-temporal images, these approaches typically require expert knowledge for\naccurate interpretation. To enable broader and more flexible access to change\ninformation by non-expert users, the task of Change Detection Visual Question\nAnswering (CDVQA) has been introduced. However, existing CDVQA methods have\nbeen developed under the assumption that training and testing datasets share\nsimilar distributions. This assumption does not hold in real-world\napplications, where domain shifts often occur. In this paper, the CDVQA task is\nrevisited with a focus on addressing domain shift. To this end, a new\nmulti-modal and multi-domain dataset, BrightVQA, is introduced to facilitate\ndomain generalization research in CDVQA. Furthermore, a novel state space\nmodel, termed Text-Conditioned State Space Model (TCSSM), is proposed. The\nTCSSM framework is designed to leverage both bi-temporal imagery and\ngeo-disaster-related textual information in an unified manner to extract\ndomain-invariant features across domains. Input-dependent parameters existing\nin TCSSM are dynamically predicted by using both bi-temporal images and\ngeo-disaster-related description, thereby facilitating the alignment between\nbi-temporal visual data and the associated textual descriptions. Extensive\nexperiments are conducted to evaluate the proposed method against\nstate-of-the-art models, and superior performance is consistently demonstrated.\nThe code and dataset will be made publicly available upon acceptance at\nhttps://github.com/Elman295/TCSSM.",
        "url": "http://arxiv.org/abs/2508.08974v1",
        "published_date": "2025-08-12T14:37:53+00:00",
        "updated_date": "2025-08-12T14:37:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Elman Ghazaei",
            "Erchan Aptoula"
        ],
        "tldr": "This paper introduces a new dataset (BrightVQA) and a Text-Conditioned State Space Model (TCSSM) for domain-generalized Change Detection Visual Question Answering, addressing the challenge of domain shift in real-world applications.",
        "tldr_zh": "本文介绍了一个新的数据集 (BrightVQA) 和一个文本条件状态空间模型 (TCSSM)，用于领域泛化的变化检测视觉问答，旨在解决现实应用中领域偏移的挑战。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation",
        "summary": "Face Morphing Attack Detection (MAD) is a critical challenge in face\nrecognition security, where attackers can fool systems by interpolating the\nidentity information of two or more individuals into a single face image,\nresulting in samples that can be verified as belonging to multiple identities\nby face recognition systems. While multimodal foundation models (FMs) like CLIP\noffer strong zero-shot capabilities by jointly modeling images and text, most\nprior works on FMs for biometric recognition have relied on fine-tuning for\nspecific downstream tasks, neglecting their potential for direct, generalizable\ndeployment. This work explores a pure zero-shot approach to MAD by leveraging\nCLIP without any additional training or fine-tuning, focusing instead on the\ndesign and aggregation of multiple textual prompts per class. By aggregating\nthe embeddings of diverse prompts, we better align the model's internal\nrepresentations with the MAD task, capturing richer and more varied cues\nindicative of bona-fide or attack samples. Our results show that prompt\naggregation substantially improves zero-shot detection performance,\ndemonstrating the effectiveness of exploiting foundation models' built-in\nmultimodal knowledge through efficient prompt engineering.",
        "url": "http://arxiv.org/abs/2508.08939v1",
        "published_date": "2025-08-12T13:47:27+00:00",
        "updated_date": "2025-08-12T13:47:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Eduarda Caldeira",
            "Fadi Boutros",
            "Naser Damer"
        ],
        "tldr": "This paper introduces a zero-shot Face Morphing Attack Detection (MAD) method using CLIP by aggregating multiple textual prompts without fine-tuning, showing improved detection performance.",
        "tldr_zh": "本文介绍了一种使用CLIP的零样本面部变形攻击检测(MAD)方法，该方法通过聚合多个文本提示而无需微调，从而提高了检测性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation",
        "summary": "Animal pose estimation is a fundamental task in computer vision, with growing\nimportance in ecological monitoring, behavioral analysis, and intelligent\nlivestock management. Compared to human pose estimation, animal pose estimation\nis more challenging due to high interspecies morphological diversity, complex\nbody structures, and limited annotated data. In this work, we introduce\nDiffPose-Animal, a novel diffusion-based framework for top-down animal pose\nestimation. Unlike traditional heatmap regression methods, DiffPose-Animal\nreformulates pose estimation as a denoising process under the generative\nframework of diffusion models. To enhance semantic guidance during keypoint\ngeneration, we leverage large language models (LLMs) to extract both global\nanatomical priors and local keypoint-wise semantics based on species-specific\nprompts. These textual priors are encoded and fused with image features via\ncross-attention modules to provide biologically meaningful constraints\nthroughout the denoising process. Additionally, a diffusion-based keypoint\ndecoder is designed to progressively refine pose predictions, improving\nrobustness to occlusion and annotation sparsity. Extensive experiments on\npublic animal pose datasets demonstrate the effectiveness and generalization\ncapability of our method, especially under challenging scenarios with diverse\nspecies, cluttered backgrounds, and incomplete keypoints.",
        "url": "http://arxiv.org/abs/2508.08783v1",
        "published_date": "2025-08-12T09:37:09+00:00",
        "updated_date": "2025-08-12T09:37:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyu Xiong",
            "Dayi Tan",
            "Wei Tian"
        ],
        "tldr": "The paper introduces DiffPose-Animal, a diffusion-based framework for animal pose estimation that utilizes LLMs to incorporate anatomical priors and refine keypoint predictions, demonstrating improved performance on public datasets.",
        "tldr_zh": "该论文介绍了DiffPose-Animal，一个基于扩散的动物姿态估计框架，它利用大型语言模型整合解剖学先验知识并改进关键点预测，在公共数据集上展示了改进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation",
        "summary": "To enhance group robustness to spurious correlations, prior work often relies\non auxiliary annotations for groups or spurious features and assumes identical\nsets of groups across source and target domains. These two requirements are\nboth unnatural and impractical in real-world settings. To overcome these\nlimitations, we propose a method that leverages the semantic structure inherent\nin class labels--specifically, superclass information--to naturally reduce\nreliance on spurious features. Our model employs gradient-based attention\nguided by a pre-trained vision-language model to disentangle\nsuperclass-relevant and irrelevant features. Then, by promoting the use of all\nsuperclass-relevant features for prediction, our approach achieves robustness\nto more complex spurious correlations without the need to annotate any source\nsamples. Experiments across diverse datasets demonstrate that our method\nsignificantly outperforms baselines in domain generalization tasks, with clear\nimprovements in both quantitative metrics and qualitative visualizations.",
        "url": "http://arxiv.org/abs/2508.08570v1",
        "published_date": "2025-08-12T02:16:04+00:00",
        "updated_date": "2025-08-12T02:16:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Chenruo Liu",
            "Hongjun Liu",
            "Zeyu Lai",
            "Yiqiu Shen",
            "Chen Zhao",
            "Qi Lei"
        ],
        "tldr": "This paper proposes a superclass-guided representation disentanglement method to mitigate spurious correlations in domain generalization without relying on auxiliary annotations, achieving improved robustness. It leverages a pre-trained vision-language model for gradient-based attention.",
        "tldr_zh": "该论文提出了一种超类引导的表征解耦方法，以减轻领域泛化中的虚假相关性，无需依赖辅助注释，并提高了鲁棒性。它利用预训练的视觉-语言模型进行基于梯度的注意力机制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Re:Verse -- Can Your VLM Read a Manga?",
        "summary": "Current Vision Language Models (VLMs) demonstrate a critical gap between\nsurface-level recognition and deep narrative reasoning when processing\nsequential visual storytelling. Through a comprehensive investigation of manga\nnarrative understanding, we reveal that while recent large multimodal models\nexcel at individual panel interpretation, they systematically fail at temporal\ncausality and cross-panel cohesion, core requirements for coherent story\ncomprehension. We introduce a novel evaluation framework that combines\nfine-grained multimodal annotation, cross-modal embedding analysis, and\nretrieval-augmented assessment to systematically characterize these\nlimitations.\n  Our methodology includes (i) a rigorous annotation protocol linking visual\nelements to narrative structure through aligned light novel text, (ii)\ncomprehensive evaluation across multiple reasoning paradigms, including direct\ninference and retrieval-augmented generation, and (iii) cross-modal similarity\nanalysis revealing fundamental misalignments in current VLMs' joint\nrepresentations. Applying this framework to Re:Zero manga across 11 chapters\nwith 308 annotated panels, we conduct the first systematic study of long-form\nnarrative understanding in VLMs through three core evaluation axes: generative\nstorytelling, contextual dialogue grounding, and temporal reasoning. Our\nfindings demonstrate that current models lack genuine story-level intelligence,\nstruggling particularly with non-linear narratives, character consistency, and\ncausal inference across extended sequences. This work establishes both the\nfoundation and practical methodology for evaluating narrative intelligence,\nwhile providing actionable insights into the capability of deep sequential\nunderstanding of Discrete Visual Narratives beyond basic recognition in\nMultimodal Models.",
        "url": "http://arxiv.org/abs/2508.08508v1",
        "published_date": "2025-08-11T22:40:05+00:00",
        "updated_date": "2025-08-11T22:40:05+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Aaditya Baranwal",
            "Madhav Kataria",
            "Naitik Agrawal",
            "Yogesh S Rawat",
            "Shruti Vyas"
        ],
        "tldr": "The paper introduces a framework for evaluating VLMs' narrative understanding using manga, revealing their limitations in temporal causality and cross-panel cohesion, and highlighting the need for improved story-level intelligence.",
        "tldr_zh": "该论文介绍了一个评估视觉语言模型叙事理解能力的框架，利用漫画揭示了它们在时间因果关系和跨面板连贯性方面的局限性，并强调了提高故事级别智能的必要性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MuGa-VTON: Multi-Garment Virtual Try-On via Diffusion Transformers with Prompt Customization",
        "summary": "Virtual try-on seeks to generate photorealistic images of individuals in\ndesired garments, a task that must simultaneously preserve personal identity\nand garment fidelity for practical use in fashion retail and personalization.\nHowever, existing methods typically handle upper and lower garments separately,\nrely on heavy preprocessing, and often fail to preserve person-specific cues\nsuch as tattoos, accessories, and body shape-resulting in limited realism and\nflexibility. To this end, we introduce MuGa-VTON, a unified multi-garment\ndiffusion framework that jointly models upper and lower garments together with\nperson identity in a shared latent space. Specifically, we proposed three key\nmodules: the Garment Representation Module (GRM) for capturing both garment\nsemantics, the Person Representation Module (PRM) for encoding identity and\npose cues, and the A-DiT fusion module, which integrates garment, person, and\ntext-prompt features through a diffusion transformer. This architecture\nsupports prompt-based customization, allowing fine-grained garment\nmodifications with minimal user input. Extensive experiments on the VITON-HD\nand DressCode benchmarks demonstrate that MuGa-VTON outperforms existing\nmethods in both qualitative and quantitative evaluations, producing\nhigh-fidelity, identity-preserving results suitable for real-world virtual\ntry-on applications.",
        "url": "http://arxiv.org/abs/2508.08488v1",
        "published_date": "2025-08-11T21:45:07+00:00",
        "updated_date": "2025-08-11T21:45:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ankan Deria",
            "Dwarikanath Mahapatra",
            "Behzad Bozorgtabar",
            "Mohna Chakraborty",
            "Snehashis Chakraborty",
            "Sudipta Roy"
        ],
        "tldr": "The paper introduces MuGa-VTON, a multi-garment virtual try-on framework using diffusion transformers that jointly models upper and lower garments with personalized cues and text-prompt customization, outperforming existing methods on benchmarks.",
        "tldr_zh": "该论文介绍了MuGa-VTON，一个使用扩散Transformer的多服装虚拟试穿框架，该框架将上装和下装与个性化提示和文本提示定制联合建模，在基准测试中优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation",
        "summary": "Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques.",
        "url": "http://arxiv.org/abs/2508.08987v1",
        "published_date": "2025-08-12T14:56:11+00:00",
        "updated_date": "2025-08-12T14:56:11+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Ding Xia",
            "Naoto Inoue",
            "Qianru Qiu",
            "Kotaro Kikuchi"
        ],
        "tldr": "The paper introduces ColorGPT, a pipeline using LLMs for color recommendation and palette generation, demonstrating improved accuracy and diversity compared to existing methods.",
        "tldr_zh": "该论文介绍了ColorGPT，一个使用大型语言模型进行颜色推荐和调色板生成的流程，与现有方法相比，在准确性和多样性方面均有所提高。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition",
        "summary": "The ability to discern subtle emotional cues is fundamental to human social\nintelligence. As artificial intelligence (AI) becomes increasingly common, AI's\nability to recognize and respond to human emotions is crucial for effective\nhuman-AI interactions. In particular, whether such systems can match or surpass\nhuman experts remains to be seen. However, the emotional intelligence of AI,\nparticularly multimodal large language models (MLLMs), remains largely\nunexplored. This study evaluates the emotion recognition abilities of MLLMs\nusing the Reading the Mind in the Eyes Test (RMET) and its multiracial\ncounterpart (MRMET), and compares their performance against human participants.\nResults show that, on average, MLLMs outperform humans in accurately\nidentifying emotions across both tests. This trend persists even when comparing\nperformance across low, medium, and expert-level performing groups. Yet when we\naggregate independent human decisions to simulate collective intelligence,\nhuman groups significantly surpass the performance of aggregated MLLM\npredictions, highlighting the wisdom of the crowd. Moreover, a collaborative\napproach (augmented intelligence) that combines human and MLLM predictions\nachieves greater accuracy than either humans or MLLMs alone. These results\nsuggest that while MLLMs exhibit strong emotion recognition at the individual\nlevel, the collective intelligence of humans and the synergistic potential of\nhuman-AI collaboration offer the most promising path toward effective emotional\nAI. We discuss the implications of these findings for the development of\nemotionally intelligent AI systems and future research directions.",
        "url": "http://arxiv.org/abs/2508.08830v1",
        "published_date": "2025-08-12T10:37:37+00:00",
        "updated_date": "2025-08-12T10:37:37+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Mustafa Akben",
            "Vinayaka Gude",
            "Haya Ajjan"
        ],
        "tldr": "This paper evaluates the emotion recognition capabilities of MLLMs, finding that while MLLMs outperform individual humans, the wisdom of crowds surpasses MLLM performance, and human-AI collaboration yields the best results.",
        "tldr_zh": "本文评估了多模态大型语言模型（MLLM）的情感识别能力，发现虽然MLLM优于个体人类，但群体智慧优于MLLM，人机协作产生最佳结果。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Think as Cardiac Sonographers: Marrying SAM with Left Ventricular Indicators Measurements According to Clinical Guidelines",
        "summary": "Left ventricular (LV) indicator measurements following clinical\nechocardiog-raphy guidelines are important for diagnosing cardiovascular\ndisease. Alt-hough existing algorithms have explored automated LV\nquantification, they can struggle to capture generic visual representations due\nto the normally small training datasets. Therefore, it is necessary to\nintroduce vision founda-tional models (VFM) with abundant knowledge. However,\nVFMs represented by the segment anything model (SAM) are usually suitable for\nsegmentation but incapable of identifying key anatomical points, which are\ncritical in LV indicator measurements. In this paper, we propose a novel\nframework named AutoSAME, combining the powerful visual understanding of SAM\nwith seg-mentation and landmark localization tasks simultaneously.\nConsequently, the framework mimics the operation of cardiac sonographers,\nachieving LV indi-cator measurements consistent with clinical guidelines. We\nfurther present fil-tered cross-branch attention (FCBA) in AutoSAME, which\nleverages relatively comprehensive features in the segmentation to enhance the\nheatmap regression (HR) of key points from the frequency domain perspective,\noptimizing the vis-ual representation learned by the latter. Moreover, we\npropose spatial-guided prompt alignment (SGPA) to automatically generate prompt\nembeddings guid-ed by spatial properties of LV, thereby improving the accuracy\nof dense pre-dictions by prior spatial knowledge. The extensive experiments on\nan echocar-diography dataset demonstrate the efficiency of each design and the\nsuperiori-ty of our AutoSAME in LV segmentation, landmark localization, and\nindicator measurements. The code will be available at\nhttps://github.com/QC-LIU-1997/AutoSAME.",
        "url": "http://arxiv.org/abs/2508.08566v1",
        "published_date": "2025-08-12T02:09:36+00:00",
        "updated_date": "2025-08-12T02:09:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tuo Liu",
            "Qinghan Yang",
            "Yu Zhang",
            "Rongjun Ge",
            "Yang Chen",
            "Guangquan Zhou"
        ],
        "tldr": "This paper introduces AutoSAME, a framework combining SAM with filtered cross-branch attention and spatial-guided prompt alignment for improved left ventricular segmentation, landmark localization, and indicator measurements in echocardiography.",
        "tldr_zh": "本文介绍了一种名为AutoSAME的框架，该框架结合了SAM与滤波跨分支注意力和空间引导提示对齐，以改进超声心动图中的左心室分割、地标定位和指标测量。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]