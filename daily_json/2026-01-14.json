[
    {
        "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
        "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
        "url": "http://arxiv.org/abs/2601.08665v1",
        "published_date": "2026-01-13T15:43:43+00:00",
        "updated_date": "2026-01-13T15:43:43+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Shaoan Wang",
            "Yuanfei Luo",
            "Xingyu Chen",
            "Aocheng Luo",
            "Dongyue Li",
            "Chang Liu",
            "Sheng Chen",
            "Yangang Zhang",
            "Junzhi Yu"
        ],
        "tldr": "The paper introduces VLingNav, a VLA model for embodied navigation that uses adaptive chain-of-thought reasoning and visual-assisted linguistic memory, achieving SOTA performance and zero-shot transfer to real-world robots.",
        "tldr_zh": "该论文介绍了VLingNav，一个用于具身导航的VLA模型，它使用自适应链式思考推理和视觉辅助语言记忆，实现了最先进的性能，并且可以零样本迁移到现实世界的机器人。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning",
        "summary": "With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.",
        "url": "http://arxiv.org/abs/2601.08617v1",
        "published_date": "2026-01-13T15:00:03+00:00",
        "updated_date": "2026-01-13T15:00:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Leo Fillioux",
            "Omprakash Chakraborty",
            "Ismail Ben Ayed",
            "Paul-Henry Cournède",
            "Stergios Christodoulidis",
            "Maria Vakalopoulou",
            "Jose Dolz"
        ],
        "tldr": "This paper introduces Semantic Orthogonal Calibration (SoC) to improve the calibration of Vision-Language Models during test-time prompt tuning by balancing prototype separation and semantic proximity, addressing overconfidence issues caused by full orthogonality constraints.",
        "tldr_zh": "本文提出了一种语义正交校准（SoC）方法，通过平衡原型分离和语义邻近性，来改善视觉语言模型在测试时提示调整期间的校准，解决了完全正交约束导致过度自信的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations",
        "summary": "Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .",
        "url": "http://arxiv.org/abs/2601.08557v1",
        "published_date": "2026-01-13T13:42:05+00:00",
        "updated_date": "2026-01-13T13:42:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sushant Gautam",
            "Cise Midoglu",
            "Vajira Thambawita",
            "Michael A. Riegler",
            "Pål Halvorsen"
        ],
        "tldr": "The paper introduces VideoHEDGE, a framework for detecting hallucinations in Video-VLMs by using entropy-based reliability scores on semantically clustered and perturbed video clips, showing promising results on the SoccerChat benchmark.",
        "tldr_zh": "该论文介绍了 VideoHEDGE，一个用于检测视频视觉语言模型（Video-VLM）中幻觉的框架。该框架通过对语义聚类和扰动的视频片段使用基于熵的可靠性评分，并在 SoccerChat 基准测试中显示出有希望的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models",
        "summary": "Reliable zero-shot detection of out-of-distribution (OOD) inputs is critical for deploying vision-language models in open-world settings. However, the lack of labeled negatives in zero-shot OOD detection necessitates proxy signals that remain effective under distribution shift. Existing negative-label methods rely on a fixed set of textual proxies, which (i) sparsely sample the semantic space beyond in-distribution (ID) classes and (ii) remain static while only visual features drift, leading to cross-modal misalignment and unstable predictions. In this paper, we propose CoEvo, a training- and annotation-free test-time framework that performs bidirectional, sample-conditioned adaptation of both textual and visual proxies. Specifically, CoEvo introduces a proxy-aligned co-evolution mechanism to maintain two evolving proxy caches, which dynamically mines contextual textual negatives guided by test images and iteratively refines visual proxies, progressively realigning cross-modal similarities and enlarging local OOD margins. Finally, we dynamically re-weight the contributions of dual-modal proxies to obtain a calibrated OOD score that is robust to distribution shift. Extensive experiments on standard benchmarks demonstrate that CoEvo achieves state-of-the-art performance, improving AUROC by 1.33% and reducing FPR95 by 45.98% on ImageNet-1K compared to strong negative-label baselines.",
        "url": "http://arxiv.org/abs/2601.08476v1",
        "published_date": "2026-01-13T12:08:26+00:00",
        "updated_date": "2026-01-13T12:08:26+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Hao Tang",
            "Yu Liu",
            "Shuanglin Yan",
            "Fei Shen",
            "Shengfeng He",
            "Jing Qin"
        ],
        "tldr": "The paper proposes CoEvo, a training-free test-time framework that uses evolving textual and visual proxies to improve zero-shot out-of-distribution (OOD) detection with vision-language models, achieving state-of-the-art performance.",
        "tldr_zh": "该论文提出了CoEvo，一个无需训练的测试时框架，它使用不断演化的文本和视觉代理来提高视觉语言模型的零样本分布外（OOD）检测能力，并实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs",
        "summary": "Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \\textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \\textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.",
        "url": "http://arxiv.org/abs/2601.08470v1",
        "published_date": "2026-01-13T11:55:31+00:00",
        "updated_date": "2026-01-13T11:55:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Takara Taniguchi",
            "Kuniaki Saito",
            "Atsushi Hashimoto"
        ],
        "tldr": "The paper introduces HazardForge, a pipeline for generating diverse and anomalous scenarios for evaluating VLMs in autonomous systems, along with MovSafeBench, a new benchmark showing performance degradation in VLMs under such conditions.",
        "tldr_zh": "该论文介绍了HazardForge，一个用于生成多样化和异常场景的流水线，用于评估自主系统中的VLM，以及MovSafeBench，一个新的基准测试表明VLM在这些条件下性能下降。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling",
        "summary": "Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.",
        "url": "http://arxiv.org/abs/2601.08467v1",
        "published_date": "2026-01-13T11:46:05+00:00",
        "updated_date": "2026-01-13T11:46:05+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Takamichi Miyata",
            "Sumiko Miyata",
            "Andrew Morris"
        ],
        "tldr": "This paper presents a subject decoupling framework for zero-shot distracted driver detection using VLMs, which disentangles driver appearance from behavior cues and orthogonalizes text embeddings for improved performance in real-world conditions.",
        "tldr_zh": "该论文提出了一种基于视觉语言模型的零样本驾驶员分心检测的主题解耦框架，该框架将驾驶员的外貌与行为线索分离，并对文本嵌入进行正交化处理，从而提高了在实际环境中的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoMa: Contextual Massing Generation with Vision-Language Models",
        "summary": "The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.",
        "url": "http://arxiv.org/abs/2601.08464v1",
        "published_date": "2026-01-13T11:44:00+00:00",
        "updated_date": "2026-01-13T11:44:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Evgenii Maslov",
            "Valentin Khrulkov",
            "Anastasia Volkova",
            "Anton Gusarov",
            "Andrey Kuznetsov",
            "Ivan Oseledets"
        ],
        "tldr": "The paper introduces CoMa-20K, a dataset for building massing generation based on functional requirements and site context, and benchmarks it using Vision-Language Models, revealing the task's complexity and VLM potential.",
        "tldr_zh": "该论文介绍了CoMa-20K数据集，用于根据功能需求和场地背景生成建筑体量，并使用视觉语言模型对其进行基准测试，揭示了任务的复杂性和VLM的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP",
        "summary": "In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.",
        "url": "http://arxiv.org/abs/2601.08420v1",
        "published_date": "2026-01-13T10:44:37+00:00",
        "updated_date": "2026-01-13T10:44:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aditya Chaudhary",
            "Sneha Barman",
            "Mainak Singha",
            "Ankit Jha",
            "Girish Mishra",
            "Biplab Banerjee"
        ],
        "tldr": "The paper introduces MMLGNet, a multimodal framework that aligns remote sensing data (HSI and LiDAR) with natural language semantics using CLIP and bi-directional contrastive learning, achieving strong performance with simple CNN-based encoders.",
        "tldr_zh": "该论文介绍了MMLGNet，一个多模态框架，它使用CLIP和双向对比学习将遥感数据（HSI和LiDAR）与自然语言语义对齐，并且仅用简单的CNN编码器就实现了强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
        "summary": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2601.08321v1",
        "published_date": "2026-01-13T08:18:49+00:00",
        "updated_date": "2026-01-13T08:18:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lichen Ma",
            "Xiaolong Fu",
            "Gaojing Zhou",
            "Zipeng Guo",
            "Ting Zhu",
            "Yichun Liu",
            "Yu Shi",
            "Jason Li",
            "Junshi Huang"
        ],
        "tldr": "The paper introduces UM-Text, a unified multimodal model for generating style-consistent visual text from natural language instructions, along with a large-scale dataset and a novel training strategy, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了UM-Text，一个统一的多模态模型，用于从自然语言指令生成风格一致的视觉文本。同时，论文还提出了一个大规模数据集和一个新的训练策略，并取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?",
        "summary": "While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a \"Scaling Law Paradox\": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.",
        "url": "http://arxiv.org/abs/2601.08292v1",
        "published_date": "2026-01-13T07:32:50+00:00",
        "updated_date": "2026-01-13T07:32:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xianfeng Wang",
            "Kaiwei Zhang",
            "Qi Jia",
            "Zijian Chen",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "tldr": "The paper introduces KidVis, a benchmark to evaluate MLLMs' fundamental visual capabilities against those of 6-year-old children, revealing a significant performance gap and a \"Scaling Law Paradox,\" suggesting that current MLLMs lack essential visual primitives.",
        "tldr_zh": "该论文介绍了 KidVis，一个用于评估 MLLM 的基本视觉能力与 6 岁儿童的视觉能力的基准。结果显示存在显著的性能差距和“缩放法则悖论”，表明当前的 MLLM 缺乏基本的视觉原语。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging",
        "summary": "Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: https://github.com/faiyazabdullah/MultimodalMedAgent",
        "url": "http://arxiv.org/abs/2601.08192v1",
        "published_date": "2026-01-13T03:44:06+00:00",
        "updated_date": "2026-01-13T03:44:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md. Faiyaz Abdullah Sayeedi",
            "Rashedur Rahman",
            "Siam Tahsin Bhuiyan",
            "Sefatul Wasi",
            "Ashraful Islam",
            "Saadia Binte Alam",
            "AKM Mahbubur Rahman"
        ],
        "tldr": "The paper introduces R^4, an agentic framework that enhances vision-language models for medical image analysis by incorporating routing, retrieval, reflection, and repair mechanisms, leading to improved performance in chest X-ray analysis without fine-tuning.",
        "tldr_zh": "该论文介绍了R^4，一个代理框架，通过整合路由、检索、反思和修复机制来增强视觉-语言模型在医学图像分析中的性能，从而在胸部X光分析中提高了性能，而无需进行微调。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards",
        "summary": "Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical \"spatial grounding bottleneck\" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a \"fluency-accuracy paradox\": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to \"over-interpretation\" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.",
        "url": "http://arxiv.org/abs/2601.08183v1",
        "published_date": "2026-01-13T03:23:11+00:00",
        "updated_date": "2026-01-13T03:23:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yan Zhu",
            "Te Luo",
            "Pei-Yao Fu",
            "Zhen Zhang",
            "Zi-Long Wang",
            "Yi-Fan Qu",
            "Zi-Han Geng",
            "Jia-Qi Xu",
            "Lu Yao",
            "Li-Yun Ma",
            "Wei Su",
            "Wei-Feng Chen",
            "Quan-Lin Li",
            "Shuo Wang",
            "Ping-Hong Zhou"
        ],
        "tldr": "The paper introduces GI-Bench, a benchmark for evaluating MLLMs in gastrointestinal endoscopy, revealing a dissociation between knowledge and experience where models excel in diagnostic reasoning but struggle with spatial grounding and factual accuracy despite fluent reporting.",
        "tldr_zh": "该论文介绍了GI-Bench，一个用于评估MLLM在胃肠内窥镜检查中表现的基准。研究发现，模型在诊断推理方面表现出色，但在空间定位和事实准确性方面存在困难，尽管报告流畅，揭示了知识与经验的脱节。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Representation Learning with Semantic-aware Instance and Sparse Token Alignments",
        "summary": "Medical contrastive vision-language pre-training (VLP) has demonstrated significant potential in improving performance on downstream tasks. Traditional approaches typically employ contrastive learning, treating paired image-report samples as positives and unpaired ones as negatives. However, in medical datasets, there can be substantial similarities between images or reports from different patients. Rigidly treating all unpaired samples as negatives, can disrupt the underlying semantic structure and negatively impact the quality of the learned representations. In this paper, we propose a multi-level alignment framework, Representation Learning with Semantic-aware Instance and Sparse Token Alignments (SISTA) by exploiting the semantic correspondence between medical image and radiology reports at two levels, i.e., image-report and patch-word levels. Specifically, we improve the conventional contrastive learning by incorporating inter-report similarity to eliminate the false negatives and introduce a method to effectively align image patches with relevant word tokens. Experimental results demonstrate the effectiveness of the proposed framework in improving transfer performance across different datasets on three downstream tasks: image classification, image segmentation, and object detection. Notably, our framework achieves significant improvements in fine-grained tasks even with limited labeled data. Codes and pre-trained models will be made available.",
        "url": "http://arxiv.org/abs/2601.08165v1",
        "published_date": "2026-01-13T02:55:48+00:00",
        "updated_date": "2026-01-13T02:55:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Phuoc-Nguyen Bui",
            "Toan Duc Nguyen",
            "Junghyun Bum",
            "Duc-Tai Le",
            "Hyunseung Choo"
        ],
        "tldr": "The paper introduces SISTA, a multi-level alignment framework for medical vision-language pre-training that incorporates inter-report similarity to eliminate false negatives and aligns image patches with relevant word tokens, leading to improved transfer performance on downstream tasks.",
        "tldr_zh": "该论文介绍了SISTA，一种用于医学视觉-语言预训练的多层次对齐框架，它结合了报告间的相似性来消除假阴性，并将图像补丁与相关的单词标记对齐，从而提高了下游任务的迁移性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage \"review\" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.",
        "url": "http://arxiv.org/abs/2601.08151v1",
        "published_date": "2026-01-13T02:26:21+00:00",
        "updated_date": "2026-01-13T02:26:21+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Shezheng Song",
            "Shasha Li",
            "Jie Yu"
        ],
        "tldr": "This paper analyzes how vision and language information are fused in MLLMs, identifies key fusion layers and attention patterns, and proposes a contrastive attention framework to improve multimodal reasoning performance without additional training.",
        "tldr_zh": "本文分析了MLLM中视觉和语言信息如何融合，识别了关键的融合层和注意力模式，并提出了一个对比注意力框架，以在不进行额外训练的情况下提高多模态推理性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Subspace Alignment for Vision-Language Model Test-time Adaptation",
        "summary": "Vision-language models (VLMs), despite their extraordinary zero-shot capabilities, are vulnerable to distribution shifts. Test-time adaptation (TTA) emerges as a predominant strategy to adapt VLMs to unlabeled test data on the fly. However, existing TTA methods heavily rely on zero-shot predictions as pseudo-labels for self-training, which can be unreliable under distribution shifts and misguide adaptation due to two fundamental limitations. First (Modality Gap), distribution shifts induce gaps between visual and textual modalities, making cross-modal relations inaccurate. Second (Visual Nuisance), visual embeddings encode rich but task-irrelevant noise that often overwhelms task-specific semantics under distribution shifts. To address these limitations, we propose SubTTA, which aligns the semantic subspaces of both modalities to enhance zero-shot predictions to better guide the TTA process. To bridge the modality gap, SubTTA extracts the principal subspaces of both modalities and aligns the visual manifold to the textual semantic anchor by minimizing their chordal distance. To eliminate visual nuisance, SubTTA projects the aligned visual features onto the task-specific textual subspace, which filters out task-irrelevant noise by constraining visual embeddings within the valid semantic span, and standard TTA is further performed on the purified space to refine the decision boundaries. Extensive experiments on various benchmarks and VLM architectures demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.",
        "url": "http://arxiv.org/abs/2601.08139v1",
        "published_date": "2026-01-13T02:02:41+00:00",
        "updated_date": "2026-01-13T02:02:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhichen Zeng",
            "Wenxuan Bao",
            "Xiao Lin",
            "Ruizhong Qiu",
            "Tianxin Wei",
            "Xuying Ning",
            "Yuchen Yan",
            "Chen Luo",
            "Monica Xiao Cheng",
            "Jingrui He",
            "Hanghang Tong"
        ],
        "tldr": "The paper introduces SubTTA, a test-time adaptation method for VLMs that aligns semantic subspaces between visual and textual modalities to improve zero-shot predictions and filter out visual noise, leading to improved performance on benchmark datasets.",
        "tldr_zh": "该论文介绍了SubTTA，一种视觉语言模型的测试时自适应方法，它通过对齐视觉和文本模态之间的语义子空间，来提高零样本预测的准确性并滤除视觉噪声，从而在基准数据集上实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs",
        "summary": "Maintaining or improving the performance of Deep Neural Networks (DNNs) through fine-tuning requires labeling newly collected inputs, a process that is often costly and time-consuming. To alleviate this problem, input selection approaches have been developed in recent years to identify small, yet highly informative subsets for labeling. Diversity-based selection is one of the most effective approaches for this purpose. However, they are often computationally intensive and lack scalability for large input sets, limiting their practical applicability. To address this challenge, we introduce Concept-Based Diversity (CBD), a highly efficient metric for image inputs that leverages Vision-Language Models (VLM). Our results show that CBD exhibits a strong correlation with Geometric Diversity (GD), an established diversity metric, while requiring only a fraction of its computation time. Building on this finding, we propose a hybrid input selection approach that combines CBD with Margin, a simple uncertainty metric. We conduct a comprehensive evaluation across a diverse set of DNN models, input sets, selection budgets, and five most effective state-of-the-art selection baselines. The results demonstrate that the CBD-based selection consistently outperforms all baselines at guiding input selection to improve the DNN model. Furthermore, the CBD-based selection approach remains highly efficient, requiring selection times close to those of simple uncertainty-based methods such as Margin, even on larger input sets like ImageNet. These results confirm not only the effectiveness and computational advantage of the CBD-based approach, particularly compared to hybrid baselines, but also its scalability in repetitive and extensive input selection scenarios.",
        "url": "http://arxiv.org/abs/2601.08024v1",
        "published_date": "2026-01-12T21:57:33+00:00",
        "updated_date": "2026-01-12T21:57:33+00:00",
        "categories": [
            "cs.CV",
            "cs.SE"
        ],
        "authors": [
            "Amin Abbasishahkoo",
            "Mahboubeh Dadkhah",
            "Lionel Briand"
        ],
        "tldr": "This paper introduces Concept-Based Diversity (CBD), a computationally efficient, VLM-leveraging approach for diversity-based input selection in DNN fine-tuning, demonstrating improved performance and scalability compared to existing methods.",
        "tldr_zh": "该论文介绍了一种基于概念的多样性（CBD）方法，它利用视觉语言模型（VLM）实现高效的基于多样性的DNN微调输入选择，并展示了与现有方法相比的改进性能和可扩展性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Representations of Text and Images Align From Layer One",
        "summary": "We show that for a variety of concepts in adapter-based vision-language models, the representations of their images and their text descriptions are meaningfully aligned from the very first layer. This contradicts the established view that such image-text alignment only appears in late layers. We show this using a new synthesis-based method inspired by DeepDream: given a textual concept such as \"Jupiter\", we extract its concept vector at a given layer, and then use optimisation to synthesise an image whose representation aligns with that vector. We apply our approach to hundreds of concepts across seven layers in Gemma 3, and find that the synthesised images often depict salient visual features of the targeted textual concepts: for example, already at layer 1, more than 50 % of images depict recognisable features of animals, activities, or seasons. Our method thus provides direct, constructive evidence of image-text alignment on a concept-by-concept and layer-by-layer basis. Unlike previous methods for measuring multimodal alignment, our approach is simple, fast, and does not require auxiliary models or datasets. It also offers a new path towards model interpretability, by providing a way to visualise a model's representation space by backtracing through its image processing components.",
        "url": "http://arxiv.org/abs/2601.08017v1",
        "published_date": "2026-01-12T21:43:29+00:00",
        "updated_date": "2026-01-12T21:43:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Evžen Wybitul",
            "Javier Rando",
            "Florian Tramèr",
            "Stanislav Fort"
        ],
        "tldr": "This paper demonstrates that image-text alignment in vision-language models occurs from the very first layer, contradicting the established belief that it only happens in later layers, using a novel DeepDream-inspired synthesis method.",
        "tldr_zh": "该论文通过一种受DeepDream启发的合成方法证明，视觉语言模型中的图像-文本对齐从第一层就开始发生，这与传统观点认为它只发生在后面的层相矛盾。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation",
        "summary": "Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.",
        "url": "http://arxiv.org/abs/2601.08010v1",
        "published_date": "2026-01-12T21:24:45+00:00",
        "updated_date": "2026-01-12T21:24:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaoyu Li",
            "Deeparghya Dutta Barua",
            "Fei Tao",
            "Pooyan Fazli"
        ],
        "tldr": "The paper introduces CASHEW, a framework to stabilize multi-step reasoning in vision-language models by iteratively aggregating reasoning trajectories with visual verification, demonstrating significant performance improvements across various benchmarks.",
        "tldr_zh": "该论文介绍了CASHEW，一个通过迭代聚合推理轨迹并进行视觉验证来稳定视觉语言模型中多步推理的框架，并在各种基准测试中展示了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
        "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.",
        "url": "http://arxiv.org/abs/2601.07812v1",
        "published_date": "2026-01-12T18:45:13+00:00",
        "updated_date": "2026-01-12T18:45:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anurag Das",
            "Adrian Bulat",
            "Alberto Baldrati",
            "Ioannis Maniadis Metaxas",
            "Bernt Schiele",
            "Georgios Tzimiropoulos",
            "Brais Martinez"
        ],
        "tldr": "The paper introduces MIMIC, a new benchmark for evaluating multi-image LVLMs, identifies their weaknesses in cross-image aggregation and concept tracking, and proposes data augmentation and attention-masking solutions.",
        "tldr_zh": "该论文介绍了MIMIC，一个用于评估多图像LVLM的新基准，发现了它们在跨图像聚合和概念跟踪方面的弱点，并提出了数据增强和注意力掩码解决方案。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding",
        "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. However, Current benchmarks for medical image understanding generally focus on the final answer while ignoring the reasoning path. An opaque process lacks reliable bases for judgment, making it difficult to assist doctors in diagnosis. To address this gap, we introduce a new M3CoTBench benchmark specifically designed to evaluate the correctness, efficiency, impact, and consistency of CoT reasoning in medical image understanding. M3CoTBench features 1) a diverse, multi-level difficulty dataset covering 24 examination types, 2) 13 varying-difficulty tasks, 3) a suite of CoT-specific evaluation metrics (correctness, efficiency, impact, and consistency) tailored to clinical reasoning, and 4) a performance analysis of multiple MLLMs. M3CoTBench systematically evaluates CoT reasoning across diverse medical imaging tasks, revealing current limitations of MLLMs in generating reliable and clinically interpretable reasoning, and aims to foster the development of transparent, trustworthy, and diagnostically accurate AI systems for healthcare. Project page at https://juntaojianggavin.github.io/projects/M3CoTBench/.",
        "url": "http://arxiv.org/abs/2601.08758v1",
        "published_date": "2026-01-13T17:42:27+00:00",
        "updated_date": "2026-01-13T17:42:27+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Juntao Jiang",
            "Jiangning Zhang",
            "Yali Bi",
            "Jinsheng Bai",
            "Weixuan Liu",
            "Weiwei Jin",
            "Zhucun Xue",
            "Yong Liu",
            "Xiaobin Hu",
            "Shuicheng Yan"
        ],
        "tldr": "The paper introduces M3CoTBench, a new benchmark for evaluating Chain-of-Thought (CoT) reasoning in medical image understanding, addressing the lack of focus on reasoning paths in existing benchmarks and revealing limitations of current MLLMs in this domain.",
        "tldr_zh": "该论文介绍了M3CoTBench，一个新的用于评估医学图像理解中思维链（CoT）推理的基准，旨在解决现有基准缺乏对推理路径关注的问题，并揭示了当前多模态大语言模型（MLLM）在该领域的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2",
        "summary": "The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.",
        "url": "http://arxiv.org/abs/2601.08408v1",
        "published_date": "2026-01-13T10:26:10+00:00",
        "updated_date": "2026-01-13T10:26:10+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yizhan Feng",
            "Hichem Snoussi",
            "Jing Teng",
            "Jian Liu",
            "Yuyang Wang",
            "Abel Cherouat",
            "Tian Wang"
        ],
        "tldr": "This paper presents an edge-optimized multimodal learning platform for UAV video understanding based on BLIP-2, integrating YOLO models and a keyframe sampling mechanism to enable real-time interaction on resource-constrained devices without task-specific fine-tuning.",
        "tldr_zh": "本文提出了一种基于BLIP-2的边缘优化多模态学习平台，用于无人机视频理解。该平台集成了YOLO模型和关键帧采样机制，能够在资源受限的设备上实现实时交互，而无需进行特定任务的微调。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation",
        "summary": "Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.",
        "url": "http://arxiv.org/abs/2601.08311v1",
        "published_date": "2026-01-13T08:00:02+00:00",
        "updated_date": "2026-01-13T08:00:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kang Fu",
            "Huiyu Duan",
            "Zicheng Zhang",
            "Yucheng Zhu",
            "Jun Zhao",
            "Xiongkuo Min",
            "Jia Wang",
            "Guangtao Zhai"
        ],
        "tldr": "The paper introduces IQARAG, a training-free framework using Retrieval-Augmented Generation (RAG) to improve Large Multimodal Models' (LMMs) Image Quality Assessment (IQA) performance by retrieving quality-variant reference images.",
        "tldr_zh": "该论文介绍了IQARAG，一种使用检索增强生成（RAG）的免训练框架，通过检索质量不同的参考图像来提高大型多模态模型（LMM）的图像质量评估（IQA）性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Knowledge-based learning in Text-RAG and Image-RAG",
        "summary": "This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con\"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.",
        "url": "http://arxiv.org/abs/2601.08226v1",
        "published_date": "2026-01-13T05:14:18+00:00",
        "updated_date": "2026-01-13T05:14:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alexander Shim",
            "Khalil Saieh",
            "Samuel Clarke"
        ],
        "tldr": "This paper explores text-based and image-based RAG using Vision Transformers and LLMs (Llama/ChatGPT) for chest x-ray disease detection, finding that text-based RAG reduces hallucinations and image-based RAG improves prediction confidence, with GPT performing better than Llama.",
        "tldr_zh": "本文研究了基于Vision Transformers和LLMs（Llama/ChatGPT）的文本和图像RAG在胸部X光疾病检测中的应用，发现文本RAG减少了幻觉，图像RAG提高了预测置信度，且GPT比Llama表现更好。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures",
        "summary": "Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.",
        "url": "http://arxiv.org/abs/2601.08026v1",
        "published_date": "2026-01-12T21:57:52+00:00",
        "updated_date": "2026-01-12T21:57:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Jifeng Song",
            "Arun Das",
            "Pan Wang",
            "Hui Ji",
            "Kun Zhao",
            "Yufei Huang"
        ],
        "tldr": "FigEx2 is a visual-conditioned framework for detecting and captioning panels in scientific compound figures, using noise-aware gated fusion and staged RL optimization, and demonstrating strong zero-shot transferability across scientific domains.",
        "tldr_zh": "FigEx2是一个视觉条件框架，用于检测和描述科学复合图中的面板。它使用噪声感知门控融合和分阶段RL优化，并展示了跨科学领域的强大零样本迁移能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding",
        "summary": "We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.",
        "url": "http://arxiv.org/abs/2601.07986v1",
        "published_date": "2026-01-12T20:36:30+00:00",
        "updated_date": "2026-01-12T20:36:30+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Haorui Yu",
            "Ramon Ruiz-Dolz",
            "Diji Yang",
            "Hang He",
            "Fengrui Zhang",
            "Qiufeng Yi"
        ],
        "tldr": "VULCA-Bench is introduced as a multicultural art-critique benchmark designed to evaluate the cultural understanding capabilities of Vision-Language Models, revealing challenges in higher-level reasoning.",
        "tldr_zh": "VULCA-Bench是一个多文化艺术评论基准，旨在评估视觉语言模型对文化的理解能力，并揭示其在高层次推理方面的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vision-Language Model for Accurate Crater Detection",
        "summary": "The European Space Agency (ESA), driven by its ambitions on planned lunar missions with the Argonaut lander, has a profound interest in reliable crater detection, since craters pose a risk to safe lunar landings. This task is usually addressed with automated crater detection algorithms (CDA) based on deep learning techniques. It is non-trivial due to the vast amount of craters of various sizes and shapes, as well as challenging conditions such as varying illumination and rugged terrain. Therefore, we propose a deep-learning CDA based on the OWLv2 model, which is built on a Vision Transformer, that has proven highly effective in various computer vision tasks. For fine-tuning, we utilize a manually labeled dataset fom the IMPACT project, that provides crater annotations on high-resolution Lunar Reconnaissance Orbiter Camera Calibrated Data Record images. We insert trainable parameters using a parameter-efficient fine-tuning strategy with Low-Rank Adaptation, and optimize a combined loss function consisting of Complete Intersection over Union (CIoU) for localization and a contrastive loss for classification. We achieve satisfactory visual results, along with a maximum recall of 94.0% and a maximum precision of 73.1% on a test dataset from IMPACT. Our method achieves reliable crater detection across challenging lunar imaging conditions, paving the way for robust crater analysis in future lunar exploration.",
        "url": "http://arxiv.org/abs/2601.07795v1",
        "published_date": "2026-01-12T18:08:17+00:00",
        "updated_date": "2026-01-12T18:08:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Patrick Bauer",
            "Marius Schwinning",
            "Florian Renk",
            "Andreas Weinmann",
            "Hichem Snoussi"
        ],
        "tldr": "This paper presents a vision-language model based on OWLv2, fine-tuned with a manually labeled lunar crater dataset, for accurate crater detection, achieving high recall and precision.",
        "tldr_zh": "本文提出了一种基于OWLv2的视觉语言模型，通过人工标注的月球陨石坑数据集进行微调，用于精确的陨石坑检测，实现了较高的召回率和精确率。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]